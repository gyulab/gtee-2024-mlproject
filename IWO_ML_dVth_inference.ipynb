{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gyulab/gtee-bti-mlproject/blob/main/IWO_ML_dVth_inference.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "WeS55towxECg"
      },
      "outputs": [],
      "source": [
        "# Dataset Call\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "dVth_xls_data = pd.read_excel(r'/content/drive/MyDrive/Colab_ML_ProfYu/csv_data/ML_BTI_Vth_dataset_MOCKUP_extended.xlsx')\n",
        "#data = dVth_xls_data.iloc[1:, 2:].T.values\n",
        "\n",
        "#[t_stress, t_rec, t_ratio, duty_cycle, clk_loops, V_ov, temperature, delta_Vth]\n",
        "# Function to clean and convert data to only numeric, ignoring non-numeric values\n",
        "def clean_numeric_data(data):\n",
        "    cleaned_data = []\n",
        "    for item in data:\n",
        "        try:\n",
        "            # Convert to float and check if it is not NaN\n",
        "            numeric_value = float(item)\n",
        "            if not np.isnan(numeric_value):\n",
        "                cleaned_data.append(numeric_value)\n",
        "        except ValueError:\n",
        "            continue\n",
        "\n",
        "    return np.array(cleaned_data)\n",
        "\n",
        "t_stress = clean_numeric_data(dVth_xls_data.iloc[1:, 2].values)\n",
        "t_rec = clean_numeric_data(dVth_xls_data.iloc[1:, 3].values)\n",
        "clk_loops = clean_numeric_data(dVth_xls_data.iloc[1:, 6].values)\n",
        "#duty_cycle = clean_numeric_data(dVth_xls_data.iloc[1:, 5].values)\n",
        "V_ov = clean_numeric_data(dVth_xls_data.iloc[1:, 7].values)\n",
        "temperature = clean_numeric_data(dVth_xls_data.iloc[1:, 8].values)\n",
        "delta_Vth = clean_numeric_data(dVth_xls_data.iloc[1:, 9].values)\n",
        "\n",
        "# t_stress, t_rec, clk_loops, V_ov, temperature\n",
        "#print((np.vstack((t_stress, t_rec)).T))\n",
        "#print(V_ov)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "GIMnZZ9yy7jz"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "def normaliz(target): #Minmax normalization\n",
        "    Min = min(target)\n",
        "    Val = target-Min\n",
        "    Val = Val\n",
        "    Max = max(Val)\n",
        "    Norm = 1/Max\n",
        "    return (Norm, Val, Min)\n",
        "\n",
        "(normt_stress, t_stress_1, Mint_stress) = normaliz(t_stress)\n",
        "(normt_rec, t_rec_1, Mint_rec) = normaliz(t_rec)\n",
        "(normclk_loops, clk_loops_1, Minclk_loops) = normaliz(clk_loops)\n",
        "(normV_ov, V_ov_1, MinV_ov) = normaliz(V_ov)\n",
        "(normtemperature, temperature_1, Mintemperature) = normaliz(temperature)\n",
        "(normdelta_Vth, delta_Vth_1, Mindelta_Vth) = normaliz(delta_Vth)\n",
        "\n",
        "T_stress = normt_stress * t_stress_1\n",
        "T_rec = normt_rec * t_rec_1\n",
        "Clk_loops = normclk_loops * clk_loops_1\n",
        "Vov = normV_ov * V_ov_1\n",
        "Temperature = normtemperature * temperature_1\n",
        "Delta_Vth = normdelta_Vth * delta_Vth_1\n",
        "\n",
        "X = np.vstack((T_stress.astype(float), T_rec.astype(float), Clk_loops.astype(float), Vov.astype(float), Temperature.astype(float))).T\n",
        "Y = np.vstack(Delta_Vth.astype(float))\n",
        "X_tensor = torch.tensor(X, dtype=torch.float64)\n",
        "Y_tensor = torch.tensor(Y, dtype=torch.float64)\n",
        "#print(np.shape(X))\n",
        "#print(np.shape(Y))\n",
        "#print(X_tensor)\n",
        "#print(Y_tensor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "gLtzD7eS3ICV",
        "outputId": "a68826b1-8ae0-4911-bf80-72668f11df6a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss (epoch:    1): 0.27850592\n",
            "Loss (epoch:    2): 0.23325545\n",
            "Loss (epoch:    3): 0.19700915\n",
            "Loss (epoch:    4): 0.16847230\n",
            "Loss (epoch:    5): 0.14614689\n",
            "Loss (epoch:    6): 0.12863065\n",
            "Loss (epoch:    7): 0.11476397\n",
            "Loss (epoch:    8): 0.10363678\n",
            "Loss (epoch:    9): 0.09455901\n",
            "Loss (epoch:   10): 0.08702281\n",
            "Loss (epoch:   11): 0.08066383\n",
            "Loss (epoch:   12): 0.07522484\n",
            "Loss (epoch:   13): 0.07052466\n",
            "Loss (epoch:   14): 0.06643394\n",
            "Loss (epoch:   15): 0.06285777\n",
            "Loss (epoch:   16): 0.05972378\n",
            "Loss (epoch:   17): 0.05697448\n",
            "Loss (epoch:   18): 0.05456239\n",
            "Loss (epoch:   19): 0.05244708\n",
            "Loss (epoch:   20): 0.05059340\n",
            "Loss (epoch:   21): 0.04897028\n",
            "Loss (epoch:   22): 0.04755007\n",
            "Loss (epoch:   23): 0.04630801\n",
            "Loss (epoch:   24): 0.04522198\n",
            "Loss (epoch:   25): 0.04427218\n",
            "Loss (epoch:   26): 0.04344093\n",
            "Loss (epoch:   27): 0.04271252\n",
            "Loss (epoch:   28): 0.04207302\n",
            "Loss (epoch:   29): 0.04151011\n",
            "Loss (epoch:   30): 0.04101302\n",
            "Loss (epoch:   31): 0.04057226\n",
            "Loss (epoch:   32): 0.04017963\n",
            "Loss (epoch:   33): 0.03982799\n",
            "Loss (epoch:   34): 0.03951119\n",
            "Loss (epoch:   35): 0.03922396\n",
            "Loss (epoch:   36): 0.03896178\n",
            "Loss (epoch:   37): 0.03872083\n",
            "Loss (epoch:   38): 0.03849786\n",
            "Loss (epoch:   39): 0.03829012\n",
            "Loss (epoch:   40): 0.03809533\n",
            "Loss (epoch:   41): 0.03791156\n",
            "Loss (epoch:   42): 0.03773720\n",
            "Loss (epoch:   43): 0.03757093\n",
            "Loss (epoch:   44): 0.03741166\n",
            "Loss (epoch:   45): 0.03725846\n",
            "Loss (epoch:   46): 0.03711060\n",
            "Loss (epoch:   47): 0.03696747\n",
            "Loss (epoch:   48): 0.03682856\n",
            "Loss (epoch:   49): 0.03669348\n",
            "Loss (epoch:   50): 0.03656187\n",
            "Loss (epoch:   51): 0.03643348\n",
            "Loss (epoch:   52): 0.03630808\n",
            "Loss (epoch:   53): 0.03618548\n",
            "Loss (epoch:   54): 0.03606555\n",
            "Loss (epoch:   55): 0.03594816\n",
            "Loss (epoch:   56): 0.03583321\n",
            "Loss (epoch:   57): 0.03572061\n",
            "Loss (epoch:   58): 0.03561029\n",
            "Loss (epoch:   59): 0.03550220\n",
            "Loss (epoch:   60): 0.03539628\n",
            "Loss (epoch:   61): 0.03529248\n",
            "Loss (epoch:   62): 0.03519076\n",
            "Loss (epoch:   63): 0.03509109\n",
            "Loss (epoch:   64): 0.03499342\n",
            "Loss (epoch:   65): 0.03489774\n",
            "Loss (epoch:   66): 0.03480400\n",
            "Loss (epoch:   67): 0.03471218\n",
            "Loss (epoch:   68): 0.03462224\n",
            "Loss (epoch:   69): 0.03453415\n",
            "Loss (epoch:   70): 0.03444790\n",
            "Loss (epoch:   71): 0.03436345\n",
            "Loss (epoch:   72): 0.03428076\n",
            "Loss (epoch:   73): 0.03419981\n",
            "Loss (epoch:   74): 0.03412058\n",
            "Loss (epoch:   75): 0.03404302\n",
            "Loss (epoch:   76): 0.03396713\n",
            "Loss (epoch:   77): 0.03389285\n",
            "Loss (epoch:   78): 0.03382017\n",
            "Loss (epoch:   79): 0.03374906\n",
            "Loss (epoch:   80): 0.03367948\n",
            "Loss (epoch:   81): 0.03361141\n",
            "Loss (epoch:   82): 0.03354481\n",
            "Loss (epoch:   83): 0.03347966\n",
            "Loss (epoch:   84): 0.03341592\n",
            "Loss (epoch:   85): 0.03335358\n",
            "Loss (epoch:   86): 0.03329259\n",
            "Loss (epoch:   87): 0.03323293\n",
            "Loss (epoch:   88): 0.03317457\n",
            "Loss (epoch:   89): 0.03311748\n",
            "Loss (epoch:   90): 0.03306162\n",
            "Loss (epoch:   91): 0.03300698\n",
            "Loss (epoch:   92): 0.03295352\n",
            "Loss (epoch:   93): 0.03290122\n",
            "Loss (epoch:   94): 0.03285005\n",
            "Loss (epoch:   95): 0.03279998\n",
            "Loss (epoch:   96): 0.03275097\n",
            "Loss (epoch:   97): 0.03270302\n",
            "Loss (epoch:   98): 0.03265608\n",
            "Loss (epoch:   99): 0.03261012\n",
            "Loss (epoch:  100): 0.03256515\n",
            "Loss (epoch:  101): 0.03252110\n",
            "Loss (epoch:  102): 0.03247798\n",
            "Loss (epoch:  103): 0.03243574\n",
            "Loss (epoch:  104): 0.03239437\n",
            "Loss (epoch:  105): 0.03235385\n",
            "Loss (epoch:  106): 0.03231414\n",
            "Loss (epoch:  107): 0.03227522\n",
            "Loss (epoch:  108): 0.03223709\n",
            "Loss (epoch:  109): 0.03219970\n",
            "Loss (epoch:  110): 0.03216304\n",
            "Loss (epoch:  111): 0.03212709\n",
            "Loss (epoch:  112): 0.03209182\n",
            "Loss (epoch:  113): 0.03205722\n",
            "Loss (epoch:  114): 0.03202327\n",
            "Loss (epoch:  115): 0.03198995\n",
            "Loss (epoch:  116): 0.03195723\n",
            "Loss (epoch:  117): 0.03192510\n",
            "Loss (epoch:  118): 0.03189354\n",
            "Loss (epoch:  119): 0.03186254\n",
            "Loss (epoch:  120): 0.03183207\n",
            "Loss (epoch:  121): 0.03180212\n",
            "Loss (epoch:  122): 0.03177268\n",
            "Loss (epoch:  123): 0.03174373\n",
            "Loss (epoch:  124): 0.03171524\n",
            "Loss (epoch:  125): 0.03168721\n",
            "Loss (epoch:  126): 0.03165962\n",
            "Loss (epoch:  127): 0.03163246\n",
            "Loss (epoch:  128): 0.03160572\n",
            "Loss (epoch:  129): 0.03157937\n",
            "Loss (epoch:  130): 0.03155342\n",
            "Loss (epoch:  131): 0.03152783\n",
            "Loss (epoch:  132): 0.03150261\n",
            "Loss (epoch:  133): 0.03147774\n",
            "Loss (epoch:  134): 0.03145320\n",
            "Loss (epoch:  135): 0.03142899\n",
            "Loss (epoch:  136): 0.03140510\n",
            "Loss (epoch:  137): 0.03138153\n",
            "Loss (epoch:  138): 0.03135824\n",
            "Loss (epoch:  139): 0.03133523\n",
            "Loss (epoch:  140): 0.03131250\n",
            "Loss (epoch:  141): 0.03129004\n",
            "Loss (epoch:  142): 0.03126783\n",
            "Loss (epoch:  143): 0.03124588\n",
            "Loss (epoch:  144): 0.03122416\n",
            "Loss (epoch:  145): 0.03120268\n",
            "Loss (epoch:  146): 0.03118142\n",
            "Loss (epoch:  147): 0.03116039\n",
            "Loss (epoch:  148): 0.03113955\n",
            "Loss (epoch:  149): 0.03111892\n",
            "Loss (epoch:  150): 0.03109849\n",
            "Loss (epoch:  151): 0.03107825\n",
            "Loss (epoch:  152): 0.03105818\n",
            "Loss (epoch:  153): 0.03103830\n",
            "Loss (epoch:  154): 0.03101858\n",
            "Loss (epoch:  155): 0.03099904\n",
            "Loss (epoch:  156): 0.03097964\n",
            "Loss (epoch:  157): 0.03096040\n",
            "Loss (epoch:  158): 0.03094132\n",
            "Loss (epoch:  159): 0.03092237\n",
            "Loss (epoch:  160): 0.03090357\n",
            "Loss (epoch:  161): 0.03088489\n",
            "Loss (epoch:  162): 0.03086636\n",
            "Loss (epoch:  163): 0.03084794\n",
            "Loss (epoch:  164): 0.03082964\n",
            "Loss (epoch:  165): 0.03081147\n",
            "Loss (epoch:  166): 0.03079340\n",
            "Loss (epoch:  167): 0.03077545\n",
            "Loss (epoch:  168): 0.03075761\n",
            "Loss (epoch:  169): 0.03073987\n",
            "Loss (epoch:  170): 0.03072222\n",
            "Loss (epoch:  171): 0.03070468\n",
            "Loss (epoch:  172): 0.03068723\n",
            "Loss (epoch:  173): 0.03066987\n",
            "Loss (epoch:  174): 0.03065259\n",
            "Loss (epoch:  175): 0.03063540\n",
            "Loss (epoch:  176): 0.03061830\n",
            "Loss (epoch:  177): 0.03060128\n",
            "Loss (epoch:  178): 0.03058433\n",
            "Loss (epoch:  179): 0.03056746\n",
            "Loss (epoch:  180): 0.03055066\n",
            "Loss (epoch:  181): 0.03053394\n",
            "Loss (epoch:  182): 0.03051728\n",
            "Loss (epoch:  183): 0.03050069\n",
            "Loss (epoch:  184): 0.03048417\n",
            "Loss (epoch:  185): 0.03046771\n",
            "Loss (epoch:  186): 0.03045131\n",
            "Loss (epoch:  187): 0.03043497\n",
            "Loss (epoch:  188): 0.03041869\n",
            "Loss (epoch:  189): 0.03040247\n",
            "Loss (epoch:  190): 0.03038630\n",
            "Loss (epoch:  191): 0.03037019\n",
            "Loss (epoch:  192): 0.03035413\n",
            "Loss (epoch:  193): 0.03033812\n",
            "Loss (epoch:  194): 0.03032216\n",
            "Loss (epoch:  195): 0.03030625\n",
            "Loss (epoch:  196): 0.03029039\n",
            "Loss (epoch:  197): 0.03027457\n",
            "Loss (epoch:  198): 0.03025880\n",
            "Loss (epoch:  199): 0.03024308\n",
            "Loss (epoch:  200): 0.03022740\n",
            "Loss (epoch:  201): 0.03021176\n",
            "Loss (epoch:  202): 0.03019616\n",
            "Loss (epoch:  203): 0.03018061\n",
            "Loss (epoch:  204): 0.03016510\n",
            "Loss (epoch:  205): 0.03014962\n",
            "Loss (epoch:  206): 0.03013419\n",
            "Loss (epoch:  207): 0.03011879\n",
            "Loss (epoch:  208): 0.03010342\n",
            "Loss (epoch:  209): 0.03008810\n",
            "Loss (epoch:  210): 0.03007281\n",
            "Loss (epoch:  211): 0.03005757\n",
            "Loss (epoch:  212): 0.03004235\n",
            "Loss (epoch:  213): 0.03002716\n",
            "Loss (epoch:  214): 0.03001201\n",
            "Loss (epoch:  215): 0.02999690\n",
            "Loss (epoch:  216): 0.02998182\n",
            "Loss (epoch:  217): 0.02996677\n",
            "Loss (epoch:  218): 0.02995175\n",
            "Loss (epoch:  219): 0.02993677\n",
            "Loss (epoch:  220): 0.02992181\n",
            "Loss (epoch:  221): 0.02990689\n",
            "Loss (epoch:  222): 0.02989200\n",
            "Loss (epoch:  223): 0.02987714\n",
            "Loss (epoch:  224): 0.02986231\n",
            "Loss (epoch:  225): 0.02984750\n",
            "Loss (epoch:  226): 0.02983273\n",
            "Loss (epoch:  227): 0.02981799\n",
            "Loss (epoch:  228): 0.02980328\n",
            "Loss (epoch:  229): 0.02978859\n",
            "Loss (epoch:  230): 0.02977394\n",
            "Loss (epoch:  231): 0.02975932\n",
            "Loss (epoch:  232): 0.02974471\n",
            "Loss (epoch:  233): 0.02973015\n",
            "Loss (epoch:  234): 0.02971561\n",
            "Loss (epoch:  235): 0.02970109\n",
            "Loss (epoch:  236): 0.02968661\n",
            "Loss (epoch:  237): 0.02967215\n",
            "Loss (epoch:  238): 0.02965773\n",
            "Loss (epoch:  239): 0.02964332\n",
            "Loss (epoch:  240): 0.02962895\n",
            "Loss (epoch:  241): 0.02961461\n",
            "Loss (epoch:  242): 0.02960029\n",
            "Loss (epoch:  243): 0.02958600\n",
            "Loss (epoch:  244): 0.02957174\n",
            "Loss (epoch:  245): 0.02955750\n",
            "Loss (epoch:  246): 0.02954330\n",
            "Loss (epoch:  247): 0.02952912\n",
            "Loss (epoch:  248): 0.02951497\n",
            "Loss (epoch:  249): 0.02950085\n",
            "Loss (epoch:  250): 0.02948675\n",
            "Loss (epoch:  251): 0.02947268\n",
            "Loss (epoch:  252): 0.02945864\n",
            "Loss (epoch:  253): 0.02944464\n",
            "Loss (epoch:  254): 0.02943065\n",
            "Loss (epoch:  255): 0.02941670\n",
            "Loss (epoch:  256): 0.02940277\n",
            "Loss (epoch:  257): 0.02938887\n",
            "Loss (epoch:  258): 0.02937500\n",
            "Loss (epoch:  259): 0.02936116\n",
            "Loss (epoch:  260): 0.02934735\n",
            "Loss (epoch:  261): 0.02933356\n",
            "Loss (epoch:  262): 0.02931980\n",
            "Loss (epoch:  263): 0.02930608\n",
            "Loss (epoch:  264): 0.02929238\n",
            "Loss (epoch:  265): 0.02927872\n",
            "Loss (epoch:  266): 0.02926508\n",
            "Loss (epoch:  267): 0.02925147\n",
            "Loss (epoch:  268): 0.02923789\n",
            "Loss (epoch:  269): 0.02922435\n",
            "Loss (epoch:  270): 0.02921083\n",
            "Loss (epoch:  271): 0.02919734\n",
            "Loss (epoch:  272): 0.02918388\n",
            "Loss (epoch:  273): 0.02917045\n",
            "Loss (epoch:  274): 0.02915705\n",
            "Loss (epoch:  275): 0.02914368\n",
            "Loss (epoch:  276): 0.02913035\n",
            "Loss (epoch:  277): 0.02911704\n",
            "Loss (epoch:  278): 0.02910377\n",
            "Loss (epoch:  279): 0.02909053\n",
            "Loss (epoch:  280): 0.02907731\n",
            "Loss (epoch:  281): 0.02906414\n",
            "Loss (epoch:  282): 0.02905099\n",
            "Loss (epoch:  283): 0.02903787\n",
            "Loss (epoch:  284): 0.02902480\n",
            "Loss (epoch:  285): 0.02901174\n",
            "Loss (epoch:  286): 0.02899872\n",
            "Loss (epoch:  287): 0.02898574\n",
            "Loss (epoch:  288): 0.02897279\n",
            "Loss (epoch:  289): 0.02895987\n",
            "Loss (epoch:  290): 0.02894698\n",
            "Loss (epoch:  291): 0.02893413\n",
            "Loss (epoch:  292): 0.02892132\n",
            "Loss (epoch:  293): 0.02890853\n",
            "Loss (epoch:  294): 0.02889578\n",
            "Loss (epoch:  295): 0.02888307\n",
            "Loss (epoch:  296): 0.02887039\n",
            "Loss (epoch:  297): 0.02885774\n",
            "Loss (epoch:  298): 0.02884513\n",
            "Loss (epoch:  299): 0.02883255\n",
            "Loss (epoch:  300): 0.02882001\n",
            "Loss (epoch:  301): 0.02880751\n",
            "Loss (epoch:  302): 0.02879504\n",
            "Loss (epoch:  303): 0.02878260\n",
            "Loss (epoch:  304): 0.02877020\n",
            "Loss (epoch:  305): 0.02875784\n",
            "Loss (epoch:  306): 0.02874551\n",
            "Loss (epoch:  307): 0.02873322\n",
            "Loss (epoch:  308): 0.02872097\n",
            "Loss (epoch:  309): 0.02870875\n",
            "Loss (epoch:  310): 0.02869657\n",
            "Loss (epoch:  311): 0.02868443\n",
            "Loss (epoch:  312): 0.02867233\n",
            "Loss (epoch:  313): 0.02866025\n",
            "Loss (epoch:  314): 0.02864822\n",
            "Loss (epoch:  315): 0.02863623\n",
            "Loss (epoch:  316): 0.02862427\n",
            "Loss (epoch:  317): 0.02861235\n",
            "Loss (epoch:  318): 0.02860047\n",
            "Loss (epoch:  319): 0.02858863\n",
            "Loss (epoch:  320): 0.02857683\n",
            "Loss (epoch:  321): 0.02856507\n",
            "Loss (epoch:  322): 0.02855334\n",
            "Loss (epoch:  323): 0.02854165\n",
            "Loss (epoch:  324): 0.02853000\n",
            "Loss (epoch:  325): 0.02851840\n",
            "Loss (epoch:  326): 0.02850682\n",
            "Loss (epoch:  327): 0.02849529\n",
            "Loss (epoch:  328): 0.02848380\n",
            "Loss (epoch:  329): 0.02847235\n",
            "Loss (epoch:  330): 0.02846094\n",
            "Loss (epoch:  331): 0.02844957\n",
            "Loss (epoch:  332): 0.02843823\n",
            "Loss (epoch:  333): 0.02842694\n",
            "Loss (epoch:  334): 0.02841569\n",
            "Loss (epoch:  335): 0.02840448\n",
            "Loss (epoch:  336): 0.02839330\n",
            "Loss (epoch:  337): 0.02838217\n",
            "Loss (epoch:  338): 0.02837108\n",
            "Loss (epoch:  339): 0.02836003\n",
            "Loss (epoch:  340): 0.02834902\n",
            "Loss (epoch:  341): 0.02833805\n",
            "Loss (epoch:  342): 0.02832713\n",
            "Loss (epoch:  343): 0.02831624\n",
            "Loss (epoch:  344): 0.02830539\n",
            "Loss (epoch:  345): 0.02829458\n",
            "Loss (epoch:  346): 0.02828383\n",
            "Loss (epoch:  347): 0.02827310\n",
            "Loss (epoch:  348): 0.02826242\n",
            "Loss (epoch:  349): 0.02825178\n",
            "Loss (epoch:  350): 0.02824118\n",
            "Loss (epoch:  351): 0.02823062\n",
            "Loss (epoch:  352): 0.02822011\n",
            "Loss (epoch:  353): 0.02820964\n",
            "Loss (epoch:  354): 0.02819920\n",
            "Loss (epoch:  355): 0.02818881\n",
            "Loss (epoch:  356): 0.02817846\n",
            "Loss (epoch:  357): 0.02816815\n",
            "Loss (epoch:  358): 0.02815789\n",
            "Loss (epoch:  359): 0.02814766\n",
            "Loss (epoch:  360): 0.02813748\n",
            "Loss (epoch:  361): 0.02812734\n",
            "Loss (epoch:  362): 0.02811724\n",
            "Loss (epoch:  363): 0.02810719\n",
            "Loss (epoch:  364): 0.02809717\n",
            "Loss (epoch:  365): 0.02808720\n",
            "Loss (epoch:  366): 0.02807727\n",
            "Loss (epoch:  367): 0.02806738\n",
            "Loss (epoch:  368): 0.02805753\n",
            "Loss (epoch:  369): 0.02804773\n",
            "Loss (epoch:  370): 0.02803797\n",
            "Loss (epoch:  371): 0.02802825\n",
            "Loss (epoch:  372): 0.02801857\n",
            "Loss (epoch:  373): 0.02800893\n",
            "Loss (epoch:  374): 0.02799934\n",
            "Loss (epoch:  375): 0.02798979\n",
            "Loss (epoch:  376): 0.02798028\n",
            "Loss (epoch:  377): 0.02797081\n",
            "Loss (epoch:  378): 0.02796138\n",
            "Loss (epoch:  379): 0.02795199\n",
            "Loss (epoch:  380): 0.02794265\n",
            "Loss (epoch:  381): 0.02793335\n",
            "Loss (epoch:  382): 0.02792410\n",
            "Loss (epoch:  383): 0.02791488\n",
            "Loss (epoch:  384): 0.02790570\n",
            "Loss (epoch:  385): 0.02789657\n",
            "Loss (epoch:  386): 0.02788748\n",
            "Loss (epoch:  387): 0.02787843\n",
            "Loss (epoch:  388): 0.02786942\n",
            "Loss (epoch:  389): 0.02786046\n",
            "Loss (epoch:  390): 0.02785153\n",
            "Loss (epoch:  391): 0.02784264\n",
            "Loss (epoch:  392): 0.02783381\n",
            "Loss (epoch:  393): 0.02782500\n",
            "Loss (epoch:  394): 0.02781625\n",
            "Loss (epoch:  395): 0.02780753\n",
            "Loss (epoch:  396): 0.02779885\n",
            "Loss (epoch:  397): 0.02779022\n",
            "Loss (epoch:  398): 0.02778163\n",
            "Loss (epoch:  399): 0.02777307\n",
            "Loss (epoch:  400): 0.02776456\n",
            "Loss (epoch:  401): 0.02775609\n",
            "Loss (epoch:  402): 0.02774766\n",
            "Loss (epoch:  403): 0.02773927\n",
            "Loss (epoch:  404): 0.02773092\n",
            "Loss (epoch:  405): 0.02772261\n",
            "Loss (epoch:  406): 0.02771435\n",
            "Loss (epoch:  407): 0.02770612\n",
            "Loss (epoch:  408): 0.02769793\n",
            "Loss (epoch:  409): 0.02768979\n",
            "Loss (epoch:  410): 0.02768169\n",
            "Loss (epoch:  411): 0.02767362\n",
            "Loss (epoch:  412): 0.02766559\n",
            "Loss (epoch:  413): 0.02765761\n",
            "Loss (epoch:  414): 0.02764966\n",
            "Loss (epoch:  415): 0.02764176\n",
            "Loss (epoch:  416): 0.02763390\n",
            "Loss (epoch:  417): 0.02762607\n",
            "Loss (epoch:  418): 0.02761829\n",
            "Loss (epoch:  419): 0.02761054\n",
            "Loss (epoch:  420): 0.02760284\n",
            "Loss (epoch:  421): 0.02759517\n",
            "Loss (epoch:  422): 0.02758755\n",
            "Loss (epoch:  423): 0.02757995\n",
            "Loss (epoch:  424): 0.02757241\n",
            "Loss (epoch:  425): 0.02756490\n",
            "Loss (epoch:  426): 0.02755742\n",
            "Loss (epoch:  427): 0.02754999\n",
            "Loss (epoch:  428): 0.02754260\n",
            "Loss (epoch:  429): 0.02753524\n",
            "Loss (epoch:  430): 0.02752792\n",
            "Loss (epoch:  431): 0.02752064\n",
            "Loss (epoch:  432): 0.02751340\n",
            "Loss (epoch:  433): 0.02750621\n",
            "Loss (epoch:  434): 0.02749903\n",
            "Loss (epoch:  435): 0.02749191\n",
            "Loss (epoch:  436): 0.02748482\n",
            "Loss (epoch:  437): 0.02747777\n",
            "Loss (epoch:  438): 0.02747075\n",
            "Loss (epoch:  439): 0.02746378\n",
            "Loss (epoch:  440): 0.02745684\n",
            "Loss (epoch:  441): 0.02744994\n",
            "Loss (epoch:  442): 0.02744307\n",
            "Loss (epoch:  443): 0.02743624\n",
            "Loss (epoch:  444): 0.02742945\n",
            "Loss (epoch:  445): 0.02742270\n",
            "Loss (epoch:  446): 0.02741598\n",
            "Loss (epoch:  447): 0.02740930\n",
            "Loss (epoch:  448): 0.02740265\n",
            "Loss (epoch:  449): 0.02739605\n",
            "Loss (epoch:  450): 0.02738947\n",
            "Loss (epoch:  451): 0.02738293\n",
            "Loss (epoch:  452): 0.02737643\n",
            "Loss (epoch:  453): 0.02736997\n",
            "Loss (epoch:  454): 0.02736354\n",
            "Loss (epoch:  455): 0.02735715\n",
            "Loss (epoch:  456): 0.02735078\n",
            "Loss (epoch:  457): 0.02734446\n",
            "Loss (epoch:  458): 0.02733816\n",
            "Loss (epoch:  459): 0.02733192\n",
            "Loss (epoch:  460): 0.02732570\n",
            "Loss (epoch:  461): 0.02731951\n",
            "Loss (epoch:  462): 0.02731336\n",
            "Loss (epoch:  463): 0.02730724\n",
            "Loss (epoch:  464): 0.02730116\n",
            "Loss (epoch:  465): 0.02729511\n",
            "Loss (epoch:  466): 0.02728910\n",
            "Loss (epoch:  467): 0.02728312\n",
            "Loss (epoch:  468): 0.02727717\n",
            "Loss (epoch:  469): 0.02727126\n",
            "Loss (epoch:  470): 0.02726538\n",
            "Loss (epoch:  471): 0.02725953\n",
            "Loss (epoch:  472): 0.02725372\n",
            "Loss (epoch:  473): 0.02724794\n",
            "Loss (epoch:  474): 0.02724219\n",
            "Loss (epoch:  475): 0.02723648\n",
            "Loss (epoch:  476): 0.02723079\n",
            "Loss (epoch:  477): 0.02722514\n",
            "Loss (epoch:  478): 0.02721953\n",
            "Loss (epoch:  479): 0.02721394\n",
            "Loss (epoch:  480): 0.02720839\n",
            "Loss (epoch:  481): 0.02720286\n",
            "Loss (epoch:  482): 0.02719738\n",
            "Loss (epoch:  483): 0.02719191\n",
            "Loss (epoch:  484): 0.02718649\n",
            "Loss (epoch:  485): 0.02718109\n",
            "Loss (epoch:  486): 0.02717572\n",
            "Loss (epoch:  487): 0.02717040\n",
            "Loss (epoch:  488): 0.02716509\n",
            "Loss (epoch:  489): 0.02715982\n",
            "Loss (epoch:  490): 0.02715458\n",
            "Loss (epoch:  491): 0.02714936\n",
            "Loss (epoch:  492): 0.02714418\n",
            "Loss (epoch:  493): 0.02713903\n",
            "Loss (epoch:  494): 0.02713392\n",
            "Loss (epoch:  495): 0.02712882\n",
            "Loss (epoch:  496): 0.02712376\n",
            "Loss (epoch:  497): 0.02711873\n",
            "Loss (epoch:  498): 0.02711373\n",
            "Loss (epoch:  499): 0.02710876\n",
            "Loss (epoch:  500): 0.02710381\n",
            "Loss (epoch:  501): 0.02709890\n",
            "Loss (epoch:  502): 0.02709401\n",
            "Loss (epoch:  503): 0.02708916\n",
            "Loss (epoch:  504): 0.02708433\n",
            "Loss (epoch:  505): 0.02707953\n",
            "Loss (epoch:  506): 0.02707475\n",
            "Loss (epoch:  507): 0.02707001\n",
            "Loss (epoch:  508): 0.02706530\n",
            "Loss (epoch:  509): 0.02706061\n",
            "Loss (epoch:  510): 0.02705595\n",
            "Loss (epoch:  511): 0.02705132\n",
            "Loss (epoch:  512): 0.02704672\n",
            "Loss (epoch:  513): 0.02704214\n",
            "Loss (epoch:  514): 0.02703759\n",
            "Loss (epoch:  515): 0.02703306\n",
            "Loss (epoch:  516): 0.02702857\n",
            "Loss (epoch:  517): 0.02702410\n",
            "Loss (epoch:  518): 0.02701966\n",
            "Loss (epoch:  519): 0.02701524\n",
            "Loss (epoch:  520): 0.02701085\n",
            "Loss (epoch:  521): 0.02700649\n",
            "Loss (epoch:  522): 0.02700215\n",
            "Loss (epoch:  523): 0.02699784\n",
            "Loss (epoch:  524): 0.02699355\n",
            "Loss (epoch:  525): 0.02698929\n",
            "Loss (epoch:  526): 0.02698506\n",
            "Loss (epoch:  527): 0.02698085\n",
            "Loss (epoch:  528): 0.02697667\n",
            "Loss (epoch:  529): 0.02697250\n",
            "Loss (epoch:  530): 0.02696838\n",
            "Loss (epoch:  531): 0.02696426\n",
            "Loss (epoch:  532): 0.02696018\n",
            "Loss (epoch:  533): 0.02695612\n",
            "Loss (epoch:  534): 0.02695209\n",
            "Loss (epoch:  535): 0.02694808\n",
            "Loss (epoch:  536): 0.02694409\n",
            "Loss (epoch:  537): 0.02694013\n",
            "Loss (epoch:  538): 0.02693619\n",
            "Loss (epoch:  539): 0.02693227\n",
            "Loss (epoch:  540): 0.02692838\n",
            "Loss (epoch:  541): 0.02692452\n",
            "Loss (epoch:  542): 0.02692067\n",
            "Loss (epoch:  543): 0.02691685\n",
            "Loss (epoch:  544): 0.02691306\n",
            "Loss (epoch:  545): 0.02690928\n",
            "Loss (epoch:  546): 0.02690553\n",
            "Loss (epoch:  547): 0.02690179\n",
            "Loss (epoch:  548): 0.02689809\n",
            "Loss (epoch:  549): 0.02689441\n",
            "Loss (epoch:  550): 0.02689074\n",
            "Loss (epoch:  551): 0.02688711\n",
            "Loss (epoch:  552): 0.02688349\n",
            "Loss (epoch:  553): 0.02687989\n",
            "Loss (epoch:  554): 0.02687632\n",
            "Loss (epoch:  555): 0.02687277\n",
            "Loss (epoch:  556): 0.02686924\n",
            "Loss (epoch:  557): 0.02686573\n",
            "Loss (epoch:  558): 0.02686224\n",
            "Loss (epoch:  559): 0.02685878\n",
            "Loss (epoch:  560): 0.02685533\n",
            "Loss (epoch:  561): 0.02685191\n",
            "Loss (epoch:  562): 0.02684850\n",
            "Loss (epoch:  563): 0.02684512\n",
            "Loss (epoch:  564): 0.02684176\n",
            "Loss (epoch:  565): 0.02683841\n",
            "Loss (epoch:  566): 0.02683510\n",
            "Loss (epoch:  567): 0.02683179\n",
            "Loss (epoch:  568): 0.02682852\n",
            "Loss (epoch:  569): 0.02682525\n",
            "Loss (epoch:  570): 0.02682201\n",
            "Loss (epoch:  571): 0.02681879\n",
            "Loss (epoch:  572): 0.02681559\n",
            "Loss (epoch:  573): 0.02681241\n",
            "Loss (epoch:  574): 0.02680924\n",
            "Loss (epoch:  575): 0.02680610\n",
            "Loss (epoch:  576): 0.02680298\n",
            "Loss (epoch:  577): 0.02679987\n",
            "Loss (epoch:  578): 0.02679679\n",
            "Loss (epoch:  579): 0.02679372\n",
            "Loss (epoch:  580): 0.02679067\n",
            "Loss (epoch:  581): 0.02678765\n",
            "Loss (epoch:  582): 0.02678463\n",
            "Loss (epoch:  583): 0.02678164\n",
            "Loss (epoch:  584): 0.02677866\n",
            "Loss (epoch:  585): 0.02677571\n",
            "Loss (epoch:  586): 0.02677277\n",
            "Loss (epoch:  587): 0.02676985\n",
            "Loss (epoch:  588): 0.02676695\n",
            "Loss (epoch:  589): 0.02676406\n",
            "Loss (epoch:  590): 0.02676119\n",
            "Loss (epoch:  591): 0.02675834\n",
            "Loss (epoch:  592): 0.02675550\n",
            "Loss (epoch:  593): 0.02675269\n",
            "Loss (epoch:  594): 0.02674990\n",
            "Loss (epoch:  595): 0.02674711\n",
            "Loss (epoch:  596): 0.02674435\n",
            "Loss (epoch:  597): 0.02674160\n",
            "Loss (epoch:  598): 0.02673887\n",
            "Loss (epoch:  599): 0.02673616\n",
            "Loss (epoch:  600): 0.02673346\n",
            "Loss (epoch:  601): 0.02673078\n",
            "Loss (epoch:  602): 0.02672812\n",
            "Loss (epoch:  603): 0.02672546\n",
            "Loss (epoch:  604): 0.02672283\n",
            "Loss (epoch:  605): 0.02672022\n",
            "Loss (epoch:  606): 0.02671762\n",
            "Loss (epoch:  607): 0.02671503\n",
            "Loss (epoch:  608): 0.02671247\n",
            "Loss (epoch:  609): 0.02670991\n",
            "Loss (epoch:  610): 0.02670737\n",
            "Loss (epoch:  611): 0.02670485\n",
            "Loss (epoch:  612): 0.02670234\n",
            "Loss (epoch:  613): 0.02669985\n",
            "Loss (epoch:  614): 0.02669737\n",
            "Loss (epoch:  615): 0.02669492\n",
            "Loss (epoch:  616): 0.02669247\n",
            "Loss (epoch:  617): 0.02669003\n",
            "Loss (epoch:  618): 0.02668762\n",
            "Loss (epoch:  619): 0.02668521\n",
            "Loss (epoch:  620): 0.02668282\n",
            "Loss (epoch:  621): 0.02668045\n",
            "Loss (epoch:  622): 0.02667809\n",
            "Loss (epoch:  623): 0.02667575\n",
            "Loss (epoch:  624): 0.02667342\n",
            "Loss (epoch:  625): 0.02667110\n",
            "Loss (epoch:  626): 0.02666880\n",
            "Loss (epoch:  627): 0.02666651\n",
            "Loss (epoch:  628): 0.02666424\n",
            "Loss (epoch:  629): 0.02666197\n",
            "Loss (epoch:  630): 0.02665972\n",
            "Loss (epoch:  631): 0.02665749\n",
            "Loss (epoch:  632): 0.02665527\n",
            "Loss (epoch:  633): 0.02665306\n",
            "Loss (epoch:  634): 0.02665087\n",
            "Loss (epoch:  635): 0.02664869\n",
            "Loss (epoch:  636): 0.02664652\n",
            "Loss (epoch:  637): 0.02664437\n",
            "Loss (epoch:  638): 0.02664222\n",
            "Loss (epoch:  639): 0.02664009\n",
            "Loss (epoch:  640): 0.02663798\n",
            "Loss (epoch:  641): 0.02663587\n",
            "Loss (epoch:  642): 0.02663378\n",
            "Loss (epoch:  643): 0.02663171\n",
            "Loss (epoch:  644): 0.02662964\n",
            "Loss (epoch:  645): 0.02662758\n",
            "Loss (epoch:  646): 0.02662554\n",
            "Loss (epoch:  647): 0.02662351\n",
            "Loss (epoch:  648): 0.02662149\n",
            "Loss (epoch:  649): 0.02661949\n",
            "Loss (epoch:  650): 0.02661749\n",
            "Loss (epoch:  651): 0.02661552\n",
            "Loss (epoch:  652): 0.02661355\n",
            "Loss (epoch:  653): 0.02661159\n",
            "Loss (epoch:  654): 0.02660964\n",
            "Loss (epoch:  655): 0.02660770\n",
            "Loss (epoch:  656): 0.02660578\n",
            "Loss (epoch:  657): 0.02660387\n",
            "Loss (epoch:  658): 0.02660197\n",
            "Loss (epoch:  659): 0.02660008\n",
            "Loss (epoch:  660): 0.02659820\n",
            "Loss (epoch:  661): 0.02659633\n",
            "Loss (epoch:  662): 0.02659447\n",
            "Loss (epoch:  663): 0.02659263\n",
            "Loss (epoch:  664): 0.02659080\n",
            "Loss (epoch:  665): 0.02658897\n",
            "Loss (epoch:  666): 0.02658716\n",
            "Loss (epoch:  667): 0.02658536\n",
            "Loss (epoch:  668): 0.02658356\n",
            "Loss (epoch:  669): 0.02658179\n",
            "Loss (epoch:  670): 0.02658001\n",
            "Loss (epoch:  671): 0.02657825\n",
            "Loss (epoch:  672): 0.02657650\n",
            "Loss (epoch:  673): 0.02657476\n",
            "Loss (epoch:  674): 0.02657303\n",
            "Loss (epoch:  675): 0.02657131\n",
            "Loss (epoch:  676): 0.02656960\n",
            "Loss (epoch:  677): 0.02656790\n",
            "Loss (epoch:  678): 0.02656622\n",
            "Loss (epoch:  679): 0.02656453\n",
            "Loss (epoch:  680): 0.02656286\n",
            "Loss (epoch:  681): 0.02656120\n",
            "Loss (epoch:  682): 0.02655955\n",
            "Loss (epoch:  683): 0.02655791\n",
            "Loss (epoch:  684): 0.02655628\n",
            "Loss (epoch:  685): 0.02655465\n",
            "Loss (epoch:  686): 0.02655304\n",
            "Loss (epoch:  687): 0.02655144\n",
            "Loss (epoch:  688): 0.02654984\n",
            "Loss (epoch:  689): 0.02654825\n",
            "Loss (epoch:  690): 0.02654667\n",
            "Loss (epoch:  691): 0.02654511\n",
            "Loss (epoch:  692): 0.02654355\n",
            "Loss (epoch:  693): 0.02654200\n",
            "Loss (epoch:  694): 0.02654046\n",
            "Loss (epoch:  695): 0.02653893\n",
            "Loss (epoch:  696): 0.02653740\n",
            "Loss (epoch:  697): 0.02653589\n",
            "Loss (epoch:  698): 0.02653438\n",
            "Loss (epoch:  699): 0.02653288\n",
            "Loss (epoch:  700): 0.02653139\n",
            "Loss (epoch:  701): 0.02652991\n",
            "Loss (epoch:  702): 0.02652844\n",
            "Loss (epoch:  703): 0.02652698\n",
            "Loss (epoch:  704): 0.02652552\n",
            "Loss (epoch:  705): 0.02652408\n",
            "Loss (epoch:  706): 0.02652263\n",
            "Loss (epoch:  707): 0.02652121\n",
            "Loss (epoch:  708): 0.02651978\n",
            "Loss (epoch:  709): 0.02651837\n",
            "Loss (epoch:  710): 0.02651695\n",
            "Loss (epoch:  711): 0.02651556\n",
            "Loss (epoch:  712): 0.02651417\n",
            "Loss (epoch:  713): 0.02651278\n",
            "Loss (epoch:  714): 0.02651140\n",
            "Loss (epoch:  715): 0.02651004\n",
            "Loss (epoch:  716): 0.02650868\n",
            "Loss (epoch:  717): 0.02650732\n",
            "Loss (epoch:  718): 0.02650598\n",
            "Loss (epoch:  719): 0.02650464\n",
            "Loss (epoch:  720): 0.02650331\n",
            "Loss (epoch:  721): 0.02650198\n",
            "Loss (epoch:  722): 0.02650066\n",
            "Loss (epoch:  723): 0.02649935\n",
            "Loss (epoch:  724): 0.02649805\n",
            "Loss (epoch:  725): 0.02649676\n",
            "Loss (epoch:  726): 0.02649547\n",
            "Loss (epoch:  727): 0.02649419\n",
            "Loss (epoch:  728): 0.02649291\n",
            "Loss (epoch:  729): 0.02649165\n",
            "Loss (epoch:  730): 0.02649039\n",
            "Loss (epoch:  731): 0.02648913\n",
            "Loss (epoch:  732): 0.02648788\n",
            "Loss (epoch:  733): 0.02648664\n",
            "Loss (epoch:  734): 0.02648541\n",
            "Loss (epoch:  735): 0.02648419\n",
            "Loss (epoch:  736): 0.02648297\n",
            "Loss (epoch:  737): 0.02648176\n",
            "Loss (epoch:  738): 0.02648055\n",
            "Loss (epoch:  739): 0.02647934\n",
            "Loss (epoch:  740): 0.02647815\n",
            "Loss (epoch:  741): 0.02647696\n",
            "Loss (epoch:  742): 0.02647578\n",
            "Loss (epoch:  743): 0.02647461\n",
            "Loss (epoch:  744): 0.02647344\n",
            "Loss (epoch:  745): 0.02647228\n",
            "Loss (epoch:  746): 0.02647112\n",
            "Loss (epoch:  747): 0.02646997\n",
            "Loss (epoch:  748): 0.02646882\n",
            "Loss (epoch:  749): 0.02646768\n",
            "Loss (epoch:  750): 0.02646655\n",
            "Loss (epoch:  751): 0.02646542\n",
            "Loss (epoch:  752): 0.02646430\n",
            "Loss (epoch:  753): 0.02646319\n",
            "Loss (epoch:  754): 0.02646208\n",
            "Loss (epoch:  755): 0.02646098\n",
            "Loss (epoch:  756): 0.02645988\n",
            "Loss (epoch:  757): 0.02645879\n",
            "Loss (epoch:  758): 0.02645771\n",
            "Loss (epoch:  759): 0.02645662\n",
            "Loss (epoch:  760): 0.02645555\n",
            "Loss (epoch:  761): 0.02645448\n",
            "Loss (epoch:  762): 0.02645342\n",
            "Loss (epoch:  763): 0.02645236\n",
            "Loss (epoch:  764): 0.02645130\n",
            "Loss (epoch:  765): 0.02645026\n",
            "Loss (epoch:  766): 0.02644922\n",
            "Loss (epoch:  767): 0.02644818\n",
            "Loss (epoch:  768): 0.02644714\n",
            "Loss (epoch:  769): 0.02644612\n",
            "Loss (epoch:  770): 0.02644510\n",
            "Loss (epoch:  771): 0.02644408\n",
            "Loss (epoch:  772): 0.02644307\n",
            "Loss (epoch:  773): 0.02644206\n",
            "Loss (epoch:  774): 0.02644106\n",
            "Loss (epoch:  775): 0.02644007\n",
            "Loss (epoch:  776): 0.02643908\n",
            "Loss (epoch:  777): 0.02643809\n",
            "Loss (epoch:  778): 0.02643711\n",
            "Loss (epoch:  779): 0.02643613\n",
            "Loss (epoch:  780): 0.02643516\n",
            "Loss (epoch:  781): 0.02643420\n",
            "Loss (epoch:  782): 0.02643324\n",
            "Loss (epoch:  783): 0.02643227\n",
            "Loss (epoch:  784): 0.02643132\n",
            "Loss (epoch:  785): 0.02643037\n",
            "Loss (epoch:  786): 0.02642943\n",
            "Loss (epoch:  787): 0.02642849\n",
            "Loss (epoch:  788): 0.02642756\n",
            "Loss (epoch:  789): 0.02642663\n",
            "Loss (epoch:  790): 0.02642571\n",
            "Loss (epoch:  791): 0.02642479\n",
            "Loss (epoch:  792): 0.02642387\n",
            "Loss (epoch:  793): 0.02642296\n",
            "Loss (epoch:  794): 0.02642205\n",
            "Loss (epoch:  795): 0.02642115\n",
            "Loss (epoch:  796): 0.02642025\n",
            "Loss (epoch:  797): 0.02641936\n",
            "Loss (epoch:  798): 0.02641847\n",
            "Loss (epoch:  799): 0.02641758\n",
            "Loss (epoch:  800): 0.02641670\n",
            "Loss (epoch:  801): 0.02641582\n",
            "Loss (epoch:  802): 0.02641495\n",
            "Loss (epoch:  803): 0.02641408\n",
            "Loss (epoch:  804): 0.02641321\n",
            "Loss (epoch:  805): 0.02641235\n",
            "Loss (epoch:  806): 0.02641150\n",
            "Loss (epoch:  807): 0.02641064\n",
            "Loss (epoch:  808): 0.02640979\n",
            "Loss (epoch:  809): 0.02640895\n",
            "Loss (epoch:  810): 0.02640811\n",
            "Loss (epoch:  811): 0.02640727\n",
            "Loss (epoch:  812): 0.02640644\n",
            "Loss (epoch:  813): 0.02640561\n",
            "Loss (epoch:  814): 0.02640479\n",
            "Loss (epoch:  815): 0.02640396\n",
            "Loss (epoch:  816): 0.02640314\n",
            "Loss (epoch:  817): 0.02640233\n",
            "Loss (epoch:  818): 0.02640152\n",
            "Loss (epoch:  819): 0.02640071\n",
            "Loss (epoch:  820): 0.02639991\n",
            "Loss (epoch:  821): 0.02639911\n",
            "Loss (epoch:  822): 0.02639831\n",
            "Loss (epoch:  823): 0.02639752\n",
            "Loss (epoch:  824): 0.02639673\n",
            "Loss (epoch:  825): 0.02639595\n",
            "Loss (epoch:  826): 0.02639516\n",
            "Loss (epoch:  827): 0.02639438\n",
            "Loss (epoch:  828): 0.02639361\n",
            "Loss (epoch:  829): 0.02639284\n",
            "Loss (epoch:  830): 0.02639207\n",
            "Loss (epoch:  831): 0.02639130\n",
            "Loss (epoch:  832): 0.02639054\n",
            "Loss (epoch:  833): 0.02638979\n",
            "Loss (epoch:  834): 0.02638903\n",
            "Loss (epoch:  835): 0.02638828\n",
            "Loss (epoch:  836): 0.02638753\n",
            "Loss (epoch:  837): 0.02638678\n",
            "Loss (epoch:  838): 0.02638604\n",
            "Loss (epoch:  839): 0.02638530\n",
            "Loss (epoch:  840): 0.02638457\n",
            "Loss (epoch:  841): 0.02638383\n",
            "Loss (epoch:  842): 0.02638310\n",
            "Loss (epoch:  843): 0.02638237\n",
            "Loss (epoch:  844): 0.02638165\n",
            "Loss (epoch:  845): 0.02638093\n",
            "Loss (epoch:  846): 0.02638021\n",
            "Loss (epoch:  847): 0.02637950\n",
            "Loss (epoch:  848): 0.02637879\n",
            "Loss (epoch:  849): 0.02637808\n",
            "Loss (epoch:  850): 0.02637737\n",
            "Loss (epoch:  851): 0.02637667\n",
            "Loss (epoch:  852): 0.02637597\n",
            "Loss (epoch:  853): 0.02637528\n",
            "Loss (epoch:  854): 0.02637458\n",
            "Loss (epoch:  855): 0.02637389\n",
            "Loss (epoch:  856): 0.02637320\n",
            "Loss (epoch:  857): 0.02637252\n",
            "Loss (epoch:  858): 0.02637184\n",
            "Loss (epoch:  859): 0.02637115\n",
            "Loss (epoch:  860): 0.02637047\n",
            "Loss (epoch:  861): 0.02636981\n",
            "Loss (epoch:  862): 0.02636913\n",
            "Loss (epoch:  863): 0.02636846\n",
            "Loss (epoch:  864): 0.02636780\n",
            "Loss (epoch:  865): 0.02636713\n",
            "Loss (epoch:  866): 0.02636647\n",
            "Loss (epoch:  867): 0.02636581\n",
            "Loss (epoch:  868): 0.02636515\n",
            "Loss (epoch:  869): 0.02636450\n",
            "Loss (epoch:  870): 0.02636385\n",
            "Loss (epoch:  871): 0.02636320\n",
            "Loss (epoch:  872): 0.02636255\n",
            "Loss (epoch:  873): 0.02636191\n",
            "Loss (epoch:  874): 0.02636127\n",
            "Loss (epoch:  875): 0.02636064\n",
            "Loss (epoch:  876): 0.02636000\n",
            "Loss (epoch:  877): 0.02635937\n",
            "Loss (epoch:  878): 0.02635873\n",
            "Loss (epoch:  879): 0.02635810\n",
            "Loss (epoch:  880): 0.02635748\n",
            "Loss (epoch:  881): 0.02635685\n",
            "Loss (epoch:  882): 0.02635624\n",
            "Loss (epoch:  883): 0.02635561\n",
            "Loss (epoch:  884): 0.02635500\n",
            "Loss (epoch:  885): 0.02635438\n",
            "Loss (epoch:  886): 0.02635377\n",
            "Loss (epoch:  887): 0.02635316\n",
            "Loss (epoch:  888): 0.02635255\n",
            "Loss (epoch:  889): 0.02635194\n",
            "Loss (epoch:  890): 0.02635134\n",
            "Loss (epoch:  891): 0.02635074\n",
            "Loss (epoch:  892): 0.02635014\n",
            "Loss (epoch:  893): 0.02634954\n",
            "Loss (epoch:  894): 0.02634895\n",
            "Loss (epoch:  895): 0.02634835\n",
            "Loss (epoch:  896): 0.02634777\n",
            "Loss (epoch:  897): 0.02634718\n",
            "Loss (epoch:  898): 0.02634659\n",
            "Loss (epoch:  899): 0.02634601\n",
            "Loss (epoch:  900): 0.02634542\n",
            "Loss (epoch:  901): 0.02634484\n",
            "Loss (epoch:  902): 0.02634426\n",
            "Loss (epoch:  903): 0.02634369\n",
            "Loss (epoch:  904): 0.02634312\n",
            "Loss (epoch:  905): 0.02634254\n",
            "Loss (epoch:  906): 0.02634197\n",
            "Loss (epoch:  907): 0.02634140\n",
            "Loss (epoch:  908): 0.02634083\n",
            "Loss (epoch:  909): 0.02634027\n",
            "Loss (epoch:  910): 0.02633970\n",
            "Loss (epoch:  911): 0.02633915\n",
            "Loss (epoch:  912): 0.02633859\n",
            "Loss (epoch:  913): 0.02633803\n",
            "Loss (epoch:  914): 0.02633748\n",
            "Loss (epoch:  915): 0.02633692\n",
            "Loss (epoch:  916): 0.02633637\n",
            "Loss (epoch:  917): 0.02633582\n",
            "Loss (epoch:  918): 0.02633528\n",
            "Loss (epoch:  919): 0.02633473\n",
            "Loss (epoch:  920): 0.02633418\n",
            "Loss (epoch:  921): 0.02633364\n",
            "Loss (epoch:  922): 0.02633310\n",
            "Loss (epoch:  923): 0.02633256\n",
            "Loss (epoch:  924): 0.02633202\n",
            "Loss (epoch:  925): 0.02633149\n",
            "Loss (epoch:  926): 0.02633096\n",
            "Loss (epoch:  927): 0.02633042\n",
            "Loss (epoch:  928): 0.02632989\n",
            "Loss (epoch:  929): 0.02632936\n",
            "Loss (epoch:  930): 0.02632884\n",
            "Loss (epoch:  931): 0.02632831\n",
            "Loss (epoch:  932): 0.02632779\n",
            "Loss (epoch:  933): 0.02632727\n",
            "Loss (epoch:  934): 0.02632675\n",
            "Loss (epoch:  935): 0.02632622\n",
            "Loss (epoch:  936): 0.02632571\n",
            "Loss (epoch:  937): 0.02632519\n",
            "Loss (epoch:  938): 0.02632468\n",
            "Loss (epoch:  939): 0.02632416\n",
            "Loss (epoch:  940): 0.02632365\n",
            "Loss (epoch:  941): 0.02632314\n",
            "Loss (epoch:  942): 0.02632263\n",
            "Loss (epoch:  943): 0.02632213\n",
            "Loss (epoch:  944): 0.02632162\n",
            "Loss (epoch:  945): 0.02632112\n",
            "Loss (epoch:  946): 0.02632062\n",
            "Loss (epoch:  947): 0.02632011\n",
            "Loss (epoch:  948): 0.02631962\n",
            "Loss (epoch:  949): 0.02631912\n",
            "Loss (epoch:  950): 0.02631862\n",
            "Loss (epoch:  951): 0.02631812\n",
            "Loss (epoch:  952): 0.02631763\n",
            "Loss (epoch:  953): 0.02631714\n",
            "Loss (epoch:  954): 0.02631665\n",
            "Loss (epoch:  955): 0.02631616\n",
            "Loss (epoch:  956): 0.02631567\n",
            "Loss (epoch:  957): 0.02631518\n",
            "Loss (epoch:  958): 0.02631470\n",
            "Loss (epoch:  959): 0.02631421\n",
            "Loss (epoch:  960): 0.02631373\n",
            "Loss (epoch:  961): 0.02631325\n",
            "Loss (epoch:  962): 0.02631276\n",
            "Loss (epoch:  963): 0.02631229\n",
            "Loss (epoch:  964): 0.02631181\n",
            "Loss (epoch:  965): 0.02631133\n",
            "Loss (epoch:  966): 0.02631086\n",
            "Loss (epoch:  967): 0.02631039\n",
            "Loss (epoch:  968): 0.02630991\n",
            "Loss (epoch:  969): 0.02630944\n",
            "Loss (epoch:  970): 0.02630897\n",
            "Loss (epoch:  971): 0.02630850\n",
            "Loss (epoch:  972): 0.02630803\n",
            "Loss (epoch:  973): 0.02630757\n",
            "Loss (epoch:  974): 0.02630710\n",
            "Loss (epoch:  975): 0.02630663\n",
            "Loss (epoch:  976): 0.02630617\n",
            "Loss (epoch:  977): 0.02630571\n",
            "Loss (epoch:  978): 0.02630525\n",
            "Loss (epoch:  979): 0.02630479\n",
            "Loss (epoch:  980): 0.02630433\n",
            "Loss (epoch:  981): 0.02630387\n",
            "Loss (epoch:  982): 0.02630342\n",
            "Loss (epoch:  983): 0.02630296\n",
            "Loss (epoch:  984): 0.02630251\n",
            "Loss (epoch:  985): 0.02630206\n",
            "Loss (epoch:  986): 0.02630161\n",
            "Loss (epoch:  987): 0.02630115\n",
            "Loss (epoch:  988): 0.02630070\n",
            "Loss (epoch:  989): 0.02630026\n",
            "Loss (epoch:  990): 0.02629981\n",
            "Loss (epoch:  991): 0.02629936\n",
            "Loss (epoch:  992): 0.02629891\n",
            "Loss (epoch:  993): 0.02629847\n",
            "Loss (epoch:  994): 0.02629802\n",
            "Loss (epoch:  995): 0.02629758\n",
            "Loss (epoch:  996): 0.02629714\n",
            "Loss (epoch:  997): 0.02629670\n",
            "Loss (epoch:  998): 0.02629626\n",
            "Loss (epoch:  999): 0.02629582\n",
            "Loss (epoch: 1000): 0.02629539\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "from re import L\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import random\n",
        "\n",
        "from torch import optim\n",
        "from torch.utils import data\n",
        "from torch.optim import lr_scheduler\n",
        "from torchvision import datasets, models, transforms\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import statistics\n",
        "import datetime\n",
        "import os\n",
        "import csv\n",
        "import math\n",
        "import time\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_tensor = torch.tensor(X, dtype=torch.float32)\n",
        "Y_tensor = torch.tensor(Y, dtype=torch.float32)\n",
        "x_train, x_test, y_train, y_test = train_test_split(X_tensor, Y_tensor, test_size=0.1, random_state=41)\n",
        "dataset = TensorDataset(x_train, y_train)\n",
        "dataloader = DataLoader(dataset, batch_size = 16)\n",
        "testdataloader = DataLoader(TensorDataset(x_test, y_test))\n",
        "\n",
        "n1 = 20\n",
        "n2 = 10\n",
        "\n",
        "# Define the neural network class\n",
        "class MLP(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MLP, self).__init__()\n",
        "        self.fc1 = torch.nn.Linear(5, n1)\n",
        "        self.fc2 = torch.nn.Linear(n1, n2)\n",
        "        self.fc3 = torch.nn.Linear(n2, 1)\n",
        "        self.dropout = torch.nn.Dropout(0.1)\n",
        "        self.tanh = torch.nn.Tanh()\n",
        "        self.bn1 = torch.nn.BatchNorm1d(n1)\n",
        "        self.bn2 = torch.nn.BatchNorm1d(n2)\n",
        "        self.bn3 = torch.nn.BatchNorm1d(1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        #x = self.bn1(x)\n",
        "        x = self.tanh(x)\n",
        "        #x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        #x = self.bn2(x)\n",
        "        x = self.tanh(x)\n",
        "        #x = self.dropout(x)\n",
        "        x = self.fc3(x)\n",
        "        #x = self.bn3(x)\n",
        "        return x\n",
        "\n",
        "# Create an instance of the MLP class\n",
        "model = MLP()\n",
        "\n",
        "def initialize_weights(m):\n",
        "    if isinstance(m, torch.nn.Linear):\n",
        "        torch.nn.init.xavier_uniform_(m.weight)\n",
        "        if m.bias is not None:\n",
        "            torch.nn.init.constant_(m.bias, 0)\n",
        "\n",
        "model.apply(initialize_weights)\n",
        "\n",
        "loss_function = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5)\n",
        "#torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "nb_epochs = 1000\n",
        "MLoss = []\n",
        "for epoch in range(0, nb_epochs):\n",
        "\n",
        "    current_loss = 0.0\n",
        "    losses = []\n",
        "    # Iterate over the dataloader for training data\n",
        "    for i, data in enumerate(dataloader, 0):\n",
        "        inputs, targets = data\n",
        "        inputs, targets = inputs.float(), targets.float()\n",
        "        targets = targets.reshape((targets.shape[0],1))\n",
        "        #zero the gradients\n",
        "        optimizer.zero_grad()\n",
        "        #perform forward pass\n",
        "        outputs = model(inputs)\n",
        "        L_weight = 3\n",
        "        #compute loss\n",
        "        batch_loss = []\n",
        "        for j in range(inputs.size(0)):\n",
        "            input_j = inputs[j].reshape((1, inputs.shape[1]))\n",
        "            if input_j[0,0]>0.3:\n",
        "                batch_loss.append(L_weight*loss_function(outputs[j], targets[j]))\n",
        "            else:\n",
        "                batch_loss.append(loss_function(outputs[j], targets[j]))\n",
        "        loss = torch.stack(batch_loss).mean()\n",
        "        losses.append(loss.item())\n",
        "        #perform backward pass\n",
        "        loss.backward()\n",
        "        #perform optimization\n",
        "        optimizer.step()\n",
        "        # Print statistics\n",
        "\n",
        "    mean_loss = sum(losses)/len(losses)\n",
        "    scheduler.step(mean_loss)\n",
        "\n",
        "    print('Loss (epoch: %4d): %.8f' %(epoch+1, mean_loss))\n",
        "    current_loss = 0.0\n",
        "    MLoss.append(mean_loss)\n",
        "\n",
        "# Process is complete.\n",
        "#print('Training process has finished.')\n",
        "\n",
        "torch.save(model, 'IWO_idvg.pt')\n",
        "torch.save(model.state_dict(), 'IWO_idvg_state_dict.pt')\n",
        "\n",
        "####### loss vs. epoch #######\n",
        "xloss = list(range(0, nb_epochs))\n",
        "plt.plot(xloss, np.log10(MLoss))\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Tw_20-o48MH",
        "outputId": "f5163c14-d232-4216-97c7-b36823719b88"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "// VerilogA for GB_lib, IWO_verliogA, veriloga\n",
            "//*******************************************************************************\n",
            "//* * School of Electrical and Computer Engineering, Georgia Institute of Technology\n",
            "//* PI: Prof. Shimeng Yu\n",
            "//* All rights reserved.\n",
            "//*\n",
            "//* Copyright of the model is maintained by the developers, and the model is distributed under\n",
            "//* the terms of the Creative Commons Attribution-NonCommercial 4.0 International Public License\n",
            "//* http://creativecommons.org/licenses/by-nc/4.0/legalcode.\n",
            "//*\n",
            "//* THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND\n",
            "//* ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\n",
            "//* WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n",
            "//* DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\n",
            "//* FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\n",
            "//* DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\n",
            "//* SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\n",
            "//* CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\n",
            "//* OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n",
            "//* OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n",
            "//*\n",
            "//* Developer:\n",
            "//*  Gihun Choe gchoe6@gatech.edu\n",
            "//********************************************************************************/\n",
            "\n",
            "`include \"constants.vams\"\n",
            "`include \"disciplines.vams\"\n",
            "\n",
            "\n",
            "module IWO_verliogA(d, g, s);\n",
            "        inout d, g, s;\n",
            "        electrical d, g, s;\n",
            "\n",
            "        //***** parameters L and W ******//\n",
            "        parameter real W = 0.1; //get parameter fom spectre\n",
            "        parameter real L = 0.05; //get parameter fom spectre\n",
            "        parameter real T_stress = 0.0001; //set on cadence as variable\n",
            "        parameter real T_rec = 0.001;     //set on cadence as variable\n",
            "        parameter real Clk_loops = 50;    //set on cadence as variable\n",
            "        parameter real V_ov = 1.7;        //set on cadence as variable\n",
            "        parameter real Temp = 25;  //set on cadence as variable\n",
            "\n",
            "        parameter MinVg = -1.0 ;\n",
            "        parameter normVg = 0.2222222222222222 ;\n",
            "        parameter MinVd = 0.01 ;\n",
            "        parameter normVd = 0.2949852507374631 ;\n",
            "        parameter MinLg = 0.05 ;\n",
            "        parameter normLg = 1.4285714285714286 ;\n",
            "        parameter MinO = 8.15e-15 ;\n",
            "        parameter normO =33613445378151.26;\n",
            "        parameter MinI = -23.98798356587402 ;\n",
            "        parameter normI =0.04615548498417793;\n",
            "\n",
            "        parameter Mint_stress = 0.0001 ;\n",
            "        parameter normt_stress = 1111.111111111111 ;\n",
            "        parameter Mint_rec = 0.0001 ;\n",
            "        parameter normt_rec = 0.07629452739355005 ;\n",
            "        parameter Minclk_loops = 100.0 ;\n",
            "        parameter normclk_loops = 0.0011111111111111111 ;\n",
            "        parameter Minv_ov = 1.0 ;\n",
            "        parameter normv_ov = 1.4285714285714286 ;\n",
            "        parameter Mintemperature = 25.0 ;\n",
            "        parameter normtemperature = 0.016666666666666666 ;\n",
            "        parameter Mindelta_Vth = 0.04321379542061602 ;\n",
            "        parameter normdelta_Vth = 1.10036881607686 ;\n",
            "\n",
            "        real hvth1_0, hvth1_1, hvth1_2, hvth1_3, hvth1_4, hvth1_5, hvth1_6, hvth1_7, hvth1_8, hvth1_9, hvth1_10, hvth1_11, hvth1_12, hvth1_13, hvth1_14, hvth1_15, hvth1_16, hvth1_17, hvth1_18, hvth1_19;\n",
            "        real hvth2_0, hvth2_1, hvth2_2, hvth2_3, hvth2_4, hvth2_5, hvth2_6, hvth2_7, hvth2_8, hvth2_9;\n",
            "\n",
            "        real Vg, Vd, Vs, Vgs, Vds, Lg, Id, Cgg, Cgsd, Vgd;\n",
            "        real Vgsraw, Vgdraw, dir;\n",
            "        real t_stress, v_ov, t_rec, clk_loops, temp, delta_Vth;\n",
            "\n",
            "        real h1_0, h1_1, h1_2, h1_3, h1_4, h1_5, h1_6, h1_7, h1_8, h1_9, h1_10, h1_11, h1_12, h1_13, h1_14, h1_15, h1_16, h1_17, h1_18, h1_19, h1_20, h1_21, h1_22, h1_23, h1_24;\n",
            "        real h2_0, h2_1, h2_2, h2_3, h2_4, h2_5, h2_6, h2_7, h2_8, h2_9, h2_10, h2_11, h2_12, h2_13, h2_14, h2_15, h2_16, y;\n",
            "        real hc1_0, hc1_1, hc1_2, hc1_3, hc1_4, hc1_5, hc1_6, hc1_7, hc1_8, hc1_9;\n",
            "        real hc1_10, hc1_11, hc1_12, hc1_13, hc1_14, hc1_15, hc1_16;\n",
            "        real hc2_0, hc2_1, hc2_2, hc2_3, hc2_4, hc2_5, hc2_6, hc2_7, hc2_8, hc2_9;\n",
            "        real hc2_10, hc2_11, hc2_12, hc2_13, hc2_14, hc2_15, hc2_16, yc, yvth;\n",
            "\n",
            "analog begin\n",
            "\n",
            "t_stress = (T_stress - Mint_stress)*normt_stress;\n",
            "v_ov = (V_ov - Minv_ov)*normv_ov;\n",
            "t_rec = (T_rec - Mint_rec)*normt_rec ;\n",
            "clk_loops = (Clk_loops - Minclk_loops)*normclk_loops ;\n",
            "temp = (Temp - Mintemperature)*normtemperature ;\n",
            "\n",
            "//******************** delta_Vth NN **********************************//\n",
            "\n",
            "hvth1_0 = tanh(-0.23620945*t_stress+-0.11422197*t_rec+-0.47228724*clk_loops+-0.14914739*v_ov+0.060948677*temp+-0.041763097);\n",
            "hvth1_1 = tanh(0.028779808*t_stress+-0.3255204*t_rec+-0.35955793*clk_loops+0.07913537*v_ov+-0.35168025*temp+0.009527014);\n",
            "hvth1_2 = tanh(-0.23698017*t_stress+-0.031253465*t_rec+0.4063394*clk_loops+0.11984312*v_ov+0.4131939*temp+0.13395193);\n",
            "hvth1_3 = tanh(-0.50531137*t_stress+0.46174458*t_rec+-0.1782618*clk_loops+0.17201678*v_ov+0.32447937*temp+-0.07247048);\n",
            "hvth1_4 = tanh(0.4709104*t_stress+0.024262382*t_rec+-0.5249183*clk_loops+0.21752039*v_ov+-0.29306147*temp+0.014862842);\n",
            "hvth1_5 = tanh(-0.02446169*t_stress+0.3921463*t_rec+-0.10286931*clk_loops+-0.22758731*v_ov+-0.31253672*temp+0.025595723);\n",
            "hvth1_6 = tanh(-0.268575*t_stress+-0.18649903*t_rec+0.07081949*clk_loops+-0.3853843*v_ov+-0.13099946*temp+0.05944069);\n",
            "hvth1_7 = tanh(0.5114621*t_stress+-0.06406127*t_rec+0.15252858*clk_loops+-0.17643532*v_ov+0.1051611*temp+0.0014484077);\n",
            "hvth1_8 = tanh(0.4923979*t_stress+-0.26999214*t_rec+0.3184994*clk_loops+0.11739618*v_ov+-0.43133062*temp+-0.028646497);\n",
            "hvth1_9 = tanh(0.055534013*t_stress+-0.43234816*t_rec+-0.39688012*clk_loops+-0.48029068*v_ov+0.27702916*temp+0.106240295);\n",
            "hvth1_10 = tanh(-0.17787331*t_stress+0.075013824*t_rec+-0.25544536*clk_loops+-0.038302206*v_ov+0.58043146*temp+0.09505023);\n",
            "hvth1_11 = tanh(0.33054048*t_stress+-0.26726386*t_rec+0.14767885*clk_loops+0.2039868*v_ov+0.25976893*temp+0.0515087);\n",
            "hvth1_12 = tanh(-0.49343204*t_stress+-0.38504344*t_rec+-0.043122437*clk_loops+0.41836444*v_ov+-0.09325249*temp+0.0034997498);\n",
            "hvth1_13 = tanh(0.18222524*t_stress+0.6223577*t_rec+-0.022172648*clk_loops+0.08148197*v_ov+-0.032767747*temp+0.074112795);\n",
            "hvth1_14 = tanh(-0.2362092*t_stress+-0.039774742*t_rec+0.26439178*clk_loops+-0.4139979*v_ov+0.18316038*temp+-0.3532865);\n",
            "hvth1_15 = tanh(-0.3070481*t_stress+-0.11789334*t_rec+-0.34454498*clk_loops+-0.13262805*v_ov+0.16419725*temp+-0.16610207);\n",
            "hvth1_16 = tanh(-0.3235092*t_stress+-0.030802485*t_rec+-0.14576414*clk_loops+0.28888705*v_ov+-0.2752622*temp+0.016559413);\n",
            "hvth1_17 = tanh(0.39718556*t_stress+0.17403921*t_rec+0.3695224*clk_loops+0.082769506*v_ov+0.37886643*temp+0.06958915);\n",
            "hvth1_18 = tanh(-0.049671166*t_stress+-0.1643029*t_rec+0.04437352*clk_loops+-0.0018665142*v_ov+-0.27600884*temp+-0.0013660155);\n",
            "hvth1_19 = tanh(-0.2352259*t_stress+-0.10144812*t_rec+0.5238022*clk_loops+0.15918866*v_ov+-0.3490547*temp+0.094069764);\n",
            "\n",
            "hvth2_0 = tanh(-0.16845305*hvth1_0+0.39756483*hvth1_1+0.16693518*hvth1_2+0.30869013*hvth1_3+-0.18108903*hvth1_4+-0.22460845*hvth1_5+-0.0035969557*hvth1_6+0.13595408*hvth1_7+0.24270168*hvth1_8+0.10163385*hvth1_9+0.18189007*hvth1_10+0.3038997*hvth1_11+-0.07428631*hvth1_12+0.42502534*hvth1_13+-0.14626172*hvth1_14+0.15082976*hvth1_15+-0.023482028*hvth1_16+-0.23518007*hvth1_17+-0.26763427*hvth1_18+0.27538553*hvth1_19+-0.03784745);\n",
            "hvth2_1 = tanh(-0.41794166*hvth1_0+0.37053975*hvth1_1+-0.26791015*hvth1_2+-0.11355441*hvth1_3+-0.5075522*hvth1_4+0.3045135*hvth1_5+0.34555352*hvth1_6+0.14713447*hvth1_7+0.43753296*hvth1_8+-0.38279983*hvth1_9+-0.046737093*hvth1_10+0.4014616*hvth1_11+-0.020086141*hvth1_12+0.24303234*hvth1_13+-0.33418557*hvth1_14+-0.5091438*hvth1_15+0.18578485*hvth1_16+-0.1603029*hvth1_17+-0.08585446*hvth1_18+0.5277006*hvth1_19+0.070012085);\n",
            "hvth2_2 = tanh(-0.14573708*hvth1_0+0.3184643*hvth1_1+0.2748625*hvth1_2+-0.23226483*hvth1_3+0.35325083*hvth1_4+0.1482115*hvth1_5+0.054981463*hvth1_6+0.07355818*hvth1_7+-0.36879742*hvth1_8+0.2377312*hvth1_9+0.25506988*hvth1_10+0.33719558*hvth1_11+0.4197101*hvth1_12+0.14284953*hvth1_13+-0.022160064*hvth1_14+0.06548938*hvth1_15+0.31598723*hvth1_16+0.26018983*hvth1_17+0.22775526*hvth1_18+0.16941279*hvth1_19+0.039906014);\n",
            "hvth2_3 = tanh(0.03997243*hvth1_0+-0.1039401*hvth1_1+0.07319529*hvth1_2+-0.4143416*hvth1_3+0.16619112*hvth1_4+0.40054905*hvth1_5+0.0026044135*hvth1_6+-0.37246403*hvth1_7+-0.12070038*hvth1_8+0.3703949*hvth1_9+-0.25257835*hvth1_10+0.41636255*hvth1_11+0.3992935*hvth1_12+-0.3524762*hvth1_13+-0.18442999*hvth1_14+-0.10957257*hvth1_15+0.35311857*hvth1_16+0.3581483*hvth1_17+0.13963126*hvth1_18+-0.18214768*hvth1_19+0.033594046);\n",
            "hvth2_4 = tanh(-0.06959928*hvth1_0+-0.16917941*hvth1_1+-0.21075183*hvth1_2+0.28207237*hvth1_3+0.15581249*hvth1_4+-0.30225942*hvth1_5+0.28091654*hvth1_6+-0.4213002*hvth1_7+-0.4188872*hvth1_8+-0.18873656*hvth1_9+-0.05960784*hvth1_10+-0.19246225*hvth1_11+-0.09820488*hvth1_12+-0.03853728*hvth1_13+-0.0039159185*hvth1_14+0.035747003*hvth1_15+0.3010332*hvth1_16+0.1148045*hvth1_17+-0.3731184*hvth1_18+0.37786087*hvth1_19+0.0018480162);\n",
            "hvth2_5 = tanh(0.3516192*hvth1_0+-0.3393518*hvth1_1+-0.2819159*hvth1_2+-0.40437645*hvth1_3+-0.41484287*hvth1_4+0.2371237*hvth1_5+0.19181025*hvth1_6+0.37355548*hvth1_7+-0.1776793*hvth1_8+0.39244145*hvth1_9+0.1692209*hvth1_10+0.42198882*hvth1_11+0.4604563*hvth1_12+0.3640072*hvth1_13+0.39516565*hvth1_14+-0.27230933*hvth1_15+-0.33197865*hvth1_16+0.34897798*hvth1_17+-0.12794481*hvth1_18+0.45925367*hvth1_19+0.039266337);\n",
            "hvth2_6 = tanh(0.18713506*hvth1_0+-0.13902345*hvth1_1+-0.29257163*hvth1_2+-0.0020112314*hvth1_3+-0.3171185*hvth1_4+0.2552745*hvth1_5+0.32808417*hvth1_6+0.23648283*hvth1_7+0.16705322*hvth1_8+0.113825805*hvth1_9+-0.23494478*hvth1_10+-0.14338814*hvth1_11+0.28235865*hvth1_12+-0.22754785*hvth1_13+0.4243886*hvth1_14+-0.3482737*hvth1_15+0.4114932*hvth1_16+0.09327023*hvth1_17+0.31020725*hvth1_18+-0.16473684*hvth1_19+-0.053053144);\n",
            "hvth2_7 = tanh(0.37427378*hvth1_0+-0.16942349*hvth1_1+-0.31311038*hvth1_2+-0.3445607*hvth1_3+0.18884918*hvth1_4+0.23512177*hvth1_5+-0.43438914*hvth1_6+-0.28158382*hvth1_7+-0.17749362*hvth1_8+0.2672613*hvth1_9+0.4329218*hvth1_10+-0.14370936*hvth1_11+-0.22237973*hvth1_12+-0.15948312*hvth1_13+-0.37799123*hvth1_14+0.0115602*hvth1_15+-0.35784122*hvth1_16+0.48435572*hvth1_17+-0.41795096*hvth1_18+0.34065259*hvth1_19+0.031116992);\n",
            "hvth2_8 = tanh(0.16162485*hvth1_0+0.20162259*hvth1_1+0.13203059*hvth1_2+-0.521588*hvth1_3+0.414747*hvth1_4+-0.10304635*hvth1_5+-0.20306803*hvth1_6+0.5009071*hvth1_7+0.07902086*hvth1_8+0.2757893*hvth1_9+-0.06971512*hvth1_10+-0.20647134*hvth1_11+-0.4602277*hvth1_12+-0.03632996*hvth1_13+-0.44813243*hvth1_14+-0.21619838*hvth1_15+0.28076693*hvth1_16+0.12535621*hvth1_17+0.43065372*hvth1_18+0.06388416*hvth1_19+0.031914692);\n",
            "hvth2_9 = tanh(-0.41457635*hvth1_0+-0.09908786*hvth1_1+-0.2014291*hvth1_2+0.4129428*hvth1_3+-0.15876462*hvth1_4+-0.1535896*hvth1_5+0.37809083*hvth1_6+0.41081986*hvth1_7+0.11348377*hvth1_8+-0.11943263*hvth1_9+0.0014336263*hvth1_10+-0.12349735*hvth1_11+-0.3121594*hvth1_12+0.14242256*hvth1_13+0.009055796*hvth1_14+0.11027221*hvth1_15+-0.41051406*hvth1_16+-0.33950076*hvth1_17+-0.44527945*hvth1_18+0.019572979*hvth1_19+0.038242202);\n",
            "\n",
            "yvth = -0.29885656*hvth2_0+0.22027107*hvth2_1+0.42393205*hvth2_2+0.51505446*hvth2_3+0.33099476*hvth2_4+0.26275173*hvth2_5+-0.26267987*hvth2_6+0.046849344*hvth2_7+-0.40909058*hvth2_8+0.35796097*hvth2_9+0.04243591;\n",
            "\n",
            "delta_Vth = (yvth - Mindelta_Vth) * normdelta_Vth;\n",
            "$strobe(\"dvth=$g\",delta_Vth);\n",
            "\n",
            "\n",
            "        Vg = V(g) ;\n",
            "        Vs = V(s) ;\n",
            "        Vd = V(d) ;\n",
            "        Vgsraw = Vg-Vs ;\n",
            "        Vgdraw = Vg-Vd ;\n",
            "\n",
            "if (Vgsraw >= Vgdraw) begin\n",
            "        Vgs = ((Vg-Vs) - MinVg) * normVg ;\n",
            "        dir = 1;\n",
            "end\n",
            "\n",
            "else begin\n",
            "        Vgs = ((Vg-Vd) - MinVg) * normVg ;\n",
            "        dir = -1;\n",
            "end\n",
            "\n",
            "        Vds = (abs(Vd-Vs) - MinVd) * normVd ;\n",
            "        Vgs = Vgs - delta_Vth ; // BTI Vth variation\n",
            "        Lg = (L -MinLg)*normLg ;\n",
            "\n",
            "\n",
            "\n",
            "//******************** C-V NN **********************************//\n",
            "hc1_0 = tanh(-0.99871427*Vgs+-0.16952373*Vds+0.32118186*Lg+0.41485423);\n",
            "hc1_1 = tanh(0.31587568*Vgs+0.13397887*Vds+-0.4541538*Lg+-0.3630942);\n",
            "hc1_2 = tanh(-0.76281804*Vgs+0.09352969*Vds+1.1961353*Lg+0.3904977);\n",
            "hc1_3 = tanh(-1.115087*Vgs+0.85752946*Vds+-0.11746484*Lg+0.5500279);\n",
            "hc1_4 = tanh(1.0741386*Vgs+0.82041687*Vds+0.19092631*Lg+-0.4009425);\n",
            "hc1_5 = tanh(-0.47921795*Vgs+-0.8749933*Vds+-0.054768667*Lg+0.62785167);\n",
            "hc1_6 = tanh(0.5449184*Vgs+-4.409165*Vds+-0.072947875*Lg+-0.31324536);\n",
            "hc1_7 = tanh(-2.9224303*Vgs+2.7675478*Vds+0.08862238*Lg+0.6493558);\n",
            "hc1_8 = tanh(0.65050656*Vgs+-0.29751927*Vds+0.1571876*Lg+-0.38088372);\n",
            "hc1_9 = tanh(-0.30384183*Vgs+0.5649165*Vds+2.6806898*Lg+0.3197917);\n",
            "hc1_10 = tanh(-0.095988505*Vgs+2.0158541*Vds+-0.42972717*Lg+-0.30388466);\n",
            "hc1_11 = tanh(6.7699738*Vgs+-0.07234483*Vds+-0.013545353*Lg+-1.3694142);\n",
            "hc1_12 = tanh(-0.3404029*Vgs+0.0443459*Vds+0.89597*Lg+0.069993004);\n",
            "hc1_13 = tanh(0.62300175*Vgs+-0.29515797*Vds+1.6753465*Lg+-0.6520838);\n",
            "hc1_14 = tanh(0.37957156*Vgs+0.2237372*Vds+0.08591952*Lg+0.13126835);\n",
            "hc1_15 = tanh(0.19949242*Vgs+-0.26481664*Vds+-0.41059187*Lg+-0.40832308);\n",
            "hc1_16 = tanh(0.98966587*Vgs+-0.24259183*Vds+0.36584845*Lg+-0.8024042);\n",
            "\n",
            "hc2_0 = tanh(-0.91327864*hc1_0+0.4696781*hc1_1+0.3302202*hc1_2+0.11393868*hc1_3+0.45070222*hc1_4+-0.2894044*hc1_5+0.55066*hc1_6+-1.6242687*hc1_7+-0.38140613*hc1_8+0.032771554*hc1_9+0.17647126);\n",
            "hc2_1 = tanh(-1.1663305*hc1_0+-0.523984*hc1_1+-0.90804136*hc1_2+-0.7418044*hc1_3+1.456171*hc1_4+-0.16802542*hc1_5+0.8235596*hc1_6+-2.2246246*hc1_7+-0.40805355*hc1_8+0.7207601*hc1_9+0.4729169);\n",
            "hc2_2 = tanh(-0.69065374*hc1_0+0.40205315*hc1_1+-0.49410668*hc1_2+0.8681325*hc1_3+0.471351*hc1_4+0.46939445*hc1_5+0.45568785*hc1_6+-0.92935294*hc1_7+-0.8209646*hc1_8+0.1158967*hc1_9+-0.075798474);\n",
            "hc2_3 = tanh(0.11535446*hc1_0+-0.06296927*hc1_1+-0.6740435*hc1_2+0.7428315*hc1_3+0.05890677*hc1_4+0.9579441*hc1_5+-0.037319*hc1_6+-0.18491426*hc1_7+-0.02981994*hc1_8+0.038347963*hc1_9+0.039531134);\n",
            "hc2_4 = tanh(0.37817463*hc1_0+-0.6811279*hc1_1+1.1388369*hc1_2+0.19983096*hc1_3+-0.20415118*hc1_4+1.3022176*hc1_5+0.22571652*hc1_6+0.1690611*hc1_7+-0.56475276*hc1_8+-0.4069731*hc1_9+0.99962974);\n",
            "hc2_5 = tanh(0.032873478*hc1_0+-0.05209407*hc1_1+-0.010839908*hc1_2+-0.13892579*hc1_3+-0.050480977*hc1_4+0.0127089145*hc1_5+0.0052771433*hc1_6+0.02029242*hc1_7+-0.08705659*hc1_8+0.0254766*hc1_9+0.025135752);\n",
            "hc2_6 = tanh(-0.34639204*hc1_0+0.06937975*hc1_1+0.18671949*hc1_2+-0.18912783*hc1_3+0.1312504*hc1_4+0.4627272*hc1_5+-0.42590702*hc1_6+-0.10966313*hc1_7+0.66083515*hc1_8+-0.050718334*hc1_9+0.08234678);\n",
            "hc2_7 = tanh(0.90656275*hc1_0+-0.037281644*hc1_1+0.77237594*hc1_2+1.4710428*hc1_3+0.13597831*hc1_4+-0.059844542*hc1_5+-0.7801535*hc1_6+3.7814677*hc1_7+-0.5976644*hc1_8+0.2721995*hc1_9+0.023777716);\n",
            "hc2_8 = tanh(0.39720625*hc1_0+-0.45262313*hc1_1+0.19873238*hc1_2+0.9750888*hc1_3+-0.9427992*hc1_4+0.4487432*hc1_5+-0.3372945*hc1_6+0.33729544*hc1_7+-0.1667088*hc1_8+-0.5707525*hc1_9+0.27954483);\n",
            "hc2_9 = tanh(0.28551984*hc1_0+-0.68350387*hc1_1+0.9916423*hc1_2+-0.8254094*hc1_3+0.09875706*hc1_4+0.47609732*hc1_5+-0.058662917*hc1_6+0.09181381*hc1_7+0.09592329*hc1_8+1.3940467*hc1_9+0.3865768);\n",
            "hc2_10 = tanh(0.098737516*hc1_0+0.060473576*hc1_1+0.42824662*hc1_2+0.15018038*hc1_3+0.082621895*hc1_4+-0.00019039502*hc1_5+-0.3321634*hc1_6+0.7936295*hc1_7+-0.041197542*hc1_8+0.6530619*hc1_9+0.1338804);\n",
            "hc2_11 = tanh(-0.3585284*hc1_0+-0.09956017*hc1_1+0.17224246*hc1_2+-0.016925728*hc1_3+-0.46462816*hc1_4+-0.5649022*hc1_5+1.251695*hc1_6+-0.4303161*hc1_7+0.48546878*hc1_8+0.22958975*hc1_9+-0.27899802);\n",
            "hc2_12 = tanh(0.8565631*hc1_0+-0.7622999*hc1_1+0.32367912*hc1_2+1.4776785*hc1_3+0.2712369*hc1_4+0.2275511*hc1_5+0.39908803*hc1_6+4.2305493*hc1_7+-0.3467536*hc1_8+0.41231114*hc1_9+0.47123823);\n",
            "hc2_13 = tanh(-0.26411077*hc1_0+-0.17583853*hc1_1+0.045439184*hc1_2+-0.13801138*hc1_3+0.03278085*hc1_4+-0.45625108*hc1_5+-0.17905861*hc1_6+0.3060186*hc1_7+-0.3361926*hc1_8+-0.050055273*hc1_9+0.17996444);\n",
            "hc2_14 = tanh(0.016094366*hc1_0+-0.013196736*hc1_1+0.4124856*hc1_2+0.22926371*hc1_3+-0.35071182*hc1_4+-0.34217188*hc1_5+-0.69466996*hc1_6+1.0563152*hc1_7+-0.18019852*hc1_8+0.061871335*hc1_9+0.09762555);\n",
            "hc2_15 = tanh(-0.18970782*hc1_0+0.3624813*hc1_1+-1.3419824*hc1_2+0.103635244*hc1_3+-0.14595217*hc1_4+-0.1899393*hc1_5+0.176524*hc1_6+-0.4428012*hc1_7+-0.39544868*hc1_8+-0.45783517*hc1_9+0.1884755);\n",
            "hc2_16 = tanh(-0.1585831*hc1_0+0.035894837*hc1_1+-0.14261873*hc1_2+0.25914294*hc1_3+0.040607046*hc1_4+0.11555795*hc1_5+0.0022548323*hc1_6+-0.002149359*hc1_7+0.07067297*hc1_8+0.019578662*hc1_9+0.16657573);\n",
            "yc = 0.17503543*hc2_0+-0.15556754*hc2_1+-0.125455*hc2_2+-0.07502612*hc2_3+-0.16671115*hc2_4+0.29854503;\n",
            "\n",
            "Cgg = (yc / normO + MinO)*W;\n",
            "Cgsd = Cgg/2 ;\n",
            "\n",
            "//******************** I-V NN **********************************//\n",
            "h1_0 = tanh(0.0023826673*Vgs+-0.0009636134*Vds+0.006639037*Lg+-0.0004692053);\n",
            "h1_1 = tanh(-7.8007555*Vgs+2.639791*Vds+-0.025273597*Lg+-1.1747514);\n",
            "h1_2 = tanh(1.9884652*Vgs+0.62111*Vds+-0.11525345*Lg+-0.14575638);\n",
            "h1_3 = tanh(0.10625471*Vgs+0.09442053*Vds+-0.052119628*Lg+1.1568379);\n",
            "h1_4 = tanh(4.058087*Vgs+-0.7568158*Vds+0.008070099*Lg+-0.4820452);\n",
            "h1_5 = tanh(-1.3065948*Vgs+-0.0492497*Vds+-0.081622526*Lg+0.20351954);\n",
            "h1_6 = tanh(-2.9507525*Vgs+-0.91150546*Vds+-0.06477873*Lg+0.09646383);\n",
            "h1_7 = tanh(-0.5886177*Vgs+0.63788587*Vds+-0.13710928*Lg+0.30260038);\n",
            "h1_8 = tanh(-0.4311161*Vgs+-0.7250705*Vds+-0.06134645*Lg+0.44054285);\n",
            "h1_9 = tanh(0.012132638*Vgs+-0.42545322*Vds+-0.66478956*Lg+0.13182367);\n",
            "h1_10 = tanh(3.289041e-05*Vgs+0.0011367714*Vds+-0.0051904405*Lg+5.4112927e-05);\n",
            "h1_11 = tanh(-0.6007774*Vgs+0.09259224*Vds+0.2170182*Lg+-0.2908748);\n",
            "h1_12 = tanh(0.28018302*Vgs+1.3014064*Vds+-0.13490067*Lg+-0.22006753);\n",
            "h1_13 = tanh(0.01864017*Vgs+-13.0928755*Vds+-0.0037254083*Lg+-0.10935691);\n",
            "h1_14 = tanh(-0.0049708197*Vgs+0.0022613218*Vds+-0.014408149*Lg+0.0011340647);\n",
            "h1_15 = tanh(-0.094654046*Vgs+-0.4174114*Vds+0.18892045*Lg+0.05248958);\n",
            "h1_16 = tanh(-0.25090316*Vgs+-0.11111481*Vds+-0.009133225*Lg+0.08377026);\n",
            "h1_17 = tanh(0.9275306*Vgs+0.40686756*Vds+0.14127344*Lg+-0.059246097);\n",
            "h1_18 = tanh(-0.27084363*Vgs+0.07915911*Vds+-10.836302*Lg+-1.1535788);\n",
            "h1_19 = tanh(1.6852072*Vgs+-0.24846223*Vds+0.049734037*Lg+-0.19625053);\n",
            "\n",
            "h2_0 = tanh(1.1139417*h1_0+-0.40033522*h1_1+0.920759*h1_2+0.47607598*h1_3+0.3978683*h1_4+0.8008105*h1_5+-0.40445566*h1_6+0.8668027*h1_7+0.23365597*h1_8+-0.28254375*h1_9+-0.63985884*h1_10+-0.62523574*h1_11+0.8338383*h1_12+-2.0540943*h1_13+1.0749483*h1_14+-1.1075479*h1_15+0.60926706*h1_16+1.1118286*h1_17+-0.8917559*h1_18+0.029025732*h1_19+0.6622382);\n",
            "h2_1 = tanh(0.37195024*h1_0+-0.761445*h1_1+0.6514153*h1_2+0.6211995*h1_3+-0.063539445*h1_4+0.31260127*h1_5+-0.3529579*h1_6+0.50422627*h1_7+0.18943883*h1_8+-0.38029787*h1_9+-0.3464503*h1_10+-0.48301366*h1_11+0.081978925*h1_12+-0.08305682*h1_13+-0.08420117*h1_14+-0.8029313*h1_15+-0.37052512*h1_16+0.4553502*h1_17+-0.71959645*h1_18+-0.17320661*h1_19+0.6566257);\n",
            "h2_2 = tanh(-0.021933522*h1_0+-0.017245224*h1_1+0.030417712*h1_2+0.2967218*h1_3+-0.048668377*h1_4+0.029245213*h1_5+0.011756416*h1_6+0.004705658*h1_7+-0.042515088*h1_8+0.010462476*h1_9+0.001331279*h1_10+0.1694541*h1_11+-0.010949079*h1_12+-0.004144526*h1_13+0.04856694*h1_14+-0.010465159*h1_15+0.24588406*h1_16+-0.028521152*h1_17+0.12161568*h1_18+0.1753255*h1_19+-0.09125355);\n",
            "h2_3 = tanh(-0.025546636*h1_0+-3.2702503*h1_1+0.46812135*h1_2+-0.25135994*h1_3+3.0725436*h1_4+0.22513995*h1_5+-1.0327209*h1_6+-0.56274736*h1_7+0.4726107*h1_8+0.38182434*h1_9+0.016403453*h1_10+-0.09128962*h1_11+0.20997792*h1_12+-0.4951615*h1_13+0.026481293*h1_14+0.6085288*h1_15+-0.09078652*h1_16+0.36613584*h1_17+0.26257026*h1_18+0.39794916*h1_19+-0.52920526);\n",
            "h2_4 = tanh(-0.44392154*h1_0+-0.5650475*h1_1+0.91716766*h1_2+0.96703196*h1_3+0.4597048*h1_4+1.452921*h1_5+-0.8115141*h1_6+0.92326456*h1_7+0.2862403*h1_8+-0.9441585*h1_9+0.22188367*h1_10+-1.0092766*h1_11+0.5495966*h1_12+0.44870532*h1_13+0.48705962*h1_14+-0.6957133*h1_15+0.27711377*h1_16+1.0138507*h1_17+-0.34337458*h1_18+-0.21548931*h1_19+0.42685974);\n",
            "h2_5 = tanh(0.0003710325*h1_0+0.5746112*h1_1+0.4144258*h1_2+0.04459103*h1_3+1.2373503*h1_4+0.12786175*h1_5+-0.16099685*h1_6+0.9826207*h1_7+-0.09701193*h1_8+-0.4379135*h1_9+-0.0035542254*h1_10+0.04205593*h1_11+0.65820116*h1_12+0.3810506*h1_13+0.0004259507*h1_14+-0.21782044*h1_15+-0.057544652*h1_16+0.24420041*h1_17+-0.10138292*h1_18+-0.2387106*h1_19+0.867847);\n",
            "h2_6 = tanh(-0.0077049686*h1_0+-0.05349668*h1_1+-0.11536639*h1_2+0.06141029*h1_3+-0.034344573*h1_4+0.21672213*h1_5+-0.09835135*h1_6+-0.25849992*h1_7+0.10576329*h1_8+-0.14288934*h1_9+0.027846925*h1_10+-0.20104973*h1_11+0.24784875*h1_12+0.03396087*h1_13+-0.016039118*h1_14+-0.03147038*h1_15+-0.023109786*h1_16+-0.118931994*h1_17+0.18256636*h1_18+0.09981429*h1_19+0.079372324);\n",
            "h2_7 = tanh(-0.009369999*h1_0+-2.4468672*h1_1+-0.41433397*h1_2+-0.6289744*h1_3+-1.8285819*h1_4+0.12905455*h1_5+0.83588725*h1_6+0.11491322*h1_7+0.3871084*h1_8+0.07153052*h1_9+0.013530584*h1_10+0.40614852*h1_11+0.10340061*h1_12+-0.04473306*h1_13+0.0075287875*h1_14+0.5593612*h1_15+0.34401885*h1_16+-0.5538749*h1_17+0.36615464*h1_18+-0.2666981*h1_19+0.20194086);\n",
            "h2_8 = tanh(-0.0017045025*h1_0+-1.1388719*h1_1+0.25932005*h1_2+-0.6482845*h1_3+2.263395*h1_4+0.115192235*h1_5+-0.40946937*h1_6+-0.038117886*h1_7+-0.03703431*h1_8+0.05965774*h1_9+0.0032721795*h1_10+0.2561857*h1_11+0.47324425*h1_12+-0.25200802*h1_13+0.0017081021*h1_14+0.29819545*h1_15+0.061945435*h1_16+0.13516657*h1_17+0.5409335*h1_18+-0.17400871*h1_19+0.11659722);\n",
            "h2_9 = tanh(0.014866875*h1_0+1.0399909*h1_1+1.0108095*h1_2+-0.0918755*h1_3+-0.7634763*h1_4+-0.49749368*h1_5+-0.7020006*h1_6+-0.16384888*h1_7+-0.07833025*h1_8+-0.23745225*h1_9+-0.023095092*h1_10+-0.29244414*h1_11+0.6255917*h1_12+0.41413808*h1_13+-0.019823061*h1_14+-1.0593235*h1_15+-0.42221433*h1_16+0.98025715*h1_17+-0.71444243*h1_18+0.84347373*h1_19+0.28510216);\n",
            "h2_10 = tanh(-0.013515852*h1_0+0.68840384*h1_1+-0.030184174*h1_2+-0.48098114*h1_3+0.08609854*h1_4+-0.50878614*h1_5+0.26547763*h1_6+-0.7151076*h1_7+0.111288495*h1_8+0.53379977*h1_9+0.022258151*h1_10+0.16971351*h1_11+-1.2486296*h1_12+0.9649942*h1_13+0.0028906881*h1_14+0.31582054*h1_15+0.24213083*h1_16+-0.27242163*h1_17+0.11330636*h1_18+0.17038865*h1_19+-0.42089102);\n",
            "h2_11 = tanh(-0.009949424*h1_0+-0.6704206*h1_1+0.13177924*h1_2+0.07316155*h1_3+-0.28207502*h1_4+-0.32401362*h1_5+0.020746209*h1_6+-0.05517531*h1_7+0.10340257*h1_8+0.29381263*h1_9+0.020497978*h1_10+0.031984854*h1_11+0.079552434*h1_12+-7.7884445*h1_13+0.01224806*h1_14+0.8700812*h1_15+-0.16562358*h1_16+0.14370379*h1_17+0.2389181*h1_18+-0.19771136*h1_19+0.33039534);\n",
            "h2_12 = tanh(-0.002431529*h1_0+0.30154988*h1_1+-0.45072728*h1_2+0.27823612*h1_3+-0.25173753*h1_4+-0.58188903*h1_5+0.39836177*h1_6+-0.060329597*h1_7+0.1667837*h1_8+-0.09419194*h1_9+0.0108116735*h1_10+-0.3581154*h1_11+0.15690842*h1_12+-0.41209573*h1_13+-0.0070331707*h1_14+-0.23976392*h1_15+0.16620094*h1_16+-0.034617994*h1_17+0.32754526*h1_18+0.8490298*h1_19+0.026407415);\n",
            "h2_13 = tanh(-0.7385622*h1_0+-0.25174448*h1_1+2.090295*h1_2+0.67499936*h1_3+0.07392262*h1_4+0.5413692*h1_5+-0.8287433*h1_6+2.2810504*h1_7+1.1917778*h1_8+1.3719273*h1_9+0.25762329*h1_10+-2.1111712*h1_11+1.6987114*h1_12+-0.437825*h1_13+1.7649944*h1_14+-2.2254016*h1_15+-2.3555171*h1_16+-0.7732424*h1_17+-1.0756916*h1_18+-0.1367373*h1_19+1.5085722);\n",
            "h2_14 = tanh(-0.002535408*h1_0+-4.2686224*h1_1+-0.045537975*h1_2+0.08043166*h1_3+1.7274952*h1_4+-0.1260494*h1_5+-0.23011239*h1_6+-0.21025337*h1_7+0.48537356*h1_8+0.39049447*h1_9+-0.006565428*h1_10+-0.293028*h1_11+-1.1253971*h1_12+-0.2890831*h1_13+0.0042421576*h1_14+0.4230598*h1_15+-0.17869289*h1_16+-0.034056116*h1_17+0.61135995*h1_18+0.6345532*h1_19+0.73144835);\n",
            "h2_15 = tanh(-0.5239093*h1_0+0.49806875*h1_1+0.53555894*h1_2+1.2645539*h1_3+1.011548*h1_4+0.39209503*h1_5+-0.78194916*h1_6+0.94610345*h1_7+0.9121405*h1_8+-1.4693716*h1_9+-0.58336824*h1_10+-1.1661471*h1_11+-0.16158457*h1_12+0.058512915*h1_13+1.3358645*h1_14+0.5742125*h1_15+-1.3325212*h1_16+1.1804186*h1_17+-1.4877121*h1_18+0.6357802*h1_19+1.1328585);\n",
            "h2_16 = tanh(0.0095406*h1_0+-0.53232694*h1_1+0.1535685*h1_2+0.26108417*h1_3+-0.08375187*h1_4+0.3462123*h1_5+-0.61755395*h1_6+0.0015145333*h1_7+-0.108780764*h1_8+0.2555477*h1_9+0.00023940884*h1_10+-0.26577523*h1_11+-0.27689674*h1_12+0.43049082*h1_13+-0.025424613*h1_14+0.24634649*h1_15+-0.22579487*h1_16+0.37249807*h1_17+-0.8058662*h1_18+-0.77451354*h1_19+0.5821276);\n",
            "y = -0.17525196*h2_0+-0.22995844*h2_1+0.0026147955*h2_2+0.091129646*h2_3+-0.4534783*h2_4+0.4119419*h2_5+-1.4205361e-05*h2_6+-0.11359831*h2_7+0.1762025*h2_8+-0.1002102*h2_9+-0.37161925*h2_10+0.2683793*h2_11+0.051897053*h2_12+0.19354156*h2_13+-0.10134394*h2_14+0.3937854*h2_15+-0.3296008*h2_16+0.3220947;\n",
            "\n",
            "        Id = pow(10, (y/normI + MinI)) ;\n",
            "\n",
            "if (Id <= 1e-15) begin //limit\n",
            "        Id = 1e-15;\n",
            "        //Id = Id;\n",
            "end\n",
            "else begin\n",
            "        Id = Id;\n",
            "end  //limit end\n",
            "\n",
            "\n",
            "        I(g, d) <+ Cgsd*ddt(Vg-Vd) ;\n",
            "        I(g, s) <+ Cgsd*ddt(Vg-Vs) ;\n",
            "\n",
            "if (Vgsraw >= Vgdraw) begin\n",
            "        I(d, s) <+ dir*Id*W ;\n",
            "\n",
            "end\n",
            "\n",
            "else begin\n",
            "        I(d, s) <+ dir*Id*W ;\n",
            "\n",
            "end\n",
            "\n",
            "end\n",
            "\n",
            "endmodule\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "with torch.no_grad():\n",
        "\n",
        "    output = []\n",
        "    # Iterate over the dataloader for training data\n",
        "    for i, data in enumerate(testdataloader, 0):\n",
        "        inputs, targets = data\n",
        "        inputs, targets = inputs.float(), targets.float()\n",
        "        targets = targets.reshape((targets.shape[0],1))\n",
        "\n",
        "        #zero the gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        #perform forward pass\n",
        "        outputs = model(inputs)\n",
        "        output.append(outputs)\n",
        "# Process is complete.\n",
        "#print('Training process has finished.')\n",
        "\n",
        "# Extract the weights and biases from the model\n",
        "weights_1 = model.fc1.weight.detach().numpy()\n",
        "bias_1 = model.fc1.bias.detach().numpy()\n",
        "weights_2 = model.fc2.weight.detach().numpy()\n",
        "bias_2 = model.fc2.bias.detach().numpy()\n",
        "weights_3 = model.fc3.weight.detach().numpy()\n",
        "bias_3 = model.fc3.bias.detach().numpy()\n",
        "\n",
        "def generate_variable_declarations(weights_shape, layer_prefix):\n",
        "    declarations = \"\"\n",
        "    num_neurons = weights_shape  # Number of neurons is determined by the first dimension of the weights matrix\n",
        "    layer_declarations = \", \".join([f\"{layer_prefix}_{i}\" for i in range(num_neurons)]) + \";\"\n",
        "    declarations += layer_declarations\n",
        "    return declarations\n",
        "\n",
        "# Use the function to generate declarations for each layer\n",
        "h1_declarations = generate_variable_declarations(weights_1.shape[0], \"hvth1\")\n",
        "h2_declarations = generate_variable_declarations(weights_2.shape[0], \"hvth2\")\n",
        "\n",
        "verilog_code = \"\"\"\n",
        "// VerilogA for GB_lib, IWO_verliogA, veriloga\n",
        "//*******************************************************************************\n",
        "//* * School of Electrical and Computer Engineering, Georgia Institute of Technology\n",
        "//* PI: Prof. Shimeng Yu\n",
        "//* All rights reserved.\n",
        "//*\n",
        "//* Copyright of the model is maintained by the developers, and the model is distributed under\n",
        "//* the terms of the Creative Commons Attribution-NonCommercial 4.0 International Public License\n",
        "//* http://creativecommons.org/licenses/by-nc/4.0/legalcode.\n",
        "//*\n",
        "//* THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND\n",
        "//* ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\n",
        "//* WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n",
        "//* DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\n",
        "//* FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\n",
        "//* DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\n",
        "//* SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\n",
        "//* CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\n",
        "//* OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n",
        "//* OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n",
        "//*\n",
        "//* Developer:\n",
        "//*  Gihun Choe gchoe6@gatech.edu\n",
        "//********************************************************************************/\n",
        "\n",
        "`include \"constants.vams\"\n",
        "`include \"disciplines.vams\"\n",
        "\n",
        "\n",
        "module IWO_verliogA(d, g, s);\n",
        "        inout d, g, s;\n",
        "        electrical d, g, s;\n",
        "\n",
        "        //***** parameters L and W ******//\n",
        "        parameter real W = 0.1; //get parameter fom spectre\n",
        "        parameter real L = 0.05; //get parameter fom spectre\n",
        "        parameter real T_stress = 0.0001; //set on cadence as variable\n",
        "        parameter real T_rec = 0.001;     //set on cadence as variable\n",
        "        parameter real Clk_loops = 50;    //set on cadence as variable\n",
        "        parameter real V_ov = 1.7;        //set on cadence as variable\n",
        "        parameter real Temp = 25;  //set on cadence as variable\n",
        "\n",
        "        parameter MinVg = -1.0 ;\n",
        "        parameter normVg = 0.2222222222222222 ;\n",
        "        parameter MinVd = 0.01 ;\n",
        "        parameter normVd = 0.2949852507374631 ;\n",
        "        parameter MinLg = 0.05 ;\n",
        "        parameter normLg = 1.4285714285714286 ;\n",
        "        parameter MinO = 8.15e-15 ;\n",
        "        parameter normO =33613445378151.26;\n",
        "        parameter MinI = -23.98798356587402 ;\n",
        "        parameter normI =0.04615548498417793;\n",
        "\n",
        "        parameter Mint_stress = {} ;\n",
        "        parameter normt_stress = {} ;\n",
        "        parameter Mint_rec = {} ;\n",
        "        parameter normt_rec = {} ;\n",
        "        parameter Minclk_loops = {} ;\n",
        "        parameter normclk_loops = {} ;\n",
        "        parameter Minv_ov = {} ;\n",
        "        parameter normv_ov = {} ;\n",
        "        parameter Mintemperature = {} ;\n",
        "        parameter normtemperature = {} ;\n",
        "        parameter Mindelta_Vth = {} ;\n",
        "        parameter normdelta_Vth = {} ;\n",
        "\n",
        "        real {}\n",
        "        real {}\n",
        "\n",
        "        real Vg, Vd, Vs, Vgs, Vds, Lg, Id, Cgg, Cgsd, Vgd;\n",
        "        real Vgsraw, Vgdraw, dir;\n",
        "        real t_stress, v_ov, t_rec, clk_loops, temp, delta_Vth;\n",
        "\n",
        "        real h1_0, h1_1, h1_2, h1_3, h1_4, h1_5, h1_6, h1_7, h1_8, h1_9, h1_10, h1_11, h1_12, h1_13, h1_14, h1_15, h1_16, h1_17, h1_18, h1_19, h1_20, h1_21, h1_22, h1_23, h1_24;\n",
        "        real h2_0, h2_1, h2_2, h2_3, h2_4, h2_5, h2_6, h2_7, h2_8, h2_9, h2_10, h2_11, h2_12, h2_13, h2_14, h2_15, h2_16, y;\n",
        "        real hc1_0, hc1_1, hc1_2, hc1_3, hc1_4, hc1_5, hc1_6, hc1_7, hc1_8, hc1_9;\n",
        "        real hc1_10, hc1_11, hc1_12, hc1_13, hc1_14, hc1_15, hc1_16;\n",
        "        real hc2_0, hc2_1, hc2_2, hc2_3, hc2_4, hc2_5, hc2_6, hc2_7, hc2_8, hc2_9;\n",
        "        real hc2_10, hc2_11, hc2_12, hc2_13, hc2_14, hc2_15, hc2_16, yc, yvth;\n",
        "\n",
        "analog begin\n",
        "\n",
        "t_stress = (T_stress - Mint_stress)*normt_stress;\n",
        "v_ov = (V_ov - Minv_ov)*normv_ov;\n",
        "t_rec = (T_rec - Mint_rec)*normt_rec ;\n",
        "clk_loops = (Clk_loops - Minclk_loops)*normclk_loops ;\n",
        "temp = (Temp - Mintemperature)*normtemperature ;\n",
        "\n",
        "//******************** delta_Vth NN **********************************//\n",
        "\n",
        "\"\"\".format(Mint_stress, normt_stress, Mint_rec, normt_rec, Minclk_loops, normclk_loops, MinV_ov, normV_ov, Mintemperature, normtemperature, Mindelta_Vth, normdelta_Vth, h1_declarations, h2_declarations)\n",
        "# V_ov = (V_ov - MinV_ov)*normV_ov ;\n",
        "# t_stress = (T_stress - Mint_stress)*normt_stress ;\n",
        "\n",
        "# Create the Verilog-A code for the 1st hidden layer\n",
        "for i in range(n1):\n",
        "    inputs = [\"t_stress\", \"t_rec\", \"clk_loops\", \"v_ov\", \"temp\"]\n",
        "    inputs = [\"*\".join([str(weights_1[i][j]), inp]) for j, inp in enumerate(inputs)]\n",
        "    inputs = \"+\".join(inputs)\n",
        "    inputs = \"+\".join([inputs, str(bias_1[i])])\n",
        "    verilog_code += \"hvth1_{} = tanh({});\\n\".format(i, inputs)\n",
        "verilog_code += \"\\n\"\n",
        "\n",
        "# Create the Verilog-A code for the 2nd hidden layer\n",
        "for i in range(n2):\n",
        "    inputs = [\"hvth1_{}\".format(j) for j in range(n1)]\n",
        "    inputs = [\"*\".join([str(weights_2[i][j]), inp]) for j, inp in enumerate(inputs)]\n",
        "    inputs = \"+\".join(inputs)\n",
        "    inputs = \"+\".join([inputs, str(bias_2[i])])\n",
        "    verilog_code += \"hvth2_{} = tanh({});\\n\".format(i, inputs)\n",
        "verilog_code += \"\\n\"\n",
        "\n",
        "# Create the Verilog-A code for the output layer\n",
        "inputs = [\"hvth2_{}\".format(i) for i in range(n2)]\n",
        "inputs = [\"*\".join([str(weights_3[0][i]), inp]) for i, inp in enumerate(inputs)]\n",
        "inputs = \"+\".join(inputs)\n",
        "inputs = \"+\".join([inputs, str(bias_3[0])])\n",
        "verilog_code += \"yvth = {};\\n\\n\".format(inputs)\n",
        "verilog_code += \"delta_Vth = (yvth - Mindelta_Vth) * normdelta_Vth;\\n\"\n",
        "verilog_code += \"\"\"$strobe(\"dvth=$g\",delta_Vth);\"\"\"\n",
        "verilog_code += \"\"\"\n",
        "\n",
        "\n",
        "        Vg = V(g) ;\n",
        "        Vs = V(s) ;\n",
        "        Vd = V(d) ;\n",
        "        Vgsraw = Vg-Vs ;\n",
        "        Vgdraw = Vg-Vd ;\n",
        "\n",
        "if (Vgsraw >= Vgdraw) begin\n",
        "        Vgs = ((Vg-Vs) - MinVg) * normVg ;\n",
        "        dir = 1;\n",
        "end\n",
        "\n",
        "else begin\n",
        "        Vgs = ((Vg-Vd) - MinVg) * normVg ;\n",
        "        dir = -1;\n",
        "end\n",
        "\n",
        "        Vds = (abs(Vd-Vs) - MinVd) * normVd ;\n",
        "        Vgs = Vgs - delta_Vth ; // BTI Vth variation\n",
        "        Lg = (L -MinLg)*normLg ;\n",
        "\n",
        "\n",
        "\n",
        "//******************** C-V NN **********************************//\n",
        "hc1_0 = tanh(-0.99871427*Vgs+-0.16952373*Vds+0.32118186*Lg+0.41485423);\n",
        "hc1_1 = tanh(0.31587568*Vgs+0.13397887*Vds+-0.4541538*Lg+-0.3630942);\n",
        "hc1_2 = tanh(-0.76281804*Vgs+0.09352969*Vds+1.1961353*Lg+0.3904977);\n",
        "hc1_3 = tanh(-1.115087*Vgs+0.85752946*Vds+-0.11746484*Lg+0.5500279);\n",
        "hc1_4 = tanh(1.0741386*Vgs+0.82041687*Vds+0.19092631*Lg+-0.4009425);\n",
        "hc1_5 = tanh(-0.47921795*Vgs+-0.8749933*Vds+-0.054768667*Lg+0.62785167);\n",
        "hc1_6 = tanh(0.5449184*Vgs+-4.409165*Vds+-0.072947875*Lg+-0.31324536);\n",
        "hc1_7 = tanh(-2.9224303*Vgs+2.7675478*Vds+0.08862238*Lg+0.6493558);\n",
        "hc1_8 = tanh(0.65050656*Vgs+-0.29751927*Vds+0.1571876*Lg+-0.38088372);\n",
        "hc1_9 = tanh(-0.30384183*Vgs+0.5649165*Vds+2.6806898*Lg+0.3197917);\n",
        "hc1_10 = tanh(-0.095988505*Vgs+2.0158541*Vds+-0.42972717*Lg+-0.30388466);\n",
        "hc1_11 = tanh(6.7699738*Vgs+-0.07234483*Vds+-0.013545353*Lg+-1.3694142);\n",
        "hc1_12 = tanh(-0.3404029*Vgs+0.0443459*Vds+0.89597*Lg+0.069993004);\n",
        "hc1_13 = tanh(0.62300175*Vgs+-0.29515797*Vds+1.6753465*Lg+-0.6520838);\n",
        "hc1_14 = tanh(0.37957156*Vgs+0.2237372*Vds+0.08591952*Lg+0.13126835);\n",
        "hc1_15 = tanh(0.19949242*Vgs+-0.26481664*Vds+-0.41059187*Lg+-0.40832308);\n",
        "hc1_16 = tanh(0.98966587*Vgs+-0.24259183*Vds+0.36584845*Lg+-0.8024042);\n",
        "\n",
        "hc2_0 = tanh(-0.91327864*hc1_0+0.4696781*hc1_1+0.3302202*hc1_2+0.11393868*hc1_3+0.45070222*hc1_4+-0.2894044*hc1_5+0.55066*hc1_6+-1.6242687*hc1_7+-0.38140613*hc1_8+0.032771554*hc1_9+0.17647126);\n",
        "hc2_1 = tanh(-1.1663305*hc1_0+-0.523984*hc1_1+-0.90804136*hc1_2+-0.7418044*hc1_3+1.456171*hc1_4+-0.16802542*hc1_5+0.8235596*hc1_6+-2.2246246*hc1_7+-0.40805355*hc1_8+0.7207601*hc1_9+0.4729169);\n",
        "hc2_2 = tanh(-0.69065374*hc1_0+0.40205315*hc1_1+-0.49410668*hc1_2+0.8681325*hc1_3+0.471351*hc1_4+0.46939445*hc1_5+0.45568785*hc1_6+-0.92935294*hc1_7+-0.8209646*hc1_8+0.1158967*hc1_9+-0.075798474);\n",
        "hc2_3 = tanh(0.11535446*hc1_0+-0.06296927*hc1_1+-0.6740435*hc1_2+0.7428315*hc1_3+0.05890677*hc1_4+0.9579441*hc1_5+-0.037319*hc1_6+-0.18491426*hc1_7+-0.02981994*hc1_8+0.038347963*hc1_9+0.039531134);\n",
        "hc2_4 = tanh(0.37817463*hc1_0+-0.6811279*hc1_1+1.1388369*hc1_2+0.19983096*hc1_3+-0.20415118*hc1_4+1.3022176*hc1_5+0.22571652*hc1_6+0.1690611*hc1_7+-0.56475276*hc1_8+-0.4069731*hc1_9+0.99962974);\n",
        "hc2_5 = tanh(0.032873478*hc1_0+-0.05209407*hc1_1+-0.010839908*hc1_2+-0.13892579*hc1_3+-0.050480977*hc1_4+0.0127089145*hc1_5+0.0052771433*hc1_6+0.02029242*hc1_7+-0.08705659*hc1_8+0.0254766*hc1_9+0.025135752);\n",
        "hc2_6 = tanh(-0.34639204*hc1_0+0.06937975*hc1_1+0.18671949*hc1_2+-0.18912783*hc1_3+0.1312504*hc1_4+0.4627272*hc1_5+-0.42590702*hc1_6+-0.10966313*hc1_7+0.66083515*hc1_8+-0.050718334*hc1_9+0.08234678);\n",
        "hc2_7 = tanh(0.90656275*hc1_0+-0.037281644*hc1_1+0.77237594*hc1_2+1.4710428*hc1_3+0.13597831*hc1_4+-0.059844542*hc1_5+-0.7801535*hc1_6+3.7814677*hc1_7+-0.5976644*hc1_8+0.2721995*hc1_9+0.023777716);\n",
        "hc2_8 = tanh(0.39720625*hc1_0+-0.45262313*hc1_1+0.19873238*hc1_2+0.9750888*hc1_3+-0.9427992*hc1_4+0.4487432*hc1_5+-0.3372945*hc1_6+0.33729544*hc1_7+-0.1667088*hc1_8+-0.5707525*hc1_9+0.27954483);\n",
        "hc2_9 = tanh(0.28551984*hc1_0+-0.68350387*hc1_1+0.9916423*hc1_2+-0.8254094*hc1_3+0.09875706*hc1_4+0.47609732*hc1_5+-0.058662917*hc1_6+0.09181381*hc1_7+0.09592329*hc1_8+1.3940467*hc1_9+0.3865768);\n",
        "hc2_10 = tanh(0.098737516*hc1_0+0.060473576*hc1_1+0.42824662*hc1_2+0.15018038*hc1_3+0.082621895*hc1_4+-0.00019039502*hc1_5+-0.3321634*hc1_6+0.7936295*hc1_7+-0.041197542*hc1_8+0.6530619*hc1_9+0.1338804);\n",
        "hc2_11 = tanh(-0.3585284*hc1_0+-0.09956017*hc1_1+0.17224246*hc1_2+-0.016925728*hc1_3+-0.46462816*hc1_4+-0.5649022*hc1_5+1.251695*hc1_6+-0.4303161*hc1_7+0.48546878*hc1_8+0.22958975*hc1_9+-0.27899802);\n",
        "hc2_12 = tanh(0.8565631*hc1_0+-0.7622999*hc1_1+0.32367912*hc1_2+1.4776785*hc1_3+0.2712369*hc1_4+0.2275511*hc1_5+0.39908803*hc1_6+4.2305493*hc1_7+-0.3467536*hc1_8+0.41231114*hc1_9+0.47123823);\n",
        "hc2_13 = tanh(-0.26411077*hc1_0+-0.17583853*hc1_1+0.045439184*hc1_2+-0.13801138*hc1_3+0.03278085*hc1_4+-0.45625108*hc1_5+-0.17905861*hc1_6+0.3060186*hc1_7+-0.3361926*hc1_8+-0.050055273*hc1_9+0.17996444);\n",
        "hc2_14 = tanh(0.016094366*hc1_0+-0.013196736*hc1_1+0.4124856*hc1_2+0.22926371*hc1_3+-0.35071182*hc1_4+-0.34217188*hc1_5+-0.69466996*hc1_6+1.0563152*hc1_7+-0.18019852*hc1_8+0.061871335*hc1_9+0.09762555);\n",
        "hc2_15 = tanh(-0.18970782*hc1_0+0.3624813*hc1_1+-1.3419824*hc1_2+0.103635244*hc1_3+-0.14595217*hc1_4+-0.1899393*hc1_5+0.176524*hc1_6+-0.4428012*hc1_7+-0.39544868*hc1_8+-0.45783517*hc1_9+0.1884755);\n",
        "hc2_16 = tanh(-0.1585831*hc1_0+0.035894837*hc1_1+-0.14261873*hc1_2+0.25914294*hc1_3+0.040607046*hc1_4+0.11555795*hc1_5+0.0022548323*hc1_6+-0.002149359*hc1_7+0.07067297*hc1_8+0.019578662*hc1_9+0.16657573);\n",
        "yc = 0.17503543*hc2_0+-0.15556754*hc2_1+-0.125455*hc2_2+-0.07502612*hc2_3+-0.16671115*hc2_4+0.29854503;\n",
        "\n",
        "Cgg = (yc / normO + MinO)*W;\n",
        "Cgsd = Cgg/2 ;\n",
        "\n",
        "//******************** I-V NN **********************************//\n",
        "h1_0 = tanh(0.0023826673*Vgs+-0.0009636134*Vds+0.006639037*Lg+-0.0004692053);\n",
        "h1_1 = tanh(-7.8007555*Vgs+2.639791*Vds+-0.025273597*Lg+-1.1747514);\n",
        "h1_2 = tanh(1.9884652*Vgs+0.62111*Vds+-0.11525345*Lg+-0.14575638);\n",
        "h1_3 = tanh(0.10625471*Vgs+0.09442053*Vds+-0.052119628*Lg+1.1568379);\n",
        "h1_4 = tanh(4.058087*Vgs+-0.7568158*Vds+0.008070099*Lg+-0.4820452);\n",
        "h1_5 = tanh(-1.3065948*Vgs+-0.0492497*Vds+-0.081622526*Lg+0.20351954);\n",
        "h1_6 = tanh(-2.9507525*Vgs+-0.91150546*Vds+-0.06477873*Lg+0.09646383);\n",
        "h1_7 = tanh(-0.5886177*Vgs+0.63788587*Vds+-0.13710928*Lg+0.30260038);\n",
        "h1_8 = tanh(-0.4311161*Vgs+-0.7250705*Vds+-0.06134645*Lg+0.44054285);\n",
        "h1_9 = tanh(0.012132638*Vgs+-0.42545322*Vds+-0.66478956*Lg+0.13182367);\n",
        "h1_10 = tanh(3.289041e-05*Vgs+0.0011367714*Vds+-0.0051904405*Lg+5.4112927e-05);\n",
        "h1_11 = tanh(-0.6007774*Vgs+0.09259224*Vds+0.2170182*Lg+-0.2908748);\n",
        "h1_12 = tanh(0.28018302*Vgs+1.3014064*Vds+-0.13490067*Lg+-0.22006753);\n",
        "h1_13 = tanh(0.01864017*Vgs+-13.0928755*Vds+-0.0037254083*Lg+-0.10935691);\n",
        "h1_14 = tanh(-0.0049708197*Vgs+0.0022613218*Vds+-0.014408149*Lg+0.0011340647);\n",
        "h1_15 = tanh(-0.094654046*Vgs+-0.4174114*Vds+0.18892045*Lg+0.05248958);\n",
        "h1_16 = tanh(-0.25090316*Vgs+-0.11111481*Vds+-0.009133225*Lg+0.08377026);\n",
        "h1_17 = tanh(0.9275306*Vgs+0.40686756*Vds+0.14127344*Lg+-0.059246097);\n",
        "h1_18 = tanh(-0.27084363*Vgs+0.07915911*Vds+-10.836302*Lg+-1.1535788);\n",
        "h1_19 = tanh(1.6852072*Vgs+-0.24846223*Vds+0.049734037*Lg+-0.19625053);\n",
        "\n",
        "h2_0 = tanh(1.1139417*h1_0+-0.40033522*h1_1+0.920759*h1_2+0.47607598*h1_3+0.3978683*h1_4+0.8008105*h1_5+-0.40445566*h1_6+0.8668027*h1_7+0.23365597*h1_8+-0.28254375*h1_9+-0.63985884*h1_10+-0.62523574*h1_11+0.8338383*h1_12+-2.0540943*h1_13+1.0749483*h1_14+-1.1075479*h1_15+0.60926706*h1_16+1.1118286*h1_17+-0.8917559*h1_18+0.029025732*h1_19+0.6622382);\n",
        "h2_1 = tanh(0.37195024*h1_0+-0.761445*h1_1+0.6514153*h1_2+0.6211995*h1_3+-0.063539445*h1_4+0.31260127*h1_5+-0.3529579*h1_6+0.50422627*h1_7+0.18943883*h1_8+-0.38029787*h1_9+-0.3464503*h1_10+-0.48301366*h1_11+0.081978925*h1_12+-0.08305682*h1_13+-0.08420117*h1_14+-0.8029313*h1_15+-0.37052512*h1_16+0.4553502*h1_17+-0.71959645*h1_18+-0.17320661*h1_19+0.6566257);\n",
        "h2_2 = tanh(-0.021933522*h1_0+-0.017245224*h1_1+0.030417712*h1_2+0.2967218*h1_3+-0.048668377*h1_4+0.029245213*h1_5+0.011756416*h1_6+0.004705658*h1_7+-0.042515088*h1_8+0.010462476*h1_9+0.001331279*h1_10+0.1694541*h1_11+-0.010949079*h1_12+-0.004144526*h1_13+0.04856694*h1_14+-0.010465159*h1_15+0.24588406*h1_16+-0.028521152*h1_17+0.12161568*h1_18+0.1753255*h1_19+-0.09125355);\n",
        "h2_3 = tanh(-0.025546636*h1_0+-3.2702503*h1_1+0.46812135*h1_2+-0.25135994*h1_3+3.0725436*h1_4+0.22513995*h1_5+-1.0327209*h1_6+-0.56274736*h1_7+0.4726107*h1_8+0.38182434*h1_9+0.016403453*h1_10+-0.09128962*h1_11+0.20997792*h1_12+-0.4951615*h1_13+0.026481293*h1_14+0.6085288*h1_15+-0.09078652*h1_16+0.36613584*h1_17+0.26257026*h1_18+0.39794916*h1_19+-0.52920526);\n",
        "h2_4 = tanh(-0.44392154*h1_0+-0.5650475*h1_1+0.91716766*h1_2+0.96703196*h1_3+0.4597048*h1_4+1.452921*h1_5+-0.8115141*h1_6+0.92326456*h1_7+0.2862403*h1_8+-0.9441585*h1_9+0.22188367*h1_10+-1.0092766*h1_11+0.5495966*h1_12+0.44870532*h1_13+0.48705962*h1_14+-0.6957133*h1_15+0.27711377*h1_16+1.0138507*h1_17+-0.34337458*h1_18+-0.21548931*h1_19+0.42685974);\n",
        "h2_5 = tanh(0.0003710325*h1_0+0.5746112*h1_1+0.4144258*h1_2+0.04459103*h1_3+1.2373503*h1_4+0.12786175*h1_5+-0.16099685*h1_6+0.9826207*h1_7+-0.09701193*h1_8+-0.4379135*h1_9+-0.0035542254*h1_10+0.04205593*h1_11+0.65820116*h1_12+0.3810506*h1_13+0.0004259507*h1_14+-0.21782044*h1_15+-0.057544652*h1_16+0.24420041*h1_17+-0.10138292*h1_18+-0.2387106*h1_19+0.867847);\n",
        "h2_6 = tanh(-0.0077049686*h1_0+-0.05349668*h1_1+-0.11536639*h1_2+0.06141029*h1_3+-0.034344573*h1_4+0.21672213*h1_5+-0.09835135*h1_6+-0.25849992*h1_7+0.10576329*h1_8+-0.14288934*h1_9+0.027846925*h1_10+-0.20104973*h1_11+0.24784875*h1_12+0.03396087*h1_13+-0.016039118*h1_14+-0.03147038*h1_15+-0.023109786*h1_16+-0.118931994*h1_17+0.18256636*h1_18+0.09981429*h1_19+0.079372324);\n",
        "h2_7 = tanh(-0.009369999*h1_0+-2.4468672*h1_1+-0.41433397*h1_2+-0.6289744*h1_3+-1.8285819*h1_4+0.12905455*h1_5+0.83588725*h1_6+0.11491322*h1_7+0.3871084*h1_8+0.07153052*h1_9+0.013530584*h1_10+0.40614852*h1_11+0.10340061*h1_12+-0.04473306*h1_13+0.0075287875*h1_14+0.5593612*h1_15+0.34401885*h1_16+-0.5538749*h1_17+0.36615464*h1_18+-0.2666981*h1_19+0.20194086);\n",
        "h2_8 = tanh(-0.0017045025*h1_0+-1.1388719*h1_1+0.25932005*h1_2+-0.6482845*h1_3+2.263395*h1_4+0.115192235*h1_5+-0.40946937*h1_6+-0.038117886*h1_7+-0.03703431*h1_8+0.05965774*h1_9+0.0032721795*h1_10+0.2561857*h1_11+0.47324425*h1_12+-0.25200802*h1_13+0.0017081021*h1_14+0.29819545*h1_15+0.061945435*h1_16+0.13516657*h1_17+0.5409335*h1_18+-0.17400871*h1_19+0.11659722);\n",
        "h2_9 = tanh(0.014866875*h1_0+1.0399909*h1_1+1.0108095*h1_2+-0.0918755*h1_3+-0.7634763*h1_4+-0.49749368*h1_5+-0.7020006*h1_6+-0.16384888*h1_7+-0.07833025*h1_8+-0.23745225*h1_9+-0.023095092*h1_10+-0.29244414*h1_11+0.6255917*h1_12+0.41413808*h1_13+-0.019823061*h1_14+-1.0593235*h1_15+-0.42221433*h1_16+0.98025715*h1_17+-0.71444243*h1_18+0.84347373*h1_19+0.28510216);\n",
        "h2_10 = tanh(-0.013515852*h1_0+0.68840384*h1_1+-0.030184174*h1_2+-0.48098114*h1_3+0.08609854*h1_4+-0.50878614*h1_5+0.26547763*h1_6+-0.7151076*h1_7+0.111288495*h1_8+0.53379977*h1_9+0.022258151*h1_10+0.16971351*h1_11+-1.2486296*h1_12+0.9649942*h1_13+0.0028906881*h1_14+0.31582054*h1_15+0.24213083*h1_16+-0.27242163*h1_17+0.11330636*h1_18+0.17038865*h1_19+-0.42089102);\n",
        "h2_11 = tanh(-0.009949424*h1_0+-0.6704206*h1_1+0.13177924*h1_2+0.07316155*h1_3+-0.28207502*h1_4+-0.32401362*h1_5+0.020746209*h1_6+-0.05517531*h1_7+0.10340257*h1_8+0.29381263*h1_9+0.020497978*h1_10+0.031984854*h1_11+0.079552434*h1_12+-7.7884445*h1_13+0.01224806*h1_14+0.8700812*h1_15+-0.16562358*h1_16+0.14370379*h1_17+0.2389181*h1_18+-0.19771136*h1_19+0.33039534);\n",
        "h2_12 = tanh(-0.002431529*h1_0+0.30154988*h1_1+-0.45072728*h1_2+0.27823612*h1_3+-0.25173753*h1_4+-0.58188903*h1_5+0.39836177*h1_6+-0.060329597*h1_7+0.1667837*h1_8+-0.09419194*h1_9+0.0108116735*h1_10+-0.3581154*h1_11+0.15690842*h1_12+-0.41209573*h1_13+-0.0070331707*h1_14+-0.23976392*h1_15+0.16620094*h1_16+-0.034617994*h1_17+0.32754526*h1_18+0.8490298*h1_19+0.026407415);\n",
        "h2_13 = tanh(-0.7385622*h1_0+-0.25174448*h1_1+2.090295*h1_2+0.67499936*h1_3+0.07392262*h1_4+0.5413692*h1_5+-0.8287433*h1_6+2.2810504*h1_7+1.1917778*h1_8+1.3719273*h1_9+0.25762329*h1_10+-2.1111712*h1_11+1.6987114*h1_12+-0.437825*h1_13+1.7649944*h1_14+-2.2254016*h1_15+-2.3555171*h1_16+-0.7732424*h1_17+-1.0756916*h1_18+-0.1367373*h1_19+1.5085722);\n",
        "h2_14 = tanh(-0.002535408*h1_0+-4.2686224*h1_1+-0.045537975*h1_2+0.08043166*h1_3+1.7274952*h1_4+-0.1260494*h1_5+-0.23011239*h1_6+-0.21025337*h1_7+0.48537356*h1_8+0.39049447*h1_9+-0.006565428*h1_10+-0.293028*h1_11+-1.1253971*h1_12+-0.2890831*h1_13+0.0042421576*h1_14+0.4230598*h1_15+-0.17869289*h1_16+-0.034056116*h1_17+0.61135995*h1_18+0.6345532*h1_19+0.73144835);\n",
        "h2_15 = tanh(-0.5239093*h1_0+0.49806875*h1_1+0.53555894*h1_2+1.2645539*h1_3+1.011548*h1_4+0.39209503*h1_5+-0.78194916*h1_6+0.94610345*h1_7+0.9121405*h1_8+-1.4693716*h1_9+-0.58336824*h1_10+-1.1661471*h1_11+-0.16158457*h1_12+0.058512915*h1_13+1.3358645*h1_14+0.5742125*h1_15+-1.3325212*h1_16+1.1804186*h1_17+-1.4877121*h1_18+0.6357802*h1_19+1.1328585);\n",
        "h2_16 = tanh(0.0095406*h1_0+-0.53232694*h1_1+0.1535685*h1_2+0.26108417*h1_3+-0.08375187*h1_4+0.3462123*h1_5+-0.61755395*h1_6+0.0015145333*h1_7+-0.108780764*h1_8+0.2555477*h1_9+0.00023940884*h1_10+-0.26577523*h1_11+-0.27689674*h1_12+0.43049082*h1_13+-0.025424613*h1_14+0.24634649*h1_15+-0.22579487*h1_16+0.37249807*h1_17+-0.8058662*h1_18+-0.77451354*h1_19+0.5821276);\n",
        "y = -0.17525196*h2_0+-0.22995844*h2_1+0.0026147955*h2_2+0.091129646*h2_3+-0.4534783*h2_4+0.4119419*h2_5+-1.4205361e-05*h2_6+-0.11359831*h2_7+0.1762025*h2_8+-0.1002102*h2_9+-0.37161925*h2_10+0.2683793*h2_11+0.051897053*h2_12+0.19354156*h2_13+-0.10134394*h2_14+0.3937854*h2_15+-0.3296008*h2_16+0.3220947;\n",
        "\n",
        "        Id = pow(10, (y/normI + MinI)) ;\n",
        "\n",
        "if (Id <= 1e-15) begin //limit\n",
        "        Id = 1e-15;\n",
        "        //Id = Id;\n",
        "end\n",
        "else begin\n",
        "        Id = Id;\n",
        "end  //limit end\n",
        "\n",
        "\n",
        "        I(g, d) <+ Cgsd*ddt(Vg-Vd) ;\n",
        "        I(g, s) <+ Cgsd*ddt(Vg-Vs) ;\n",
        "\n",
        "if (Vgsraw >= Vgdraw) begin\n",
        "        I(d, s) <+ dir*Id*W ;\n",
        "\n",
        "end\n",
        "\n",
        "else begin\n",
        "        I(d, s) <+ dir*Id*W ;\n",
        "\n",
        "end\n",
        "\n",
        "end\n",
        "\n",
        "endmodule\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "print(verilog_code)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 576
        },
        "id": "jlLiTNqU9gKw",
        "outputId": "b37692fd-6518-4c80-a194-46c8339ad64e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.10036881607686\n",
            "0.04321379542061602\n",
            "(29, 1, 1)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted delta_V: 0.4370737671852112V\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-26-0ddd2f296d2b>:19: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  print(\"Predicted delta_V: {}V\".format(float(pred_y)))\n"
          ]
        }
      ],
      "source": [
        "output1 = np.array(output)/normdelta_Vth + Mindelta_Vth\n",
        "ytest1 = np.array(y_test)/normdelta_Vth + Mindelta_Vth\n",
        "print(normdelta_Vth)\n",
        "print(Mindelta_Vth)\n",
        "print(np.shape(output1))\n",
        "\n",
        "plt.scatter(output1, ytest1)\n",
        "a = [min(ytest1), max(ytest1)]\n",
        "b = [min(ytest1), max(ytest1)]\n",
        "plt.plot(a,b,'k')\n",
        "plt.scatter(ytest1,output1)\n",
        "plt.xlabel(\"TCAD [A/um]\")\n",
        "plt.ylabel(\"ML-Prediction [A/um]\")\n",
        "plt.show()\n",
        "\n",
        "tp = [0.0001, 0.0001, 1000, 1.7, 25] # [t_st, t_rec, clk_loops, V_ov, temperature]\n",
        "new_var = torch.FloatTensor([(tp[0]-Mint_stress)*normt_stress, (tp[1]-Mint_rec)*normt_rec, (tp[2]-Minclk_loops)*normclk_loops, (tp[3]-MinV_ov)*normV_ov, (tp[4] - Mintemperature)* normtemperature])\n",
        "pred_y=model(new_var).data.numpy()\n",
        "print(\"Predicted delta_V: {}V\".format(float(pred_y)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mCmumHX7By8e"
      },
      "source": [
        "    \n",
        "    ===========================================\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Layer 1**"
      ],
      "metadata": {
        "id": "_svAlxE1RRYo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from re import L\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import random\n",
        "\n",
        "from torch import optim\n",
        "from torch.utils import data\n",
        "from torch.optim import lr_scheduler\n",
        "from torchvision import datasets, models, transforms\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import statistics\n",
        "import datetime\n",
        "import os\n",
        "import csv\n",
        "import math\n",
        "import time\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_tensor = torch.tensor(X, dtype=torch.float64)\n",
        "Y_tensor = torch.tensor(Y, dtype=torch.float64)\n",
        "x_train, x_test, y_train, y_test = train_test_split(X_tensor, Y_tensor, test_size=0.1, random_state=41)\n",
        "dataset = TensorDataset(x_train, y_train)\n",
        "dataloader = DataLoader(dataset, batch_size = 32)\n",
        "testdataloader = DataLoader(TensorDataset(x_test, y_test))\n",
        "\n",
        "n1 = 20\n",
        "#n2 = 10\n",
        "\n",
        "# Define the neural network class\n",
        "class MLP(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MLP, self).__init__()\n",
        "        self.fc1 = torch.nn.Linear(5, n1)\n",
        "        self.fc2 = torch.nn.Linear(n1, n2)\n",
        "        self.fc3 = torch.nn.Linear(n2, 1)\n",
        "        self.fc4 = torch.nn.Linear(n1, 1)\n",
        "        self.dropout = torch.nn.Dropout(0.1)\n",
        "        self.tanh = torch.nn.Tanh()\n",
        "        self.bn1 = torch.nn.BatchNorm1d(n1)\n",
        "        self.bn2 = torch.nn.BatchNorm1d(n2)\n",
        "        self.bn3 = torch.nn.BatchNorm1d(1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        #x = self.bn1(x)\n",
        "        x = self.tanh(x)\n",
        "        #x = self.dropout(x)\n",
        "        #x = self.fc2(x)\n",
        "        #x = self.bn2(x)\n",
        "        #x = self.tanh(x)\n",
        "        #x = self.dropout(x)\n",
        "        #x = self.fc3(x)\n",
        "        #x = self.bn3(x)\n",
        "        x = self.fc4(x)\n",
        "        return x\n",
        "\n",
        "# Create an instance of the MLP class\n",
        "model = MLP()\n",
        "\n",
        "def initialize_weights(m):\n",
        "    if isinstance(m, torch.nn.Linear):\n",
        "        torch.nn.init.xavier_uniform_(m.weight)\n",
        "        if m.bias is not None:\n",
        "            torch.nn.init.constant_(m.bias, 0)\n",
        "\n",
        "model.apply(initialize_weights)\n",
        "\n",
        "loss_function = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5)\n",
        "#torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "nb_epochs = 1000\n",
        "MLoss = []\n",
        "for epoch in range(0, nb_epochs):\n",
        "\n",
        "    current_loss = 0.0\n",
        "    losses = []\n",
        "    # Iterate over the dataloader for training data\n",
        "    for i, data in enumerate(dataloader, 0):\n",
        "        inputs, targets = data\n",
        "        inputs, targets = inputs.float(), targets.float()\n",
        "        targets = targets.reshape((targets.shape[0],1))\n",
        "        #zero the gradients\n",
        "        optimizer.zero_grad()\n",
        "        #perform forward pass\n",
        "        outputs = model(inputs)\n",
        "        L_weight = 3\n",
        "        #compute loss\n",
        "        batch_loss = []\n",
        "        for j in range(inputs.size(0)):\n",
        "            input_j = inputs[j].reshape((1, inputs.shape[1]))\n",
        "            if input_j[0,0]>0.3:\n",
        "                batch_loss.append(L_weight*loss_function(outputs[j], targets[j]))\n",
        "            else:\n",
        "                batch_loss.append(loss_function(outputs[j], targets[j]))\n",
        "        loss = torch.stack(batch_loss).mean()\n",
        "        losses.append(loss.item())\n",
        "        #perform backward pass\n",
        "        loss.backward()\n",
        "        #perform optimization\n",
        "        optimizer.step()\n",
        "        # Print statistics\n",
        "\n",
        "    mean_loss = sum(losses)/len(losses)\n",
        "    scheduler.step(mean_loss)\n",
        "\n",
        "    print('Loss (epoch: %4d): %.8f' %(epoch+1, mean_loss))\n",
        "    current_loss = 0.0\n",
        "    MLoss.append(mean_loss)\n",
        "\n",
        "# Process is complete.\n",
        "#print('Training process has finished.')\n",
        "\n",
        "torch.save(model, 'IWO_idvg.pt')\n",
        "torch.save(model.state_dict(), 'IWO_idvg_state_dict.pt')\n",
        "\n",
        "####### loss vs. epoch #######\n",
        "xloss = list(range(0, nb_epochs))\n",
        "plt.plot(xloss, np.log10(MLoss))\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "_UsVAZBFRUtV",
        "outputId": "45e5e7b9-cc28-4417-f825-40ce7f599caf"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss (epoch:    1): 1.20045621\n",
            "Loss (epoch:    2): 1.15505596\n",
            "Loss (epoch:    3): 1.11135498\n",
            "Loss (epoch:    4): 1.06911688\n",
            "Loss (epoch:    5): 1.02834294\n",
            "Loss (epoch:    6): 0.98902388\n",
            "Loss (epoch:    7): 0.95113640\n",
            "Loss (epoch:    8): 0.91464892\n",
            "Loss (epoch:    9): 0.87952644\n",
            "Loss (epoch:   10): 0.84573260\n",
            "Loss (epoch:   11): 0.81323153\n",
            "Loss (epoch:   12): 0.78198754\n",
            "Loss (epoch:   13): 0.75196593\n",
            "Loss (epoch:   14): 0.72313262\n",
            "Loss (epoch:   15): 0.69545401\n",
            "Loss (epoch:   16): 0.66889699\n",
            "Loss (epoch:   17): 0.64342882\n",
            "Loss (epoch:   18): 0.61901712\n",
            "Loss (epoch:   19): 0.59562984\n",
            "Loss (epoch:   20): 0.57323520\n",
            "Loss (epoch:   21): 0.55180176\n",
            "Loss (epoch:   22): 0.53129842\n",
            "Loss (epoch:   23): 0.51169426\n",
            "Loss (epoch:   24): 0.49295860\n",
            "Loss (epoch:   25): 0.47506112\n",
            "Loss (epoch:   26): 0.45797173\n",
            "Loss (epoch:   27): 0.44166082\n",
            "Loss (epoch:   28): 0.42609887\n",
            "Loss (epoch:   29): 0.41125691\n",
            "Loss (epoch:   30): 0.39710635\n",
            "Loss (epoch:   31): 0.38361906\n",
            "Loss (epoch:   32): 0.37076722\n",
            "Loss (epoch:   33): 0.35852364\n",
            "Loss (epoch:   34): 0.34686167\n",
            "Loss (epoch:   35): 0.33575511\n",
            "Loss (epoch:   36): 0.32517836\n",
            "Loss (epoch:   37): 0.31510637\n",
            "Loss (epoch:   38): 0.30551489\n",
            "Loss (epoch:   39): 0.29638031\n",
            "Loss (epoch:   40): 0.28767964\n",
            "Loss (epoch:   41): 0.27939065\n",
            "Loss (epoch:   42): 0.27149173\n",
            "Loss (epoch:   43): 0.26396220\n",
            "Loss (epoch:   44): 0.25678220\n",
            "Loss (epoch:   45): 0.24993244\n",
            "Loss (epoch:   46): 0.24339451\n",
            "Loss (epoch:   47): 0.23715082\n",
            "Loss (epoch:   48): 0.23118449\n",
            "Loss (epoch:   49): 0.22547951\n",
            "Loss (epoch:   50): 0.22002056\n",
            "Loss (epoch:   51): 0.21479322\n",
            "Loss (epoch:   52): 0.20978373\n",
            "Loss (epoch:   53): 0.20497906\n",
            "Loss (epoch:   54): 0.20036697\n",
            "Loss (epoch:   55): 0.19593586\n",
            "Loss (epoch:   56): 0.19167491\n",
            "Loss (epoch:   57): 0.18757383\n",
            "Loss (epoch:   58): 0.18362302\n",
            "Loss (epoch:   59): 0.17981353\n",
            "Loss (epoch:   60): 0.17613691\n",
            "Loss (epoch:   61): 0.17258538\n",
            "Loss (epoch:   62): 0.16915160\n",
            "Loss (epoch:   63): 0.16582879\n",
            "Loss (epoch:   64): 0.16261049\n",
            "Loss (epoch:   65): 0.15949086\n",
            "Loss (epoch:   66): 0.15646446\n",
            "Loss (epoch:   67): 0.15352622\n",
            "Loss (epoch:   68): 0.15067139\n",
            "Loss (epoch:   69): 0.14789557\n",
            "Loss (epoch:   70): 0.14519474\n",
            "Loss (epoch:   71): 0.14256521\n",
            "Loss (epoch:   72): 0.14000343\n",
            "Loss (epoch:   73): 0.13750627\n",
            "Loss (epoch:   74): 0.13507073\n",
            "Loss (epoch:   75): 0.13269406\n",
            "Loss (epoch:   76): 0.13037375\n",
            "Loss (epoch:   77): 0.12810742\n",
            "Loss (epoch:   78): 0.12589294\n",
            "Loss (epoch:   79): 0.12372829\n",
            "Loss (epoch:   80): 0.12161157\n",
            "Loss (epoch:   81): 0.11954108\n",
            "Loss (epoch:   82): 0.11751516\n",
            "Loss (epoch:   83): 0.11553242\n",
            "Loss (epoch:   84): 0.11359141\n",
            "Loss (epoch:   85): 0.11169082\n",
            "Loss (epoch:   86): 0.10982949\n",
            "Loss (epoch:   87): 0.10800628\n",
            "Loss (epoch:   88): 0.10622014\n",
            "Loss (epoch:   89): 0.10447008\n",
            "Loss (epoch:   90): 0.10275520\n",
            "Loss (epoch:   91): 0.10107462\n",
            "Loss (epoch:   92): 0.09942750\n",
            "Loss (epoch:   93): 0.09781309\n",
            "Loss (epoch:   94): 0.09623062\n",
            "Loss (epoch:   95): 0.09467944\n",
            "Loss (epoch:   96): 0.09315886\n",
            "Loss (epoch:   97): 0.09166827\n",
            "Loss (epoch:   98): 0.09020703\n",
            "Loss (epoch:   99): 0.08877462\n",
            "Loss (epoch:  100): 0.08737046\n",
            "Loss (epoch:  101): 0.08599402\n",
            "Loss (epoch:  102): 0.08464480\n",
            "Loss (epoch:  103): 0.08332229\n",
            "Loss (epoch:  104): 0.08202605\n",
            "Loss (epoch:  105): 0.08075561\n",
            "Loss (epoch:  106): 0.07951049\n",
            "Loss (epoch:  107): 0.07829029\n",
            "Loss (epoch:  108): 0.07709459\n",
            "Loss (epoch:  109): 0.07592295\n",
            "Loss (epoch:  110): 0.07477499\n",
            "Loss (epoch:  111): 0.07365031\n",
            "Loss (epoch:  112): 0.07254854\n",
            "Loss (epoch:  113): 0.07146929\n",
            "Loss (epoch:  114): 0.07041217\n",
            "Loss (epoch:  115): 0.06937687\n",
            "Loss (epoch:  116): 0.06836301\n",
            "Loss (epoch:  117): 0.06737024\n",
            "Loss (epoch:  118): 0.06639822\n",
            "Loss (epoch:  119): 0.06544660\n",
            "Loss (epoch:  120): 0.06451506\n",
            "Loss (epoch:  121): 0.06360326\n",
            "Loss (epoch:  122): 0.06271088\n",
            "Loss (epoch:  123): 0.06183761\n",
            "Loss (epoch:  124): 0.06098311\n",
            "Loss (epoch:  125): 0.06014709\n",
            "Loss (epoch:  126): 0.05932923\n",
            "Loss (epoch:  127): 0.05852922\n",
            "Loss (epoch:  128): 0.05774678\n",
            "Loss (epoch:  129): 0.05698160\n",
            "Loss (epoch:  130): 0.05623340\n",
            "Loss (epoch:  131): 0.05550189\n",
            "Loss (epoch:  132): 0.05478675\n",
            "Loss (epoch:  133): 0.05408773\n",
            "Loss (epoch:  134): 0.05340454\n",
            "Loss (epoch:  135): 0.05273690\n",
            "Loss (epoch:  136): 0.05208454\n",
            "Loss (epoch:  137): 0.05144719\n",
            "Loss (epoch:  138): 0.05082459\n",
            "Loss (epoch:  139): 0.05021646\n",
            "Loss (epoch:  140): 0.04962255\n",
            "Loss (epoch:  141): 0.04904259\n",
            "Loss (epoch:  142): 0.04847633\n",
            "Loss (epoch:  143): 0.04792352\n",
            "Loss (epoch:  144): 0.04738389\n",
            "Loss (epoch:  145): 0.04685723\n",
            "Loss (epoch:  146): 0.04634326\n",
            "Loss (epoch:  147): 0.04584178\n",
            "Loss (epoch:  148): 0.04535252\n",
            "Loss (epoch:  149): 0.04487524\n",
            "Loss (epoch:  150): 0.04440973\n",
            "Loss (epoch:  151): 0.04395575\n",
            "Loss (epoch:  152): 0.04351309\n",
            "Loss (epoch:  153): 0.04308149\n",
            "Loss (epoch:  154): 0.04266077\n",
            "Loss (epoch:  155): 0.04225070\n",
            "Loss (epoch:  156): 0.04185105\n",
            "Loss (epoch:  157): 0.04146161\n",
            "Loss (epoch:  158): 0.04108220\n",
            "Loss (epoch:  159): 0.04071256\n",
            "Loss (epoch:  160): 0.04035253\n",
            "Loss (epoch:  161): 0.04000190\n",
            "Loss (epoch:  162): 0.03966046\n",
            "Loss (epoch:  163): 0.03932803\n",
            "Loss (epoch:  164): 0.03900441\n",
            "Loss (epoch:  165): 0.03868942\n",
            "Loss (epoch:  166): 0.03838286\n",
            "Loss (epoch:  167): 0.03808454\n",
            "Loss (epoch:  168): 0.03779429\n",
            "Loss (epoch:  169): 0.03751194\n",
            "Loss (epoch:  170): 0.03723729\n",
            "Loss (epoch:  171): 0.03697019\n",
            "Loss (epoch:  172): 0.03671047\n",
            "Loss (epoch:  173): 0.03645795\n",
            "Loss (epoch:  174): 0.03621246\n",
            "Loss (epoch:  175): 0.03597385\n",
            "Loss (epoch:  176): 0.03574196\n",
            "Loss (epoch:  177): 0.03551663\n",
            "Loss (epoch:  178): 0.03529770\n",
            "Loss (epoch:  179): 0.03508503\n",
            "Loss (epoch:  180): 0.03487846\n",
            "Loss (epoch:  181): 0.03467785\n",
            "Loss (epoch:  182): 0.03448305\n",
            "Loss (epoch:  183): 0.03429393\n",
            "Loss (epoch:  184): 0.03411034\n",
            "Loss (epoch:  185): 0.03393216\n",
            "Loss (epoch:  186): 0.03375923\n",
            "Loss (epoch:  187): 0.03359143\n",
            "Loss (epoch:  188): 0.03342864\n",
            "Loss (epoch:  189): 0.03327073\n",
            "Loss (epoch:  190): 0.03311757\n",
            "Loss (epoch:  191): 0.03296904\n",
            "Loss (epoch:  192): 0.03282503\n",
            "Loss (epoch:  193): 0.03268541\n",
            "Loss (epoch:  194): 0.03255007\n",
            "Loss (epoch:  195): 0.03241889\n",
            "Loss (epoch:  196): 0.03229177\n",
            "Loss (epoch:  197): 0.03216860\n",
            "Loss (epoch:  198): 0.03204928\n",
            "Loss (epoch:  199): 0.03193369\n",
            "Loss (epoch:  200): 0.03182174\n",
            "Loss (epoch:  201): 0.03171334\n",
            "Loss (epoch:  202): 0.03160837\n",
            "Loss (epoch:  203): 0.03150675\n",
            "Loss (epoch:  204): 0.03140839\n",
            "Loss (epoch:  205): 0.03131319\n",
            "Loss (epoch:  206): 0.03122107\n",
            "Loss (epoch:  207): 0.03113193\n",
            "Loss (epoch:  208): 0.03104569\n",
            "Loss (epoch:  209): 0.03096227\n",
            "Loss (epoch:  210): 0.03088159\n",
            "Loss (epoch:  211): 0.03080357\n",
            "Loss (epoch:  212): 0.03072814\n",
            "Loss (epoch:  213): 0.03065520\n",
            "Loss (epoch:  214): 0.03058470\n",
            "Loss (epoch:  215): 0.03051656\n",
            "Loss (epoch:  216): 0.03045070\n",
            "Loss (epoch:  217): 0.03038707\n",
            "Loss (epoch:  218): 0.03032559\n",
            "Loss (epoch:  219): 0.03026619\n",
            "Loss (epoch:  220): 0.03020882\n",
            "Loss (epoch:  221): 0.03015341\n",
            "Loss (epoch:  222): 0.03009990\n",
            "Loss (epoch:  223): 0.03004824\n",
            "Loss (epoch:  224): 0.02999836\n",
            "Loss (epoch:  225): 0.02995021\n",
            "Loss (epoch:  226): 0.02990373\n",
            "Loss (epoch:  227): 0.02985887\n",
            "Loss (epoch:  228): 0.02981558\n",
            "Loss (epoch:  229): 0.02977382\n",
            "Loss (epoch:  230): 0.02973351\n",
            "Loss (epoch:  231): 0.02969464\n",
            "Loss (epoch:  232): 0.02965713\n",
            "Loss (epoch:  233): 0.02962095\n",
            "Loss (epoch:  234): 0.02958607\n",
            "Loss (epoch:  235): 0.02955243\n",
            "Loss (epoch:  236): 0.02951999\n",
            "Loss (epoch:  237): 0.02948870\n",
            "Loss (epoch:  238): 0.02945854\n",
            "Loss (epoch:  239): 0.02942947\n",
            "Loss (epoch:  240): 0.02940145\n",
            "Loss (epoch:  241): 0.02937443\n",
            "Loss (epoch:  242): 0.02934839\n",
            "Loss (epoch:  243): 0.02932329\n",
            "Loss (epoch:  244): 0.02929910\n",
            "Loss (epoch:  245): 0.02927579\n",
            "Loss (epoch:  246): 0.02925334\n",
            "Loss (epoch:  247): 0.02923169\n",
            "Loss (epoch:  248): 0.02921083\n",
            "Loss (epoch:  249): 0.02919073\n",
            "Loss (epoch:  250): 0.02917136\n",
            "Loss (epoch:  251): 0.02915270\n",
            "Loss (epoch:  252): 0.02913472\n",
            "Loss (epoch:  253): 0.02911740\n",
            "Loss (epoch:  254): 0.02910070\n",
            "Loss (epoch:  255): 0.02908461\n",
            "Loss (epoch:  256): 0.02906911\n",
            "Loss (epoch:  257): 0.02905417\n",
            "Loss (epoch:  258): 0.02903977\n",
            "Loss (epoch:  259): 0.02902589\n",
            "Loss (epoch:  260): 0.02901252\n",
            "Loss (epoch:  261): 0.02899963\n",
            "Loss (epoch:  262): 0.02898720\n",
            "Loss (epoch:  263): 0.02897522\n",
            "Loss (epoch:  264): 0.02896366\n",
            "Loss (epoch:  265): 0.02895252\n",
            "Loss (epoch:  266): 0.02894178\n",
            "Loss (epoch:  267): 0.02893141\n",
            "Loss (epoch:  268): 0.02892141\n",
            "Loss (epoch:  269): 0.02891177\n",
            "Loss (epoch:  270): 0.02890245\n",
            "Loss (epoch:  271): 0.02889346\n",
            "Loss (epoch:  272): 0.02888479\n",
            "Loss (epoch:  273): 0.02887640\n",
            "Loss (epoch:  274): 0.02886831\n",
            "Loss (epoch:  275): 0.02886048\n",
            "Loss (epoch:  276): 0.02885292\n",
            "Loss (epoch:  277): 0.02884562\n",
            "Loss (epoch:  278): 0.02883855\n",
            "Loss (epoch:  279): 0.02883171\n",
            "Loss (epoch:  280): 0.02882510\n",
            "Loss (epoch:  281): 0.02881869\n",
            "Loss (epoch:  282): 0.02881249\n",
            "Loss (epoch:  283): 0.02880648\n",
            "Loss (epoch:  284): 0.02880066\n",
            "Loss (epoch:  285): 0.02879501\n",
            "Loss (epoch:  286): 0.02878954\n",
            "Loss (epoch:  287): 0.02878423\n",
            "Loss (epoch:  288): 0.02877907\n",
            "Loss (epoch:  289): 0.02877405\n",
            "Loss (epoch:  290): 0.02876918\n",
            "Loss (epoch:  291): 0.02876445\n",
            "Loss (epoch:  292): 0.02875985\n",
            "Loss (epoch:  293): 0.02875536\n",
            "Loss (epoch:  294): 0.02875099\n",
            "Loss (epoch:  295): 0.02874674\n",
            "Loss (epoch:  296): 0.02874259\n",
            "Loss (epoch:  297): 0.02873855\n",
            "Loss (epoch:  298): 0.02873460\n",
            "Loss (epoch:  299): 0.02873075\n",
            "Loss (epoch:  300): 0.02872698\n",
            "Loss (epoch:  301): 0.02872330\n",
            "Loss (epoch:  302): 0.02871969\n",
            "Loss (epoch:  303): 0.02871616\n",
            "Loss (epoch:  304): 0.02871271\n",
            "Loss (epoch:  305): 0.02870931\n",
            "Loss (epoch:  306): 0.02870599\n",
            "Loss (epoch:  307): 0.02870273\n",
            "Loss (epoch:  308): 0.02869952\n",
            "Loss (epoch:  309): 0.02869637\n",
            "Loss (epoch:  310): 0.02869328\n",
            "Loss (epoch:  311): 0.02869023\n",
            "Loss (epoch:  312): 0.02868723\n",
            "Loss (epoch:  313): 0.02868428\n",
            "Loss (epoch:  314): 0.02868137\n",
            "Loss (epoch:  315): 0.02867849\n",
            "Loss (epoch:  316): 0.02867566\n",
            "Loss (epoch:  317): 0.02867286\n",
            "Loss (epoch:  318): 0.02867009\n",
            "Loss (epoch:  319): 0.02866736\n",
            "Loss (epoch:  320): 0.02866465\n",
            "Loss (epoch:  321): 0.02866198\n",
            "Loss (epoch:  322): 0.02865933\n",
            "Loss (epoch:  323): 0.02865670\n",
            "Loss (epoch:  324): 0.02865410\n",
            "Loss (epoch:  325): 0.02865152\n",
            "Loss (epoch:  326): 0.02864895\n",
            "Loss (epoch:  327): 0.02864641\n",
            "Loss (epoch:  328): 0.02864389\n",
            "Loss (epoch:  329): 0.02864138\n",
            "Loss (epoch:  330): 0.02863889\n",
            "Loss (epoch:  331): 0.02863641\n",
            "Loss (epoch:  332): 0.02863394\n",
            "Loss (epoch:  333): 0.02863149\n",
            "Loss (epoch:  334): 0.02862904\n",
            "Loss (epoch:  335): 0.02862661\n",
            "Loss (epoch:  336): 0.02862419\n",
            "Loss (epoch:  337): 0.02862178\n",
            "Loss (epoch:  338): 0.02861937\n",
            "Loss (epoch:  339): 0.02861697\n",
            "Loss (epoch:  340): 0.02861458\n",
            "Loss (epoch:  341): 0.02861219\n",
            "Loss (epoch:  342): 0.02860981\n",
            "Loss (epoch:  343): 0.02860743\n",
            "Loss (epoch:  344): 0.02860506\n",
            "Loss (epoch:  345): 0.02860269\n",
            "Loss (epoch:  346): 0.02860032\n",
            "Loss (epoch:  347): 0.02859795\n",
            "Loss (epoch:  348): 0.02859559\n",
            "Loss (epoch:  349): 0.02859323\n",
            "Loss (epoch:  350): 0.02859086\n",
            "Loss (epoch:  351): 0.02858851\n",
            "Loss (epoch:  352): 0.02858614\n",
            "Loss (epoch:  353): 0.02858379\n",
            "Loss (epoch:  354): 0.02858142\n",
            "Loss (epoch:  355): 0.02857906\n",
            "Loss (epoch:  356): 0.02857670\n",
            "Loss (epoch:  357): 0.02857433\n",
            "Loss (epoch:  358): 0.02857196\n",
            "Loss (epoch:  359): 0.02856959\n",
            "Loss (epoch:  360): 0.02856723\n",
            "Loss (epoch:  361): 0.02856485\n",
            "Loss (epoch:  362): 0.02856247\n",
            "Loss (epoch:  363): 0.02856010\n",
            "Loss (epoch:  364): 0.02855771\n",
            "Loss (epoch:  365): 0.02855533\n",
            "Loss (epoch:  366): 0.02855294\n",
            "Loss (epoch:  367): 0.02855054\n",
            "Loss (epoch:  368): 0.02854815\n",
            "Loss (epoch:  369): 0.02854575\n",
            "Loss (epoch:  370): 0.02854334\n",
            "Loss (epoch:  371): 0.02854094\n",
            "Loss (epoch:  372): 0.02853852\n",
            "Loss (epoch:  373): 0.02853610\n",
            "Loss (epoch:  374): 0.02853368\n",
            "Loss (epoch:  375): 0.02853125\n",
            "Loss (epoch:  376): 0.02852882\n",
            "Loss (epoch:  377): 0.02852638\n",
            "Loss (epoch:  378): 0.02852394\n",
            "Loss (epoch:  379): 0.02852150\n",
            "Loss (epoch:  380): 0.02851905\n",
            "Loss (epoch:  381): 0.02851659\n",
            "Loss (epoch:  382): 0.02851413\n",
            "Loss (epoch:  383): 0.02851166\n",
            "Loss (epoch:  384): 0.02850919\n",
            "Loss (epoch:  385): 0.02850672\n",
            "Loss (epoch:  386): 0.02850423\n",
            "Loss (epoch:  387): 0.02850174\n",
            "Loss (epoch:  388): 0.02849925\n",
            "Loss (epoch:  389): 0.02849675\n",
            "Loss (epoch:  390): 0.02849424\n",
            "Loss (epoch:  391): 0.02849174\n",
            "Loss (epoch:  392): 0.02848922\n",
            "Loss (epoch:  393): 0.02848670\n",
            "Loss (epoch:  394): 0.02848417\n",
            "Loss (epoch:  395): 0.02848163\n",
            "Loss (epoch:  396): 0.02847910\n",
            "Loss (epoch:  397): 0.02847655\n",
            "Loss (epoch:  398): 0.02847400\n",
            "Loss (epoch:  399): 0.02847144\n",
            "Loss (epoch:  400): 0.02846888\n",
            "Loss (epoch:  401): 0.02846631\n",
            "Loss (epoch:  402): 0.02846374\n",
            "Loss (epoch:  403): 0.02846116\n",
            "Loss (epoch:  404): 0.02845857\n",
            "Loss (epoch:  405): 0.02845599\n",
            "Loss (epoch:  406): 0.02845339\n",
            "Loss (epoch:  407): 0.02845078\n",
            "Loss (epoch:  408): 0.02844818\n",
            "Loss (epoch:  409): 0.02844556\n",
            "Loss (epoch:  410): 0.02844294\n",
            "Loss (epoch:  411): 0.02844031\n",
            "Loss (epoch:  412): 0.02843768\n",
            "Loss (epoch:  413): 0.02843504\n",
            "Loss (epoch:  414): 0.02843240\n",
            "Loss (epoch:  415): 0.02842975\n",
            "Loss (epoch:  416): 0.02842710\n",
            "Loss (epoch:  417): 0.02842444\n",
            "Loss (epoch:  418): 0.02842177\n",
            "Loss (epoch:  419): 0.02841910\n",
            "Loss (epoch:  420): 0.02841642\n",
            "Loss (epoch:  421): 0.02841374\n",
            "Loss (epoch:  422): 0.02841105\n",
            "Loss (epoch:  423): 0.02840835\n",
            "Loss (epoch:  424): 0.02840565\n",
            "Loss (epoch:  425): 0.02840294\n",
            "Loss (epoch:  426): 0.02840023\n",
            "Loss (epoch:  427): 0.02839751\n",
            "Loss (epoch:  428): 0.02839479\n",
            "Loss (epoch:  429): 0.02839206\n",
            "Loss (epoch:  430): 0.02838933\n",
            "Loss (epoch:  431): 0.02838658\n",
            "Loss (epoch:  432): 0.02838384\n",
            "Loss (epoch:  433): 0.02838108\n",
            "Loss (epoch:  434): 0.02837833\n",
            "Loss (epoch:  435): 0.02837556\n",
            "Loss (epoch:  436): 0.02837280\n",
            "Loss (epoch:  437): 0.02837002\n",
            "Loss (epoch:  438): 0.02836724\n",
            "Loss (epoch:  439): 0.02836446\n",
            "Loss (epoch:  440): 0.02836166\n",
            "Loss (epoch:  441): 0.02835887\n",
            "Loss (epoch:  442): 0.02835607\n",
            "Loss (epoch:  443): 0.02835326\n",
            "Loss (epoch:  444): 0.02835045\n",
            "Loss (epoch:  445): 0.02834763\n",
            "Loss (epoch:  446): 0.02834481\n",
            "Loss (epoch:  447): 0.02834198\n",
            "Loss (epoch:  448): 0.02833914\n",
            "Loss (epoch:  449): 0.02833631\n",
            "Loss (epoch:  450): 0.02833346\n",
            "Loss (epoch:  451): 0.02833061\n",
            "Loss (epoch:  452): 0.02832775\n",
            "Loss (epoch:  453): 0.02832489\n",
            "Loss (epoch:  454): 0.02832203\n",
            "Loss (epoch:  455): 0.02831916\n",
            "Loss (epoch:  456): 0.02831628\n",
            "Loss (epoch:  457): 0.02831340\n",
            "Loss (epoch:  458): 0.02831051\n",
            "Loss (epoch:  459): 0.02830762\n",
            "Loss (epoch:  460): 0.02830473\n",
            "Loss (epoch:  461): 0.02830183\n",
            "Loss (epoch:  462): 0.02829892\n",
            "Loss (epoch:  463): 0.02829601\n",
            "Loss (epoch:  464): 0.02829309\n",
            "Loss (epoch:  465): 0.02829017\n",
            "Loss (epoch:  466): 0.02828724\n",
            "Loss (epoch:  467): 0.02828431\n",
            "Loss (epoch:  468): 0.02828138\n",
            "Loss (epoch:  469): 0.02827844\n",
            "Loss (epoch:  470): 0.02827549\n",
            "Loss (epoch:  471): 0.02827254\n",
            "Loss (epoch:  472): 0.02826958\n",
            "Loss (epoch:  473): 0.02826662\n",
            "Loss (epoch:  474): 0.02826366\n",
            "Loss (epoch:  475): 0.02826069\n",
            "Loss (epoch:  476): 0.02825772\n",
            "Loss (epoch:  477): 0.02825474\n",
            "Loss (epoch:  478): 0.02825175\n",
            "Loss (epoch:  479): 0.02824876\n",
            "Loss (epoch:  480): 0.02824577\n",
            "Loss (epoch:  481): 0.02824278\n",
            "Loss (epoch:  482): 0.02823977\n",
            "Loss (epoch:  483): 0.02823677\n",
            "Loss (epoch:  484): 0.02823376\n",
            "Loss (epoch:  485): 0.02823074\n",
            "Loss (epoch:  486): 0.02822772\n",
            "Loss (epoch:  487): 0.02822470\n",
            "Loss (epoch:  488): 0.02822167\n",
            "Loss (epoch:  489): 0.02821864\n",
            "Loss (epoch:  490): 0.02821560\n",
            "Loss (epoch:  491): 0.02821256\n",
            "Loss (epoch:  492): 0.02820951\n",
            "Loss (epoch:  493): 0.02820646\n",
            "Loss (epoch:  494): 0.02820341\n",
            "Loss (epoch:  495): 0.02820035\n",
            "Loss (epoch:  496): 0.02819729\n",
            "Loss (epoch:  497): 0.02819422\n",
            "Loss (epoch:  498): 0.02819115\n",
            "Loss (epoch:  499): 0.02818807\n",
            "Loss (epoch:  500): 0.02818500\n",
            "Loss (epoch:  501): 0.02818191\n",
            "Loss (epoch:  502): 0.02817883\n",
            "Loss (epoch:  503): 0.02817574\n",
            "Loss (epoch:  504): 0.02817264\n",
            "Loss (epoch:  505): 0.02816955\n",
            "Loss (epoch:  506): 0.02816644\n",
            "Loss (epoch:  507): 0.02816334\n",
            "Loss (epoch:  508): 0.02816023\n",
            "Loss (epoch:  509): 0.02815711\n",
            "Loss (epoch:  510): 0.02815400\n",
            "Loss (epoch:  511): 0.02815087\n",
            "Loss (epoch:  512): 0.02814775\n",
            "Loss (epoch:  513): 0.02814462\n",
            "Loss (epoch:  514): 0.02814149\n",
            "Loss (epoch:  515): 0.02813836\n",
            "Loss (epoch:  516): 0.02813521\n",
            "Loss (epoch:  517): 0.02813208\n",
            "Loss (epoch:  518): 0.02812893\n",
            "Loss (epoch:  519): 0.02812578\n",
            "Loss (epoch:  520): 0.02812263\n",
            "Loss (epoch:  521): 0.02811947\n",
            "Loss (epoch:  522): 0.02811631\n",
            "Loss (epoch:  523): 0.02811315\n",
            "Loss (epoch:  524): 0.02810999\n",
            "Loss (epoch:  525): 0.02810681\n",
            "Loss (epoch:  526): 0.02810364\n",
            "Loss (epoch:  527): 0.02810047\n",
            "Loss (epoch:  528): 0.02809729\n",
            "Loss (epoch:  529): 0.02809410\n",
            "Loss (epoch:  530): 0.02809092\n",
            "Loss (epoch:  531): 0.02808774\n",
            "Loss (epoch:  532): 0.02808454\n",
            "Loss (epoch:  533): 0.02808135\n",
            "Loss (epoch:  534): 0.02807815\n",
            "Loss (epoch:  535): 0.02807495\n",
            "Loss (epoch:  536): 0.02807175\n",
            "Loss (epoch:  537): 0.02806854\n",
            "Loss (epoch:  538): 0.02806533\n",
            "Loss (epoch:  539): 0.02806212\n",
            "Loss (epoch:  540): 0.02805891\n",
            "Loss (epoch:  541): 0.02805569\n",
            "Loss (epoch:  542): 0.02805247\n",
            "Loss (epoch:  543): 0.02804925\n",
            "Loss (epoch:  544): 0.02804603\n",
            "Loss (epoch:  545): 0.02804280\n",
            "Loss (epoch:  546): 0.02803957\n",
            "Loss (epoch:  547): 0.02803634\n",
            "Loss (epoch:  548): 0.02803310\n",
            "Loss (epoch:  549): 0.02802987\n",
            "Loss (epoch:  550): 0.02802663\n",
            "Loss (epoch:  551): 0.02802338\n",
            "Loss (epoch:  552): 0.02802014\n",
            "Loss (epoch:  553): 0.02801689\n",
            "Loss (epoch:  554): 0.02801364\n",
            "Loss (epoch:  555): 0.02801039\n",
            "Loss (epoch:  556): 0.02800714\n",
            "Loss (epoch:  557): 0.02800388\n",
            "Loss (epoch:  558): 0.02800062\n",
            "Loss (epoch:  559): 0.02799736\n",
            "Loss (epoch:  560): 0.02799410\n",
            "Loss (epoch:  561): 0.02799084\n",
            "Loss (epoch:  562): 0.02798757\n",
            "Loss (epoch:  563): 0.02798431\n",
            "Loss (epoch:  564): 0.02798104\n",
            "Loss (epoch:  565): 0.02797776\n",
            "Loss (epoch:  566): 0.02797449\n",
            "Loss (epoch:  567): 0.02797121\n",
            "Loss (epoch:  568): 0.02796794\n",
            "Loss (epoch:  569): 0.02796466\n",
            "Loss (epoch:  570): 0.02796138\n",
            "Loss (epoch:  571): 0.02795809\n",
            "Loss (epoch:  572): 0.02795481\n",
            "Loss (epoch:  573): 0.02795153\n",
            "Loss (epoch:  574): 0.02794824\n",
            "Loss (epoch:  575): 0.02794495\n",
            "Loss (epoch:  576): 0.02794166\n",
            "Loss (epoch:  577): 0.02793837\n",
            "Loss (epoch:  578): 0.02793508\n",
            "Loss (epoch:  579): 0.02793178\n",
            "Loss (epoch:  580): 0.02792848\n",
            "Loss (epoch:  581): 0.02792519\n",
            "Loss (epoch:  582): 0.02792189\n",
            "Loss (epoch:  583): 0.02791859\n",
            "Loss (epoch:  584): 0.02791529\n",
            "Loss (epoch:  585): 0.02791198\n",
            "Loss (epoch:  586): 0.02790868\n",
            "Loss (epoch:  587): 0.02790538\n",
            "Loss (epoch:  588): 0.02790207\n",
            "Loss (epoch:  589): 0.02789876\n",
            "Loss (epoch:  590): 0.02789545\n",
            "Loss (epoch:  591): 0.02789214\n",
            "Loss (epoch:  592): 0.02788884\n",
            "Loss (epoch:  593): 0.02788552\n",
            "Loss (epoch:  594): 0.02788222\n",
            "Loss (epoch:  595): 0.02787890\n",
            "Loss (epoch:  596): 0.02787559\n",
            "Loss (epoch:  597): 0.02787227\n",
            "Loss (epoch:  598): 0.02786896\n",
            "Loss (epoch:  599): 0.02786564\n",
            "Loss (epoch:  600): 0.02786233\n",
            "Loss (epoch:  601): 0.02785902\n",
            "Loss (epoch:  602): 0.02785570\n",
            "Loss (epoch:  603): 0.02785238\n",
            "Loss (epoch:  604): 0.02784906\n",
            "Loss (epoch:  605): 0.02784574\n",
            "Loss (epoch:  606): 0.02784242\n",
            "Loss (epoch:  607): 0.02783910\n",
            "Loss (epoch:  608): 0.02783578\n",
            "Loss (epoch:  609): 0.02783246\n",
            "Loss (epoch:  610): 0.02782914\n",
            "Loss (epoch:  611): 0.02782582\n",
            "Loss (epoch:  612): 0.02782250\n",
            "Loss (epoch:  613): 0.02781918\n",
            "Loss (epoch:  614): 0.02781586\n",
            "Loss (epoch:  615): 0.02781254\n",
            "Loss (epoch:  616): 0.02780921\n",
            "Loss (epoch:  617): 0.02780589\n",
            "Loss (epoch:  618): 0.02780257\n",
            "Loss (epoch:  619): 0.02779925\n",
            "Loss (epoch:  620): 0.02779592\n",
            "Loss (epoch:  621): 0.02779260\n",
            "Loss (epoch:  622): 0.02778928\n",
            "Loss (epoch:  623): 0.02778596\n",
            "Loss (epoch:  624): 0.02778264\n",
            "Loss (epoch:  625): 0.02777932\n",
            "Loss (epoch:  626): 0.02777599\n",
            "Loss (epoch:  627): 0.02777267\n",
            "Loss (epoch:  628): 0.02776935\n",
            "Loss (epoch:  629): 0.02776604\n",
            "Loss (epoch:  630): 0.02776271\n",
            "Loss (epoch:  631): 0.02775939\n",
            "Loss (epoch:  632): 0.02775608\n",
            "Loss (epoch:  633): 0.02775275\n",
            "Loss (epoch:  634): 0.02774944\n",
            "Loss (epoch:  635): 0.02774612\n",
            "Loss (epoch:  636): 0.02774280\n",
            "Loss (epoch:  637): 0.02773949\n",
            "Loss (epoch:  638): 0.02773617\n",
            "Loss (epoch:  639): 0.02773285\n",
            "Loss (epoch:  640): 0.02772954\n",
            "Loss (epoch:  641): 0.02772622\n",
            "Loss (epoch:  642): 0.02772291\n",
            "Loss (epoch:  643): 0.02771960\n",
            "Loss (epoch:  644): 0.02771628\n",
            "Loss (epoch:  645): 0.02771297\n",
            "Loss (epoch:  646): 0.02770966\n",
            "Loss (epoch:  647): 0.02770635\n",
            "Loss (epoch:  648): 0.02770305\n",
            "Loss (epoch:  649): 0.02769974\n",
            "Loss (epoch:  650): 0.02769643\n",
            "Loss (epoch:  651): 0.02769312\n",
            "Loss (epoch:  652): 0.02768982\n",
            "Loss (epoch:  653): 0.02768651\n",
            "Loss (epoch:  654): 0.02768321\n",
            "Loss (epoch:  655): 0.02767991\n",
            "Loss (epoch:  656): 0.02767661\n",
            "Loss (epoch:  657): 0.02767331\n",
            "Loss (epoch:  658): 0.02767001\n",
            "Loss (epoch:  659): 0.02766671\n",
            "Loss (epoch:  660): 0.02766342\n",
            "Loss (epoch:  661): 0.02766012\n",
            "Loss (epoch:  662): 0.02765683\n",
            "Loss (epoch:  663): 0.02765354\n",
            "Loss (epoch:  664): 0.02765025\n",
            "Loss (epoch:  665): 0.02764696\n",
            "Loss (epoch:  666): 0.02764367\n",
            "Loss (epoch:  667): 0.02764039\n",
            "Loss (epoch:  668): 0.02763710\n",
            "Loss (epoch:  669): 0.02763382\n",
            "Loss (epoch:  670): 0.02763054\n",
            "Loss (epoch:  671): 0.02762725\n",
            "Loss (epoch:  672): 0.02762398\n",
            "Loss (epoch:  673): 0.02762070\n",
            "Loss (epoch:  674): 0.02761743\n",
            "Loss (epoch:  675): 0.02761416\n",
            "Loss (epoch:  676): 0.02761088\n",
            "Loss (epoch:  677): 0.02760761\n",
            "Loss (epoch:  678): 0.02760434\n",
            "Loss (epoch:  679): 0.02760108\n",
            "Loss (epoch:  680): 0.02759781\n",
            "Loss (epoch:  681): 0.02759455\n",
            "Loss (epoch:  682): 0.02759129\n",
            "Loss (epoch:  683): 0.02758803\n",
            "Loss (epoch:  684): 0.02758477\n",
            "Loss (epoch:  685): 0.02758151\n",
            "Loss (epoch:  686): 0.02757826\n",
            "Loss (epoch:  687): 0.02757501\n",
            "Loss (epoch:  688): 0.02757175\n",
            "Loss (epoch:  689): 0.02756851\n",
            "Loss (epoch:  690): 0.02756526\n",
            "Loss (epoch:  691): 0.02756202\n",
            "Loss (epoch:  692): 0.02755877\n",
            "Loss (epoch:  693): 0.02755554\n",
            "Loss (epoch:  694): 0.02755230\n",
            "Loss (epoch:  695): 0.02754907\n",
            "Loss (epoch:  696): 0.02754583\n",
            "Loss (epoch:  697): 0.02754260\n",
            "Loss (epoch:  698): 0.02753937\n",
            "Loss (epoch:  699): 0.02753614\n",
            "Loss (epoch:  700): 0.02753292\n",
            "Loss (epoch:  701): 0.02752970\n",
            "Loss (epoch:  702): 0.02752648\n",
            "Loss (epoch:  703): 0.02752326\n",
            "Loss (epoch:  704): 0.02752005\n",
            "Loss (epoch:  705): 0.02751683\n",
            "Loss (epoch:  706): 0.02751362\n",
            "Loss (epoch:  707): 0.02751042\n",
            "Loss (epoch:  708): 0.02750721\n",
            "Loss (epoch:  709): 0.02750401\n",
            "Loss (epoch:  710): 0.02750081\n",
            "Loss (epoch:  711): 0.02749761\n",
            "Loss (epoch:  712): 0.02749441\n",
            "Loss (epoch:  713): 0.02749122\n",
            "Loss (epoch:  714): 0.02748803\n",
            "Loss (epoch:  715): 0.02748484\n",
            "Loss (epoch:  716): 0.02748166\n",
            "Loss (epoch:  717): 0.02747848\n",
            "Loss (epoch:  718): 0.02747530\n",
            "Loss (epoch:  719): 0.02747212\n",
            "Loss (epoch:  720): 0.02746894\n",
            "Loss (epoch:  721): 0.02746577\n",
            "Loss (epoch:  722): 0.02746260\n",
            "Loss (epoch:  723): 0.02745944\n",
            "Loss (epoch:  724): 0.02745627\n",
            "Loss (epoch:  725): 0.02745311\n",
            "Loss (epoch:  726): 0.02744995\n",
            "Loss (epoch:  727): 0.02744679\n",
            "Loss (epoch:  728): 0.02744364\n",
            "Loss (epoch:  729): 0.02744050\n",
            "Loss (epoch:  730): 0.02743735\n",
            "Loss (epoch:  731): 0.02743420\n",
            "Loss (epoch:  732): 0.02743106\n",
            "Loss (epoch:  733): 0.02742792\n",
            "Loss (epoch:  734): 0.02742479\n",
            "Loss (epoch:  735): 0.02742165\n",
            "Loss (epoch:  736): 0.02741852\n",
            "Loss (epoch:  737): 0.02741540\n",
            "Loss (epoch:  738): 0.02741227\n",
            "Loss (epoch:  739): 0.02740916\n",
            "Loss (epoch:  740): 0.02740604\n",
            "Loss (epoch:  741): 0.02740292\n",
            "Loss (epoch:  742): 0.02739981\n",
            "Loss (epoch:  743): 0.02739671\n",
            "Loss (epoch:  744): 0.02739360\n",
            "Loss (epoch:  745): 0.02739050\n",
            "Loss (epoch:  746): 0.02738739\n",
            "Loss (epoch:  747): 0.02738430\n",
            "Loss (epoch:  748): 0.02738120\n",
            "Loss (epoch:  749): 0.02737812\n",
            "Loss (epoch:  750): 0.02737503\n",
            "Loss (epoch:  751): 0.02737195\n",
            "Loss (epoch:  752): 0.02736887\n",
            "Loss (epoch:  753): 0.02736579\n",
            "Loss (epoch:  754): 0.02736271\n",
            "Loss (epoch:  755): 0.02735964\n",
            "Loss (epoch:  756): 0.02735658\n",
            "Loss (epoch:  757): 0.02735351\n",
            "Loss (epoch:  758): 0.02735045\n",
            "Loss (epoch:  759): 0.02734739\n",
            "Loss (epoch:  760): 0.02734434\n",
            "Loss (epoch:  761): 0.02734129\n",
            "Loss (epoch:  762): 0.02733824\n",
            "Loss (epoch:  763): 0.02733520\n",
            "Loss (epoch:  764): 0.02733215\n",
            "Loss (epoch:  765): 0.02732911\n",
            "Loss (epoch:  766): 0.02732608\n",
            "Loss (epoch:  767): 0.02732305\n",
            "Loss (epoch:  768): 0.02732002\n",
            "Loss (epoch:  769): 0.02731700\n",
            "Loss (epoch:  770): 0.02731398\n",
            "Loss (epoch:  771): 0.02731096\n",
            "Loss (epoch:  772): 0.02730795\n",
            "Loss (epoch:  773): 0.02730494\n",
            "Loss (epoch:  774): 0.02730193\n",
            "Loss (epoch:  775): 0.02729892\n",
            "Loss (epoch:  776): 0.02729593\n",
            "Loss (epoch:  777): 0.02729293\n",
            "Loss (epoch:  778): 0.02728994\n",
            "Loss (epoch:  779): 0.02728695\n",
            "Loss (epoch:  780): 0.02728396\n",
            "Loss (epoch:  781): 0.02728098\n",
            "Loss (epoch:  782): 0.02727800\n",
            "Loss (epoch:  783): 0.02727502\n",
            "Loss (epoch:  784): 0.02727205\n",
            "Loss (epoch:  785): 0.02726908\n",
            "Loss (epoch:  786): 0.02726612\n",
            "Loss (epoch:  787): 0.02726316\n",
            "Loss (epoch:  788): 0.02726020\n",
            "Loss (epoch:  789): 0.02725725\n",
            "Loss (epoch:  790): 0.02725429\n",
            "Loss (epoch:  791): 0.02725135\n",
            "Loss (epoch:  792): 0.02724840\n",
            "Loss (epoch:  793): 0.02724547\n",
            "Loss (epoch:  794): 0.02724253\n",
            "Loss (epoch:  795): 0.02723960\n",
            "Loss (epoch:  796): 0.02723667\n",
            "Loss (epoch:  797): 0.02723374\n",
            "Loss (epoch:  798): 0.02723082\n",
            "Loss (epoch:  799): 0.02722791\n",
            "Loss (epoch:  800): 0.02722499\n",
            "Loss (epoch:  801): 0.02722208\n",
            "Loss (epoch:  802): 0.02721917\n",
            "Loss (epoch:  803): 0.02721627\n",
            "Loss (epoch:  804): 0.02721337\n",
            "Loss (epoch:  805): 0.02721048\n",
            "Loss (epoch:  806): 0.02720758\n",
            "Loss (epoch:  807): 0.02720470\n",
            "Loss (epoch:  808): 0.02720181\n",
            "Loss (epoch:  809): 0.02719893\n",
            "Loss (epoch:  810): 0.02719605\n",
            "Loss (epoch:  811): 0.02719318\n",
            "Loss (epoch:  812): 0.02719031\n",
            "Loss (epoch:  813): 0.02718744\n",
            "Loss (epoch:  814): 0.02718458\n",
            "Loss (epoch:  815): 0.02718172\n",
            "Loss (epoch:  816): 0.02717887\n",
            "Loss (epoch:  817): 0.02717602\n",
            "Loss (epoch:  818): 0.02717318\n",
            "Loss (epoch:  819): 0.02717033\n",
            "Loss (epoch:  820): 0.02716749\n",
            "Loss (epoch:  821): 0.02716465\n",
            "Loss (epoch:  822): 0.02716182\n",
            "Loss (epoch:  823): 0.02715899\n",
            "Loss (epoch:  824): 0.02715617\n",
            "Loss (epoch:  825): 0.02715335\n",
            "Loss (epoch:  826): 0.02715053\n",
            "Loss (epoch:  827): 0.02714772\n",
            "Loss (epoch:  828): 0.02714491\n",
            "Loss (epoch:  829): 0.02714211\n",
            "Loss (epoch:  830): 0.02713931\n",
            "Loss (epoch:  831): 0.02713650\n",
            "Loss (epoch:  832): 0.02713372\n",
            "Loss (epoch:  833): 0.02713092\n",
            "Loss (epoch:  834): 0.02712814\n",
            "Loss (epoch:  835): 0.02712536\n",
            "Loss (epoch:  836): 0.02712258\n",
            "Loss (epoch:  837): 0.02711980\n",
            "Loss (epoch:  838): 0.02711703\n",
            "Loss (epoch:  839): 0.02711427\n",
            "Loss (epoch:  840): 0.02711150\n",
            "Loss (epoch:  841): 0.02710874\n",
            "Loss (epoch:  842): 0.02710599\n",
            "Loss (epoch:  843): 0.02710324\n",
            "Loss (epoch:  844): 0.02710049\n",
            "Loss (epoch:  845): 0.02709775\n",
            "Loss (epoch:  846): 0.02709501\n",
            "Loss (epoch:  847): 0.02709227\n",
            "Loss (epoch:  848): 0.02708954\n",
            "Loss (epoch:  849): 0.02708681\n",
            "Loss (epoch:  850): 0.02708409\n",
            "Loss (epoch:  851): 0.02708137\n",
            "Loss (epoch:  852): 0.02707865\n",
            "Loss (epoch:  853): 0.02707594\n",
            "Loss (epoch:  854): 0.02707323\n",
            "Loss (epoch:  855): 0.02707053\n",
            "Loss (epoch:  856): 0.02706783\n",
            "Loss (epoch:  857): 0.02706513\n",
            "Loss (epoch:  858): 0.02706243\n",
            "Loss (epoch:  859): 0.02705975\n",
            "Loss (epoch:  860): 0.02705706\n",
            "Loss (epoch:  861): 0.02705438\n",
            "Loss (epoch:  862): 0.02705170\n",
            "Loss (epoch:  863): 0.02704903\n",
            "Loss (epoch:  864): 0.02704636\n",
            "Loss (epoch:  865): 0.02704369\n",
            "Loss (epoch:  866): 0.02704103\n",
            "Loss (epoch:  867): 0.02703837\n",
            "Loss (epoch:  868): 0.02703572\n",
            "Loss (epoch:  869): 0.02703307\n",
            "Loss (epoch:  870): 0.02703043\n",
            "Loss (epoch:  871): 0.02702778\n",
            "Loss (epoch:  872): 0.02702514\n",
            "Loss (epoch:  873): 0.02702251\n",
            "Loss (epoch:  874): 0.02701988\n",
            "Loss (epoch:  875): 0.02701725\n",
            "Loss (epoch:  876): 0.02701463\n",
            "Loss (epoch:  877): 0.02701201\n",
            "Loss (epoch:  878): 0.02700939\n",
            "Loss (epoch:  879): 0.02700678\n",
            "Loss (epoch:  880): 0.02700417\n",
            "Loss (epoch:  881): 0.02700157\n",
            "Loss (epoch:  882): 0.02699897\n",
            "Loss (epoch:  883): 0.02699638\n",
            "Loss (epoch:  884): 0.02699378\n",
            "Loss (epoch:  885): 0.02699120\n",
            "Loss (epoch:  886): 0.02698861\n",
            "Loss (epoch:  887): 0.02698603\n",
            "Loss (epoch:  888): 0.02698345\n",
            "Loss (epoch:  889): 0.02698088\n",
            "Loss (epoch:  890): 0.02697831\n",
            "Loss (epoch:  891): 0.02697575\n",
            "Loss (epoch:  892): 0.02697319\n",
            "Loss (epoch:  893): 0.02697063\n",
            "Loss (epoch:  894): 0.02696808\n",
            "Loss (epoch:  895): 0.02696553\n",
            "Loss (epoch:  896): 0.02696299\n",
            "Loss (epoch:  897): 0.02696045\n",
            "Loss (epoch:  898): 0.02695791\n",
            "Loss (epoch:  899): 0.02695538\n",
            "Loss (epoch:  900): 0.02695284\n",
            "Loss (epoch:  901): 0.02695032\n",
            "Loss (epoch:  902): 0.02694780\n",
            "Loss (epoch:  903): 0.02694528\n",
            "Loss (epoch:  904): 0.02694277\n",
            "Loss (epoch:  905): 0.02694026\n",
            "Loss (epoch:  906): 0.02693775\n",
            "Loss (epoch:  907): 0.02693525\n",
            "Loss (epoch:  908): 0.02693275\n",
            "Loss (epoch:  909): 0.02693025\n",
            "Loss (epoch:  910): 0.02692776\n",
            "Loss (epoch:  911): 0.02692527\n",
            "Loss (epoch:  912): 0.02692279\n",
            "Loss (epoch:  913): 0.02692031\n",
            "Loss (epoch:  914): 0.02691784\n",
            "Loss (epoch:  915): 0.02691537\n",
            "Loss (epoch:  916): 0.02691290\n",
            "Loss (epoch:  917): 0.02691043\n",
            "Loss (epoch:  918): 0.02690798\n",
            "Loss (epoch:  919): 0.02690552\n",
            "Loss (epoch:  920): 0.02690307\n",
            "Loss (epoch:  921): 0.02690062\n",
            "Loss (epoch:  922): 0.02689817\n",
            "Loss (epoch:  923): 0.02689573\n",
            "Loss (epoch:  924): 0.02689329\n",
            "Loss (epoch:  925): 0.02689086\n",
            "Loss (epoch:  926): 0.02688843\n",
            "Loss (epoch:  927): 0.02688600\n",
            "Loss (epoch:  928): 0.02688358\n",
            "Loss (epoch:  929): 0.02688117\n",
            "Loss (epoch:  930): 0.02687875\n",
            "Loss (epoch:  931): 0.02687634\n",
            "Loss (epoch:  932): 0.02687394\n",
            "Loss (epoch:  933): 0.02687153\n",
            "Loss (epoch:  934): 0.02686913\n",
            "Loss (epoch:  935): 0.02686674\n",
            "Loss (epoch:  936): 0.02686435\n",
            "Loss (epoch:  937): 0.02686196\n",
            "Loss (epoch:  938): 0.02685957\n",
            "Loss (epoch:  939): 0.02685720\n",
            "Loss (epoch:  940): 0.02685482\n",
            "Loss (epoch:  941): 0.02685245\n",
            "Loss (epoch:  942): 0.02685008\n",
            "Loss (epoch:  943): 0.02684771\n",
            "Loss (epoch:  944): 0.02684535\n",
            "Loss (epoch:  945): 0.02684299\n",
            "Loss (epoch:  946): 0.02684064\n",
            "Loss (epoch:  947): 0.02683829\n",
            "Loss (epoch:  948): 0.02683594\n",
            "Loss (epoch:  949): 0.02683360\n",
            "Loss (epoch:  950): 0.02683126\n",
            "Loss (epoch:  951): 0.02682893\n",
            "Loss (epoch:  952): 0.02682660\n",
            "Loss (epoch:  953): 0.02682427\n",
            "Loss (epoch:  954): 0.02682194\n",
            "Loss (epoch:  955): 0.02681963\n",
            "Loss (epoch:  956): 0.02681731\n",
            "Loss (epoch:  957): 0.02681500\n",
            "Loss (epoch:  958): 0.02681269\n",
            "Loss (epoch:  959): 0.02681038\n",
            "Loss (epoch:  960): 0.02680808\n",
            "Loss (epoch:  961): 0.02680578\n",
            "Loss (epoch:  962): 0.02680349\n",
            "Loss (epoch:  963): 0.02680119\n",
            "Loss (epoch:  964): 0.02679891\n",
            "Loss (epoch:  965): 0.02679663\n",
            "Loss (epoch:  966): 0.02679435\n",
            "Loss (epoch:  967): 0.02679207\n",
            "Loss (epoch:  968): 0.02678980\n",
            "Loss (epoch:  969): 0.02678753\n",
            "Loss (epoch:  970): 0.02678527\n",
            "Loss (epoch:  971): 0.02678301\n",
            "Loss (epoch:  972): 0.02678075\n",
            "Loss (epoch:  973): 0.02677849\n",
            "Loss (epoch:  974): 0.02677624\n",
            "Loss (epoch:  975): 0.02677399\n",
            "Loss (epoch:  976): 0.02677175\n",
            "Loss (epoch:  977): 0.02676952\n",
            "Loss (epoch:  978): 0.02676728\n",
            "Loss (epoch:  979): 0.02676505\n",
            "Loss (epoch:  980): 0.02676282\n",
            "Loss (epoch:  981): 0.02676059\n",
            "Loss (epoch:  982): 0.02675837\n",
            "Loss (epoch:  983): 0.02675615\n",
            "Loss (epoch:  984): 0.02675394\n",
            "Loss (epoch:  985): 0.02675173\n",
            "Loss (epoch:  986): 0.02674952\n",
            "Loss (epoch:  987): 0.02674732\n",
            "Loss (epoch:  988): 0.02674513\n",
            "Loss (epoch:  989): 0.02674293\n",
            "Loss (epoch:  990): 0.02674073\n",
            "Loss (epoch:  991): 0.02673855\n",
            "Loss (epoch:  992): 0.02673636\n",
            "Loss (epoch:  993): 0.02673418\n",
            "Loss (epoch:  994): 0.02673200\n",
            "Loss (epoch:  995): 0.02672982\n",
            "Loss (epoch:  996): 0.02672765\n",
            "Loss (epoch:  997): 0.02672549\n",
            "Loss (epoch:  998): 0.02672332\n",
            "Loss (epoch:  999): 0.02672116\n",
            "Loss (epoch: 1000): 0.02671900\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "\n",
        "    output = []\n",
        "    # Iterate over the dataloader for training data\n",
        "    for i, data in enumerate(testdataloader, 0):\n",
        "        inputs, targets = data\n",
        "        inputs, targets = inputs.float(), targets.float()\n",
        "        targets = targets.reshape((targets.shape[0],1))\n",
        "\n",
        "        #zero the gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        #perform forward pass\n",
        "        outputs = model(inputs)\n",
        "        output.append(outputs)\n",
        "# Process is complete.\n",
        "#print('Training process has finished.')\n",
        "\n",
        "# Extract the weights and biases from the model\n",
        "weights_1 = model.fc1.weight.detach().numpy()\n",
        "bias_1 = model.fc1.bias.detach().numpy()\n",
        "#weights_2 = model.fc2.weight.detach().numpy()\n",
        "#bias_2 = model.fc2.bias.detach().numpy()\n",
        "#weights_3 = model.fc3.weight.detach().numpy()\n",
        "#bias_3 = model.fc3.bias.detach().numpy()\n",
        "weights_4 = model.fc4.weight.detach().numpy()\n",
        "bias_4 = model.fc4.bias.detach().numpy()\n",
        "\n",
        "def generate_variable_declarations(weights_shape, layer_prefix):\n",
        "    declarations = \"\"\n",
        "    num_neurons = weights_shape  # Number of neurons is determined by the first dimension of the weights matrix\n",
        "    layer_declarations = \", \".join([f\"{layer_prefix}_{i}\" for i in range(num_neurons)]) + \";\"\n",
        "    declarations += layer_declarations\n",
        "    return declarations\n",
        "\n",
        "# Use the function to generate declarations for each layer\n",
        "h1_declarations = generate_variable_declarations(weights_1.shape[0], \"hvth1\")\n",
        "#2_declarations = generate_variable_declarations(weights_2.shape[0], \"hvth2\")\n",
        "\n",
        "verilog_code = \"\"\"\n",
        "// VerilogA for GB_lib, IWO_verliogA, veriloga\n",
        "//*******************************************************************************\n",
        "//* * School of Electrical and Computer Engineering, Georgia Institute of Technology\n",
        "//* PI: Prof. Shimeng Yu\n",
        "//* All rights reserved.\n",
        "//*\n",
        "//* Copyright of the model is maintained by the developers, and the model is distributed under\n",
        "//* the terms of the Creative Commons Attribution-NonCommercial 4.0 International Public License\n",
        "//* http://creativecommons.org/licenses/by-nc/4.0/legalcode.\n",
        "//*\n",
        "//* THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND\n",
        "//* ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\n",
        "//* WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n",
        "//* DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\n",
        "//* FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\n",
        "//* DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\n",
        "//* SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\n",
        "//* CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\n",
        "//* OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n",
        "//* OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n",
        "//*\n",
        "//* Developer:\n",
        "//*  Gihun Choe gchoe6@gatech.edu\n",
        "//********************************************************************************/\n",
        "\n",
        "`include \"constants.vams\"\n",
        "`include \"disciplines.vams\"\n",
        "\n",
        "\n",
        "module IWO_verliogA(d, g, s);\n",
        "        inout d, g, s;\n",
        "        electrical d, g, s;\n",
        "\n",
        "        //***** parameters L and W ******//\n",
        "        parameter real W = 0.1; //get parameter fom spectre\n",
        "        parameter real L = 0.05; //get parameter fom spectre\n",
        "        parameter real T_stress = 0.0001; //set on cadence as variable\n",
        "        parameter real T_rec = 0.001;     //set on cadence as variable\n",
        "        parameter real Clk_loops = 50;    //set on cadence as variable\n",
        "        parameter real V_ov = 1.7;        //set on cadence as variable\n",
        "        parameter real Temp = 25;  //set on cadence as variable\n",
        "\n",
        "        parameter MinVg = -1.0 ;\n",
        "        parameter normVg = 0.2222222222222222 ;\n",
        "        parameter MinVd = 0.01 ;\n",
        "        parameter normVd = 0.2949852507374631 ;\n",
        "        parameter MinLg = 0.05 ;\n",
        "        parameter normLg = 1.4285714285714286 ;\n",
        "        parameter MinO = 8.15e-15 ;\n",
        "        parameter normO =33613445378151.26;\n",
        "        parameter MinI = -23.98798356587402 ;\n",
        "        parameter normI =0.04615548498417793;\n",
        "\n",
        "        parameter Mint_stress = {} ;\n",
        "        parameter normt_stress = {} ;\n",
        "        parameter Mint_rec = {} ;\n",
        "        parameter normt_rec = {} ;\n",
        "        parameter Minclk_loops = {} ;\n",
        "        parameter normclk_loops = {} ;\n",
        "        parameter Minv_ov = {} ;\n",
        "        parameter normv_ov = {} ;\n",
        "        parameter Mintemperature = {} ;\n",
        "        parameter normtemperature = {} ;\n",
        "        parameter Mindelta_Vth = {} ;\n",
        "        parameter normdelta_Vth = {} ;\n",
        "\n",
        "        real {}\n",
        "        real {}\n",
        "\n",
        "        real Vg, Vd, Vs, Vgs, Vds, Lg, Id, Cgg, Cgsd, Vgd;\n",
        "        real Vgsraw, Vgdraw, dir;\n",
        "        real t_stress, v_ov, t_rec, clk_loops, temp, delta_Vth;\n",
        "\n",
        "        real h1_0, h1_1, h1_2, h1_3, h1_4, h1_5, h1_6, h1_7, h1_8, h1_9, h1_10, h1_11, h1_12, h1_13, h1_14, h1_15, h1_16, h1_17, h1_18, h1_19, h1_20, h1_21, h1_22, h1_23, h1_24;\n",
        "        real h2_0, h2_1, h2_2, h2_3, h2_4, h2_5, h2_6, h2_7, h2_8, h2_9, h2_10, h2_11, h2_12, h2_13, h2_14, h2_15, h2_16, y;\n",
        "        real hc1_0, hc1_1, hc1_2, hc1_3, hc1_4, hc1_5, hc1_6, hc1_7, hc1_8, hc1_9;\n",
        "        real hc1_10, hc1_11, hc1_12, hc1_13, hc1_14, hc1_15, hc1_16;\n",
        "        real hc2_0, hc2_1, hc2_2, hc2_3, hc2_4, hc2_5, hc2_6, hc2_7, hc2_8, hc2_9;\n",
        "        real hc2_10, hc2_11, hc2_12, hc2_13, hc2_14, hc2_15, hc2_16, yc, yvth;\n",
        "\n",
        "analog begin\n",
        "\n",
        "t_stress = (T_stress - Mint_stress)*normt_stress;\n",
        "v_ov = (V_ov - Minv_ov)*normv_ov;\n",
        "t_rec = (T_rec - Mint_rec)*normt_rec ;\n",
        "clk_loops = (Clk_loops - Minclk_loops)*normclk_loops ;\n",
        "temp = (Temp - Mintemperature)*normtemperature ;\n",
        "\n",
        "//******************** delta_Vth NN **********************************//\n",
        "\n",
        "\"\"\".format(Mint_stress, normt_stress, Mint_rec, normt_rec, Minclk_loops, normclk_loops, MinV_ov, normV_ov, Mintemperature, normtemperature, Mindelta_Vth, normdelta_Vth, h1_declarations, h2_declarations)\n",
        "# V_ov = (V_ov - MinV_ov)*normV_ov ;\n",
        "# t_stress = (T_stress - Mint_stress)*normt_stress ;\n",
        "\n",
        "# Create the Verilog-A code for the 1st hidden layer\n",
        "for i in range(n1):\n",
        "    inputs = [\"t_stress\", \"t_rec\", \"clk_loops\", \"v_ov\", \"temp\"]\n",
        "    inputs = [\"*\".join([str(weights_1[i][j]), inp]) for j, inp in enumerate(inputs)]\n",
        "    inputs = \"+\".join(inputs)\n",
        "    inputs = \"+\".join([inputs, str(bias_1[i])])\n",
        "    verilog_code += \"hvth1_{} = tanh({});\\n\".format(i, inputs)\n",
        "verilog_code += \"\\n\"\n",
        "\n",
        "\"\"\"\n",
        "# Create the Verilog-A code for the 2nd hidden layer\n",
        "for i in range(n2):\n",
        "    inputs = [\"h1_{}\".format(j) for j in range(n1)]\n",
        "    inputs = [\"*\".join([str(weights_2[i][j]), inp]) for j, inp in enumerate(inputs)]\n",
        "    inputs = \"+\".join(inputs)\n",
        "    inputs = \"+\".join([inputs, str(bias_2[i])])\n",
        "    verilog_code += \"hvth2_{} = tanh({});\\n\".format(i, inputs)\n",
        "verilog_code += \"\\n\"\n",
        "\"\"\"\n",
        "# Create the Verilog-A code for the output layer\n",
        "inputs = [\"hvth1_{}\".format(i) for i in range(n1)]\n",
        "inputs = [\"*\".join([str(weights_4[0][i]), inp]) for i, inp in enumerate(inputs)]\n",
        "inputs = \"+\".join(inputs)\n",
        "inputs = \"+\".join([inputs, str(bias_4[0])])\n",
        "verilog_code += \"yvth = {};\\n\\n\".format(inputs)\n",
        "verilog_code += \"delta_Vth = (yvth - Mindelta_Vth) * normdelta_Vth;\"\n",
        "verilog_code += \"\"\"\n",
        "\n",
        "\n",
        "        Vg = V(g) ;\n",
        "        Vs = V(s) ;\n",
        "        Vd = V(d) ;\n",
        "        Vgsraw = Vg-Vs ;\n",
        "        Vgdraw = Vg-Vd ;\n",
        "\n",
        "if (Vgsraw >= Vgdraw) begin\n",
        "        Vgs = ((Vg-Vs) - MinVg) * normVg ;\n",
        "        dir = 1;\n",
        "end\n",
        "\n",
        "else begin\n",
        "        Vgs = ((Vg-Vd) - MinVg) * normVg ;\n",
        "        dir = -1;\n",
        "end\n",
        "\n",
        "        Vds = (abs(Vd-Vs) - MinVd) * normVd ;\n",
        "        Vgs = Vgs - delta_Vth ; // BTI Vth variation\n",
        "        Lg = (L -MinLg)*normLg ;\n",
        "\n",
        "\n",
        "\n",
        "//******************** C-V NN **********************************//\n",
        "hc1_0 = tanh(-0.99871427*Vgs+-0.16952373*Vds+0.32118186*Lg+0.41485423);\n",
        "hc1_1 = tanh(0.31587568*Vgs+0.13397887*Vds+-0.4541538*Lg+-0.3630942);\n",
        "hc1_2 = tanh(-0.76281804*Vgs+0.09352969*Vds+1.1961353*Lg+0.3904977);\n",
        "hc1_3 = tanh(-1.115087*Vgs+0.85752946*Vds+-0.11746484*Lg+0.5500279);\n",
        "hc1_4 = tanh(1.0741386*Vgs+0.82041687*Vds+0.19092631*Lg+-0.4009425);\n",
        "hc1_5 = tanh(-0.47921795*Vgs+-0.8749933*Vds+-0.054768667*Lg+0.62785167);\n",
        "hc1_6 = tanh(0.5449184*Vgs+-4.409165*Vds+-0.072947875*Lg+-0.31324536);\n",
        "hc1_7 = tanh(-2.9224303*Vgs+2.7675478*Vds+0.08862238*Lg+0.6493558);\n",
        "hc1_8 = tanh(0.65050656*Vgs+-0.29751927*Vds+0.1571876*Lg+-0.38088372);\n",
        "hc1_9 = tanh(-0.30384183*Vgs+0.5649165*Vds+2.6806898*Lg+0.3197917);\n",
        "hc1_10 = tanh(-0.095988505*Vgs+2.0158541*Vds+-0.42972717*Lg+-0.30388466);\n",
        "hc1_11 = tanh(6.7699738*Vgs+-0.07234483*Vds+-0.013545353*Lg+-1.3694142);\n",
        "hc1_12 = tanh(-0.3404029*Vgs+0.0443459*Vds+0.89597*Lg+0.069993004);\n",
        "hc1_13 = tanh(0.62300175*Vgs+-0.29515797*Vds+1.6753465*Lg+-0.6520838);\n",
        "hc1_14 = tanh(0.37957156*Vgs+0.2237372*Vds+0.08591952*Lg+0.13126835);\n",
        "hc1_15 = tanh(0.19949242*Vgs+-0.26481664*Vds+-0.41059187*Lg+-0.40832308);\n",
        "hc1_16 = tanh(0.98966587*Vgs+-0.24259183*Vds+0.36584845*Lg+-0.8024042);\n",
        "\n",
        "hc2_0 = tanh(-0.91327864*hc1_0+0.4696781*hc1_1+0.3302202*hc1_2+0.11393868*hc1_3+0.45070222*hc1_4+-0.2894044*hc1_5+0.55066*hc1_6+-1.6242687*hc1_7+-0.38140613*hc1_8+0.032771554*hc1_9+0.17647126);\n",
        "hc2_1 = tanh(-1.1663305*hc1_0+-0.523984*hc1_1+-0.90804136*hc1_2+-0.7418044*hc1_3+1.456171*hc1_4+-0.16802542*hc1_5+0.8235596*hc1_6+-2.2246246*hc1_7+-0.40805355*hc1_8+0.7207601*hc1_9+0.4729169);\n",
        "hc2_2 = tanh(-0.69065374*hc1_0+0.40205315*hc1_1+-0.49410668*hc1_2+0.8681325*hc1_3+0.471351*hc1_4+0.46939445*hc1_5+0.45568785*hc1_6+-0.92935294*hc1_7+-0.8209646*hc1_8+0.1158967*hc1_9+-0.075798474);\n",
        "hc2_3 = tanh(0.11535446*hc1_0+-0.06296927*hc1_1+-0.6740435*hc1_2+0.7428315*hc1_3+0.05890677*hc1_4+0.9579441*hc1_5+-0.037319*hc1_6+-0.18491426*hc1_7+-0.02981994*hc1_8+0.038347963*hc1_9+0.039531134);\n",
        "hc2_4 = tanh(0.37817463*hc1_0+-0.6811279*hc1_1+1.1388369*hc1_2+0.19983096*hc1_3+-0.20415118*hc1_4+1.3022176*hc1_5+0.22571652*hc1_6+0.1690611*hc1_7+-0.56475276*hc1_8+-0.4069731*hc1_9+0.99962974);\n",
        "hc2_5 = tanh(0.032873478*hc1_0+-0.05209407*hc1_1+-0.010839908*hc1_2+-0.13892579*hc1_3+-0.050480977*hc1_4+0.0127089145*hc1_5+0.0052771433*hc1_6+0.02029242*hc1_7+-0.08705659*hc1_8+0.0254766*hc1_9+0.025135752);\n",
        "hc2_6 = tanh(-0.34639204*hc1_0+0.06937975*hc1_1+0.18671949*hc1_2+-0.18912783*hc1_3+0.1312504*hc1_4+0.4627272*hc1_5+-0.42590702*hc1_6+-0.10966313*hc1_7+0.66083515*hc1_8+-0.050718334*hc1_9+0.08234678);\n",
        "hc2_7 = tanh(0.90656275*hc1_0+-0.037281644*hc1_1+0.77237594*hc1_2+1.4710428*hc1_3+0.13597831*hc1_4+-0.059844542*hc1_5+-0.7801535*hc1_6+3.7814677*hc1_7+-0.5976644*hc1_8+0.2721995*hc1_9+0.023777716);\n",
        "hc2_8 = tanh(0.39720625*hc1_0+-0.45262313*hc1_1+0.19873238*hc1_2+0.9750888*hc1_3+-0.9427992*hc1_4+0.4487432*hc1_5+-0.3372945*hc1_6+0.33729544*hc1_7+-0.1667088*hc1_8+-0.5707525*hc1_9+0.27954483);\n",
        "hc2_9 = tanh(0.28551984*hc1_0+-0.68350387*hc1_1+0.9916423*hc1_2+-0.8254094*hc1_3+0.09875706*hc1_4+0.47609732*hc1_5+-0.058662917*hc1_6+0.09181381*hc1_7+0.09592329*hc1_8+1.3940467*hc1_9+0.3865768);\n",
        "hc2_10 = tanh(0.098737516*hc1_0+0.060473576*hc1_1+0.42824662*hc1_2+0.15018038*hc1_3+0.082621895*hc1_4+-0.00019039502*hc1_5+-0.3321634*hc1_6+0.7936295*hc1_7+-0.041197542*hc1_8+0.6530619*hc1_9+0.1338804);\n",
        "hc2_11 = tanh(-0.3585284*hc1_0+-0.09956017*hc1_1+0.17224246*hc1_2+-0.016925728*hc1_3+-0.46462816*hc1_4+-0.5649022*hc1_5+1.251695*hc1_6+-0.4303161*hc1_7+0.48546878*hc1_8+0.22958975*hc1_9+-0.27899802);\n",
        "hc2_12 = tanh(0.8565631*hc1_0+-0.7622999*hc1_1+0.32367912*hc1_2+1.4776785*hc1_3+0.2712369*hc1_4+0.2275511*hc1_5+0.39908803*hc1_6+4.2305493*hc1_7+-0.3467536*hc1_8+0.41231114*hc1_9+0.47123823);\n",
        "hc2_13 = tanh(-0.26411077*hc1_0+-0.17583853*hc1_1+0.045439184*hc1_2+-0.13801138*hc1_3+0.03278085*hc1_4+-0.45625108*hc1_5+-0.17905861*hc1_6+0.3060186*hc1_7+-0.3361926*hc1_8+-0.050055273*hc1_9+0.17996444);\n",
        "hc2_14 = tanh(0.016094366*hc1_0+-0.013196736*hc1_1+0.4124856*hc1_2+0.22926371*hc1_3+-0.35071182*hc1_4+-0.34217188*hc1_5+-0.69466996*hc1_6+1.0563152*hc1_7+-0.18019852*hc1_8+0.061871335*hc1_9+0.09762555);\n",
        "hc2_15 = tanh(-0.18970782*hc1_0+0.3624813*hc1_1+-1.3419824*hc1_2+0.103635244*hc1_3+-0.14595217*hc1_4+-0.1899393*hc1_5+0.176524*hc1_6+-0.4428012*hc1_7+-0.39544868*hc1_8+-0.45783517*hc1_9+0.1884755);\n",
        "hc2_16 = tanh(-0.1585831*hc1_0+0.035894837*hc1_1+-0.14261873*hc1_2+0.25914294*hc1_3+0.040607046*hc1_4+0.11555795*hc1_5+0.0022548323*hc1_6+-0.002149359*hc1_7+0.07067297*hc1_8+0.019578662*hc1_9+0.16657573);\n",
        "yc = 0.17503543*hc2_0+-0.15556754*hc2_1+-0.125455*hc2_2+-0.07502612*hc2_3+-0.16671115*hc2_4+0.29854503;\n",
        "\n",
        "Cgg = (yc / normO + MinO)*W;\n",
        "Cgsd = Cgg/2 ;\n",
        "\n",
        "//******************** I-V NN **********************************//\n",
        "h1_0 = tanh(0.0023826673*Vgs+-0.0009636134*Vds+0.006639037*Lg+-0.0004692053);\n",
        "h1_1 = tanh(-7.8007555*Vgs+2.639791*Vds+-0.025273597*Lg+-1.1747514);\n",
        "h1_2 = tanh(1.9884652*Vgs+0.62111*Vds+-0.11525345*Lg+-0.14575638);\n",
        "h1_3 = tanh(0.10625471*Vgs+0.09442053*Vds+-0.052119628*Lg+1.1568379);\n",
        "h1_4 = tanh(4.058087*Vgs+-0.7568158*Vds+0.008070099*Lg+-0.4820452);\n",
        "h1_5 = tanh(-1.3065948*Vgs+-0.0492497*Vds+-0.081622526*Lg+0.20351954);\n",
        "h1_6 = tanh(-2.9507525*Vgs+-0.91150546*Vds+-0.06477873*Lg+0.09646383);\n",
        "h1_7 = tanh(-0.5886177*Vgs+0.63788587*Vds+-0.13710928*Lg+0.30260038);\n",
        "h1_8 = tanh(-0.4311161*Vgs+-0.7250705*Vds+-0.06134645*Lg+0.44054285);\n",
        "h1_9 = tanh(0.012132638*Vgs+-0.42545322*Vds+-0.66478956*Lg+0.13182367);\n",
        "h1_10 = tanh(3.289041e-05*Vgs+0.0011367714*Vds+-0.0051904405*Lg+5.4112927e-05);\n",
        "h1_11 = tanh(-0.6007774*Vgs+0.09259224*Vds+0.2170182*Lg+-0.2908748);\n",
        "h1_12 = tanh(0.28018302*Vgs+1.3014064*Vds+-0.13490067*Lg+-0.22006753);\n",
        "h1_13 = tanh(0.01864017*Vgs+-13.0928755*Vds+-0.0037254083*Lg+-0.10935691);\n",
        "h1_14 = tanh(-0.0049708197*Vgs+0.0022613218*Vds+-0.014408149*Lg+0.0011340647);\n",
        "h1_15 = tanh(-0.094654046*Vgs+-0.4174114*Vds+0.18892045*Lg+0.05248958);\n",
        "h1_16 = tanh(-0.25090316*Vgs+-0.11111481*Vds+-0.009133225*Lg+0.08377026);\n",
        "h1_17 = tanh(0.9275306*Vgs+0.40686756*Vds+0.14127344*Lg+-0.059246097);\n",
        "h1_18 = tanh(-0.27084363*Vgs+0.07915911*Vds+-10.836302*Lg+-1.1535788);\n",
        "h1_19 = tanh(1.6852072*Vgs+-0.24846223*Vds+0.049734037*Lg+-0.19625053);\n",
        "\n",
        "h2_0 = tanh(1.1139417*h1_0+-0.40033522*h1_1+0.920759*h1_2+0.47607598*h1_3+0.3978683*h1_4+0.8008105*h1_5+-0.40445566*h1_6+0.8668027*h1_7+0.23365597*h1_8+-0.28254375*h1_9+-0.63985884*h1_10+-0.62523574*h1_11+0.8338383*h1_12+-2.0540943*h1_13+1.0749483*h1_14+-1.1075479*h1_15+0.60926706*h1_16+1.1118286*h1_17+-0.8917559*h1_18+0.029025732*h1_19+0.6622382);\n",
        "h2_1 = tanh(0.37195024*h1_0+-0.761445*h1_1+0.6514153*h1_2+0.6211995*h1_3+-0.063539445*h1_4+0.31260127*h1_5+-0.3529579*h1_6+0.50422627*h1_7+0.18943883*h1_8+-0.38029787*h1_9+-0.3464503*h1_10+-0.48301366*h1_11+0.081978925*h1_12+-0.08305682*h1_13+-0.08420117*h1_14+-0.8029313*h1_15+-0.37052512*h1_16+0.4553502*h1_17+-0.71959645*h1_18+-0.17320661*h1_19+0.6566257);\n",
        "h2_2 = tanh(-0.021933522*h1_0+-0.017245224*h1_1+0.030417712*h1_2+0.2967218*h1_3+-0.048668377*h1_4+0.029245213*h1_5+0.011756416*h1_6+0.004705658*h1_7+-0.042515088*h1_8+0.010462476*h1_9+0.001331279*h1_10+0.1694541*h1_11+-0.010949079*h1_12+-0.004144526*h1_13+0.04856694*h1_14+-0.010465159*h1_15+0.24588406*h1_16+-0.028521152*h1_17+0.12161568*h1_18+0.1753255*h1_19+-0.09125355);\n",
        "h2_3 = tanh(-0.025546636*h1_0+-3.2702503*h1_1+0.46812135*h1_2+-0.25135994*h1_3+3.0725436*h1_4+0.22513995*h1_5+-1.0327209*h1_6+-0.56274736*h1_7+0.4726107*h1_8+0.38182434*h1_9+0.016403453*h1_10+-0.09128962*h1_11+0.20997792*h1_12+-0.4951615*h1_13+0.026481293*h1_14+0.6085288*h1_15+-0.09078652*h1_16+0.36613584*h1_17+0.26257026*h1_18+0.39794916*h1_19+-0.52920526);\n",
        "h2_4 = tanh(-0.44392154*h1_0+-0.5650475*h1_1+0.91716766*h1_2+0.96703196*h1_3+0.4597048*h1_4+1.452921*h1_5+-0.8115141*h1_6+0.92326456*h1_7+0.2862403*h1_8+-0.9441585*h1_9+0.22188367*h1_10+-1.0092766*h1_11+0.5495966*h1_12+0.44870532*h1_13+0.48705962*h1_14+-0.6957133*h1_15+0.27711377*h1_16+1.0138507*h1_17+-0.34337458*h1_18+-0.21548931*h1_19+0.42685974);\n",
        "h2_5 = tanh(0.0003710325*h1_0+0.5746112*h1_1+0.4144258*h1_2+0.04459103*h1_3+1.2373503*h1_4+0.12786175*h1_5+-0.16099685*h1_6+0.9826207*h1_7+-0.09701193*h1_8+-0.4379135*h1_9+-0.0035542254*h1_10+0.04205593*h1_11+0.65820116*h1_12+0.3810506*h1_13+0.0004259507*h1_14+-0.21782044*h1_15+-0.057544652*h1_16+0.24420041*h1_17+-0.10138292*h1_18+-0.2387106*h1_19+0.867847);\n",
        "h2_6 = tanh(-0.0077049686*h1_0+-0.05349668*h1_1+-0.11536639*h1_2+0.06141029*h1_3+-0.034344573*h1_4+0.21672213*h1_5+-0.09835135*h1_6+-0.25849992*h1_7+0.10576329*h1_8+-0.14288934*h1_9+0.027846925*h1_10+-0.20104973*h1_11+0.24784875*h1_12+0.03396087*h1_13+-0.016039118*h1_14+-0.03147038*h1_15+-0.023109786*h1_16+-0.118931994*h1_17+0.18256636*h1_18+0.09981429*h1_19+0.079372324);\n",
        "h2_7 = tanh(-0.009369999*h1_0+-2.4468672*h1_1+-0.41433397*h1_2+-0.6289744*h1_3+-1.8285819*h1_4+0.12905455*h1_5+0.83588725*h1_6+0.11491322*h1_7+0.3871084*h1_8+0.07153052*h1_9+0.013530584*h1_10+0.40614852*h1_11+0.10340061*h1_12+-0.04473306*h1_13+0.0075287875*h1_14+0.5593612*h1_15+0.34401885*h1_16+-0.5538749*h1_17+0.36615464*h1_18+-0.2666981*h1_19+0.20194086);\n",
        "h2_8 = tanh(-0.0017045025*h1_0+-1.1388719*h1_1+0.25932005*h1_2+-0.6482845*h1_3+2.263395*h1_4+0.115192235*h1_5+-0.40946937*h1_6+-0.038117886*h1_7+-0.03703431*h1_8+0.05965774*h1_9+0.0032721795*h1_10+0.2561857*h1_11+0.47324425*h1_12+-0.25200802*h1_13+0.0017081021*h1_14+0.29819545*h1_15+0.061945435*h1_16+0.13516657*h1_17+0.5409335*h1_18+-0.17400871*h1_19+0.11659722);\n",
        "h2_9 = tanh(0.014866875*h1_0+1.0399909*h1_1+1.0108095*h1_2+-0.0918755*h1_3+-0.7634763*h1_4+-0.49749368*h1_5+-0.7020006*h1_6+-0.16384888*h1_7+-0.07833025*h1_8+-0.23745225*h1_9+-0.023095092*h1_10+-0.29244414*h1_11+0.6255917*h1_12+0.41413808*h1_13+-0.019823061*h1_14+-1.0593235*h1_15+-0.42221433*h1_16+0.98025715*h1_17+-0.71444243*h1_18+0.84347373*h1_19+0.28510216);\n",
        "h2_10 = tanh(-0.013515852*h1_0+0.68840384*h1_1+-0.030184174*h1_2+-0.48098114*h1_3+0.08609854*h1_4+-0.50878614*h1_5+0.26547763*h1_6+-0.7151076*h1_7+0.111288495*h1_8+0.53379977*h1_9+0.022258151*h1_10+0.16971351*h1_11+-1.2486296*h1_12+0.9649942*h1_13+0.0028906881*h1_14+0.31582054*h1_15+0.24213083*h1_16+-0.27242163*h1_17+0.11330636*h1_18+0.17038865*h1_19+-0.42089102);\n",
        "h2_11 = tanh(-0.009949424*h1_0+-0.6704206*h1_1+0.13177924*h1_2+0.07316155*h1_3+-0.28207502*h1_4+-0.32401362*h1_5+0.020746209*h1_6+-0.05517531*h1_7+0.10340257*h1_8+0.29381263*h1_9+0.020497978*h1_10+0.031984854*h1_11+0.079552434*h1_12+-7.7884445*h1_13+0.01224806*h1_14+0.8700812*h1_15+-0.16562358*h1_16+0.14370379*h1_17+0.2389181*h1_18+-0.19771136*h1_19+0.33039534);\n",
        "h2_12 = tanh(-0.002431529*h1_0+0.30154988*h1_1+-0.45072728*h1_2+0.27823612*h1_3+-0.25173753*h1_4+-0.58188903*h1_5+0.39836177*h1_6+-0.060329597*h1_7+0.1667837*h1_8+-0.09419194*h1_9+0.0108116735*h1_10+-0.3581154*h1_11+0.15690842*h1_12+-0.41209573*h1_13+-0.0070331707*h1_14+-0.23976392*h1_15+0.16620094*h1_16+-0.034617994*h1_17+0.32754526*h1_18+0.8490298*h1_19+0.026407415);\n",
        "h2_13 = tanh(-0.7385622*h1_0+-0.25174448*h1_1+2.090295*h1_2+0.67499936*h1_3+0.07392262*h1_4+0.5413692*h1_5+-0.8287433*h1_6+2.2810504*h1_7+1.1917778*h1_8+1.3719273*h1_9+0.25762329*h1_10+-2.1111712*h1_11+1.6987114*h1_12+-0.437825*h1_13+1.7649944*h1_14+-2.2254016*h1_15+-2.3555171*h1_16+-0.7732424*h1_17+-1.0756916*h1_18+-0.1367373*h1_19+1.5085722);\n",
        "h2_14 = tanh(-0.002535408*h1_0+-4.2686224*h1_1+-0.045537975*h1_2+0.08043166*h1_3+1.7274952*h1_4+-0.1260494*h1_5+-0.23011239*h1_6+-0.21025337*h1_7+0.48537356*h1_8+0.39049447*h1_9+-0.006565428*h1_10+-0.293028*h1_11+-1.1253971*h1_12+-0.2890831*h1_13+0.0042421576*h1_14+0.4230598*h1_15+-0.17869289*h1_16+-0.034056116*h1_17+0.61135995*h1_18+0.6345532*h1_19+0.73144835);\n",
        "h2_15 = tanh(-0.5239093*h1_0+0.49806875*h1_1+0.53555894*h1_2+1.2645539*h1_3+1.011548*h1_4+0.39209503*h1_5+-0.78194916*h1_6+0.94610345*h1_7+0.9121405*h1_8+-1.4693716*h1_9+-0.58336824*h1_10+-1.1661471*h1_11+-0.16158457*h1_12+0.058512915*h1_13+1.3358645*h1_14+0.5742125*h1_15+-1.3325212*h1_16+1.1804186*h1_17+-1.4877121*h1_18+0.6357802*h1_19+1.1328585);\n",
        "h2_16 = tanh(0.0095406*h1_0+-0.53232694*h1_1+0.1535685*h1_2+0.26108417*h1_3+-0.08375187*h1_4+0.3462123*h1_5+-0.61755395*h1_6+0.0015145333*h1_7+-0.108780764*h1_8+0.2555477*h1_9+0.00023940884*h1_10+-0.26577523*h1_11+-0.27689674*h1_12+0.43049082*h1_13+-0.025424613*h1_14+0.24634649*h1_15+-0.22579487*h1_16+0.37249807*h1_17+-0.8058662*h1_18+-0.77451354*h1_19+0.5821276);\n",
        "y = -0.17525196*h2_0+-0.22995844*h2_1+0.0026147955*h2_2+0.091129646*h2_3+-0.4534783*h2_4+0.4119419*h2_5+-1.4205361e-05*h2_6+-0.11359831*h2_7+0.1762025*h2_8+-0.1002102*h2_9+-0.37161925*h2_10+0.2683793*h2_11+0.051897053*h2_12+0.19354156*h2_13+-0.10134394*h2_14+0.3937854*h2_15+-0.3296008*h2_16+0.3220947;\n",
        "\n",
        "        Id = pow(10, (y/normI + MinI)) ;\n",
        "\n",
        "if (Id <= 1e-15) begin //limit\n",
        "        Id = 1e-15;\n",
        "        //Id = Id;\n",
        "end\n",
        "else begin\n",
        "        Id = Id;\n",
        "end  //limit end\n",
        "\n",
        "\n",
        "        I(g, d) <+ Cgsd*ddt(Vg-Vd) ;\n",
        "        I(g, s) <+ Cgsd*ddt(Vg-Vs) ;\n",
        "\n",
        "if (Vgsraw >= Vgdraw) begin\n",
        "        I(d, s) <+ dir*Id*W ;\n",
        "\n",
        "end\n",
        "\n",
        "else begin\n",
        "        I(d, s) <+ dir*Id*W ;\n",
        "\n",
        "end\n",
        "\n",
        "end\n",
        "\n",
        "endmodule\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "print(verilog_code)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bbMxJW3-Ras7",
        "outputId": "bf846a1d-77da-49e6-857a-ca608c61e728"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "// VerilogA for GB_lib, IWO_verliogA, veriloga\n",
            "//*******************************************************************************\n",
            "//* * School of Electrical and Computer Engineering, Georgia Institute of Technology\n",
            "//* PI: Prof. Shimeng Yu\n",
            "//* All rights reserved.\n",
            "//*\n",
            "//* Copyright of the model is maintained by the developers, and the model is distributed under\n",
            "//* the terms of the Creative Commons Attribution-NonCommercial 4.0 International Public License\n",
            "//* http://creativecommons.org/licenses/by-nc/4.0/legalcode.\n",
            "//*\n",
            "//* THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND\n",
            "//* ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\n",
            "//* WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n",
            "//* DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\n",
            "//* FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\n",
            "//* DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\n",
            "//* SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\n",
            "//* CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\n",
            "//* OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n",
            "//* OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n",
            "//*\n",
            "//* Developer:\n",
            "//*  Gihun Choe gchoe6@gatech.edu\n",
            "//********************************************************************************/\n",
            "\n",
            "`include \"constants.vams\"\n",
            "`include \"disciplines.vams\"\n",
            "\n",
            "\n",
            "module IWO_verliogA(d, g, s);\n",
            "        inout d, g, s;\n",
            "        electrical d, g, s;\n",
            "\n",
            "        //***** parameters L and W ******//\n",
            "        parameter real W = 0.1; //get parameter fom spectre\n",
            "        parameter real L = 0.05; //get parameter fom spectre\n",
            "        parameter real T_stress = 0.0001; //set on cadence as variable\n",
            "        parameter real T_rec = 0.001;     //set on cadence as variable\n",
            "        parameter real Clk_loops = 50;    //set on cadence as variable\n",
            "        parameter real V_ov = 1.7;        //set on cadence as variable\n",
            "        parameter real Temp = 25;  //set on cadence as variable\n",
            "\n",
            "        parameter MinVg = -1.0 ;\n",
            "        parameter normVg = 0.2222222222222222 ;\n",
            "        parameter MinVd = 0.01 ;\n",
            "        parameter normVd = 0.2949852507374631 ;\n",
            "        parameter MinLg = 0.05 ;\n",
            "        parameter normLg = 1.4285714285714286 ;\n",
            "        parameter MinO = 8.15e-15 ;\n",
            "        parameter normO =33613445378151.26;\n",
            "        parameter MinI = -23.98798356587402 ;\n",
            "        parameter normI =0.04615548498417793;\n",
            "\n",
            "        parameter Mint_stress = 0.0001 ;\n",
            "        parameter normt_stress = 1111.111111111111 ;\n",
            "        parameter Mint_rec = 0.0001 ;\n",
            "        parameter normt_rec = 0.07629452739355005 ;\n",
            "        parameter Minclk_loops = 100.0 ;\n",
            "        parameter normclk_loops = 0.0011111111111111111 ;\n",
            "        parameter Minv_ov = 1.0 ;\n",
            "        parameter normv_ov = 1.4285714285714286 ;\n",
            "        parameter Mintemperature = 25.0 ;\n",
            "        parameter normtemperature = 0.016666666666666666 ;\n",
            "        parameter Mindelta_Vth = 0.04321379542061602 ;\n",
            "        parameter normdelta_Vth = 1.10036881607686 ;\n",
            "\n",
            "        real hvth1_0, hvth1_1, hvth1_2, hvth1_3, hvth1_4, hvth1_5, hvth1_6, hvth1_7, hvth1_8, hvth1_9, hvth1_10, hvth1_11, hvth1_12, hvth1_13, hvth1_14, hvth1_15, hvth1_16, hvth1_17, hvth1_18, hvth1_19;\n",
            "        real hvth2_0, hvth2_1, hvth2_2, hvth2_3, hvth2_4, hvth2_5, hvth2_6, hvth2_7, hvth2_8, hvth2_9;\n",
            "\n",
            "        real Vg, Vd, Vs, Vgs, Vds, Lg, Id, Cgg, Cgsd, Vgd;\n",
            "        real Vgsraw, Vgdraw, dir;\n",
            "        real t_stress, v_ov, t_rec, clk_loops, temp, delta_Vth;\n",
            "\n",
            "        real h1_0, h1_1, h1_2, h1_3, h1_4, h1_5, h1_6, h1_7, h1_8, h1_9, h1_10, h1_11, h1_12, h1_13, h1_14, h1_15, h1_16, h1_17, h1_18, h1_19, h1_20, h1_21, h1_22, h1_23, h1_24;\n",
            "        real h2_0, h2_1, h2_2, h2_3, h2_4, h2_5, h2_6, h2_7, h2_8, h2_9, h2_10, h2_11, h2_12, h2_13, h2_14, h2_15, h2_16, y;\n",
            "        real hc1_0, hc1_1, hc1_2, hc1_3, hc1_4, hc1_5, hc1_6, hc1_7, hc1_8, hc1_9;\n",
            "        real hc1_10, hc1_11, hc1_12, hc1_13, hc1_14, hc1_15, hc1_16;\n",
            "        real hc2_0, hc2_1, hc2_2, hc2_3, hc2_4, hc2_5, hc2_6, hc2_7, hc2_8, hc2_9;\n",
            "        real hc2_10, hc2_11, hc2_12, hc2_13, hc2_14, hc2_15, hc2_16, yc, yvth;\n",
            "\n",
            "analog begin\n",
            "\n",
            "t_stress = (T_stress - Mint_stress)*normt_stress;\n",
            "v_ov = (V_ov - Minv_ov)*normv_ov;\n",
            "t_rec = (T_rec - Mint_rec)*normt_rec ;\n",
            "clk_loops = (Clk_loops - Minclk_loops)*normclk_loops ;\n",
            "temp = (Temp - Mintemperature)*normtemperature ;\n",
            "\n",
            "//******************** delta_Vth NN **********************************//\n",
            "\n",
            "hvth1_0 = tanh(0.272744*t_stress+-0.23056228*t_rec+0.596634*clk_loops+-0.24343318*v_ov+-0.13230233*temp+0.048297387);\n",
            "hvth1_1 = tanh(0.245181*t_stress+0.21064457*t_rec+-0.07516779*clk_loops+0.40903565*v_ov+0.3082128*temp+0.059504695);\n",
            "hvth1_2 = tanh(0.32406178*t_stress+0.0961214*t_rec+0.01575127*clk_loops+-0.009446217*v_ov+-0.23892635*temp+-0.028230919);\n",
            "hvth1_3 = tanh(-0.36218816*t_stress+0.1578473*t_rec+-0.27259*clk_loops+-0.052113354*v_ov+0.11496024*temp+0.06155471);\n",
            "hvth1_4 = tanh(-0.13945356*t_stress+-0.021458378*t_rec+-0.40719503*clk_loops+-0.03505379*v_ov+-0.34112245*temp+0.035970967);\n",
            "hvth1_5 = tanh(0.42872298*t_stress+-0.45401916*t_rec+0.29580662*clk_loops+0.14504935*v_ov+-0.09959237*temp+-0.06627877);\n",
            "hvth1_6 = tanh(0.23034811*t_stress+-0.35950583*t_rec+0.11901722*clk_loops+0.11415793*v_ov+0.13242759*temp+0.05198286);\n",
            "hvth1_7 = tanh(0.06712985*t_stress+-0.062374733*t_rec+-0.28906003*clk_loops+0.024265185*v_ov+-0.06899681*temp+-0.034705576);\n",
            "hvth1_8 = tanh(-0.3795196*t_stress+0.043707807*t_rec+0.17164321*clk_loops+0.09797319*v_ov+0.06505784*temp+-0.031417992);\n",
            "hvth1_9 = tanh(0.28494462*t_stress+0.36375752*t_rec+0.093011215*clk_loops+0.31761396*v_ov+-0.5277346*temp+-0.078400396);\n",
            "hvth1_10 = tanh(-0.013580938*t_stress+-0.43107393*t_rec+0.28892088*clk_loops+0.22873798*v_ov+0.25575206*temp+-0.023186358);\n",
            "hvth1_11 = tanh(0.2528077*t_stress+-0.20586371*t_rec+0.13156222*clk_loops+-0.3816571*v_ov+0.30369946*temp+-0.04340843);\n",
            "hvth1_12 = tanh(0.18532023*t_stress+0.24751016*t_rec+0.08201023*clk_loops+-0.25347483*v_ov+-0.4244604*temp+0.004803747);\n",
            "hvth1_13 = tanh(-0.022441879*t_stress+0.033056706*t_rec+-0.38571253*clk_loops+-0.23792677*v_ov+0.3504729*temp+-0.07746615);\n",
            "hvth1_14 = tanh(0.10063724*t_stress+-0.45871386*t_rec+0.23451798*clk_loops+-0.03935696*v_ov+0.036486935*temp+0.037222292);\n",
            "hvth1_15 = tanh(-0.16819571*t_stress+-0.4405411*t_rec+-0.114860095*clk_loops+0.0030280957*v_ov+0.30049363*temp+-0.043794256);\n",
            "hvth1_16 = tanh(-0.06282347*t_stress+0.030551529*t_rec+-0.51957816*clk_loops+-0.47829595*v_ov+0.3119373*temp+-0.096701674);\n",
            "hvth1_17 = tanh(0.10372053*t_stress+0.084576525*t_rec+-0.22177602*clk_loops+-0.26837832*v_ov+0.54554504*temp+0.09300423);\n",
            "hvth1_18 = tanh(-0.14441638*t_stress+-0.41340104*t_rec+-0.05324316*clk_loops+0.32211384*v_ov+0.18434624*temp+0.03877211);\n",
            "hvth1_19 = tanh(-0.42406812*t_stress+-0.40645334*t_rec+0.015821397*clk_loops+0.39822432*v_ov+0.011161399*temp+0.019219667);\n",
            "\n",
            "yvth = 0.36008114*hvth1_0+0.45734045*hvth1_1+-0.18510826*hvth1_2+0.4497314*hvth1_3+0.3650353*hvth1_4+-0.36643317*hvth1_5+0.20361336*hvth1_6+-0.580298*hvth1_7+-0.029452145*hvth1_8+-0.21224296*hvth1_9+-0.311336*hvth1_10+-0.12667896*hvth1_11+0.0036873992*hvth1_12+-0.4226459*hvth1_13+0.42364526*hvth1_14+-0.04215827*hvth1_15+-0.05251629*hvth1_16+0.24671972*hvth1_17+0.016773598*hvth1_18+0.296929*hvth1_19+0.04334759;\n",
            "\n",
            "delta_Vth = (yvth - Mindelta_Vth) * normdelta_Vth;\n",
            "\n",
            "\n",
            "        Vg = V(g) ;\n",
            "        Vs = V(s) ;\n",
            "        Vd = V(d) ;\n",
            "        Vgsraw = Vg-Vs ;\n",
            "        Vgdraw = Vg-Vd ;\n",
            "\n",
            "if (Vgsraw >= Vgdraw) begin\n",
            "        Vgs = ((Vg-Vs) - MinVg) * normVg ;\n",
            "        dir = 1;\n",
            "end\n",
            "\n",
            "else begin\n",
            "        Vgs = ((Vg-Vd) - MinVg) * normVg ;\n",
            "        dir = -1;\n",
            "end\n",
            "\n",
            "        Vds = (abs(Vd-Vs) - MinVd) * normVd ;\n",
            "        Vgs = Vgs - delta_Vth ; // BTI Vth variation\n",
            "        Lg = (L -MinLg)*normLg ;\n",
            "\n",
            "\n",
            "\n",
            "//******************** C-V NN **********************************//\n",
            "hc1_0 = tanh(-0.99871427*Vgs+-0.16952373*Vds+0.32118186*Lg+0.41485423);\n",
            "hc1_1 = tanh(0.31587568*Vgs+0.13397887*Vds+-0.4541538*Lg+-0.3630942);\n",
            "hc1_2 = tanh(-0.76281804*Vgs+0.09352969*Vds+1.1961353*Lg+0.3904977);\n",
            "hc1_3 = tanh(-1.115087*Vgs+0.85752946*Vds+-0.11746484*Lg+0.5500279);\n",
            "hc1_4 = tanh(1.0741386*Vgs+0.82041687*Vds+0.19092631*Lg+-0.4009425);\n",
            "hc1_5 = tanh(-0.47921795*Vgs+-0.8749933*Vds+-0.054768667*Lg+0.62785167);\n",
            "hc1_6 = tanh(0.5449184*Vgs+-4.409165*Vds+-0.072947875*Lg+-0.31324536);\n",
            "hc1_7 = tanh(-2.9224303*Vgs+2.7675478*Vds+0.08862238*Lg+0.6493558);\n",
            "hc1_8 = tanh(0.65050656*Vgs+-0.29751927*Vds+0.1571876*Lg+-0.38088372);\n",
            "hc1_9 = tanh(-0.30384183*Vgs+0.5649165*Vds+2.6806898*Lg+0.3197917);\n",
            "hc1_10 = tanh(-0.095988505*Vgs+2.0158541*Vds+-0.42972717*Lg+-0.30388466);\n",
            "hc1_11 = tanh(6.7699738*Vgs+-0.07234483*Vds+-0.013545353*Lg+-1.3694142);\n",
            "hc1_12 = tanh(-0.3404029*Vgs+0.0443459*Vds+0.89597*Lg+0.069993004);\n",
            "hc1_13 = tanh(0.62300175*Vgs+-0.29515797*Vds+1.6753465*Lg+-0.6520838);\n",
            "hc1_14 = tanh(0.37957156*Vgs+0.2237372*Vds+0.08591952*Lg+0.13126835);\n",
            "hc1_15 = tanh(0.19949242*Vgs+-0.26481664*Vds+-0.41059187*Lg+-0.40832308);\n",
            "hc1_16 = tanh(0.98966587*Vgs+-0.24259183*Vds+0.36584845*Lg+-0.8024042);\n",
            "\n",
            "hc2_0 = tanh(-0.91327864*hc1_0+0.4696781*hc1_1+0.3302202*hc1_2+0.11393868*hc1_3+0.45070222*hc1_4+-0.2894044*hc1_5+0.55066*hc1_6+-1.6242687*hc1_7+-0.38140613*hc1_8+0.032771554*hc1_9+0.17647126);\n",
            "hc2_1 = tanh(-1.1663305*hc1_0+-0.523984*hc1_1+-0.90804136*hc1_2+-0.7418044*hc1_3+1.456171*hc1_4+-0.16802542*hc1_5+0.8235596*hc1_6+-2.2246246*hc1_7+-0.40805355*hc1_8+0.7207601*hc1_9+0.4729169);\n",
            "hc2_2 = tanh(-0.69065374*hc1_0+0.40205315*hc1_1+-0.49410668*hc1_2+0.8681325*hc1_3+0.471351*hc1_4+0.46939445*hc1_5+0.45568785*hc1_6+-0.92935294*hc1_7+-0.8209646*hc1_8+0.1158967*hc1_9+-0.075798474);\n",
            "hc2_3 = tanh(0.11535446*hc1_0+-0.06296927*hc1_1+-0.6740435*hc1_2+0.7428315*hc1_3+0.05890677*hc1_4+0.9579441*hc1_5+-0.037319*hc1_6+-0.18491426*hc1_7+-0.02981994*hc1_8+0.038347963*hc1_9+0.039531134);\n",
            "hc2_4 = tanh(0.37817463*hc1_0+-0.6811279*hc1_1+1.1388369*hc1_2+0.19983096*hc1_3+-0.20415118*hc1_4+1.3022176*hc1_5+0.22571652*hc1_6+0.1690611*hc1_7+-0.56475276*hc1_8+-0.4069731*hc1_9+0.99962974);\n",
            "hc2_5 = tanh(0.032873478*hc1_0+-0.05209407*hc1_1+-0.010839908*hc1_2+-0.13892579*hc1_3+-0.050480977*hc1_4+0.0127089145*hc1_5+0.0052771433*hc1_6+0.02029242*hc1_7+-0.08705659*hc1_8+0.0254766*hc1_9+0.025135752);\n",
            "hc2_6 = tanh(-0.34639204*hc1_0+0.06937975*hc1_1+0.18671949*hc1_2+-0.18912783*hc1_3+0.1312504*hc1_4+0.4627272*hc1_5+-0.42590702*hc1_6+-0.10966313*hc1_7+0.66083515*hc1_8+-0.050718334*hc1_9+0.08234678);\n",
            "hc2_7 = tanh(0.90656275*hc1_0+-0.037281644*hc1_1+0.77237594*hc1_2+1.4710428*hc1_3+0.13597831*hc1_4+-0.059844542*hc1_5+-0.7801535*hc1_6+3.7814677*hc1_7+-0.5976644*hc1_8+0.2721995*hc1_9+0.023777716);\n",
            "hc2_8 = tanh(0.39720625*hc1_0+-0.45262313*hc1_1+0.19873238*hc1_2+0.9750888*hc1_3+-0.9427992*hc1_4+0.4487432*hc1_5+-0.3372945*hc1_6+0.33729544*hc1_7+-0.1667088*hc1_8+-0.5707525*hc1_9+0.27954483);\n",
            "hc2_9 = tanh(0.28551984*hc1_0+-0.68350387*hc1_1+0.9916423*hc1_2+-0.8254094*hc1_3+0.09875706*hc1_4+0.47609732*hc1_5+-0.058662917*hc1_6+0.09181381*hc1_7+0.09592329*hc1_8+1.3940467*hc1_9+0.3865768);\n",
            "hc2_10 = tanh(0.098737516*hc1_0+0.060473576*hc1_1+0.42824662*hc1_2+0.15018038*hc1_3+0.082621895*hc1_4+-0.00019039502*hc1_5+-0.3321634*hc1_6+0.7936295*hc1_7+-0.041197542*hc1_8+0.6530619*hc1_9+0.1338804);\n",
            "hc2_11 = tanh(-0.3585284*hc1_0+-0.09956017*hc1_1+0.17224246*hc1_2+-0.016925728*hc1_3+-0.46462816*hc1_4+-0.5649022*hc1_5+1.251695*hc1_6+-0.4303161*hc1_7+0.48546878*hc1_8+0.22958975*hc1_9+-0.27899802);\n",
            "hc2_12 = tanh(0.8565631*hc1_0+-0.7622999*hc1_1+0.32367912*hc1_2+1.4776785*hc1_3+0.2712369*hc1_4+0.2275511*hc1_5+0.39908803*hc1_6+4.2305493*hc1_7+-0.3467536*hc1_8+0.41231114*hc1_9+0.47123823);\n",
            "hc2_13 = tanh(-0.26411077*hc1_0+-0.17583853*hc1_1+0.045439184*hc1_2+-0.13801138*hc1_3+0.03278085*hc1_4+-0.45625108*hc1_5+-0.17905861*hc1_6+0.3060186*hc1_7+-0.3361926*hc1_8+-0.050055273*hc1_9+0.17996444);\n",
            "hc2_14 = tanh(0.016094366*hc1_0+-0.013196736*hc1_1+0.4124856*hc1_2+0.22926371*hc1_3+-0.35071182*hc1_4+-0.34217188*hc1_5+-0.69466996*hc1_6+1.0563152*hc1_7+-0.18019852*hc1_8+0.061871335*hc1_9+0.09762555);\n",
            "hc2_15 = tanh(-0.18970782*hc1_0+0.3624813*hc1_1+-1.3419824*hc1_2+0.103635244*hc1_3+-0.14595217*hc1_4+-0.1899393*hc1_5+0.176524*hc1_6+-0.4428012*hc1_7+-0.39544868*hc1_8+-0.45783517*hc1_9+0.1884755);\n",
            "hc2_16 = tanh(-0.1585831*hc1_0+0.035894837*hc1_1+-0.14261873*hc1_2+0.25914294*hc1_3+0.040607046*hc1_4+0.11555795*hc1_5+0.0022548323*hc1_6+-0.002149359*hc1_7+0.07067297*hc1_8+0.019578662*hc1_9+0.16657573);\n",
            "yc = 0.17503543*hc2_0+-0.15556754*hc2_1+-0.125455*hc2_2+-0.07502612*hc2_3+-0.16671115*hc2_4+0.29854503;\n",
            "\n",
            "Cgg = (yc / normO + MinO)*W;\n",
            "Cgsd = Cgg/2 ;\n",
            "\n",
            "//******************** I-V NN **********************************//\n",
            "h1_0 = tanh(0.0023826673*Vgs+-0.0009636134*Vds+0.006639037*Lg+-0.0004692053);\n",
            "h1_1 = tanh(-7.8007555*Vgs+2.639791*Vds+-0.025273597*Lg+-1.1747514);\n",
            "h1_2 = tanh(1.9884652*Vgs+0.62111*Vds+-0.11525345*Lg+-0.14575638);\n",
            "h1_3 = tanh(0.10625471*Vgs+0.09442053*Vds+-0.052119628*Lg+1.1568379);\n",
            "h1_4 = tanh(4.058087*Vgs+-0.7568158*Vds+0.008070099*Lg+-0.4820452);\n",
            "h1_5 = tanh(-1.3065948*Vgs+-0.0492497*Vds+-0.081622526*Lg+0.20351954);\n",
            "h1_6 = tanh(-2.9507525*Vgs+-0.91150546*Vds+-0.06477873*Lg+0.09646383);\n",
            "h1_7 = tanh(-0.5886177*Vgs+0.63788587*Vds+-0.13710928*Lg+0.30260038);\n",
            "h1_8 = tanh(-0.4311161*Vgs+-0.7250705*Vds+-0.06134645*Lg+0.44054285);\n",
            "h1_9 = tanh(0.012132638*Vgs+-0.42545322*Vds+-0.66478956*Lg+0.13182367);\n",
            "h1_10 = tanh(3.289041e-05*Vgs+0.0011367714*Vds+-0.0051904405*Lg+5.4112927e-05);\n",
            "h1_11 = tanh(-0.6007774*Vgs+0.09259224*Vds+0.2170182*Lg+-0.2908748);\n",
            "h1_12 = tanh(0.28018302*Vgs+1.3014064*Vds+-0.13490067*Lg+-0.22006753);\n",
            "h1_13 = tanh(0.01864017*Vgs+-13.0928755*Vds+-0.0037254083*Lg+-0.10935691);\n",
            "h1_14 = tanh(-0.0049708197*Vgs+0.0022613218*Vds+-0.014408149*Lg+0.0011340647);\n",
            "h1_15 = tanh(-0.094654046*Vgs+-0.4174114*Vds+0.18892045*Lg+0.05248958);\n",
            "h1_16 = tanh(-0.25090316*Vgs+-0.11111481*Vds+-0.009133225*Lg+0.08377026);\n",
            "h1_17 = tanh(0.9275306*Vgs+0.40686756*Vds+0.14127344*Lg+-0.059246097);\n",
            "h1_18 = tanh(-0.27084363*Vgs+0.07915911*Vds+-10.836302*Lg+-1.1535788);\n",
            "h1_19 = tanh(1.6852072*Vgs+-0.24846223*Vds+0.049734037*Lg+-0.19625053);\n",
            "\n",
            "h2_0 = tanh(1.1139417*h1_0+-0.40033522*h1_1+0.920759*h1_2+0.47607598*h1_3+0.3978683*h1_4+0.8008105*h1_5+-0.40445566*h1_6+0.8668027*h1_7+0.23365597*h1_8+-0.28254375*h1_9+-0.63985884*h1_10+-0.62523574*h1_11+0.8338383*h1_12+-2.0540943*h1_13+1.0749483*h1_14+-1.1075479*h1_15+0.60926706*h1_16+1.1118286*h1_17+-0.8917559*h1_18+0.029025732*h1_19+0.6622382);\n",
            "h2_1 = tanh(0.37195024*h1_0+-0.761445*h1_1+0.6514153*h1_2+0.6211995*h1_3+-0.063539445*h1_4+0.31260127*h1_5+-0.3529579*h1_6+0.50422627*h1_7+0.18943883*h1_8+-0.38029787*h1_9+-0.3464503*h1_10+-0.48301366*h1_11+0.081978925*h1_12+-0.08305682*h1_13+-0.08420117*h1_14+-0.8029313*h1_15+-0.37052512*h1_16+0.4553502*h1_17+-0.71959645*h1_18+-0.17320661*h1_19+0.6566257);\n",
            "h2_2 = tanh(-0.021933522*h1_0+-0.017245224*h1_1+0.030417712*h1_2+0.2967218*h1_3+-0.048668377*h1_4+0.029245213*h1_5+0.011756416*h1_6+0.004705658*h1_7+-0.042515088*h1_8+0.010462476*h1_9+0.001331279*h1_10+0.1694541*h1_11+-0.010949079*h1_12+-0.004144526*h1_13+0.04856694*h1_14+-0.010465159*h1_15+0.24588406*h1_16+-0.028521152*h1_17+0.12161568*h1_18+0.1753255*h1_19+-0.09125355);\n",
            "h2_3 = tanh(-0.025546636*h1_0+-3.2702503*h1_1+0.46812135*h1_2+-0.25135994*h1_3+3.0725436*h1_4+0.22513995*h1_5+-1.0327209*h1_6+-0.56274736*h1_7+0.4726107*h1_8+0.38182434*h1_9+0.016403453*h1_10+-0.09128962*h1_11+0.20997792*h1_12+-0.4951615*h1_13+0.026481293*h1_14+0.6085288*h1_15+-0.09078652*h1_16+0.36613584*h1_17+0.26257026*h1_18+0.39794916*h1_19+-0.52920526);\n",
            "h2_4 = tanh(-0.44392154*h1_0+-0.5650475*h1_1+0.91716766*h1_2+0.96703196*h1_3+0.4597048*h1_4+1.452921*h1_5+-0.8115141*h1_6+0.92326456*h1_7+0.2862403*h1_8+-0.9441585*h1_9+0.22188367*h1_10+-1.0092766*h1_11+0.5495966*h1_12+0.44870532*h1_13+0.48705962*h1_14+-0.6957133*h1_15+0.27711377*h1_16+1.0138507*h1_17+-0.34337458*h1_18+-0.21548931*h1_19+0.42685974);\n",
            "h2_5 = tanh(0.0003710325*h1_0+0.5746112*h1_1+0.4144258*h1_2+0.04459103*h1_3+1.2373503*h1_4+0.12786175*h1_5+-0.16099685*h1_6+0.9826207*h1_7+-0.09701193*h1_8+-0.4379135*h1_9+-0.0035542254*h1_10+0.04205593*h1_11+0.65820116*h1_12+0.3810506*h1_13+0.0004259507*h1_14+-0.21782044*h1_15+-0.057544652*h1_16+0.24420041*h1_17+-0.10138292*h1_18+-0.2387106*h1_19+0.867847);\n",
            "h2_6 = tanh(-0.0077049686*h1_0+-0.05349668*h1_1+-0.11536639*h1_2+0.06141029*h1_3+-0.034344573*h1_4+0.21672213*h1_5+-0.09835135*h1_6+-0.25849992*h1_7+0.10576329*h1_8+-0.14288934*h1_9+0.027846925*h1_10+-0.20104973*h1_11+0.24784875*h1_12+0.03396087*h1_13+-0.016039118*h1_14+-0.03147038*h1_15+-0.023109786*h1_16+-0.118931994*h1_17+0.18256636*h1_18+0.09981429*h1_19+0.079372324);\n",
            "h2_7 = tanh(-0.009369999*h1_0+-2.4468672*h1_1+-0.41433397*h1_2+-0.6289744*h1_3+-1.8285819*h1_4+0.12905455*h1_5+0.83588725*h1_6+0.11491322*h1_7+0.3871084*h1_8+0.07153052*h1_9+0.013530584*h1_10+0.40614852*h1_11+0.10340061*h1_12+-0.04473306*h1_13+0.0075287875*h1_14+0.5593612*h1_15+0.34401885*h1_16+-0.5538749*h1_17+0.36615464*h1_18+-0.2666981*h1_19+0.20194086);\n",
            "h2_8 = tanh(-0.0017045025*h1_0+-1.1388719*h1_1+0.25932005*h1_2+-0.6482845*h1_3+2.263395*h1_4+0.115192235*h1_5+-0.40946937*h1_6+-0.038117886*h1_7+-0.03703431*h1_8+0.05965774*h1_9+0.0032721795*h1_10+0.2561857*h1_11+0.47324425*h1_12+-0.25200802*h1_13+0.0017081021*h1_14+0.29819545*h1_15+0.061945435*h1_16+0.13516657*h1_17+0.5409335*h1_18+-0.17400871*h1_19+0.11659722);\n",
            "h2_9 = tanh(0.014866875*h1_0+1.0399909*h1_1+1.0108095*h1_2+-0.0918755*h1_3+-0.7634763*h1_4+-0.49749368*h1_5+-0.7020006*h1_6+-0.16384888*h1_7+-0.07833025*h1_8+-0.23745225*h1_9+-0.023095092*h1_10+-0.29244414*h1_11+0.6255917*h1_12+0.41413808*h1_13+-0.019823061*h1_14+-1.0593235*h1_15+-0.42221433*h1_16+0.98025715*h1_17+-0.71444243*h1_18+0.84347373*h1_19+0.28510216);\n",
            "h2_10 = tanh(-0.013515852*h1_0+0.68840384*h1_1+-0.030184174*h1_2+-0.48098114*h1_3+0.08609854*h1_4+-0.50878614*h1_5+0.26547763*h1_6+-0.7151076*h1_7+0.111288495*h1_8+0.53379977*h1_9+0.022258151*h1_10+0.16971351*h1_11+-1.2486296*h1_12+0.9649942*h1_13+0.0028906881*h1_14+0.31582054*h1_15+0.24213083*h1_16+-0.27242163*h1_17+0.11330636*h1_18+0.17038865*h1_19+-0.42089102);\n",
            "h2_11 = tanh(-0.009949424*h1_0+-0.6704206*h1_1+0.13177924*h1_2+0.07316155*h1_3+-0.28207502*h1_4+-0.32401362*h1_5+0.020746209*h1_6+-0.05517531*h1_7+0.10340257*h1_8+0.29381263*h1_9+0.020497978*h1_10+0.031984854*h1_11+0.079552434*h1_12+-7.7884445*h1_13+0.01224806*h1_14+0.8700812*h1_15+-0.16562358*h1_16+0.14370379*h1_17+0.2389181*h1_18+-0.19771136*h1_19+0.33039534);\n",
            "h2_12 = tanh(-0.002431529*h1_0+0.30154988*h1_1+-0.45072728*h1_2+0.27823612*h1_3+-0.25173753*h1_4+-0.58188903*h1_5+0.39836177*h1_6+-0.060329597*h1_7+0.1667837*h1_8+-0.09419194*h1_9+0.0108116735*h1_10+-0.3581154*h1_11+0.15690842*h1_12+-0.41209573*h1_13+-0.0070331707*h1_14+-0.23976392*h1_15+0.16620094*h1_16+-0.034617994*h1_17+0.32754526*h1_18+0.8490298*h1_19+0.026407415);\n",
            "h2_13 = tanh(-0.7385622*h1_0+-0.25174448*h1_1+2.090295*h1_2+0.67499936*h1_3+0.07392262*h1_4+0.5413692*h1_5+-0.8287433*h1_6+2.2810504*h1_7+1.1917778*h1_8+1.3719273*h1_9+0.25762329*h1_10+-2.1111712*h1_11+1.6987114*h1_12+-0.437825*h1_13+1.7649944*h1_14+-2.2254016*h1_15+-2.3555171*h1_16+-0.7732424*h1_17+-1.0756916*h1_18+-0.1367373*h1_19+1.5085722);\n",
            "h2_14 = tanh(-0.002535408*h1_0+-4.2686224*h1_1+-0.045537975*h1_2+0.08043166*h1_3+1.7274952*h1_4+-0.1260494*h1_5+-0.23011239*h1_6+-0.21025337*h1_7+0.48537356*h1_8+0.39049447*h1_9+-0.006565428*h1_10+-0.293028*h1_11+-1.1253971*h1_12+-0.2890831*h1_13+0.0042421576*h1_14+0.4230598*h1_15+-0.17869289*h1_16+-0.034056116*h1_17+0.61135995*h1_18+0.6345532*h1_19+0.73144835);\n",
            "h2_15 = tanh(-0.5239093*h1_0+0.49806875*h1_1+0.53555894*h1_2+1.2645539*h1_3+1.011548*h1_4+0.39209503*h1_5+-0.78194916*h1_6+0.94610345*h1_7+0.9121405*h1_8+-1.4693716*h1_9+-0.58336824*h1_10+-1.1661471*h1_11+-0.16158457*h1_12+0.058512915*h1_13+1.3358645*h1_14+0.5742125*h1_15+-1.3325212*h1_16+1.1804186*h1_17+-1.4877121*h1_18+0.6357802*h1_19+1.1328585);\n",
            "h2_16 = tanh(0.0095406*h1_0+-0.53232694*h1_1+0.1535685*h1_2+0.26108417*h1_3+-0.08375187*h1_4+0.3462123*h1_5+-0.61755395*h1_6+0.0015145333*h1_7+-0.108780764*h1_8+0.2555477*h1_9+0.00023940884*h1_10+-0.26577523*h1_11+-0.27689674*h1_12+0.43049082*h1_13+-0.025424613*h1_14+0.24634649*h1_15+-0.22579487*h1_16+0.37249807*h1_17+-0.8058662*h1_18+-0.77451354*h1_19+0.5821276);\n",
            "y = -0.17525196*h2_0+-0.22995844*h2_1+0.0026147955*h2_2+0.091129646*h2_3+-0.4534783*h2_4+0.4119419*h2_5+-1.4205361e-05*h2_6+-0.11359831*h2_7+0.1762025*h2_8+-0.1002102*h2_9+-0.37161925*h2_10+0.2683793*h2_11+0.051897053*h2_12+0.19354156*h2_13+-0.10134394*h2_14+0.3937854*h2_15+-0.3296008*h2_16+0.3220947;\n",
            "\n",
            "        Id = pow(10, (y/normI + MinI)) ;\n",
            "\n",
            "if (Id <= 1e-15) begin //limit\n",
            "        Id = 1e-15;\n",
            "        //Id = Id;\n",
            "end\n",
            "else begin\n",
            "        Id = Id;\n",
            "end  //limit end\n",
            "\n",
            "\n",
            "        I(g, d) <+ Cgsd*ddt(Vg-Vd) ;\n",
            "        I(g, s) <+ Cgsd*ddt(Vg-Vs) ;\n",
            "\n",
            "if (Vgsraw >= Vgdraw) begin\n",
            "        I(d, s) <+ dir*Id*W ;\n",
            "\n",
            "end\n",
            "\n",
            "else begin\n",
            "        I(d, s) <+ dir*Id*W ;\n",
            "\n",
            "end\n",
            "\n",
            "end\n",
            "\n",
            "endmodule\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O8ZOvUesBszP"
      },
      "source": [
        "**Deprecated Version (Linear Regression)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ASPvpASs1kUi",
        "outputId": "c5c7ddd0-a242-42d6-a36f-32f8004863b4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [50/5000], Loss: 216.5890\n",
            "Epoch [100/5000], Loss: 35.0012\n",
            "Epoch [150/5000], Loss: 14.2757\n",
            "Epoch [200/5000], Loss: 5.4854\n",
            "Epoch [250/5000], Loss: 2.1224\n",
            "Epoch [300/5000], Loss: 1.0208\n",
            "Epoch [350/5000], Loss: 0.6775\n",
            "Epoch [400/5000], Loss: 0.5532\n",
            "Epoch [450/5000], Loss: 0.4914\n",
            "Epoch [500/5000], Loss: 0.4525\n",
            "Epoch [550/5000], Loss: 0.4252\n",
            "Epoch [600/5000], Loss: 0.4048\n",
            "Epoch [650/5000], Loss: 0.3887\n",
            "Epoch [700/5000], Loss: 0.3751\n",
            "Epoch [750/5000], Loss: 0.3628\n",
            "Epoch [800/5000], Loss: 0.3512\n",
            "Epoch [850/5000], Loss: 0.3399\n",
            "Epoch [900/5000], Loss: 0.3286\n",
            "Epoch [950/5000], Loss: 0.3174\n",
            "Epoch [1000/5000], Loss: 0.3062\n",
            "Epoch [1050/5000], Loss: 0.2950\n",
            "Epoch [1100/5000], Loss: 0.2838\n",
            "Epoch [1150/5000], Loss: 0.2727\n",
            "Epoch [1200/5000], Loss: 0.2616\n",
            "Epoch [1250/5000], Loss: 0.2506\n",
            "Epoch [1300/5000], Loss: 0.2397\n",
            "Epoch [1350/5000], Loss: 0.2289\n",
            "Epoch [1400/5000], Loss: 0.2183\n",
            "Epoch [1450/5000], Loss: 0.2078\n",
            "Epoch [1500/5000], Loss: 0.1976\n",
            "Epoch [1550/5000], Loss: 0.1875\n",
            "Epoch [1600/5000], Loss: 0.1777\n",
            "Epoch [1650/5000], Loss: 0.1682\n",
            "Epoch [1700/5000], Loss: 0.1589\n",
            "Epoch [1750/5000], Loss: 0.1499\n",
            "Epoch [1800/5000], Loss: 0.1411\n",
            "Epoch [1850/5000], Loss: 0.1327\n",
            "Epoch [1900/5000], Loss: 0.1245\n",
            "Epoch [1950/5000], Loss: 0.1167\n",
            "Epoch [2000/5000], Loss: 0.1092\n",
            "Epoch [2050/5000], Loss: 0.1020\n",
            "Epoch [2100/5000], Loss: 0.0951\n",
            "Epoch [2150/5000], Loss: 0.0886\n",
            "Epoch [2200/5000], Loss: 0.0823\n",
            "Epoch [2250/5000], Loss: 0.0764\n",
            "Epoch [2300/5000], Loss: 0.0708\n",
            "Epoch [2350/5000], Loss: 0.0655\n",
            "Epoch [2400/5000], Loss: 0.0606\n",
            "Epoch [2450/5000], Loss: 0.0559\n",
            "Epoch [2500/5000], Loss: 0.0515\n",
            "Epoch [2550/5000], Loss: 0.0475\n",
            "Epoch [2600/5000], Loss: 0.0437\n",
            "Epoch [2650/5000], Loss: 0.0401\n",
            "Epoch [2700/5000], Loss: 0.0369\n",
            "Epoch [2750/5000], Loss: 0.0339\n",
            "Epoch [2800/5000], Loss: 0.0311\n",
            "Epoch [2850/5000], Loss: 0.0286\n",
            "Epoch [2900/5000], Loss: 0.0262\n",
            "Epoch [2950/5000], Loss: 0.0241\n",
            "Epoch [3000/5000], Loss: 0.0222\n",
            "Epoch [3050/5000], Loss: 0.0204\n",
            "Epoch [3100/5000], Loss: 0.0189\n",
            "Epoch [3150/5000], Loss: 0.0175\n",
            "Epoch [3200/5000], Loss: 0.0162\n",
            "Epoch [3250/5000], Loss: 0.0151\n",
            "Epoch [3300/5000], Loss: 0.0141\n",
            "Epoch [3350/5000], Loss: 0.0132\n",
            "Epoch [3400/5000], Loss: 0.0124\n",
            "Epoch [3450/5000], Loss: 0.0117\n",
            "Epoch [3500/5000], Loss: 0.0111\n",
            "Epoch [3550/5000], Loss: 0.0105\n",
            "Epoch [3600/5000], Loss: 0.0101\n",
            "Epoch [3650/5000], Loss: 0.0097\n",
            "Epoch [3700/5000], Loss: 0.0094\n",
            "Epoch [3750/5000], Loss: 0.0091\n",
            "Epoch [3800/5000], Loss: 0.0088\n",
            "Epoch [3850/5000], Loss: 0.0086\n",
            "Epoch [3900/5000], Loss: 0.0084\n",
            "Epoch [3950/5000], Loss: 0.0083\n",
            "Epoch [4000/5000], Loss: 0.0082\n",
            "Epoch [4050/5000], Loss: 0.0081\n",
            "Epoch [4100/5000], Loss: 0.0080\n",
            "Epoch [4150/5000], Loss: 0.0079\n",
            "Epoch [4200/5000], Loss: 0.0078\n",
            "Epoch [4250/5000], Loss: 0.0078\n",
            "Epoch [4300/5000], Loss: 0.0078\n",
            "Epoch [4350/5000], Loss: 0.0077\n",
            "Epoch [4400/5000], Loss: 0.0077\n",
            "Epoch [4450/5000], Loss: 0.0077\n",
            "Epoch [4500/5000], Loss: 0.0077\n",
            "Epoch [4550/5000], Loss: 0.0077\n",
            "Epoch [4600/5000], Loss: 0.0077\n",
            "Epoch [4650/5000], Loss: 0.0077\n",
            "Epoch [4700/5000], Loss: 0.0076\n",
            "Epoch [4750/5000], Loss: 0.0076\n",
            "Epoch [4800/5000], Loss: 0.0076\n",
            "Epoch [4850/5000], Loss: 0.0076\n",
            "Epoch [4900/5000], Loss: 0.0076\n",
            "Epoch [4950/5000], Loss: 0.0076\n",
            "Epoch [5000/5000], Loss: 0.0076\n",
            "Learned linear coefficients (h1, h2, h3, h4, h5, h6): [[3.2403252e-01 1.2588283e-04 1.4926398e-01 1.6216135e-06 5.8017345e-03\n",
            "  3.4838660e-05]]\n",
            "Learned bias: [0.369416]\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "X_tensor = torch.tensor(X, dtype=torch.float32)\n",
        "Y_tensor = torch.tensor(Y, dtype=torch.float32)\n",
        "\n",
        "# Define the model\n",
        "class LinearModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LinearModel, self).__init__()\n",
        "        self.linear = nn.Linear(6, 1)  # 6 inputs (t_stress, t_rec, etc.), 1 output (deltaV)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linear(x)\n",
        "\n",
        "# Instantiate the model\n",
        "model = LinearModel()\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 5000\n",
        "for epoch in range(num_epochs):\n",
        "    # Forward pass\n",
        "    outputs = model(X_tensor)\n",
        "    loss = criterion(outputs, Y_tensor)\n",
        "\n",
        "    # Backward and optimize\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if (epoch+1) % 50 == 0:\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "# Print learned weights and biases\n",
        "print(\"Learned linear coefficients (h1, h2, h3, h4, h5, h6):\", model.linear.weight.data.numpy())\n",
        "print(\"Learned bias:\", model.linear.bias.data.numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rjD077d3381W",
        "outputId": "ead20c1e-027c-46e5-ac60-530137e872f3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Estimated Output delta_V: 0.45497649908065796\n"
          ]
        }
      ],
      "source": [
        "input_array = np.array([[0.0001, 0.0002, 0.5, 100, 1.7, 25]])  # Sample input\n",
        "\n",
        "# Convert input data to a tensor\n",
        "input_tensor = torch.tensor(input_array, dtype=torch.float32)\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# No need to track gradients for making predictions\n",
        "with torch.no_grad():\n",
        "    # Predict the output using the model\n",
        "    predicted_output = model(input_tensor)\n",
        "\n",
        "# Print the predicted delta_V value\n",
        "print(\"Estimated Output delta_V:\", predicted_output.item())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9OZZWfblrhuz",
        "outputId": "3b31f421-93f6-4151-b35e-2e8776ffba7b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 10/100, Loss: 0.048087798058986664\n",
            "Epoch 20/100, Loss: 0.07116486877202988\n",
            "Epoch 30/100, Loss: 0.08174236863851547\n",
            "Epoch 40/100, Loss: 0.10612807422876358\n",
            "Epoch 50/100, Loss: 0.05975770205259323\n",
            "Epoch 60/100, Loss: 0.04767672345042229\n",
            "Epoch 70/100, Loss: 0.06521065533161163\n",
            "Epoch 80/100, Loss: 0.04492233693599701\n",
            "Epoch 90/100, Loss: 0.09233981370925903\n",
            "Epoch 100/100, Loss: 0.052101973444223404\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import numpy as np\n",
        "\n",
        "# Define the MLP neural network class\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MLP, self).__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(5, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)\n",
        "\n",
        "# Example data\n",
        "np.random.seed(42)\n",
        "x_data = np.random.rand(1000, 5)  # 5D Input (t_st, t_rec, t_cycle, Vov, temp)\n",
        "y_data = np.random.rand(1000, 1)  # 1D Output(deltaV)\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "x_tensor = torch.FloatTensor(x_data)\n",
        "y_tensor = torch.FloatTensor(y_data)\n",
        "\n",
        "# Create dataset and dataloader\n",
        "dataset = TensorDataset(x_tensor, y_tensor)\n",
        "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "# Initialize the MLP\n",
        "model = MLP()\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 100\n",
        "for epoch in range(num_epochs):\n",
        "    for inputs, targets in dataloader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}')\n",
        "\n",
        "# Save the trained model\n",
        "torch.save(model.state_dict(), 'mlp_regression_model.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vGO6_LmkvEpO",
        "outputId": "10734133-2889-4575-d0c0-d57ccc05b958"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predicted delta_V: 26.089859008789062V\n"
          ]
        }
      ],
      "source": [
        "input_features = [0.01, 0.01, 0.001, 1.7, 300] # 5D Input (t_st, t_rec, t_cycle, Vov, temp)\n",
        "input_tensor = torch.FloatTensor([input_features])\n",
        "\n",
        "# Inference\n",
        "with torch.no_grad():\n",
        "    output = model(input_tensor)\n",
        "\n",
        "print(\"Predicted delta_V: {}V\".format(output.item()))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "197YwTf3VfAoIKLyiMET0e8Bp9tePnvKt",
      "authorship_tag": "ABX9TyONInXaeTuKKAFgepiD95BI",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}