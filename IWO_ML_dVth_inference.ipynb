{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gyulab/gtee-bti-mlproject/blob/main/IWO_ML_dVth_inference.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "WeS55towxECg"
      },
      "outputs": [],
      "source": [
        "# Dataset Call\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "dVth_xls_data = pd.read_excel(r'/content/drive/MyDrive/Colab_ML_ProfYu/csv_data/ML_BTI_Vth_dataset_MOCKUP_extended.xlsx')\n",
        "#data = dVth_xls_data.iloc[1:, 2:].T.values\n",
        "\n",
        "#[t_stress, t_rec, t_ratio, duty_cycle, clk_loops, V_ov, temperature, delta_Vth]\n",
        "# Function to clean and convert data to only numeric, ignoring non-numeric values\n",
        "def clean_numeric_data(data):\n",
        "    # Initialize an empty list to hold cleaned data\n",
        "    cleaned_data = []\n",
        "\n",
        "    # Iterate over each element in the data\n",
        "    for item in data:\n",
        "        # Try converting each item to float, append if successful\n",
        "        try:\n",
        "            # Convert to float and check if it is not NaN\n",
        "            numeric_value = float(item)\n",
        "            if not np.isnan(numeric_value):\n",
        "                cleaned_data.append(numeric_value)\n",
        "        except ValueError:\n",
        "            # Ignore items that cannot be converted to float\n",
        "            continue\n",
        "\n",
        "    return np.array(cleaned_data)\n",
        "\n",
        "# Apply the cleaning function to each array in your dataset\n",
        "t_stress = clean_numeric_data(dVth_xls_data.iloc[1:, 2].values)\n",
        "t_rec = clean_numeric_data(dVth_xls_data.iloc[1:, 3].values)\n",
        "clk_loops = clean_numeric_data(dVth_xls_data.iloc[1:, 6].values)\n",
        "#duty_cycle = clean_numeric_data(dVth_xls_data.iloc[1:, 5].values)\n",
        "V_ov = clean_numeric_data(dVth_xls_data.iloc[1:, 7].values)\n",
        "temperature = clean_numeric_data(dVth_xls_data.iloc[1:, 8].values)\n",
        "delta_Vth = clean_numeric_data(dVth_xls_data.iloc[1:, 9].values)\n",
        "\n",
        "\n",
        "# t_stress, t_rec, clk_loops, V_ov, temperature\n",
        "#print((np.vstack((t_stress, t_rec, t_ratio)).T))\n",
        "#print(V_ov)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "GIMnZZ9yy7jz"
      },
      "outputs": [],
      "source": [
        "def normaliz(target): #Minmax normalization\n",
        "    Min = min(target)\n",
        "    Val = target-Min\n",
        "    Val = Val\n",
        "    Max = max(Val)\n",
        "    if Max == 0:\n",
        "        Norm = 1\n",
        "        Val = target\n",
        "    else:\n",
        "        Norm = 1/Max\n",
        "    return (Norm, Val, Min)\n",
        "\n",
        "(normt_stress, t_stress_1, Mint_stress) = normaliz(t_stress)\n",
        "(normt_rec, t_rec_1, Mint_rec) = normaliz(t_rec)\n",
        "(normclk_loops, clk_loops_1, Minclk_loops) = normaliz(clk_loops)\n",
        "(normV_ov, V_ov_1, MinV_ov) = normaliz(V_ov)\n",
        "(normtemperature, temperature_1, Mintemperature) = normaliz(temperature)\n",
        "(normdelta_Vth, delta_Vth_1, Mindelta_Vth) = normaliz(delta_Vth)\n",
        "\n",
        "T_stress = normt_stress * t_stress_1\n",
        "T_rec = normt_rec * t_rec_1\n",
        "Clk_loops = normclk_loops * clk_loops_1\n",
        "Vov = normV_ov * V_ov_1\n",
        "Temperature = normtemperature * temperature_1\n",
        "Delta_Vth = normdelta_Vth * delta_Vth_1\n",
        "\n",
        "X = np.vstack((T_stress.astype(float), T_rec.astype(float), Clk_loops.astype(float), Vov.astype(float), Temperature.astype(float))).T\n",
        "Y = Delta_Vth.astype(float)\n",
        "#print(Vov)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "gLtzD7eS3ICV",
        "outputId": "b2cbdb1e-f81c-40f6-bb1d-ee0a24e010a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss (epoch:    1): 0.79334805\n",
            "Loss (epoch:    2): 0.35976149\n",
            "Loss (epoch:    3): 0.20870446\n",
            "Loss (epoch:    4): 0.14258644\n",
            "Loss (epoch:    5): 0.09249035\n",
            "Loss (epoch:    6): 0.06581431\n",
            "Loss (epoch:    7): 0.05319629\n",
            "Loss (epoch:    8): 0.04458164\n",
            "Loss (epoch:    9): 0.03951419\n",
            "Loss (epoch:   10): 0.03675977\n",
            "Loss (epoch:   11): 0.03522578\n",
            "Loss (epoch:   12): 0.03441461\n",
            "Loss (epoch:   13): 0.03390816\n",
            "Loss (epoch:   14): 0.03356373\n",
            "Loss (epoch:   15): 0.03333106\n",
            "Loss (epoch:   16): 0.03317736\n",
            "Loss (epoch:   17): 0.03305805\n",
            "Loss (epoch:   18): 0.03294651\n",
            "Loss (epoch:   19): 0.03284103\n",
            "Loss (epoch:   20): 0.03274351\n",
            "Loss (epoch:   21): 0.03265025\n",
            "Loss (epoch:   22): 0.03255807\n",
            "Loss (epoch:   23): 0.03246754\n",
            "Loss (epoch:   24): 0.03237944\n",
            "Loss (epoch:   25): 0.03229320\n",
            "Loss (epoch:   26): 0.03220827\n",
            "Loss (epoch:   27): 0.03212476\n",
            "Loss (epoch:   28): 0.03204274\n",
            "Loss (epoch:   29): 0.03196196\n",
            "Loss (epoch:   30): 0.03188229\n",
            "Loss (epoch:   31): 0.03180365\n",
            "Loss (epoch:   32): 0.03172599\n",
            "Loss (epoch:   33): 0.03164916\n",
            "Loss (epoch:   34): 0.03157309\n",
            "Loss (epoch:   35): 0.03149771\n",
            "Loss (epoch:   36): 0.03142297\n",
            "Loss (epoch:   37): 0.03134882\n",
            "Loss (epoch:   38): 0.03127520\n",
            "Loss (epoch:   39): 0.03120209\n",
            "Loss (epoch:   40): 0.03112948\n",
            "Loss (epoch:   41): 0.03105732\n",
            "Loss (epoch:   42): 0.03098562\n",
            "Loss (epoch:   43): 0.03091436\n",
            "Loss (epoch:   44): 0.03084353\n",
            "Loss (epoch:   45): 0.03077314\n",
            "Loss (epoch:   46): 0.03070317\n",
            "Loss (epoch:   47): 0.03063364\n",
            "Loss (epoch:   48): 0.03056454\n",
            "Loss (epoch:   49): 0.03049589\n",
            "Loss (epoch:   50): 0.03042768\n",
            "Loss (epoch:   51): 0.03035992\n",
            "Loss (epoch:   52): 0.03029263\n",
            "Loss (epoch:   53): 0.03022579\n",
            "Loss (epoch:   54): 0.03015944\n",
            "Loss (epoch:   55): 0.03009356\n",
            "Loss (epoch:   56): 0.03002817\n",
            "Loss (epoch:   57): 0.02996328\n",
            "Loss (epoch:   58): 0.02989890\n",
            "Loss (epoch:   59): 0.02983503\n",
            "Loss (epoch:   60): 0.02977168\n",
            "Loss (epoch:   61): 0.02970886\n",
            "Loss (epoch:   62): 0.02964656\n",
            "Loss (epoch:   63): 0.02958481\n",
            "Loss (epoch:   64): 0.02952361\n",
            "Loss (epoch:   65): 0.02946295\n",
            "Loss (epoch:   66): 0.02940285\n",
            "Loss (epoch:   67): 0.02934333\n",
            "Loss (epoch:   68): 0.02928437\n",
            "Loss (epoch:   69): 0.02922598\n",
            "Loss (epoch:   70): 0.02916818\n",
            "Loss (epoch:   71): 0.02911095\n",
            "Loss (epoch:   72): 0.02905432\n",
            "Loss (epoch:   73): 0.02899827\n",
            "Loss (epoch:   74): 0.02894282\n",
            "Loss (epoch:   75): 0.02888797\n",
            "Loss (epoch:   76): 0.02883372\n",
            "Loss (epoch:   77): 0.02878007\n",
            "Loss (epoch:   78): 0.02872701\n",
            "Loss (epoch:   79): 0.02867458\n",
            "Loss (epoch:   80): 0.02862274\n",
            "Loss (epoch:   81): 0.02857151\n",
            "Loss (epoch:   82): 0.02852090\n",
            "Loss (epoch:   83): 0.02847089\n",
            "Loss (epoch:   84): 0.02842150\n",
            "Loss (epoch:   85): 0.02837271\n",
            "Loss (epoch:   86): 0.02832453\n",
            "Loss (epoch:   87): 0.02827697\n",
            "Loss (epoch:   88): 0.02823001\n",
            "Loss (epoch:   89): 0.02818365\n",
            "Loss (epoch:   90): 0.02813790\n",
            "Loss (epoch:   91): 0.02809276\n",
            "Loss (epoch:   92): 0.02804822\n",
            "Loss (epoch:   93): 0.02800427\n",
            "Loss (epoch:   94): 0.02796093\n",
            "Loss (epoch:   95): 0.02791818\n",
            "Loss (epoch:   96): 0.02787602\n",
            "Loss (epoch:   97): 0.02783444\n",
            "Loss (epoch:   98): 0.02779346\n",
            "Loss (epoch:   99): 0.02775306\n",
            "Loss (epoch:  100): 0.02771323\n",
            "Loss (epoch:  101): 0.02767398\n",
            "Loss (epoch:  102): 0.02763531\n",
            "Loss (epoch:  103): 0.02759719\n",
            "Loss (epoch:  104): 0.02755964\n",
            "Loss (epoch:  105): 0.02752265\n",
            "Loss (epoch:  106): 0.02748621\n",
            "Loss (epoch:  107): 0.02745032\n",
            "Loss (epoch:  108): 0.02741498\n",
            "Loss (epoch:  109): 0.02738018\n",
            "Loss (epoch:  110): 0.02734591\n",
            "Loss (epoch:  111): 0.02731218\n",
            "Loss (epoch:  112): 0.02727896\n",
            "Loss (epoch:  113): 0.02724627\n",
            "Loss (epoch:  114): 0.02721409\n",
            "Loss (epoch:  115): 0.02718242\n",
            "Loss (epoch:  116): 0.02715126\n",
            "Loss (epoch:  117): 0.02712060\n",
            "Loss (epoch:  118): 0.02709043\n",
            "Loss (epoch:  119): 0.02706074\n",
            "Loss (epoch:  120): 0.02703155\n",
            "Loss (epoch:  121): 0.02700283\n",
            "Loss (epoch:  122): 0.02697459\n",
            "Loss (epoch:  123): 0.02694681\n",
            "Loss (epoch:  124): 0.02691950\n",
            "Loss (epoch:  125): 0.02689263\n",
            "Loss (epoch:  126): 0.02686622\n",
            "Loss (epoch:  127): 0.02684025\n",
            "Loss (epoch:  128): 0.02681473\n",
            "Loss (epoch:  129): 0.02678963\n",
            "Loss (epoch:  130): 0.02676498\n",
            "Loss (epoch:  131): 0.02674073\n",
            "Loss (epoch:  132): 0.02671691\n",
            "Loss (epoch:  133): 0.02669350\n",
            "Loss (epoch:  134): 0.02667050\n",
            "Loss (epoch:  135): 0.02664789\n",
            "Loss (epoch:  136): 0.02662568\n",
            "Loss (epoch:  137): 0.02660387\n",
            "Loss (epoch:  138): 0.02658243\n",
            "Loss (epoch:  139): 0.02656138\n",
            "Loss (epoch:  140): 0.02654070\n",
            "Loss (epoch:  141): 0.02652039\n",
            "Loss (epoch:  142): 0.02650044\n",
            "Loss (epoch:  143): 0.02648084\n",
            "Loss (epoch:  144): 0.02646160\n",
            "Loss (epoch:  145): 0.02644271\n",
            "Loss (epoch:  146): 0.02642416\n",
            "Loss (epoch:  147): 0.02640594\n",
            "Loss (epoch:  148): 0.02638805\n",
            "Loss (epoch:  149): 0.02637049\n",
            "Loss (epoch:  150): 0.02635325\n",
            "Loss (epoch:  151): 0.02633633\n",
            "Loss (epoch:  152): 0.02631971\n",
            "Loss (epoch:  153): 0.02630340\n",
            "Loss (epoch:  154): 0.02628739\n",
            "Loss (epoch:  155): 0.02627167\n",
            "Loss (epoch:  156): 0.02625625\n",
            "Loss (epoch:  157): 0.02624111\n",
            "Loss (epoch:  158): 0.02622624\n",
            "Loss (epoch:  159): 0.02621167\n",
            "Loss (epoch:  160): 0.02619734\n",
            "Loss (epoch:  161): 0.02618330\n",
            "Loss (epoch:  162): 0.02616950\n",
            "Loss (epoch:  163): 0.02615597\n",
            "Loss (epoch:  164): 0.02614268\n",
            "Loss (epoch:  165): 0.02612965\n",
            "Loss (epoch:  166): 0.02611685\n",
            "Loss (epoch:  167): 0.02610429\n",
            "Loss (epoch:  168): 0.02609197\n",
            "Loss (epoch:  169): 0.02607988\n",
            "Loss (epoch:  170): 0.02606801\n",
            "Loss (epoch:  171): 0.02605635\n",
            "Loss (epoch:  172): 0.02604492\n",
            "Loss (epoch:  173): 0.02603369\n",
            "Loss (epoch:  174): 0.02602268\n",
            "Loss (epoch:  175): 0.02601187\n",
            "Loss (epoch:  176): 0.02600125\n",
            "Loss (epoch:  177): 0.02599084\n",
            "Loss (epoch:  178): 0.02598061\n",
            "Loss (epoch:  179): 0.02597057\n",
            "Loss (epoch:  180): 0.02596072\n",
            "Loss (epoch:  181): 0.02595104\n",
            "Loss (epoch:  182): 0.02594154\n",
            "Loss (epoch:  183): 0.02593222\n",
            "Loss (epoch:  184): 0.02592306\n",
            "Loss (epoch:  185): 0.02591407\n",
            "Loss (epoch:  186): 0.02590524\n",
            "Loss (epoch:  187): 0.02589657\n",
            "Loss (epoch:  188): 0.02588806\n",
            "Loss (epoch:  189): 0.02587970\n",
            "Loss (epoch:  190): 0.02587149\n",
            "Loss (epoch:  191): 0.02586343\n",
            "Loss (epoch:  192): 0.02585551\n",
            "Loss (epoch:  193): 0.02584772\n",
            "Loss (epoch:  194): 0.02584008\n",
            "Loss (epoch:  195): 0.02583256\n",
            "Loss (epoch:  196): 0.02582519\n",
            "Loss (epoch:  197): 0.02581793\n",
            "Loss (epoch:  198): 0.02581081\n",
            "Loss (epoch:  199): 0.02580381\n",
            "Loss (epoch:  200): 0.02579693\n",
            "Loss (epoch:  201): 0.02579016\n",
            "Loss (epoch:  202): 0.02578351\n",
            "Loss (epoch:  203): 0.02577698\n",
            "Loss (epoch:  204): 0.02577056\n",
            "Loss (epoch:  205): 0.02576423\n",
            "Loss (epoch:  206): 0.02575803\n",
            "Loss (epoch:  207): 0.02575192\n",
            "Loss (epoch:  208): 0.02574592\n",
            "Loss (epoch:  209): 0.02574001\n",
            "Loss (epoch:  210): 0.02573420\n",
            "Loss (epoch:  211): 0.02572849\n",
            "Loss (epoch:  212): 0.02572287\n",
            "Loss (epoch:  213): 0.02571733\n",
            "Loss (epoch:  214): 0.02571189\n",
            "Loss (epoch:  215): 0.02570654\n",
            "Loss (epoch:  216): 0.02570127\n",
            "Loss (epoch:  217): 0.02569609\n",
            "Loss (epoch:  218): 0.02569099\n",
            "Loss (epoch:  219): 0.02568596\n",
            "Loss (epoch:  220): 0.02568103\n",
            "Loss (epoch:  221): 0.02567615\n",
            "Loss (epoch:  222): 0.02567137\n",
            "Loss (epoch:  223): 0.02566665\n",
            "Loss (epoch:  224): 0.02566200\n",
            "Loss (epoch:  225): 0.02565743\n",
            "Loss (epoch:  226): 0.02565292\n",
            "Loss (epoch:  227): 0.02564849\n",
            "Loss (epoch:  228): 0.02564412\n",
            "Loss (epoch:  229): 0.02563981\n",
            "Loss (epoch:  230): 0.02563557\n",
            "Loss (epoch:  231): 0.02563139\n",
            "Loss (epoch:  232): 0.02562727\n",
            "Loss (epoch:  233): 0.02562322\n",
            "Loss (epoch:  234): 0.02561921\n",
            "Loss (epoch:  235): 0.02561527\n",
            "Loss (epoch:  236): 0.02561139\n",
            "Loss (epoch:  237): 0.02560756\n",
            "Loss (epoch:  238): 0.02560379\n",
            "Loss (epoch:  239): 0.02560007\n",
            "Loss (epoch:  240): 0.02559641\n",
            "Loss (epoch:  241): 0.02559278\n",
            "Loss (epoch:  242): 0.02558922\n",
            "Loss (epoch:  243): 0.02558570\n",
            "Loss (epoch:  244): 0.02558224\n",
            "Loss (epoch:  245): 0.02557882\n",
            "Loss (epoch:  246): 0.02557544\n",
            "Loss (epoch:  247): 0.02557211\n",
            "Loss (epoch:  248): 0.02556884\n",
            "Loss (epoch:  249): 0.02556560\n",
            "Loss (epoch:  250): 0.02556239\n",
            "Loss (epoch:  251): 0.02555924\n",
            "Loss (epoch:  252): 0.02555614\n",
            "Loss (epoch:  253): 0.02555306\n",
            "Loss (epoch:  254): 0.02555004\n",
            "Loss (epoch:  255): 0.02554704\n",
            "Loss (epoch:  256): 0.02554410\n",
            "Loss (epoch:  257): 0.02554119\n",
            "Loss (epoch:  258): 0.02553831\n",
            "Loss (epoch:  259): 0.02553547\n",
            "Loss (epoch:  260): 0.02553267\n",
            "Loss (epoch:  261): 0.02552991\n",
            "Loss (epoch:  262): 0.02552717\n",
            "Loss (epoch:  263): 0.02552448\n",
            "Loss (epoch:  264): 0.02552182\n",
            "Loss (epoch:  265): 0.02551919\n",
            "Loss (epoch:  266): 0.02551659\n",
            "Loss (epoch:  267): 0.02551403\n",
            "Loss (epoch:  268): 0.02551150\n",
            "Loss (epoch:  269): 0.02550900\n",
            "Loss (epoch:  270): 0.02550652\n",
            "Loss (epoch:  271): 0.02550408\n",
            "Loss (epoch:  272): 0.02550167\n",
            "Loss (epoch:  273): 0.02549929\n",
            "Loss (epoch:  274): 0.02549694\n",
            "Loss (epoch:  275): 0.02549461\n",
            "Loss (epoch:  276): 0.02549231\n",
            "Loss (epoch:  277): 0.02549004\n",
            "Loss (epoch:  278): 0.02548780\n",
            "Loss (epoch:  279): 0.02548558\n",
            "Loss (epoch:  280): 0.02548339\n",
            "Loss (epoch:  281): 0.02548122\n",
            "Loss (epoch:  282): 0.02547909\n",
            "Loss (epoch:  283): 0.02547696\n",
            "Loss (epoch:  284): 0.02547487\n",
            "Loss (epoch:  285): 0.02547281\n",
            "Loss (epoch:  286): 0.02547077\n",
            "Loss (epoch:  287): 0.02546875\n",
            "Loss (epoch:  288): 0.02546675\n",
            "Loss (epoch:  289): 0.02546477\n",
            "Loss (epoch:  290): 0.02546283\n",
            "Loss (epoch:  291): 0.02546089\n",
            "Loss (epoch:  292): 0.02545898\n",
            "Loss (epoch:  293): 0.02545709\n",
            "Loss (epoch:  294): 0.02545523\n",
            "Loss (epoch:  295): 0.02545338\n",
            "Loss (epoch:  296): 0.02545156\n",
            "Loss (epoch:  297): 0.02544975\n",
            "Loss (epoch:  298): 0.02544796\n",
            "Loss (epoch:  299): 0.02544620\n",
            "Loss (epoch:  300): 0.02544445\n",
            "Loss (epoch:  301): 0.02544273\n",
            "Loss (epoch:  302): 0.02544101\n",
            "Loss (epoch:  303): 0.02543932\n",
            "Loss (epoch:  304): 0.02543764\n",
            "Loss (epoch:  305): 0.02543599\n",
            "Loss (epoch:  306): 0.02543435\n",
            "Loss (epoch:  307): 0.02543273\n",
            "Loss (epoch:  308): 0.02543112\n",
            "Loss (epoch:  309): 0.02542953\n",
            "Loss (epoch:  310): 0.02542797\n",
            "Loss (epoch:  311): 0.02542641\n",
            "Loss (epoch:  312): 0.02542487\n",
            "Loss (epoch:  313): 0.02542335\n",
            "Loss (epoch:  314): 0.02542184\n",
            "Loss (epoch:  315): 0.02542035\n",
            "Loss (epoch:  316): 0.02541886\n",
            "Loss (epoch:  317): 0.02541740\n",
            "Loss (epoch:  318): 0.02541596\n",
            "Loss (epoch:  319): 0.02541452\n",
            "Loss (epoch:  320): 0.02541310\n",
            "Loss (epoch:  321): 0.02541169\n",
            "Loss (epoch:  322): 0.02541030\n",
            "Loss (epoch:  323): 0.02540893\n",
            "Loss (epoch:  324): 0.02540756\n",
            "Loss (epoch:  325): 0.02540621\n",
            "Loss (epoch:  326): 0.02540487\n",
            "Loss (epoch:  327): 0.02540354\n",
            "Loss (epoch:  328): 0.02540223\n",
            "Loss (epoch:  329): 0.02540093\n",
            "Loss (epoch:  330): 0.02539964\n",
            "Loss (epoch:  331): 0.02539837\n",
            "Loss (epoch:  332): 0.02539711\n",
            "Loss (epoch:  333): 0.02539585\n",
            "Loss (epoch:  334): 0.02539461\n",
            "Loss (epoch:  335): 0.02539339\n",
            "Loss (epoch:  336): 0.02539217\n",
            "Loss (epoch:  337): 0.02539096\n",
            "Loss (epoch:  338): 0.02538977\n",
            "Loss (epoch:  339): 0.02538857\n",
            "Loss (epoch:  340): 0.02538740\n",
            "Loss (epoch:  341): 0.02538624\n",
            "Loss (epoch:  342): 0.02538509\n",
            "Loss (epoch:  343): 0.02538395\n",
            "Loss (epoch:  344): 0.02538281\n",
            "Loss (epoch:  345): 0.02538169\n",
            "Loss (epoch:  346): 0.02538058\n",
            "Loss (epoch:  347): 0.02537947\n",
            "Loss (epoch:  348): 0.02537838\n",
            "Loss (epoch:  349): 0.02537730\n",
            "Loss (epoch:  350): 0.02537622\n",
            "Loss (epoch:  351): 0.02537515\n",
            "Loss (epoch:  352): 0.02537410\n",
            "Loss (epoch:  353): 0.02537305\n",
            "Loss (epoch:  354): 0.02537201\n",
            "Loss (epoch:  355): 0.02537098\n",
            "Loss (epoch:  356): 0.02536995\n",
            "Loss (epoch:  357): 0.02536894\n",
            "Loss (epoch:  358): 0.02536793\n",
            "Loss (epoch:  359): 0.02536694\n",
            "Loss (epoch:  360): 0.02536594\n",
            "Loss (epoch:  361): 0.02536496\n",
            "Loss (epoch:  362): 0.02536399\n",
            "Loss (epoch:  363): 0.02536302\n",
            "Loss (epoch:  364): 0.02536206\n",
            "Loss (epoch:  365): 0.02536111\n",
            "Loss (epoch:  366): 0.02536017\n",
            "Loss (epoch:  367): 0.02535923\n",
            "Loss (epoch:  368): 0.02535830\n",
            "Loss (epoch:  369): 0.02535737\n",
            "Loss (epoch:  370): 0.02535646\n",
            "Loss (epoch:  371): 0.02535555\n",
            "Loss (epoch:  372): 0.02535465\n",
            "Loss (epoch:  373): 0.02535375\n",
            "Loss (epoch:  374): 0.02535286\n",
            "Loss (epoch:  375): 0.02535198\n",
            "Loss (epoch:  376): 0.02535109\n",
            "Loss (epoch:  377): 0.02535023\n",
            "Loss (epoch:  378): 0.02534937\n",
            "Loss (epoch:  379): 0.02534851\n",
            "Loss (epoch:  380): 0.02534766\n",
            "Loss (epoch:  381): 0.02534681\n",
            "Loss (epoch:  382): 0.02534597\n",
            "Loss (epoch:  383): 0.02534514\n",
            "Loss (epoch:  384): 0.02534431\n",
            "Loss (epoch:  385): 0.02534348\n",
            "Loss (epoch:  386): 0.02534266\n",
            "Loss (epoch:  387): 0.02534185\n",
            "Loss (epoch:  388): 0.02534104\n",
            "Loss (epoch:  389): 0.02534024\n",
            "Loss (epoch:  390): 0.02533945\n",
            "Loss (epoch:  391): 0.02533865\n",
            "Loss (epoch:  392): 0.02533787\n",
            "Loss (epoch:  393): 0.02533708\n",
            "Loss (epoch:  394): 0.02533631\n",
            "Loss (epoch:  395): 0.02533554\n",
            "Loss (epoch:  396): 0.02533477\n",
            "Loss (epoch:  397): 0.02533400\n",
            "Loss (epoch:  398): 0.02533324\n",
            "Loss (epoch:  399): 0.02533249\n",
            "Loss (epoch:  400): 0.02533174\n",
            "Loss (epoch:  401): 0.02533099\n",
            "Loss (epoch:  402): 0.02533025\n",
            "Loss (epoch:  403): 0.02532951\n",
            "Loss (epoch:  404): 0.02532879\n",
            "Loss (epoch:  405): 0.02532806\n",
            "Loss (epoch:  406): 0.02532733\n",
            "Loss (epoch:  407): 0.02532661\n",
            "Loss (epoch:  408): 0.02532589\n",
            "Loss (epoch:  409): 0.02532518\n",
            "Loss (epoch:  410): 0.02532448\n",
            "Loss (epoch:  411): 0.02532377\n",
            "Loss (epoch:  412): 0.02532306\n",
            "Loss (epoch:  413): 0.02532237\n",
            "Loss (epoch:  414): 0.02532168\n",
            "Loss (epoch:  415): 0.02532099\n",
            "Loss (epoch:  416): 0.02532029\n",
            "Loss (epoch:  417): 0.02531962\n",
            "Loss (epoch:  418): 0.02531893\n",
            "Loss (epoch:  419): 0.02531825\n",
            "Loss (epoch:  420): 0.02531759\n",
            "Loss (epoch:  421): 0.02531691\n",
            "Loss (epoch:  422): 0.02531625\n",
            "Loss (epoch:  423): 0.02531558\n",
            "Loss (epoch:  424): 0.02531492\n",
            "Loss (epoch:  425): 0.02531427\n",
            "Loss (epoch:  426): 0.02531361\n",
            "Loss (epoch:  427): 0.02531295\n",
            "Loss (epoch:  428): 0.02531230\n",
            "Loss (epoch:  429): 0.02531167\n",
            "Loss (epoch:  430): 0.02531102\n",
            "Loss (epoch:  431): 0.02531038\n",
            "Loss (epoch:  432): 0.02530975\n",
            "Loss (epoch:  433): 0.02530911\n",
            "Loss (epoch:  434): 0.02530847\n",
            "Loss (epoch:  435): 0.02530784\n",
            "Loss (epoch:  436): 0.02530722\n",
            "Loss (epoch:  437): 0.02530659\n",
            "Loss (epoch:  438): 0.02530597\n",
            "Loss (epoch:  439): 0.02530535\n",
            "Loss (epoch:  440): 0.02530473\n",
            "Loss (epoch:  441): 0.02530412\n",
            "Loss (epoch:  442): 0.02530350\n",
            "Loss (epoch:  443): 0.02530290\n",
            "Loss (epoch:  444): 0.02530228\n",
            "Loss (epoch:  445): 0.02530168\n",
            "Loss (epoch:  446): 0.02530107\n",
            "Loss (epoch:  447): 0.02530047\n",
            "Loss (epoch:  448): 0.02529987\n",
            "Loss (epoch:  449): 0.02529928\n",
            "Loss (epoch:  450): 0.02529868\n",
            "Loss (epoch:  451): 0.02529809\n",
            "Loss (epoch:  452): 0.02529749\n",
            "Loss (epoch:  453): 0.02529691\n",
            "Loss (epoch:  454): 0.02529632\n",
            "Loss (epoch:  455): 0.02529574\n",
            "Loss (epoch:  456): 0.02529514\n",
            "Loss (epoch:  457): 0.02529457\n",
            "Loss (epoch:  458): 0.02529399\n",
            "Loss (epoch:  459): 0.02529341\n",
            "Loss (epoch:  460): 0.02529283\n",
            "Loss (epoch:  461): 0.02529225\n",
            "Loss (epoch:  462): 0.02529169\n",
            "Loss (epoch:  463): 0.02529112\n",
            "Loss (epoch:  464): 0.02529054\n",
            "Loss (epoch:  465): 0.02528998\n",
            "Loss (epoch:  466): 0.02528942\n",
            "Loss (epoch:  467): 0.02528884\n",
            "Loss (epoch:  468): 0.02528828\n",
            "Loss (epoch:  469): 0.02528772\n",
            "Loss (epoch:  470): 0.02528716\n",
            "Loss (epoch:  471): 0.02528660\n",
            "Loss (epoch:  472): 0.02528604\n",
            "Loss (epoch:  473): 0.02528549\n",
            "Loss (epoch:  474): 0.02528493\n",
            "Loss (epoch:  475): 0.02528438\n",
            "Loss (epoch:  476): 0.02528383\n",
            "Loss (epoch:  477): 0.02528328\n",
            "Loss (epoch:  478): 0.02528273\n",
            "Loss (epoch:  479): 0.02528218\n",
            "Loss (epoch:  480): 0.02528164\n",
            "Loss (epoch:  481): 0.02528109\n",
            "Loss (epoch:  482): 0.02528055\n",
            "Loss (epoch:  483): 0.02528001\n",
            "Loss (epoch:  484): 0.02527946\n",
            "Loss (epoch:  485): 0.02527892\n",
            "Loss (epoch:  486): 0.02527838\n",
            "Loss (epoch:  487): 0.02527785\n",
            "Loss (epoch:  488): 0.02527730\n",
            "Loss (epoch:  489): 0.02527677\n",
            "Loss (epoch:  490): 0.02527624\n",
            "Loss (epoch:  491): 0.02527571\n",
            "Loss (epoch:  492): 0.02527517\n",
            "Loss (epoch:  493): 0.02527464\n",
            "Loss (epoch:  494): 0.02527411\n",
            "Loss (epoch:  495): 0.02527358\n",
            "Loss (epoch:  496): 0.02527306\n",
            "Loss (epoch:  497): 0.02527252\n",
            "Loss (epoch:  498): 0.02527200\n",
            "Loss (epoch:  499): 0.02527147\n",
            "Loss (epoch:  500): 0.02527094\n",
            "Loss (epoch:  501): 0.02527042\n",
            "Loss (epoch:  502): 0.02526990\n",
            "Loss (epoch:  503): 0.02526937\n",
            "Loss (epoch:  504): 0.02526885\n",
            "Loss (epoch:  505): 0.02526833\n",
            "Loss (epoch:  506): 0.02526781\n",
            "Loss (epoch:  507): 0.02526729\n",
            "Loss (epoch:  508): 0.02526677\n",
            "Loss (epoch:  509): 0.02526626\n",
            "Loss (epoch:  510): 0.02526574\n",
            "Loss (epoch:  511): 0.02526523\n",
            "Loss (epoch:  512): 0.02526470\n",
            "Loss (epoch:  513): 0.02526420\n",
            "Loss (epoch:  514): 0.02526368\n",
            "Loss (epoch:  515): 0.02526317\n",
            "Loss (epoch:  516): 0.02526266\n",
            "Loss (epoch:  517): 0.02526214\n",
            "Loss (epoch:  518): 0.02526163\n",
            "Loss (epoch:  519): 0.02526112\n",
            "Loss (epoch:  520): 0.02526061\n",
            "Loss (epoch:  521): 0.02526010\n",
            "Loss (epoch:  522): 0.02525959\n",
            "Loss (epoch:  523): 0.02525909\n",
            "Loss (epoch:  524): 0.02525857\n",
            "Loss (epoch:  525): 0.02525807\n",
            "Loss (epoch:  526): 0.02525756\n",
            "Loss (epoch:  527): 0.02525705\n",
            "Loss (epoch:  528): 0.02525655\n",
            "Loss (epoch:  529): 0.02525604\n",
            "Loss (epoch:  530): 0.02525554\n",
            "Loss (epoch:  531): 0.02525504\n",
            "Loss (epoch:  532): 0.02525453\n",
            "Loss (epoch:  533): 0.02525403\n",
            "Loss (epoch:  534): 0.02525353\n",
            "Loss (epoch:  535): 0.02525303\n",
            "Loss (epoch:  536): 0.02525252\n",
            "Loss (epoch:  537): 0.02525202\n",
            "Loss (epoch:  538): 0.02525153\n",
            "Loss (epoch:  539): 0.02525102\n",
            "Loss (epoch:  540): 0.02525052\n",
            "Loss (epoch:  541): 0.02525002\n",
            "Loss (epoch:  542): 0.02524952\n",
            "Loss (epoch:  543): 0.02524903\n",
            "Loss (epoch:  544): 0.02524853\n",
            "Loss (epoch:  545): 0.02524803\n",
            "Loss (epoch:  546): 0.02524753\n",
            "Loss (epoch:  547): 0.02524703\n",
            "Loss (epoch:  548): 0.02524654\n",
            "Loss (epoch:  549): 0.02524604\n",
            "Loss (epoch:  550): 0.02524554\n",
            "Loss (epoch:  551): 0.02524505\n",
            "Loss (epoch:  552): 0.02524454\n",
            "Loss (epoch:  553): 0.02524405\n",
            "Loss (epoch:  554): 0.02524356\n",
            "Loss (epoch:  555): 0.02524307\n",
            "Loss (epoch:  556): 0.02524257\n",
            "Loss (epoch:  557): 0.02524207\n",
            "Loss (epoch:  558): 0.02524158\n",
            "Loss (epoch:  559): 0.02524109\n",
            "Loss (epoch:  560): 0.02524059\n",
            "Loss (epoch:  561): 0.02524010\n",
            "Loss (epoch:  562): 0.02523961\n",
            "Loss (epoch:  563): 0.02523911\n",
            "Loss (epoch:  564): 0.02523863\n",
            "Loss (epoch:  565): 0.02523813\n",
            "Loss (epoch:  566): 0.02523764\n",
            "Loss (epoch:  567): 0.02523715\n",
            "Loss (epoch:  568): 0.02523665\n",
            "Loss (epoch:  569): 0.02523616\n",
            "Loss (epoch:  570): 0.02523567\n",
            "Loss (epoch:  571): 0.02523518\n",
            "Loss (epoch:  572): 0.02523469\n",
            "Loss (epoch:  573): 0.02523420\n",
            "Loss (epoch:  574): 0.02523371\n",
            "Loss (epoch:  575): 0.02523322\n",
            "Loss (epoch:  576): 0.02523273\n",
            "Loss (epoch:  577): 0.02523224\n",
            "Loss (epoch:  578): 0.02523174\n",
            "Loss (epoch:  579): 0.02523125\n",
            "Loss (epoch:  580): 0.02523076\n",
            "Loss (epoch:  581): 0.02523027\n",
            "Loss (epoch:  582): 0.02522979\n",
            "Loss (epoch:  583): 0.02522929\n",
            "Loss (epoch:  584): 0.02522880\n",
            "Loss (epoch:  585): 0.02522832\n",
            "Loss (epoch:  586): 0.02522782\n",
            "Loss (epoch:  587): 0.02522733\n",
            "Loss (epoch:  588): 0.02522684\n",
            "Loss (epoch:  589): 0.02522636\n",
            "Loss (epoch:  590): 0.02522586\n",
            "Loss (epoch:  591): 0.02522538\n",
            "Loss (epoch:  592): 0.02522489\n",
            "Loss (epoch:  593): 0.02522440\n",
            "Loss (epoch:  594): 0.02522390\n",
            "Loss (epoch:  595): 0.02522342\n",
            "Loss (epoch:  596): 0.02522294\n",
            "Loss (epoch:  597): 0.02522244\n",
            "Loss (epoch:  598): 0.02522196\n",
            "Loss (epoch:  599): 0.02522147\n",
            "Loss (epoch:  600): 0.02522098\n",
            "Loss (epoch:  601): 0.02522049\n",
            "Loss (epoch:  602): 0.02522000\n",
            "Loss (epoch:  603): 0.02521951\n",
            "Loss (epoch:  604): 0.02521902\n",
            "Loss (epoch:  605): 0.02521854\n",
            "Loss (epoch:  606): 0.02521804\n",
            "Loss (epoch:  607): 0.02521755\n",
            "Loss (epoch:  608): 0.02521706\n",
            "Loss (epoch:  609): 0.02521657\n",
            "Loss (epoch:  610): 0.02521608\n",
            "Loss (epoch:  611): 0.02521559\n",
            "Loss (epoch:  612): 0.02521510\n",
            "Loss (epoch:  613): 0.02521462\n",
            "Loss (epoch:  614): 0.02521412\n",
            "Loss (epoch:  615): 0.02521364\n",
            "Loss (epoch:  616): 0.02521315\n",
            "Loss (epoch:  617): 0.02521266\n",
            "Loss (epoch:  618): 0.02521217\n",
            "Loss (epoch:  619): 0.02521168\n",
            "Loss (epoch:  620): 0.02521119\n",
            "Loss (epoch:  621): 0.02521070\n",
            "Loss (epoch:  622): 0.02521021\n",
            "Loss (epoch:  623): 0.02520973\n",
            "Loss (epoch:  624): 0.02520924\n",
            "Loss (epoch:  625): 0.02520874\n",
            "Loss (epoch:  626): 0.02520825\n",
            "Loss (epoch:  627): 0.02520776\n",
            "Loss (epoch:  628): 0.02520728\n",
            "Loss (epoch:  629): 0.02520678\n",
            "Loss (epoch:  630): 0.02520630\n",
            "Loss (epoch:  631): 0.02520580\n",
            "Loss (epoch:  632): 0.02520531\n",
            "Loss (epoch:  633): 0.02520482\n",
            "Loss (epoch:  634): 0.02520433\n",
            "Loss (epoch:  635): 0.02520384\n",
            "Loss (epoch:  636): 0.02520335\n",
            "Loss (epoch:  637): 0.02520287\n",
            "Loss (epoch:  638): 0.02520236\n",
            "Loss (epoch:  639): 0.02520188\n",
            "Loss (epoch:  640): 0.02520139\n",
            "Loss (epoch:  641): 0.02520089\n",
            "Loss (epoch:  642): 0.02520040\n",
            "Loss (epoch:  643): 0.02519991\n",
            "Loss (epoch:  644): 0.02519941\n",
            "Loss (epoch:  645): 0.02519892\n",
            "Loss (epoch:  646): 0.02519843\n",
            "Loss (epoch:  647): 0.02519794\n",
            "Loss (epoch:  648): 0.02519745\n",
            "Loss (epoch:  649): 0.02519695\n",
            "Loss (epoch:  650): 0.02519646\n",
            "Loss (epoch:  651): 0.02519597\n",
            "Loss (epoch:  652): 0.02519548\n",
            "Loss (epoch:  653): 0.02519498\n",
            "Loss (epoch:  654): 0.02519449\n",
            "Loss (epoch:  655): 0.02519399\n",
            "Loss (epoch:  656): 0.02519350\n",
            "Loss (epoch:  657): 0.02519301\n",
            "Loss (epoch:  658): 0.02519252\n",
            "Loss (epoch:  659): 0.02519202\n",
            "Loss (epoch:  660): 0.02519153\n",
            "Loss (epoch:  661): 0.02519103\n",
            "Loss (epoch:  662): 0.02519054\n",
            "Loss (epoch:  663): 0.02519004\n",
            "Loss (epoch:  664): 0.02518955\n",
            "Loss (epoch:  665): 0.02518905\n",
            "Loss (epoch:  666): 0.02518856\n",
            "Loss (epoch:  667): 0.02518806\n",
            "Loss (epoch:  668): 0.02518757\n",
            "Loss (epoch:  669): 0.02518707\n",
            "Loss (epoch:  670): 0.02518657\n",
            "Loss (epoch:  671): 0.02518607\n",
            "Loss (epoch:  672): 0.02518558\n",
            "Loss (epoch:  673): 0.02518508\n",
            "Loss (epoch:  674): 0.02518459\n",
            "Loss (epoch:  675): 0.02518409\n",
            "Loss (epoch:  676): 0.02518359\n",
            "Loss (epoch:  677): 0.02518309\n",
            "Loss (epoch:  678): 0.02518260\n",
            "Loss (epoch:  679): 0.02518210\n",
            "Loss (epoch:  680): 0.02518160\n",
            "Loss (epoch:  681): 0.02518110\n",
            "Loss (epoch:  682): 0.02518060\n",
            "Loss (epoch:  683): 0.02518011\n",
            "Loss (epoch:  684): 0.02517960\n",
            "Loss (epoch:  685): 0.02517911\n",
            "Loss (epoch:  686): 0.02517861\n",
            "Loss (epoch:  687): 0.02517811\n",
            "Loss (epoch:  688): 0.02517760\n",
            "Loss (epoch:  689): 0.02517711\n",
            "Loss (epoch:  690): 0.02517661\n",
            "Loss (epoch:  691): 0.02517611\n",
            "Loss (epoch:  692): 0.02517561\n",
            "Loss (epoch:  693): 0.02517511\n",
            "Loss (epoch:  694): 0.02517461\n",
            "Loss (epoch:  695): 0.02517410\n",
            "Loss (epoch:  696): 0.02517361\n",
            "Loss (epoch:  697): 0.02517310\n",
            "Loss (epoch:  698): 0.02517260\n",
            "Loss (epoch:  699): 0.02517210\n",
            "Loss (epoch:  700): 0.02517159\n",
            "Loss (epoch:  701): 0.02517110\n",
            "Loss (epoch:  702): 0.02517059\n",
            "Loss (epoch:  703): 0.02517009\n",
            "Loss (epoch:  704): 0.02516959\n",
            "Loss (epoch:  705): 0.02516908\n",
            "Loss (epoch:  706): 0.02516858\n",
            "Loss (epoch:  707): 0.02516808\n",
            "Loss (epoch:  708): 0.02516757\n",
            "Loss (epoch:  709): 0.02516706\n",
            "Loss (epoch:  710): 0.02516656\n",
            "Loss (epoch:  711): 0.02516605\n",
            "Loss (epoch:  712): 0.02516555\n",
            "Loss (epoch:  713): 0.02516505\n",
            "Loss (epoch:  714): 0.02516454\n",
            "Loss (epoch:  715): 0.02516403\n",
            "Loss (epoch:  716): 0.02516353\n",
            "Loss (epoch:  717): 0.02516302\n",
            "Loss (epoch:  718): 0.02516251\n",
            "Loss (epoch:  719): 0.02516200\n",
            "Loss (epoch:  720): 0.02516150\n",
            "Loss (epoch:  721): 0.02516100\n",
            "Loss (epoch:  722): 0.02516049\n",
            "Loss (epoch:  723): 0.02515998\n",
            "Loss (epoch:  724): 0.02515947\n",
            "Loss (epoch:  725): 0.02515896\n",
            "Loss (epoch:  726): 0.02515846\n",
            "Loss (epoch:  727): 0.02515795\n",
            "Loss (epoch:  728): 0.02515743\n",
            "Loss (epoch:  729): 0.02515692\n",
            "Loss (epoch:  730): 0.02515642\n",
            "Loss (epoch:  731): 0.02515591\n",
            "Loss (epoch:  732): 0.02515540\n",
            "Loss (epoch:  733): 0.02515489\n",
            "Loss (epoch:  734): 0.02515438\n",
            "Loss (epoch:  735): 0.02515387\n",
            "Loss (epoch:  736): 0.02515335\n",
            "Loss (epoch:  737): 0.02515285\n",
            "Loss (epoch:  738): 0.02515233\n",
            "Loss (epoch:  739): 0.02515182\n",
            "Loss (epoch:  740): 0.02515131\n",
            "Loss (epoch:  741): 0.02515079\n",
            "Loss (epoch:  742): 0.02515028\n",
            "Loss (epoch:  743): 0.02514978\n",
            "Loss (epoch:  744): 0.02514926\n",
            "Loss (epoch:  745): 0.02514875\n",
            "Loss (epoch:  746): 0.02514823\n",
            "Loss (epoch:  747): 0.02514772\n",
            "Loss (epoch:  748): 0.02514721\n",
            "Loss (epoch:  749): 0.02514669\n",
            "Loss (epoch:  750): 0.02514618\n",
            "Loss (epoch:  751): 0.02514567\n",
            "Loss (epoch:  752): 0.02514515\n",
            "Loss (epoch:  753): 0.02514464\n",
            "Loss (epoch:  754): 0.02514412\n",
            "Loss (epoch:  755): 0.02514361\n",
            "Loss (epoch:  756): 0.02514309\n",
            "Loss (epoch:  757): 0.02514257\n",
            "Loss (epoch:  758): 0.02514205\n",
            "Loss (epoch:  759): 0.02514154\n",
            "Loss (epoch:  760): 0.02514102\n",
            "Loss (epoch:  761): 0.02514051\n",
            "Loss (epoch:  762): 0.02513999\n",
            "Loss (epoch:  763): 0.02513947\n",
            "Loss (epoch:  764): 0.02513896\n",
            "Loss (epoch:  765): 0.02513843\n",
            "Loss (epoch:  766): 0.02513792\n",
            "Loss (epoch:  767): 0.02513740\n",
            "Loss (epoch:  768): 0.02513688\n",
            "Loss (epoch:  769): 0.02513636\n",
            "Loss (epoch:  770): 0.02513584\n",
            "Loss (epoch:  771): 0.02513532\n",
            "Loss (epoch:  772): 0.02513480\n",
            "Loss (epoch:  773): 0.02513429\n",
            "Loss (epoch:  774): 0.02513376\n",
            "Loss (epoch:  775): 0.02513325\n",
            "Loss (epoch:  776): 0.02513272\n",
            "Loss (epoch:  777): 0.02513220\n",
            "Loss (epoch:  778): 0.02513168\n",
            "Loss (epoch:  779): 0.02513116\n",
            "Loss (epoch:  780): 0.02513063\n",
            "Loss (epoch:  781): 0.02513011\n",
            "Loss (epoch:  782): 0.02512959\n",
            "Loss (epoch:  783): 0.02512907\n",
            "Loss (epoch:  784): 0.02512855\n",
            "Loss (epoch:  785): 0.02512802\n",
            "Loss (epoch:  786): 0.02512751\n",
            "Loss (epoch:  787): 0.02512697\n",
            "Loss (epoch:  788): 0.02512646\n",
            "Loss (epoch:  789): 0.02512593\n",
            "Loss (epoch:  790): 0.02512541\n",
            "Loss (epoch:  791): 0.02512488\n",
            "Loss (epoch:  792): 0.02512436\n",
            "Loss (epoch:  793): 0.02512384\n",
            "Loss (epoch:  794): 0.02512331\n",
            "Loss (epoch:  795): 0.02512279\n",
            "Loss (epoch:  796): 0.02512226\n",
            "Loss (epoch:  797): 0.02512174\n",
            "Loss (epoch:  798): 0.02512121\n",
            "Loss (epoch:  799): 0.02512068\n",
            "Loss (epoch:  800): 0.02512016\n",
            "Loss (epoch:  801): 0.02511963\n",
            "Loss (epoch:  802): 0.02511911\n",
            "Loss (epoch:  803): 0.02511858\n",
            "Loss (epoch:  804): 0.02511805\n",
            "Loss (epoch:  805): 0.02511753\n",
            "Loss (epoch:  806): 0.02511700\n",
            "Loss (epoch:  807): 0.02511647\n",
            "Loss (epoch:  808): 0.02511594\n",
            "Loss (epoch:  809): 0.02511542\n",
            "Loss (epoch:  810): 0.02511489\n",
            "Loss (epoch:  811): 0.02511436\n",
            "Loss (epoch:  812): 0.02511383\n",
            "Loss (epoch:  813): 0.02511330\n",
            "Loss (epoch:  814): 0.02511277\n",
            "Loss (epoch:  815): 0.02511225\n",
            "Loss (epoch:  816): 0.02511172\n",
            "Loss (epoch:  817): 0.02511118\n",
            "Loss (epoch:  818): 0.02511066\n",
            "Loss (epoch:  819): 0.02511013\n",
            "Loss (epoch:  820): 0.02510960\n",
            "Loss (epoch:  821): 0.02510907\n",
            "Loss (epoch:  822): 0.02510854\n",
            "Loss (epoch:  823): 0.02510801\n",
            "Loss (epoch:  824): 0.02510748\n",
            "Loss (epoch:  825): 0.02510695\n",
            "Loss (epoch:  826): 0.02510642\n",
            "Loss (epoch:  827): 0.02510590\n",
            "Loss (epoch:  828): 0.02510535\n",
            "Loss (epoch:  829): 0.02510482\n",
            "Loss (epoch:  830): 0.02510429\n",
            "Loss (epoch:  831): 0.02510376\n",
            "Loss (epoch:  832): 0.02510323\n",
            "Loss (epoch:  833): 0.02510270\n",
            "Loss (epoch:  834): 0.02510217\n",
            "Loss (epoch:  835): 0.02510164\n",
            "Loss (epoch:  836): 0.02510111\n",
            "Loss (epoch:  837): 0.02510057\n",
            "Loss (epoch:  838): 0.02510004\n",
            "Loss (epoch:  839): 0.02509950\n",
            "Loss (epoch:  840): 0.02509898\n",
            "Loss (epoch:  841): 0.02509844\n",
            "Loss (epoch:  842): 0.02509791\n",
            "Loss (epoch:  843): 0.02509738\n",
            "Loss (epoch:  844): 0.02509685\n",
            "Loss (epoch:  845): 0.02509631\n",
            "Loss (epoch:  846): 0.02509578\n",
            "Loss (epoch:  847): 0.02509525\n",
            "Loss (epoch:  848): 0.02509471\n",
            "Loss (epoch:  849): 0.02509418\n",
            "Loss (epoch:  850): 0.02509365\n",
            "Loss (epoch:  851): 0.02509311\n",
            "Loss (epoch:  852): 0.02509258\n",
            "Loss (epoch:  853): 0.02509205\n",
            "Loss (epoch:  854): 0.02509151\n",
            "Loss (epoch:  855): 0.02509098\n",
            "Loss (epoch:  856): 0.02509044\n",
            "Loss (epoch:  857): 0.02508991\n",
            "Loss (epoch:  858): 0.02508938\n",
            "Loss (epoch:  859): 0.02508885\n",
            "Loss (epoch:  860): 0.02508831\n",
            "Loss (epoch:  861): 0.02508778\n",
            "Loss (epoch:  862): 0.02508724\n",
            "Loss (epoch:  863): 0.02508671\n",
            "Loss (epoch:  864): 0.02508617\n",
            "Loss (epoch:  865): 0.02508564\n",
            "Loss (epoch:  866): 0.02508511\n",
            "Loss (epoch:  867): 0.02508457\n",
            "Loss (epoch:  868): 0.02508404\n",
            "Loss (epoch:  869): 0.02508350\n",
            "Loss (epoch:  870): 0.02508297\n",
            "Loss (epoch:  871): 0.02508243\n",
            "Loss (epoch:  872): 0.02508191\n",
            "Loss (epoch:  873): 0.02508137\n",
            "Loss (epoch:  874): 0.02508083\n",
            "Loss (epoch:  875): 0.02508030\n",
            "Loss (epoch:  876): 0.02507976\n",
            "Loss (epoch:  877): 0.02507923\n",
            "Loss (epoch:  878): 0.02507870\n",
            "Loss (epoch:  879): 0.02507817\n",
            "Loss (epoch:  880): 0.02507763\n",
            "Loss (epoch:  881): 0.02507710\n",
            "Loss (epoch:  882): 0.02507657\n",
            "Loss (epoch:  883): 0.02507603\n",
            "Loss (epoch:  884): 0.02507550\n",
            "Loss (epoch:  885): 0.02507496\n",
            "Loss (epoch:  886): 0.02507443\n",
            "Loss (epoch:  887): 0.02507390\n",
            "Loss (epoch:  888): 0.02507337\n",
            "Loss (epoch:  889): 0.02507283\n",
            "Loss (epoch:  890): 0.02507230\n",
            "Loss (epoch:  891): 0.02507176\n",
            "Loss (epoch:  892): 0.02507124\n",
            "Loss (epoch:  893): 0.02507071\n",
            "Loss (epoch:  894): 0.02507017\n",
            "Loss (epoch:  895): 0.02506964\n",
            "Loss (epoch:  896): 0.02506911\n",
            "Loss (epoch:  897): 0.02506857\n",
            "Loss (epoch:  898): 0.02506804\n",
            "Loss (epoch:  899): 0.02506751\n",
            "Loss (epoch:  900): 0.02506698\n",
            "Loss (epoch:  901): 0.02506645\n",
            "Loss (epoch:  902): 0.02506592\n",
            "Loss (epoch:  903): 0.02506539\n",
            "Loss (epoch:  904): 0.02506486\n",
            "Loss (epoch:  905): 0.02506433\n",
            "Loss (epoch:  906): 0.02506380\n",
            "Loss (epoch:  907): 0.02506327\n",
            "Loss (epoch:  908): 0.02506274\n",
            "Loss (epoch:  909): 0.02506221\n",
            "Loss (epoch:  910): 0.02506168\n",
            "Loss (epoch:  911): 0.02506116\n",
            "Loss (epoch:  912): 0.02506062\n",
            "Loss (epoch:  913): 0.02506010\n",
            "Loss (epoch:  914): 0.02505957\n",
            "Loss (epoch:  915): 0.02505904\n",
            "Loss (epoch:  916): 0.02505852\n",
            "Loss (epoch:  917): 0.02505799\n",
            "Loss (epoch:  918): 0.02505746\n",
            "Loss (epoch:  919): 0.02505693\n",
            "Loss (epoch:  920): 0.02505641\n",
            "Loss (epoch:  921): 0.02505588\n",
            "Loss (epoch:  922): 0.02505535\n",
            "Loss (epoch:  923): 0.02505483\n",
            "Loss (epoch:  924): 0.02505430\n",
            "Loss (epoch:  925): 0.02505377\n",
            "Loss (epoch:  926): 0.02505326\n",
            "Loss (epoch:  927): 0.02505273\n",
            "Loss (epoch:  928): 0.02505221\n",
            "Loss (epoch:  929): 0.02505168\n",
            "Loss (epoch:  930): 0.02505116\n",
            "Loss (epoch:  931): 0.02505063\n",
            "Loss (epoch:  932): 0.02505012\n",
            "Loss (epoch:  933): 0.02504959\n",
            "Loss (epoch:  934): 0.02504907\n",
            "Loss (epoch:  935): 0.02504854\n",
            "Loss (epoch:  936): 0.02504802\n",
            "Loss (epoch:  937): 0.02504751\n",
            "Loss (epoch:  938): 0.02504699\n",
            "Loss (epoch:  939): 0.02504647\n",
            "Loss (epoch:  940): 0.02504595\n",
            "Loss (epoch:  941): 0.02504543\n",
            "Loss (epoch:  942): 0.02504491\n",
            "Loss (epoch:  943): 0.02504440\n",
            "Loss (epoch:  944): 0.02504388\n",
            "Loss (epoch:  945): 0.02504337\n",
            "Loss (epoch:  946): 0.02504285\n",
            "Loss (epoch:  947): 0.02504233\n",
            "Loss (epoch:  948): 0.02504182\n",
            "Loss (epoch:  949): 0.02504130\n",
            "Loss (epoch:  950): 0.02504079\n",
            "Loss (epoch:  951): 0.02504027\n",
            "Loss (epoch:  952): 0.02503976\n",
            "Loss (epoch:  953): 0.02503925\n",
            "Loss (epoch:  954): 0.02503874\n",
            "Loss (epoch:  955): 0.02503822\n",
            "Loss (epoch:  956): 0.02503772\n",
            "Loss (epoch:  957): 0.02503720\n",
            "Loss (epoch:  958): 0.02503670\n",
            "Loss (epoch:  959): 0.02503618\n",
            "Loss (epoch:  960): 0.02503568\n",
            "Loss (epoch:  961): 0.02503517\n",
            "Loss (epoch:  962): 0.02503467\n",
            "Loss (epoch:  963): 0.02503416\n",
            "Loss (epoch:  964): 0.02503366\n",
            "Loss (epoch:  965): 0.02503315\n",
            "Loss (epoch:  966): 0.02503264\n",
            "Loss (epoch:  967): 0.02503214\n",
            "Loss (epoch:  968): 0.02503164\n",
            "Loss (epoch:  969): 0.02503113\n",
            "Loss (epoch:  970): 0.02503063\n",
            "Loss (epoch:  971): 0.02503013\n",
            "Loss (epoch:  972): 0.02502963\n",
            "Loss (epoch:  973): 0.02502913\n",
            "Loss (epoch:  974): 0.02502863\n",
            "Loss (epoch:  975): 0.02502813\n",
            "Loss (epoch:  976): 0.02502763\n",
            "Loss (epoch:  977): 0.02502714\n",
            "Loss (epoch:  978): 0.02502664\n",
            "Loss (epoch:  979): 0.02502614\n",
            "Loss (epoch:  980): 0.02502565\n",
            "Loss (epoch:  981): 0.02502515\n",
            "Loss (epoch:  982): 0.02502466\n",
            "Loss (epoch:  983): 0.02502417\n",
            "Loss (epoch:  984): 0.02502367\n",
            "Loss (epoch:  985): 0.02502319\n",
            "Loss (epoch:  986): 0.02502270\n",
            "Loss (epoch:  987): 0.02502220\n",
            "Loss (epoch:  988): 0.02502171\n",
            "Loss (epoch:  989): 0.02502123\n",
            "Loss (epoch:  990): 0.02502074\n",
            "Loss (epoch:  991): 0.02502026\n",
            "Loss (epoch:  992): 0.02501977\n",
            "Loss (epoch:  993): 0.02501929\n",
            "Loss (epoch:  994): 0.02501880\n",
            "Loss (epoch:  995): 0.02501832\n",
            "Loss (epoch:  996): 0.02501783\n",
            "Loss (epoch:  997): 0.02501736\n",
            "Loss (epoch:  998): 0.02501687\n",
            "Loss (epoch:  999): 0.02501639\n",
            "Loss (epoch: 1000): 0.02501592\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "from re import L\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import random\n",
        "\n",
        "from torch import optim\n",
        "from torch.utils import data\n",
        "from torch.optim import lr_scheduler\n",
        "from torchvision import datasets, models, transforms\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import statistics\n",
        "import datetime\n",
        "import os\n",
        "import csv\n",
        "import math\n",
        "import time\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_tensor = torch.tensor(X, dtype=torch.float32)\n",
        "Y_tensor = torch.tensor(Y, dtype=torch.float32)\n",
        "x_train, x_test, y_train, y_test = train_test_split(X_tensor, Y_tensor, test_size=0.1, random_state=41)\n",
        "dataset = TensorDataset(x_train, y_train)\n",
        "dataloader = DataLoader(dataset, batch_size = 32)\n",
        "testdataloader = DataLoader(TensorDataset(x_test, y_test))\n",
        "\n",
        "n1 = 20\n",
        "n2 = 10\n",
        "\n",
        "# Define the neural network class\n",
        "class MLP(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MLP, self).__init__()\n",
        "        self.fc1 = torch.nn.Linear(5, n1)\n",
        "        self.fc2 = torch.nn.Linear(n1, n2)\n",
        "        self.fc3 = torch.nn.Linear(n2, 1)\n",
        "        self.dropout = torch.nn.Dropout(0.1)\n",
        "        self.tanh = torch.nn.Tanh()\n",
        "        self.bn1 = torch.nn.BatchNorm1d(n1)\n",
        "        self.bn2 = torch.nn.BatchNorm1d(n2)\n",
        "        self.bn3 = torch.nn.BatchNorm1d(1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        #x = self.bn1(x)\n",
        "        x = self.tanh(x)\n",
        "        #x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        #x = self.bn2(x)\n",
        "        x = self.tanh(x)\n",
        "        #x = self.dropout(x)\n",
        "        x = self.fc3(x)\n",
        "        #x = self.bn3(x)\n",
        "        return x\n",
        "\n",
        "# Create an instance of the MLP class\n",
        "model = MLP()\n",
        "\n",
        "def initialize_weights(m):\n",
        "    if isinstance(m, torch.nn.Linear):\n",
        "        torch.nn.init.xavier_uniform_(m.weight)\n",
        "        if m.bias is not None:\n",
        "            torch.nn.init.constant_(m.bias, 0)\n",
        "\n",
        "model.apply(initialize_weights)\n",
        "\n",
        "loss_function = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5)\n",
        "#torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "nb_epochs = 1000\n",
        "MLoss = []\n",
        "for epoch in range(0, nb_epochs):\n",
        "\n",
        "    current_loss = 0.0\n",
        "    losses = []\n",
        "    # Iterate over the dataloader for training data\n",
        "    for i, data in enumerate(dataloader, 0):\n",
        "        inputs, targets = data\n",
        "        inputs, targets = inputs.float(), targets.float()\n",
        "        targets = targets.reshape((targets.shape[0],1))\n",
        "        #zero the gradients\n",
        "        optimizer.zero_grad()\n",
        "        #perform forward pass\n",
        "        outputs = model(inputs)\n",
        "        L_weight = 3\n",
        "        #compute loss\n",
        "        batch_loss = []\n",
        "        for j in range(inputs.size(0)):\n",
        "            input_j = inputs[j].reshape((1, inputs.shape[1]))\n",
        "            if input_j[0,0]>0.3:\n",
        "                batch_loss.append(L_weight*loss_function(outputs[j], targets[j]))\n",
        "            else:\n",
        "                batch_loss.append(loss_function(outputs[j], targets[j]))\n",
        "        loss = torch.stack(batch_loss).mean()\n",
        "        losses.append(loss.item())\n",
        "        #perform backward pass\n",
        "        loss.backward()\n",
        "        #perform optimization\n",
        "        optimizer.step()\n",
        "        # Print statistics\n",
        "\n",
        "    mean_loss = sum(losses)/len(losses)\n",
        "    scheduler.step(mean_loss)\n",
        "\n",
        "    print('Loss (epoch: %4d): %.8f' %(epoch+1, mean_loss))\n",
        "    current_loss = 0.0\n",
        "    MLoss.append(mean_loss)\n",
        "\n",
        "# Process is complete.\n",
        "#print('Training process has finished.')\n",
        "\n",
        "torch.save(model, 'IWO_idvg.pt')\n",
        "torch.save(model.state_dict(), 'IWO_idvg_state_dict.pt')\n",
        "\n",
        "####### loss vs. epoch #######\n",
        "xloss = list(range(0, nb_epochs))\n",
        "plt.plot(xloss, np.log10(MLoss))\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Tw_20-o48MH",
        "outputId": "db31e181-de04-42e8-d62c-47a31b4a5dee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "// VerilogA for GB_lib, IWO_verliogA, veriloga\n",
            "//*******************************************************************************\n",
            "//* * School of Electrical and Computer Engineering, Georgia Institute of Technology\n",
            "//* PI: Prof. Shimeng Yu\n",
            "//* All rights reserved.\n",
            "//*\n",
            "//* Copyright of the model is maintained by the developers, and the model is distributed under\n",
            "//* the terms of the Creative Commons Attribution-NonCommercial 4.0 International Public License\n",
            "//* http://creativecommons.org/licenses/by-nc/4.0/legalcode.\n",
            "//*\n",
            "//* THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND\n",
            "//* ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\n",
            "//* WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n",
            "//* DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\n",
            "//* FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\n",
            "//* DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\n",
            "//* SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\n",
            "//* CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\n",
            "//* OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n",
            "//* OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n",
            "//*\n",
            "//* Developer:\n",
            "//*  Gihun Choe gchoe6@gatech.edu\n",
            "//********************************************************************************/\n",
            "\n",
            "`include \"constants.vams\"\n",
            "`include \"disciplines.vams\"\n",
            "\n",
            "\n",
            "module IWO_verliogA(d, g, s);\n",
            "        inout d, g, s;\n",
            "        electrical d, g, s;\n",
            "\n",
            "        //***** parameters L and W ******//\n",
            "        parameter real W = 0.1; //get parameter fom spectre\n",
            "        parameter real L = 0.05; //get parameter fom spectre\n",
            "        parameter real T_stress = 0.0001; //set on cadence as variable\n",
            "        parameter real T_rec = 0.001;     //set on cadence as variable\n",
            "        parameter real Clk_loops = 50;    //set on cadence as variable\n",
            "        parameter real V_ov = 1.7;        //set on cadence as variable\n",
            "        parameter real Temp = 25;  //set on cadence as variable\n",
            "\n",
            "        parameter MinVg = -1.0 ;\n",
            "        parameter normVg = 0.2222222222222222 ;\n",
            "        parameter MinVd = 0.01 ;\n",
            "        parameter normVd = 0.2949852507374631 ;\n",
            "        parameter MinLg = 0.05 ;\n",
            "        parameter normLg = 1.4285714285714286 ;\n",
            "        parameter MinO = 8.15e-15 ;\n",
            "        parameter normO =33613445378151.26;\n",
            "        parameter MinI = -23.98798356587402 ;\n",
            "        parameter normI =0.04615548498417793;\n",
            "\n",
            "        parameter Mint_stress = 0.0001 ;\n",
            "        parameter normt_stress = 1111.111111111111 ;\n",
            "        parameter Mint_rec = 0.0001 ;\n",
            "        parameter normt_rec = 0.07629452739355005 ;\n",
            "        parameter Minclk_loops = 100.0 ;\n",
            "        parameter normclk_loops = 0.0011111111111111111 ;\n",
            "        parameter Minv_ov = 1.0 ;\n",
            "        parameter normv_ov = 1.4285714285714286 ;\n",
            "        parameter Mintemperature = 25.0 ;\n",
            "        parameter normtemperature = 0.016666666666666666 ;\n",
            "        parameter Mindelta_Vth = 0.04321379542061602 ;\n",
            "        parameter normdelta_Vth = 1.10036881607686 ;\n",
            "\n",
            "        real hvth1_0, hvth1_1, hvth1_2, hvth1_3, hvth1_4, hvth1_5, hvth1_6, hvth1_7, hvth1_8, hvth1_9, hvth1_10, hvth1_11, hvth1_12, hvth1_13, hvth1_14, hvth1_15, hvth1_16, hvth1_17, hvth1_18, hvth1_19;\n",
            "        real hvth2_0, hvth2_1, hvth2_2, hvth2_3, hvth2_4, hvth2_5, hvth2_6, hvth2_7, hvth2_8, hvth2_9;\n",
            "\n",
            "        real Vg, Vd, Vs, Vgs, Vds, Lg, Id, Cgg, Cgsd, Vgd;\n",
            "        real Vgsraw, Vgdraw, dir;\n",
            "        real t_stress, v_ov, t_rec, clk_loops, temp, delta_Vth;\n",
            "\n",
            "        real h1_0, h1_1, h1_2, h1_3, h1_4, h1_5, h1_6, h1_7, h1_8, h1_9, h1_10, h1_11, h1_12, h1_13, h1_14, h1_15, h1_16, h1_17, h1_18, h1_19, h1_20, h1_21, h1_22, h1_23, h1_24;\n",
            "        real h2_0, h2_1, h2_2, h2_3, h2_4, h2_5, h2_6, h2_7, h2_8, h2_9, h2_10, h2_11, h2_12, h2_13, h2_14, h2_15, h2_16, y;\n",
            "        real hc1_0, hc1_1, hc1_2, hc1_3, hc1_4, hc1_5, hc1_6, hc1_7, hc1_8, hc1_9;\n",
            "        real hc1_10, hc1_11, hc1_12, hc1_13, hc1_14, hc1_15, hc1_16;\n",
            "        real hc2_0, hc2_1, hc2_2, hc2_3, hc2_4, hc2_5, hc2_6, hc2_7, hc2_8, hc2_9;\n",
            "        real hc2_10, hc2_11, hc2_12, hc2_13, hc2_14, hc2_15, hc2_16, yc, yvth;\n",
            "\n",
            "analog begin\n",
            "\n",
            "t_stress = (T_stress - Mint_stress)*normt_stress;\n",
            "v_ov = (V_ov - Minv_ov)*normv_ov;\n",
            "t_rec = (T_rec - Mint_rec)*normt_rec ;\n",
            "clk_loops = (Clk_loops - Minclk_loops)*normclk_loops ;\n",
            "temp = (Temp - Mintemperature)*normtemperature ;\n",
            "\n",
            "//******************** delta_Vth NN **********************************//\n",
            "\n",
            "hvth1_0 = tanh(0.2500816*t_stress+0.3494927*t_rec+-0.027227858*clk_loops+0.19615911*v_ov+-0.12638812*temp+-0.10495603);\n",
            "hvth1_1 = tanh(0.6339284*t_stress+-0.09373678*t_rec+-0.04278805*clk_loops+-0.12487813*v_ov+-0.121251374*temp+-0.0003817874);\n",
            "hvth1_2 = tanh(0.22474506*t_stress+0.28950128*t_rec+0.30563113*clk_loops+-0.15888163*v_ov+0.05780247*temp+0.05209613);\n",
            "hvth1_3 = tanh(0.4275834*t_stress+0.20436187*t_rec+-0.12034748*clk_loops+-0.12077719*v_ov+0.11329896*temp+-0.096425034);\n",
            "hvth1_4 = tanh(0.42832723*t_stress+-0.23254706*t_rec+-0.05830425*clk_loops+-0.2999877*v_ov+0.052400496*temp+0.063853726);\n",
            "hvth1_5 = tanh(0.11975818*t_stress+-0.095110685*t_rec+0.3201763*clk_loops+-0.331158*v_ov+-0.2563358*temp+0.013856548);\n",
            "hvth1_6 = tanh(-0.27721095*t_stress+-0.0615577*t_rec+0.47190538*clk_loops+0.16667077*v_ov+-0.17281374*temp+-0.032367684);\n",
            "hvth1_7 = tanh(-0.08809367*t_stress+-0.43541688*t_rec+-0.08767599*clk_loops+0.2334363*v_ov+-0.11216702*temp+0.04557218);\n",
            "hvth1_8 = tanh(0.11537128*t_stress+0.062496774*t_rec+-0.53504914*clk_loops+-0.51772684*v_ov+-0.6574143*temp+-0.01823639);\n",
            "hvth1_9 = tanh(-0.15813704*t_stress+-0.35906208*t_rec+-0.37548852*clk_loops+0.22874445*v_ov+0.2433172*temp+0.060471945);\n",
            "hvth1_10 = tanh(-0.12911297*t_stress+0.026989972*t_rec+0.37391135*clk_loops+-0.19842574*v_ov+0.19676876*temp+-0.02631906);\n",
            "hvth1_11 = tanh(0.5649682*t_stress+0.13145167*t_rec+-0.37169528*clk_loops+-0.26134253*v_ov+-0.045833785*temp+0.056586515);\n",
            "hvth1_12 = tanh(-0.35522515*t_stress+0.12236696*t_rec+0.21175715*clk_loops+0.30376297*v_ov+-0.16240065*temp+0.005025978);\n",
            "hvth1_13 = tanh(0.18278329*t_stress+-0.05047196*t_rec+-0.0859669*clk_loops+0.08623643*v_ov+-0.29378998*temp+0.024062997);\n",
            "hvth1_14 = tanh(0.32751706*t_stress+0.18366009*t_rec+0.096448384*clk_loops+-0.010355521*v_ov+0.37254447*temp+0.073224336);\n",
            "hvth1_15 = tanh(-0.51555645*t_stress+-0.020809859*t_rec+0.15948933*clk_loops+-0.27451771*v_ov+0.39268005*temp+0.058021);\n",
            "hvth1_16 = tanh(0.26681328*t_stress+-0.09101257*t_rec+0.14451417*clk_loops+-0.072999075*v_ov+0.23352014*temp+0.041466154);\n",
            "hvth1_17 = tanh(-0.2325035*t_stress+-0.12786983*t_rec+-0.33691865*clk_loops+-0.22249557*v_ov+0.455452*temp+-0.08384482);\n",
            "hvth1_18 = tanh(-0.5573306*t_stress+0.4057994*t_rec+0.29177016*clk_loops+0.0020979329*v_ov+-0.20111765*temp+0.027436657);\n",
            "hvth1_19 = tanh(0.6195811*t_stress+0.14640138*t_rec+-0.12379404*clk_loops+0.3302253*v_ov+0.6246317*temp+0.09103866);\n",
            "\n",
            "hvth2_0 = tanh(0.17270666*hvth1_0+-0.34011972*hvth1_1+0.4371278*hvth1_2+0.41912073*hvth1_3+-0.24767792*hvth1_4+0.40022418*hvth1_5+0.1793341*hvth1_6+0.20722637*hvth1_7+0.40731052*hvth1_8+0.15173836*hvth1_9+-0.042169962*hvth1_10+-0.41135603*hvth1_11+0.28384554*hvth1_12+-0.029474933*hvth1_13+-0.12779967*hvth1_14+-0.050527636*hvth1_15+-0.21522577*hvth1_16+0.23777719*hvth1_17+0.15183246*hvth1_18+-0.040784758*hvth1_19+0.0);\n",
            "hvth2_1 = tanh(-0.22859553*hvth1_0+0.24716857*hvth1_1+0.22223951*hvth1_2+0.34368125*hvth1_3+0.42784247*hvth1_4+-0.2456955*hvth1_5+-0.17638066*hvth1_6+-0.35605368*hvth1_7+-0.22429451*hvth1_8+-0.14731048*hvth1_9+0.08837506*hvth1_10+-0.2598452*hvth1_11+-0.06377956*hvth1_12+0.2525853*hvth1_13+0.08960022*hvth1_14+-0.16882923*hvth1_15+-0.40668344*hvth1_16+0.17757858*hvth1_17+-0.24081154*hvth1_18+-0.24997614*hvth1_19+0.0);\n",
            "hvth2_2 = tanh(0.2682705*hvth1_0+-0.32610282*hvth1_1+-0.040697113*hvth1_2+0.3338228*hvth1_3+-0.022291299*hvth1_4+-0.18770692*hvth1_5+-0.2665801*hvth1_6+0.10602869*hvth1_7+-0.28880066*hvth1_8+0.32070723*hvth1_9+0.18742149*hvth1_10+0.2943585*hvth1_11+0.22059354*hvth1_12+-0.35726988*hvth1_13+0.3256968*hvth1_14+0.29186782*hvth1_15+0.41783014*hvth1_16+0.339807*hvth1_17+0.31847143*hvth1_18+0.36603713*hvth1_19+0.0);\n",
            "hvth2_3 = tanh(0.3542863*hvth1_0+-0.379325*hvth1_1+-0.40217283*hvth1_2+0.016445637*hvth1_3+-0.3623793*hvth1_4+-0.3578421*hvth1_5+-0.29185107*hvth1_6+0.30237907*hvth1_7+0.43807966*hvth1_8+-0.09926468*hvth1_9+-0.32253727*hvth1_10+-0.10149643*hvth1_11+-0.26388487*hvth1_12+0.41032237*hvth1_13+0.056470003*hvth1_14+-0.40349096*hvth1_15+0.37542042*hvth1_16+0.3310997*hvth1_17+0.26615426*hvth1_18+-0.09913731*hvth1_19+0.0);\n",
            "hvth2_4 = tanh(-0.24667613*hvth1_0+-0.058163833*hvth1_1+0.06383986*hvth1_2+-0.37177512*hvth1_3+0.0446856*hvth1_4+-0.36173028*hvth1_5+0.21119767*hvth1_6+-0.30145353*hvth1_7+-0.42818713*hvth1_8+-0.25230518*hvth1_9+0.30806705*hvth1_10+-0.32797956*hvth1_11+0.22593711*hvth1_12+0.22140208*hvth1_13+-0.3205072*hvth1_14+0.18510577*hvth1_15+-0.0068621626*hvth1_16+0.06575893*hvth1_17+-0.053846467*hvth1_18+0.023813037*hvth1_19+0.0);\n",
            "hvth2_5 = tanh(-0.41172636*hvth1_0+0.34832785*hvth1_1+-0.11237293*hvth1_2+0.39814532*hvth1_3+0.35799104*hvth1_4+0.25872746*hvth1_5+-0.33624282*hvth1_6+-0.1764498*hvth1_7+-0.24915013*hvth1_8+0.05245177*hvth1_9+-0.41766536*hvth1_10+-0.23651923*hvth1_11+0.4426811*hvth1_12+0.306324*hvth1_13+-0.22403601*hvth1_14+0.41765308*hvth1_15+-0.37815368*hvth1_16+0.27441671*hvth1_17+0.28034282*hvth1_18+0.010528697*hvth1_19+0.0);\n",
            "hvth2_6 = tanh(0.25725627*hvth1_0+-0.37493828*hvth1_1+-0.02329687*hvth1_2+0.33084428*hvth1_3+0.2794393*hvth1_4+0.12967774*hvth1_5+0.43502*hvth1_6+-0.43869346*hvth1_7+0.34158027*hvth1_8+-0.16200604*hvth1_9+-0.17610121*hvth1_10+-0.18896961*hvth1_11+0.29401192*hvth1_12+-0.1460996*hvth1_13+0.23362444*hvth1_14+0.34898478*hvth1_15+-0.21936844*hvth1_16+0.14692828*hvth1_17+-0.11317906*hvth1_18+-0.4358763*hvth1_19+0.0);\n",
            "hvth2_7 = tanh(-0.26494595*hvth1_0+-0.022311399*hvth1_1+-0.07004143*hvth1_2+-0.01637036*hvth1_3+0.28586504*hvth1_4+-0.30775362*hvth1_5+-0.32960692*hvth1_6+-0.27362505*hvth1_7+-0.04034408*hvth1_8+0.31855118*hvth1_9+0.005922592*hvth1_10+0.3370948*hvth1_11+0.41997856*hvth1_12+0.0032313478*hvth1_13+-0.08441568*hvth1_14+0.21488655*hvth1_15+-0.390195*hvth1_16+0.15421157*hvth1_17+0.4205441*hvth1_18+0.17269078*hvth1_19+0.0);\n",
            "hvth2_8 = tanh(-0.42390275*hvth1_0+0.11486004*hvth1_1+0.26284823*hvth1_2+-0.279445*hvth1_3+0.3672366*hvth1_4+-0.38071597*hvth1_5+0.043164607*hvth1_6+-0.108552165*hvth1_7+-0.057544775*hvth1_8+-0.024343172*hvth1_9+0.4045549*hvth1_10+-0.20511211*hvth1_11+-0.37478426*hvth1_12+0.22530521*hvth1_13+-0.13667244*hvth1_14+0.11946465*hvth1_15+0.27932298*hvth1_16+0.13578938*hvth1_17+0.28981158*hvth1_18+0.23301274*hvth1_19+0.0);\n",
            "hvth2_9 = tanh(0.16433956*hvth1_0+0.16434708*hvth1_1+0.06569709*hvth1_2+0.25689733*hvth1_3+-0.027893858*hvth1_4+0.13014443*hvth1_5+-0.28307945*hvth1_6+-0.15091491*hvth1_7+0.32388148*hvth1_8+-0.14006276*hvth1_9+0.27057698*hvth1_10+0.34000888*hvth1_11+0.07253952*hvth1_12+-0.011075571*hvth1_13+0.13443285*hvth1_14+0.3685629*hvth1_15+0.08753129*hvth1_16+-0.3434052*hvth1_17+-0.31684908*hvth1_18+-0.27923536*hvth1_19+0.0);\n",
            "\n",
            "yvth = 0.6739452*hvth2_0+0.52863467*hvth2_1+0.06017954*hvth2_2+0.6665096*hvth2_3+0.5849437*hvth2_4+-0.15776896*hvth2_5+0.48868543*hvth2_6+0.30221155*hvth2_7+0.32078847*hvth2_8+0.36096588*hvth2_9+0.0;\n",
            "\n",
            "delta_Vth = (yvth - Mindelta_Vth) * normdelta_Vth;\n",
            "$strobe(\"dvth=$g\",delta_Vth);\n",
            "\n",
            "\n",
            "        Vg = V(g) ;\n",
            "        Vs = V(s) ;\n",
            "        Vd = V(d) ;\n",
            "        Vgsraw = Vg-Vs ;\n",
            "        Vgdraw = Vg-Vd ;\n",
            "\n",
            "if (Vgsraw >= Vgdraw) begin\n",
            "        Vgs = ((Vg-Vs) - MinVg) * normVg ;\n",
            "        dir = 1;\n",
            "end\n",
            "\n",
            "else begin\n",
            "        Vgs = ((Vg-Vd) - MinVg) * normVg ;\n",
            "        dir = -1;\n",
            "end\n",
            "\n",
            "        Vds = (abs(Vd-Vs) - MinVd) * normVd ;\n",
            "        Vgs = Vgs - delta_Vth ; // BTI Vth variation\n",
            "        Lg = (L -MinLg)*normLg ;\n",
            "\n",
            "\n",
            "\n",
            "//******************** C-V NN **********************************//\n",
            "hc1_0 = tanh(-0.99871427*Vgs+-0.16952373*Vds+0.32118186*Lg+0.41485423);\n",
            "hc1_1 = tanh(0.31587568*Vgs+0.13397887*Vds+-0.4541538*Lg+-0.3630942);\n",
            "hc1_2 = tanh(-0.76281804*Vgs+0.09352969*Vds+1.1961353*Lg+0.3904977);\n",
            "hc1_3 = tanh(-1.115087*Vgs+0.85752946*Vds+-0.11746484*Lg+0.5500279);\n",
            "hc1_4 = tanh(1.0741386*Vgs+0.82041687*Vds+0.19092631*Lg+-0.4009425);\n",
            "hc1_5 = tanh(-0.47921795*Vgs+-0.8749933*Vds+-0.054768667*Lg+0.62785167);\n",
            "hc1_6 = tanh(0.5449184*Vgs+-4.409165*Vds+-0.072947875*Lg+-0.31324536);\n",
            "hc1_7 = tanh(-2.9224303*Vgs+2.7675478*Vds+0.08862238*Lg+0.6493558);\n",
            "hc1_8 = tanh(0.65050656*Vgs+-0.29751927*Vds+0.1571876*Lg+-0.38088372);\n",
            "hc1_9 = tanh(-0.30384183*Vgs+0.5649165*Vds+2.6806898*Lg+0.3197917);\n",
            "hc1_10 = tanh(-0.095988505*Vgs+2.0158541*Vds+-0.42972717*Lg+-0.30388466);\n",
            "hc1_11 = tanh(6.7699738*Vgs+-0.07234483*Vds+-0.013545353*Lg+-1.3694142);\n",
            "hc1_12 = tanh(-0.3404029*Vgs+0.0443459*Vds+0.89597*Lg+0.069993004);\n",
            "hc1_13 = tanh(0.62300175*Vgs+-0.29515797*Vds+1.6753465*Lg+-0.6520838);\n",
            "hc1_14 = tanh(0.37957156*Vgs+0.2237372*Vds+0.08591952*Lg+0.13126835);\n",
            "hc1_15 = tanh(0.19949242*Vgs+-0.26481664*Vds+-0.41059187*Lg+-0.40832308);\n",
            "hc1_16 = tanh(0.98966587*Vgs+-0.24259183*Vds+0.36584845*Lg+-0.8024042);\n",
            "\n",
            "hc2_0 = tanh(-0.91327864*hc1_0+0.4696781*hc1_1+0.3302202*hc1_2+0.11393868*hc1_3+0.45070222*hc1_4+-0.2894044*hc1_5+0.55066*hc1_6+-1.6242687*hc1_7+-0.38140613*hc1_8+0.032771554*hc1_9+0.17647126);\n",
            "hc2_1 = tanh(-1.1663305*hc1_0+-0.523984*hc1_1+-0.90804136*hc1_2+-0.7418044*hc1_3+1.456171*hc1_4+-0.16802542*hc1_5+0.8235596*hc1_6+-2.2246246*hc1_7+-0.40805355*hc1_8+0.7207601*hc1_9+0.4729169);\n",
            "hc2_2 = tanh(-0.69065374*hc1_0+0.40205315*hc1_1+-0.49410668*hc1_2+0.8681325*hc1_3+0.471351*hc1_4+0.46939445*hc1_5+0.45568785*hc1_6+-0.92935294*hc1_7+-0.8209646*hc1_8+0.1158967*hc1_9+-0.075798474);\n",
            "hc2_3 = tanh(0.11535446*hc1_0+-0.06296927*hc1_1+-0.6740435*hc1_2+0.7428315*hc1_3+0.05890677*hc1_4+0.9579441*hc1_5+-0.037319*hc1_6+-0.18491426*hc1_7+-0.02981994*hc1_8+0.038347963*hc1_9+0.039531134);\n",
            "hc2_4 = tanh(0.37817463*hc1_0+-0.6811279*hc1_1+1.1388369*hc1_2+0.19983096*hc1_3+-0.20415118*hc1_4+1.3022176*hc1_5+0.22571652*hc1_6+0.1690611*hc1_7+-0.56475276*hc1_8+-0.4069731*hc1_9+0.99962974);\n",
            "hc2_5 = tanh(0.032873478*hc1_0+-0.05209407*hc1_1+-0.010839908*hc1_2+-0.13892579*hc1_3+-0.050480977*hc1_4+0.0127089145*hc1_5+0.0052771433*hc1_6+0.02029242*hc1_7+-0.08705659*hc1_8+0.0254766*hc1_9+0.025135752);\n",
            "hc2_6 = tanh(-0.34639204*hc1_0+0.06937975*hc1_1+0.18671949*hc1_2+-0.18912783*hc1_3+0.1312504*hc1_4+0.4627272*hc1_5+-0.42590702*hc1_6+-0.10966313*hc1_7+0.66083515*hc1_8+-0.050718334*hc1_9+0.08234678);\n",
            "hc2_7 = tanh(0.90656275*hc1_0+-0.037281644*hc1_1+0.77237594*hc1_2+1.4710428*hc1_3+0.13597831*hc1_4+-0.059844542*hc1_5+-0.7801535*hc1_6+3.7814677*hc1_7+-0.5976644*hc1_8+0.2721995*hc1_9+0.023777716);\n",
            "hc2_8 = tanh(0.39720625*hc1_0+-0.45262313*hc1_1+0.19873238*hc1_2+0.9750888*hc1_3+-0.9427992*hc1_4+0.4487432*hc1_5+-0.3372945*hc1_6+0.33729544*hc1_7+-0.1667088*hc1_8+-0.5707525*hc1_9+0.27954483);\n",
            "hc2_9 = tanh(0.28551984*hc1_0+-0.68350387*hc1_1+0.9916423*hc1_2+-0.8254094*hc1_3+0.09875706*hc1_4+0.47609732*hc1_5+-0.058662917*hc1_6+0.09181381*hc1_7+0.09592329*hc1_8+1.3940467*hc1_9+0.3865768);\n",
            "hc2_10 = tanh(0.098737516*hc1_0+0.060473576*hc1_1+0.42824662*hc1_2+0.15018038*hc1_3+0.082621895*hc1_4+-0.00019039502*hc1_5+-0.3321634*hc1_6+0.7936295*hc1_7+-0.041197542*hc1_8+0.6530619*hc1_9+0.1338804);\n",
            "hc2_11 = tanh(-0.3585284*hc1_0+-0.09956017*hc1_1+0.17224246*hc1_2+-0.016925728*hc1_3+-0.46462816*hc1_4+-0.5649022*hc1_5+1.251695*hc1_6+-0.4303161*hc1_7+0.48546878*hc1_8+0.22958975*hc1_9+-0.27899802);\n",
            "hc2_12 = tanh(0.8565631*hc1_0+-0.7622999*hc1_1+0.32367912*hc1_2+1.4776785*hc1_3+0.2712369*hc1_4+0.2275511*hc1_5+0.39908803*hc1_6+4.2305493*hc1_7+-0.3467536*hc1_8+0.41231114*hc1_9+0.47123823);\n",
            "hc2_13 = tanh(-0.26411077*hc1_0+-0.17583853*hc1_1+0.045439184*hc1_2+-0.13801138*hc1_3+0.03278085*hc1_4+-0.45625108*hc1_5+-0.17905861*hc1_6+0.3060186*hc1_7+-0.3361926*hc1_8+-0.050055273*hc1_9+0.17996444);\n",
            "hc2_14 = tanh(0.016094366*hc1_0+-0.013196736*hc1_1+0.4124856*hc1_2+0.22926371*hc1_3+-0.35071182*hc1_4+-0.34217188*hc1_5+-0.69466996*hc1_6+1.0563152*hc1_7+-0.18019852*hc1_8+0.061871335*hc1_9+0.09762555);\n",
            "hc2_15 = tanh(-0.18970782*hc1_0+0.3624813*hc1_1+-1.3419824*hc1_2+0.103635244*hc1_3+-0.14595217*hc1_4+-0.1899393*hc1_5+0.176524*hc1_6+-0.4428012*hc1_7+-0.39544868*hc1_8+-0.45783517*hc1_9+0.1884755);\n",
            "hc2_16 = tanh(-0.1585831*hc1_0+0.035894837*hc1_1+-0.14261873*hc1_2+0.25914294*hc1_3+0.040607046*hc1_4+0.11555795*hc1_5+0.0022548323*hc1_6+-0.002149359*hc1_7+0.07067297*hc1_8+0.019578662*hc1_9+0.16657573);\n",
            "yc = 0.17503543*hc2_0+-0.15556754*hc2_1+-0.125455*hc2_2+-0.07502612*hc2_3+-0.16671115*hc2_4+0.29854503;\n",
            "\n",
            "Cgg = (yc / normO + MinO)*W;\n",
            "Cgsd = Cgg/2 ;\n",
            "\n",
            "//******************** I-V NN **********************************//\n",
            "h1_0 = tanh(0.0023826673*Vgs+-0.0009636134*Vds+0.006639037*Lg+-0.0004692053);\n",
            "h1_1 = tanh(-7.8007555*Vgs+2.639791*Vds+-0.025273597*Lg+-1.1747514);\n",
            "h1_2 = tanh(1.9884652*Vgs+0.62111*Vds+-0.11525345*Lg+-0.14575638);\n",
            "h1_3 = tanh(0.10625471*Vgs+0.09442053*Vds+-0.052119628*Lg+1.1568379);\n",
            "h1_4 = tanh(4.058087*Vgs+-0.7568158*Vds+0.008070099*Lg+-0.4820452);\n",
            "h1_5 = tanh(-1.3065948*Vgs+-0.0492497*Vds+-0.081622526*Lg+0.20351954);\n",
            "h1_6 = tanh(-2.9507525*Vgs+-0.91150546*Vds+-0.06477873*Lg+0.09646383);\n",
            "h1_7 = tanh(-0.5886177*Vgs+0.63788587*Vds+-0.13710928*Lg+0.30260038);\n",
            "h1_8 = tanh(-0.4311161*Vgs+-0.7250705*Vds+-0.06134645*Lg+0.44054285);\n",
            "h1_9 = tanh(0.012132638*Vgs+-0.42545322*Vds+-0.66478956*Lg+0.13182367);\n",
            "h1_10 = tanh(3.289041e-05*Vgs+0.0011367714*Vds+-0.0051904405*Lg+5.4112927e-05);\n",
            "h1_11 = tanh(-0.6007774*Vgs+0.09259224*Vds+0.2170182*Lg+-0.2908748);\n",
            "h1_12 = tanh(0.28018302*Vgs+1.3014064*Vds+-0.13490067*Lg+-0.22006753);\n",
            "h1_13 = tanh(0.01864017*Vgs+-13.0928755*Vds+-0.0037254083*Lg+-0.10935691);\n",
            "h1_14 = tanh(-0.0049708197*Vgs+0.0022613218*Vds+-0.014408149*Lg+0.0011340647);\n",
            "h1_15 = tanh(-0.094654046*Vgs+-0.4174114*Vds+0.18892045*Lg+0.05248958);\n",
            "h1_16 = tanh(-0.25090316*Vgs+-0.11111481*Vds+-0.009133225*Lg+0.08377026);\n",
            "h1_17 = tanh(0.9275306*Vgs+0.40686756*Vds+0.14127344*Lg+-0.059246097);\n",
            "h1_18 = tanh(-0.27084363*Vgs+0.07915911*Vds+-10.836302*Lg+-1.1535788);\n",
            "h1_19 = tanh(1.6852072*Vgs+-0.24846223*Vds+0.049734037*Lg+-0.19625053);\n",
            "\n",
            "h2_0 = tanh(1.1139417*h1_0+-0.40033522*h1_1+0.920759*h1_2+0.47607598*h1_3+0.3978683*h1_4+0.8008105*h1_5+-0.40445566*h1_6+0.8668027*h1_7+0.23365597*h1_8+-0.28254375*h1_9+-0.63985884*h1_10+-0.62523574*h1_11+0.8338383*h1_12+-2.0540943*h1_13+1.0749483*h1_14+-1.1075479*h1_15+0.60926706*h1_16+1.1118286*h1_17+-0.8917559*h1_18+0.029025732*h1_19+0.6622382);\n",
            "h2_1 = tanh(0.37195024*h1_0+-0.761445*h1_1+0.6514153*h1_2+0.6211995*h1_3+-0.063539445*h1_4+0.31260127*h1_5+-0.3529579*h1_6+0.50422627*h1_7+0.18943883*h1_8+-0.38029787*h1_9+-0.3464503*h1_10+-0.48301366*h1_11+0.081978925*h1_12+-0.08305682*h1_13+-0.08420117*h1_14+-0.8029313*h1_15+-0.37052512*h1_16+0.4553502*h1_17+-0.71959645*h1_18+-0.17320661*h1_19+0.6566257);\n",
            "h2_2 = tanh(-0.021933522*h1_0+-0.017245224*h1_1+0.030417712*h1_2+0.2967218*h1_3+-0.048668377*h1_4+0.029245213*h1_5+0.011756416*h1_6+0.004705658*h1_7+-0.042515088*h1_8+0.010462476*h1_9+0.001331279*h1_10+0.1694541*h1_11+-0.010949079*h1_12+-0.004144526*h1_13+0.04856694*h1_14+-0.010465159*h1_15+0.24588406*h1_16+-0.028521152*h1_17+0.12161568*h1_18+0.1753255*h1_19+-0.09125355);\n",
            "h2_3 = tanh(-0.025546636*h1_0+-3.2702503*h1_1+0.46812135*h1_2+-0.25135994*h1_3+3.0725436*h1_4+0.22513995*h1_5+-1.0327209*h1_6+-0.56274736*h1_7+0.4726107*h1_8+0.38182434*h1_9+0.016403453*h1_10+-0.09128962*h1_11+0.20997792*h1_12+-0.4951615*h1_13+0.026481293*h1_14+0.6085288*h1_15+-0.09078652*h1_16+0.36613584*h1_17+0.26257026*h1_18+0.39794916*h1_19+-0.52920526);\n",
            "h2_4 = tanh(-0.44392154*h1_0+-0.5650475*h1_1+0.91716766*h1_2+0.96703196*h1_3+0.4597048*h1_4+1.452921*h1_5+-0.8115141*h1_6+0.92326456*h1_7+0.2862403*h1_8+-0.9441585*h1_9+0.22188367*h1_10+-1.0092766*h1_11+0.5495966*h1_12+0.44870532*h1_13+0.48705962*h1_14+-0.6957133*h1_15+0.27711377*h1_16+1.0138507*h1_17+-0.34337458*h1_18+-0.21548931*h1_19+0.42685974);\n",
            "h2_5 = tanh(0.0003710325*h1_0+0.5746112*h1_1+0.4144258*h1_2+0.04459103*h1_3+1.2373503*h1_4+0.12786175*h1_5+-0.16099685*h1_6+0.9826207*h1_7+-0.09701193*h1_8+-0.4379135*h1_9+-0.0035542254*h1_10+0.04205593*h1_11+0.65820116*h1_12+0.3810506*h1_13+0.0004259507*h1_14+-0.21782044*h1_15+-0.057544652*h1_16+0.24420041*h1_17+-0.10138292*h1_18+-0.2387106*h1_19+0.867847);\n",
            "h2_6 = tanh(-0.0077049686*h1_0+-0.05349668*h1_1+-0.11536639*h1_2+0.06141029*h1_3+-0.034344573*h1_4+0.21672213*h1_5+-0.09835135*h1_6+-0.25849992*h1_7+0.10576329*h1_8+-0.14288934*h1_9+0.027846925*h1_10+-0.20104973*h1_11+0.24784875*h1_12+0.03396087*h1_13+-0.016039118*h1_14+-0.03147038*h1_15+-0.023109786*h1_16+-0.118931994*h1_17+0.18256636*h1_18+0.09981429*h1_19+0.079372324);\n",
            "h2_7 = tanh(-0.009369999*h1_0+-2.4468672*h1_1+-0.41433397*h1_2+-0.6289744*h1_3+-1.8285819*h1_4+0.12905455*h1_5+0.83588725*h1_6+0.11491322*h1_7+0.3871084*h1_8+0.07153052*h1_9+0.013530584*h1_10+0.40614852*h1_11+0.10340061*h1_12+-0.04473306*h1_13+0.0075287875*h1_14+0.5593612*h1_15+0.34401885*h1_16+-0.5538749*h1_17+0.36615464*h1_18+-0.2666981*h1_19+0.20194086);\n",
            "h2_8 = tanh(-0.0017045025*h1_0+-1.1388719*h1_1+0.25932005*h1_2+-0.6482845*h1_3+2.263395*h1_4+0.115192235*h1_5+-0.40946937*h1_6+-0.038117886*h1_7+-0.03703431*h1_8+0.05965774*h1_9+0.0032721795*h1_10+0.2561857*h1_11+0.47324425*h1_12+-0.25200802*h1_13+0.0017081021*h1_14+0.29819545*h1_15+0.061945435*h1_16+0.13516657*h1_17+0.5409335*h1_18+-0.17400871*h1_19+0.11659722);\n",
            "h2_9 = tanh(0.014866875*h1_0+1.0399909*h1_1+1.0108095*h1_2+-0.0918755*h1_3+-0.7634763*h1_4+-0.49749368*h1_5+-0.7020006*h1_6+-0.16384888*h1_7+-0.07833025*h1_8+-0.23745225*h1_9+-0.023095092*h1_10+-0.29244414*h1_11+0.6255917*h1_12+0.41413808*h1_13+-0.019823061*h1_14+-1.0593235*h1_15+-0.42221433*h1_16+0.98025715*h1_17+-0.71444243*h1_18+0.84347373*h1_19+0.28510216);\n",
            "h2_10 = tanh(-0.013515852*h1_0+0.68840384*h1_1+-0.030184174*h1_2+-0.48098114*h1_3+0.08609854*h1_4+-0.50878614*h1_5+0.26547763*h1_6+-0.7151076*h1_7+0.111288495*h1_8+0.53379977*h1_9+0.022258151*h1_10+0.16971351*h1_11+-1.2486296*h1_12+0.9649942*h1_13+0.0028906881*h1_14+0.31582054*h1_15+0.24213083*h1_16+-0.27242163*h1_17+0.11330636*h1_18+0.17038865*h1_19+-0.42089102);\n",
            "h2_11 = tanh(-0.009949424*h1_0+-0.6704206*h1_1+0.13177924*h1_2+0.07316155*h1_3+-0.28207502*h1_4+-0.32401362*h1_5+0.020746209*h1_6+-0.05517531*h1_7+0.10340257*h1_8+0.29381263*h1_9+0.020497978*h1_10+0.031984854*h1_11+0.079552434*h1_12+-7.7884445*h1_13+0.01224806*h1_14+0.8700812*h1_15+-0.16562358*h1_16+0.14370379*h1_17+0.2389181*h1_18+-0.19771136*h1_19+0.33039534);\n",
            "h2_12 = tanh(-0.002431529*h1_0+0.30154988*h1_1+-0.45072728*h1_2+0.27823612*h1_3+-0.25173753*h1_4+-0.58188903*h1_5+0.39836177*h1_6+-0.060329597*h1_7+0.1667837*h1_8+-0.09419194*h1_9+0.0108116735*h1_10+-0.3581154*h1_11+0.15690842*h1_12+-0.41209573*h1_13+-0.0070331707*h1_14+-0.23976392*h1_15+0.16620094*h1_16+-0.034617994*h1_17+0.32754526*h1_18+0.8490298*h1_19+0.026407415);\n",
            "h2_13 = tanh(-0.7385622*h1_0+-0.25174448*h1_1+2.090295*h1_2+0.67499936*h1_3+0.07392262*h1_4+0.5413692*h1_5+-0.8287433*h1_6+2.2810504*h1_7+1.1917778*h1_8+1.3719273*h1_9+0.25762329*h1_10+-2.1111712*h1_11+1.6987114*h1_12+-0.437825*h1_13+1.7649944*h1_14+-2.2254016*h1_15+-2.3555171*h1_16+-0.7732424*h1_17+-1.0756916*h1_18+-0.1367373*h1_19+1.5085722);\n",
            "h2_14 = tanh(-0.002535408*h1_0+-4.2686224*h1_1+-0.045537975*h1_2+0.08043166*h1_3+1.7274952*h1_4+-0.1260494*h1_5+-0.23011239*h1_6+-0.21025337*h1_7+0.48537356*h1_8+0.39049447*h1_9+-0.006565428*h1_10+-0.293028*h1_11+-1.1253971*h1_12+-0.2890831*h1_13+0.0042421576*h1_14+0.4230598*h1_15+-0.17869289*h1_16+-0.034056116*h1_17+0.61135995*h1_18+0.6345532*h1_19+0.73144835);\n",
            "h2_15 = tanh(-0.5239093*h1_0+0.49806875*h1_1+0.53555894*h1_2+1.2645539*h1_3+1.011548*h1_4+0.39209503*h1_5+-0.78194916*h1_6+0.94610345*h1_7+0.9121405*h1_8+-1.4693716*h1_9+-0.58336824*h1_10+-1.1661471*h1_11+-0.16158457*h1_12+0.058512915*h1_13+1.3358645*h1_14+0.5742125*h1_15+-1.3325212*h1_16+1.1804186*h1_17+-1.4877121*h1_18+0.6357802*h1_19+1.1328585);\n",
            "h2_16 = tanh(0.0095406*h1_0+-0.53232694*h1_1+0.1535685*h1_2+0.26108417*h1_3+-0.08375187*h1_4+0.3462123*h1_5+-0.61755395*h1_6+0.0015145333*h1_7+-0.108780764*h1_8+0.2555477*h1_9+0.00023940884*h1_10+-0.26577523*h1_11+-0.27689674*h1_12+0.43049082*h1_13+-0.025424613*h1_14+0.24634649*h1_15+-0.22579487*h1_16+0.37249807*h1_17+-0.8058662*h1_18+-0.77451354*h1_19+0.5821276);\n",
            "y = -0.17525196*h2_0+-0.22995844*h2_1+0.0026147955*h2_2+0.091129646*h2_3+-0.4534783*h2_4+0.4119419*h2_5+-1.4205361e-05*h2_6+-0.11359831*h2_7+0.1762025*h2_8+-0.1002102*h2_9+-0.37161925*h2_10+0.2683793*h2_11+0.051897053*h2_12+0.19354156*h2_13+-0.10134394*h2_14+0.3937854*h2_15+-0.3296008*h2_16+0.3220947;\n",
            "\n",
            "        Id = pow(10, (y/normI + MinI)) ;\n",
            "\n",
            "if (Id <= 1e-15) begin //limit\n",
            "        Id = 1e-15;\n",
            "        //Id = Id;\n",
            "end\n",
            "else begin\n",
            "        Id = Id;\n",
            "end  //limit end\n",
            "\n",
            "\n",
            "        I(g, d) <+ Cgsd*ddt(Vg-Vd) ;\n",
            "        I(g, s) <+ Cgsd*ddt(Vg-Vs) ;\n",
            "\n",
            "if (Vgsraw >= Vgdraw) begin\n",
            "        I(d, s) <+ dir*Id*W ;\n",
            "\n",
            "end\n",
            "\n",
            "else begin\n",
            "        I(d, s) <+ dir*Id*W ;\n",
            "\n",
            "end\n",
            "\n",
            "end\n",
            "\n",
            "endmodule\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "with torch.no_grad():\n",
        "\n",
        "    output = []\n",
        "    # Iterate over the dataloader for training data\n",
        "    for i, data in enumerate(testdataloader, 0):\n",
        "        inputs, targets = data\n",
        "        inputs, targets = inputs.float(), targets.float()\n",
        "        targets = targets.reshape((targets.shape[0],1))\n",
        "\n",
        "        #zero the gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        #perform forward pass\n",
        "        outputs = model(inputs)\n",
        "        output.append(outputs)\n",
        "# Process is complete.\n",
        "#print('Training process has finished.')\n",
        "\n",
        "# Extract the weights and biases from the model\n",
        "weights_1 = model.fc1.weight.detach().numpy()\n",
        "bias_1 = model.fc1.bias.detach().numpy()\n",
        "weights_2 = model.fc2.weight.detach().numpy()\n",
        "bias_2 = model.fc2.bias.detach().numpy()\n",
        "weights_3 = model.fc3.weight.detach().numpy()\n",
        "bias_3 = model.fc3.bias.detach().numpy()\n",
        "\n",
        "def generate_variable_declarations(weights_shape, layer_prefix):\n",
        "    declarations = \"\"\n",
        "    num_neurons = weights_shape  # Number of neurons is determined by the first dimension of the weights matrix\n",
        "    layer_declarations = \", \".join([f\"{layer_prefix}_{i}\" for i in range(num_neurons)]) + \";\"\n",
        "    declarations += layer_declarations\n",
        "    return declarations\n",
        "\n",
        "# Use the function to generate declarations for each layer\n",
        "h1_declarations = generate_variable_declarations(weights_1.shape[0], \"hvth1\")\n",
        "h2_declarations = generate_variable_declarations(weights_2.shape[0], \"hvth2\")\n",
        "\n",
        "verilog_code = \"\"\"\n",
        "// VerilogA for GB_lib, IWO_verliogA, veriloga\n",
        "//*******************************************************************************\n",
        "//* * School of Electrical and Computer Engineering, Georgia Institute of Technology\n",
        "//* PI: Prof. Shimeng Yu\n",
        "//* All rights reserved.\n",
        "//*\n",
        "//* Copyright of the model is maintained by the developers, and the model is distributed under\n",
        "//* the terms of the Creative Commons Attribution-NonCommercial 4.0 International Public License\n",
        "//* http://creativecommons.org/licenses/by-nc/4.0/legalcode.\n",
        "//*\n",
        "//* THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND\n",
        "//* ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\n",
        "//* WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n",
        "//* DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\n",
        "//* FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\n",
        "//* DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\n",
        "//* SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\n",
        "//* CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\n",
        "//* OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n",
        "//* OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n",
        "//*\n",
        "//* Developer:\n",
        "//*  Gihun Choe gchoe6@gatech.edu\n",
        "//********************************************************************************/\n",
        "\n",
        "`include \"constants.vams\"\n",
        "`include \"disciplines.vams\"\n",
        "\n",
        "\n",
        "module IWO_verliogA(d, g, s);\n",
        "        inout d, g, s;\n",
        "        electrical d, g, s;\n",
        "\n",
        "        //***** parameters L and W ******//\n",
        "        parameter real W = 0.1; //get parameter fom spectre\n",
        "        parameter real L = 0.05; //get parameter fom spectre\n",
        "        parameter real T_stress = 0.0001; //set on cadence as variable\n",
        "        parameter real T_rec = 0.001;     //set on cadence as variable\n",
        "        parameter real Clk_loops = 50;    //set on cadence as variable\n",
        "        parameter real V_ov = 1.7;        //set on cadence as variable\n",
        "        parameter real Temp = 25;  //set on cadence as variable\n",
        "\n",
        "        parameter MinVg = -1.0 ;\n",
        "        parameter normVg = 0.2222222222222222 ;\n",
        "        parameter MinVd = 0.01 ;\n",
        "        parameter normVd = 0.2949852507374631 ;\n",
        "        parameter MinLg = 0.05 ;\n",
        "        parameter normLg = 1.4285714285714286 ;\n",
        "        parameter MinO = 8.15e-15 ;\n",
        "        parameter normO =33613445378151.26;\n",
        "        parameter MinI = -23.98798356587402 ;\n",
        "        parameter normI =0.04615548498417793;\n",
        "\n",
        "        parameter Mint_stress = {} ;\n",
        "        parameter normt_stress = {} ;\n",
        "        parameter Mint_rec = {} ;\n",
        "        parameter normt_rec = {} ;\n",
        "        parameter Minclk_loops = {} ;\n",
        "        parameter normclk_loops = {} ;\n",
        "        parameter Minv_ov = {} ;\n",
        "        parameter normv_ov = {} ;\n",
        "        parameter Mintemperature = {} ;\n",
        "        parameter normtemperature = {} ;\n",
        "        parameter Mindelta_Vth = {} ;\n",
        "        parameter normdelta_Vth = {} ;\n",
        "\n",
        "        real {}\n",
        "        real {}\n",
        "\n",
        "        real Vg, Vd, Vs, Vgs, Vds, Lg, Id, Cgg, Cgsd, Vgd;\n",
        "        real Vgsraw, Vgdraw, dir;\n",
        "        real t_stress, v_ov, t_rec, clk_loops, temp, delta_Vth;\n",
        "\n",
        "        real h1_0, h1_1, h1_2, h1_3, h1_4, h1_5, h1_6, h1_7, h1_8, h1_9, h1_10, h1_11, h1_12, h1_13, h1_14, h1_15, h1_16, h1_17, h1_18, h1_19, h1_20, h1_21, h1_22, h1_23, h1_24;\n",
        "        real h2_0, h2_1, h2_2, h2_3, h2_4, h2_5, h2_6, h2_7, h2_8, h2_9, h2_10, h2_11, h2_12, h2_13, h2_14, h2_15, h2_16, y;\n",
        "        real hc1_0, hc1_1, hc1_2, hc1_3, hc1_4, hc1_5, hc1_6, hc1_7, hc1_8, hc1_9;\n",
        "        real hc1_10, hc1_11, hc1_12, hc1_13, hc1_14, hc1_15, hc1_16;\n",
        "        real hc2_0, hc2_1, hc2_2, hc2_3, hc2_4, hc2_5, hc2_6, hc2_7, hc2_8, hc2_9;\n",
        "        real hc2_10, hc2_11, hc2_12, hc2_13, hc2_14, hc2_15, hc2_16, yc, yvth;\n",
        "\n",
        "analog begin\n",
        "\n",
        "t_stress = (T_stress - Mint_stress)*normt_stress;\n",
        "v_ov = (V_ov - Minv_ov)*normv_ov;\n",
        "t_rec = (T_rec - Mint_rec)*normt_rec ;\n",
        "clk_loops = (Clk_loops - Minclk_loops)*normclk_loops ;\n",
        "temp = (Temp - Mintemperature)*normtemperature ;\n",
        "\n",
        "//******************** delta_Vth NN **********************************//\n",
        "\n",
        "\"\"\".format(Mint_stress, normt_stress, Mint_rec, normt_rec, Minclk_loops, normclk_loops, MinV_ov, normV_ov, Mintemperature, normtemperature, Mindelta_Vth, normdelta_Vth, h1_declarations, h2_declarations)\n",
        "# V_ov = (V_ov - MinV_ov)*normV_ov ;\n",
        "# t_stress = (T_stress - Mint_stress)*normt_stress ;\n",
        "\n",
        "# Create the Verilog-A code for the 1st hidden layer\n",
        "for i in range(n1):\n",
        "    inputs = [\"t_stress\", \"t_rec\", \"clk_loops\", \"v_ov\", \"temp\"]\n",
        "    inputs = [\"*\".join([str(weights_1[i][j]), inp]) for j, inp in enumerate(inputs)]\n",
        "    inputs = \"+\".join(inputs)\n",
        "    inputs = \"+\".join([inputs, str(bias_1[i])])\n",
        "    verilog_code += \"hvth1_{} = tanh({});\\n\".format(i, inputs)\n",
        "verilog_code += \"\\n\"\n",
        "\n",
        "# Create the Verilog-A code for the 2nd hidden layer\n",
        "for i in range(n2):\n",
        "    inputs = [\"hvth1_{}\".format(j) for j in range(n1)]\n",
        "    inputs = [\"*\".join([str(weights_2[i][j]), inp]) for j, inp in enumerate(inputs)]\n",
        "    inputs = \"+\".join(inputs)\n",
        "    inputs = \"+\".join([inputs, str(bias_2[i])])\n",
        "    verilog_code += \"hvth2_{} = tanh({});\\n\".format(i, inputs)\n",
        "verilog_code += \"\\n\"\n",
        "\n",
        "# Create the Verilog-A code for the output layer\n",
        "inputs = [\"hvth2_{}\".format(i) for i in range(n2)]\n",
        "inputs = [\"*\".join([str(weights_3[0][i]), inp]) for i, inp in enumerate(inputs)]\n",
        "inputs = \"+\".join(inputs)\n",
        "inputs = \"+\".join([inputs, str(bias_3[0])])\n",
        "verilog_code += \"yvth = {};\\n\\n\".format(inputs)\n",
        "verilog_code += \"delta_Vth = (yvth - Mindelta_Vth) * normdelta_Vth;\\n\"\n",
        "verilog_code += \"\"\"$strobe(\"dvth=$g\",delta_Vth);\"\"\"\n",
        "verilog_code += \"\"\"\n",
        "\n",
        "\n",
        "        Vg = V(g) ;\n",
        "        Vs = V(s) ;\n",
        "        Vd = V(d) ;\n",
        "        Vgsraw = Vg-Vs ;\n",
        "        Vgdraw = Vg-Vd ;\n",
        "\n",
        "if (Vgsraw >= Vgdraw) begin\n",
        "        Vgs = ((Vg-Vs) - MinVg) * normVg ;\n",
        "        dir = 1;\n",
        "end\n",
        "\n",
        "else begin\n",
        "        Vgs = ((Vg-Vd) - MinVg) * normVg ;\n",
        "        dir = -1;\n",
        "end\n",
        "\n",
        "        Vds = (abs(Vd-Vs) - MinVd) * normVd ;\n",
        "        Vgs = Vgs - delta_Vth ; // BTI Vth variation\n",
        "        Lg = (L -MinLg)*normLg ;\n",
        "\n",
        "\n",
        "\n",
        "//******************** C-V NN **********************************//\n",
        "hc1_0 = tanh(-0.99871427*Vgs+-0.16952373*Vds+0.32118186*Lg+0.41485423);\n",
        "hc1_1 = tanh(0.31587568*Vgs+0.13397887*Vds+-0.4541538*Lg+-0.3630942);\n",
        "hc1_2 = tanh(-0.76281804*Vgs+0.09352969*Vds+1.1961353*Lg+0.3904977);\n",
        "hc1_3 = tanh(-1.115087*Vgs+0.85752946*Vds+-0.11746484*Lg+0.5500279);\n",
        "hc1_4 = tanh(1.0741386*Vgs+0.82041687*Vds+0.19092631*Lg+-0.4009425);\n",
        "hc1_5 = tanh(-0.47921795*Vgs+-0.8749933*Vds+-0.054768667*Lg+0.62785167);\n",
        "hc1_6 = tanh(0.5449184*Vgs+-4.409165*Vds+-0.072947875*Lg+-0.31324536);\n",
        "hc1_7 = tanh(-2.9224303*Vgs+2.7675478*Vds+0.08862238*Lg+0.6493558);\n",
        "hc1_8 = tanh(0.65050656*Vgs+-0.29751927*Vds+0.1571876*Lg+-0.38088372);\n",
        "hc1_9 = tanh(-0.30384183*Vgs+0.5649165*Vds+2.6806898*Lg+0.3197917);\n",
        "hc1_10 = tanh(-0.095988505*Vgs+2.0158541*Vds+-0.42972717*Lg+-0.30388466);\n",
        "hc1_11 = tanh(6.7699738*Vgs+-0.07234483*Vds+-0.013545353*Lg+-1.3694142);\n",
        "hc1_12 = tanh(-0.3404029*Vgs+0.0443459*Vds+0.89597*Lg+0.069993004);\n",
        "hc1_13 = tanh(0.62300175*Vgs+-0.29515797*Vds+1.6753465*Lg+-0.6520838);\n",
        "hc1_14 = tanh(0.37957156*Vgs+0.2237372*Vds+0.08591952*Lg+0.13126835);\n",
        "hc1_15 = tanh(0.19949242*Vgs+-0.26481664*Vds+-0.41059187*Lg+-0.40832308);\n",
        "hc1_16 = tanh(0.98966587*Vgs+-0.24259183*Vds+0.36584845*Lg+-0.8024042);\n",
        "\n",
        "hc2_0 = tanh(-0.91327864*hc1_0+0.4696781*hc1_1+0.3302202*hc1_2+0.11393868*hc1_3+0.45070222*hc1_4+-0.2894044*hc1_5+0.55066*hc1_6+-1.6242687*hc1_7+-0.38140613*hc1_8+0.032771554*hc1_9+0.17647126);\n",
        "hc2_1 = tanh(-1.1663305*hc1_0+-0.523984*hc1_1+-0.90804136*hc1_2+-0.7418044*hc1_3+1.456171*hc1_4+-0.16802542*hc1_5+0.8235596*hc1_6+-2.2246246*hc1_7+-0.40805355*hc1_8+0.7207601*hc1_9+0.4729169);\n",
        "hc2_2 = tanh(-0.69065374*hc1_0+0.40205315*hc1_1+-0.49410668*hc1_2+0.8681325*hc1_3+0.471351*hc1_4+0.46939445*hc1_5+0.45568785*hc1_6+-0.92935294*hc1_7+-0.8209646*hc1_8+0.1158967*hc1_9+-0.075798474);\n",
        "hc2_3 = tanh(0.11535446*hc1_0+-0.06296927*hc1_1+-0.6740435*hc1_2+0.7428315*hc1_3+0.05890677*hc1_4+0.9579441*hc1_5+-0.037319*hc1_6+-0.18491426*hc1_7+-0.02981994*hc1_8+0.038347963*hc1_9+0.039531134);\n",
        "hc2_4 = tanh(0.37817463*hc1_0+-0.6811279*hc1_1+1.1388369*hc1_2+0.19983096*hc1_3+-0.20415118*hc1_4+1.3022176*hc1_5+0.22571652*hc1_6+0.1690611*hc1_7+-0.56475276*hc1_8+-0.4069731*hc1_9+0.99962974);\n",
        "hc2_5 = tanh(0.032873478*hc1_0+-0.05209407*hc1_1+-0.010839908*hc1_2+-0.13892579*hc1_3+-0.050480977*hc1_4+0.0127089145*hc1_5+0.0052771433*hc1_6+0.02029242*hc1_7+-0.08705659*hc1_8+0.0254766*hc1_9+0.025135752);\n",
        "hc2_6 = tanh(-0.34639204*hc1_0+0.06937975*hc1_1+0.18671949*hc1_2+-0.18912783*hc1_3+0.1312504*hc1_4+0.4627272*hc1_5+-0.42590702*hc1_6+-0.10966313*hc1_7+0.66083515*hc1_8+-0.050718334*hc1_9+0.08234678);\n",
        "hc2_7 = tanh(0.90656275*hc1_0+-0.037281644*hc1_1+0.77237594*hc1_2+1.4710428*hc1_3+0.13597831*hc1_4+-0.059844542*hc1_5+-0.7801535*hc1_6+3.7814677*hc1_7+-0.5976644*hc1_8+0.2721995*hc1_9+0.023777716);\n",
        "hc2_8 = tanh(0.39720625*hc1_0+-0.45262313*hc1_1+0.19873238*hc1_2+0.9750888*hc1_3+-0.9427992*hc1_4+0.4487432*hc1_5+-0.3372945*hc1_6+0.33729544*hc1_7+-0.1667088*hc1_8+-0.5707525*hc1_9+0.27954483);\n",
        "hc2_9 = tanh(0.28551984*hc1_0+-0.68350387*hc1_1+0.9916423*hc1_2+-0.8254094*hc1_3+0.09875706*hc1_4+0.47609732*hc1_5+-0.058662917*hc1_6+0.09181381*hc1_7+0.09592329*hc1_8+1.3940467*hc1_9+0.3865768);\n",
        "hc2_10 = tanh(0.098737516*hc1_0+0.060473576*hc1_1+0.42824662*hc1_2+0.15018038*hc1_3+0.082621895*hc1_4+-0.00019039502*hc1_5+-0.3321634*hc1_6+0.7936295*hc1_7+-0.041197542*hc1_8+0.6530619*hc1_9+0.1338804);\n",
        "hc2_11 = tanh(-0.3585284*hc1_0+-0.09956017*hc1_1+0.17224246*hc1_2+-0.016925728*hc1_3+-0.46462816*hc1_4+-0.5649022*hc1_5+1.251695*hc1_6+-0.4303161*hc1_7+0.48546878*hc1_8+0.22958975*hc1_9+-0.27899802);\n",
        "hc2_12 = tanh(0.8565631*hc1_0+-0.7622999*hc1_1+0.32367912*hc1_2+1.4776785*hc1_3+0.2712369*hc1_4+0.2275511*hc1_5+0.39908803*hc1_6+4.2305493*hc1_7+-0.3467536*hc1_8+0.41231114*hc1_9+0.47123823);\n",
        "hc2_13 = tanh(-0.26411077*hc1_0+-0.17583853*hc1_1+0.045439184*hc1_2+-0.13801138*hc1_3+0.03278085*hc1_4+-0.45625108*hc1_5+-0.17905861*hc1_6+0.3060186*hc1_7+-0.3361926*hc1_8+-0.050055273*hc1_9+0.17996444);\n",
        "hc2_14 = tanh(0.016094366*hc1_0+-0.013196736*hc1_1+0.4124856*hc1_2+0.22926371*hc1_3+-0.35071182*hc1_4+-0.34217188*hc1_5+-0.69466996*hc1_6+1.0563152*hc1_7+-0.18019852*hc1_8+0.061871335*hc1_9+0.09762555);\n",
        "hc2_15 = tanh(-0.18970782*hc1_0+0.3624813*hc1_1+-1.3419824*hc1_2+0.103635244*hc1_3+-0.14595217*hc1_4+-0.1899393*hc1_5+0.176524*hc1_6+-0.4428012*hc1_7+-0.39544868*hc1_8+-0.45783517*hc1_9+0.1884755);\n",
        "hc2_16 = tanh(-0.1585831*hc1_0+0.035894837*hc1_1+-0.14261873*hc1_2+0.25914294*hc1_3+0.040607046*hc1_4+0.11555795*hc1_5+0.0022548323*hc1_6+-0.002149359*hc1_7+0.07067297*hc1_8+0.019578662*hc1_9+0.16657573);\n",
        "yc = 0.17503543*hc2_0+-0.15556754*hc2_1+-0.125455*hc2_2+-0.07502612*hc2_3+-0.16671115*hc2_4+0.29854503;\n",
        "\n",
        "Cgg = (yc / normO + MinO)*W;\n",
        "Cgsd = Cgg/2 ;\n",
        "\n",
        "//******************** I-V NN **********************************//\n",
        "h1_0 = tanh(0.0023826673*Vgs+-0.0009636134*Vds+0.006639037*Lg+-0.0004692053);\n",
        "h1_1 = tanh(-7.8007555*Vgs+2.639791*Vds+-0.025273597*Lg+-1.1747514);\n",
        "h1_2 = tanh(1.9884652*Vgs+0.62111*Vds+-0.11525345*Lg+-0.14575638);\n",
        "h1_3 = tanh(0.10625471*Vgs+0.09442053*Vds+-0.052119628*Lg+1.1568379);\n",
        "h1_4 = tanh(4.058087*Vgs+-0.7568158*Vds+0.008070099*Lg+-0.4820452);\n",
        "h1_5 = tanh(-1.3065948*Vgs+-0.0492497*Vds+-0.081622526*Lg+0.20351954);\n",
        "h1_6 = tanh(-2.9507525*Vgs+-0.91150546*Vds+-0.06477873*Lg+0.09646383);\n",
        "h1_7 = tanh(-0.5886177*Vgs+0.63788587*Vds+-0.13710928*Lg+0.30260038);\n",
        "h1_8 = tanh(-0.4311161*Vgs+-0.7250705*Vds+-0.06134645*Lg+0.44054285);\n",
        "h1_9 = tanh(0.012132638*Vgs+-0.42545322*Vds+-0.66478956*Lg+0.13182367);\n",
        "h1_10 = tanh(3.289041e-05*Vgs+0.0011367714*Vds+-0.0051904405*Lg+5.4112927e-05);\n",
        "h1_11 = tanh(-0.6007774*Vgs+0.09259224*Vds+0.2170182*Lg+-0.2908748);\n",
        "h1_12 = tanh(0.28018302*Vgs+1.3014064*Vds+-0.13490067*Lg+-0.22006753);\n",
        "h1_13 = tanh(0.01864017*Vgs+-13.0928755*Vds+-0.0037254083*Lg+-0.10935691);\n",
        "h1_14 = tanh(-0.0049708197*Vgs+0.0022613218*Vds+-0.014408149*Lg+0.0011340647);\n",
        "h1_15 = tanh(-0.094654046*Vgs+-0.4174114*Vds+0.18892045*Lg+0.05248958);\n",
        "h1_16 = tanh(-0.25090316*Vgs+-0.11111481*Vds+-0.009133225*Lg+0.08377026);\n",
        "h1_17 = tanh(0.9275306*Vgs+0.40686756*Vds+0.14127344*Lg+-0.059246097);\n",
        "h1_18 = tanh(-0.27084363*Vgs+0.07915911*Vds+-10.836302*Lg+-1.1535788);\n",
        "h1_19 = tanh(1.6852072*Vgs+-0.24846223*Vds+0.049734037*Lg+-0.19625053);\n",
        "\n",
        "h2_0 = tanh(1.1139417*h1_0+-0.40033522*h1_1+0.920759*h1_2+0.47607598*h1_3+0.3978683*h1_4+0.8008105*h1_5+-0.40445566*h1_6+0.8668027*h1_7+0.23365597*h1_8+-0.28254375*h1_9+-0.63985884*h1_10+-0.62523574*h1_11+0.8338383*h1_12+-2.0540943*h1_13+1.0749483*h1_14+-1.1075479*h1_15+0.60926706*h1_16+1.1118286*h1_17+-0.8917559*h1_18+0.029025732*h1_19+0.6622382);\n",
        "h2_1 = tanh(0.37195024*h1_0+-0.761445*h1_1+0.6514153*h1_2+0.6211995*h1_3+-0.063539445*h1_4+0.31260127*h1_5+-0.3529579*h1_6+0.50422627*h1_7+0.18943883*h1_8+-0.38029787*h1_9+-0.3464503*h1_10+-0.48301366*h1_11+0.081978925*h1_12+-0.08305682*h1_13+-0.08420117*h1_14+-0.8029313*h1_15+-0.37052512*h1_16+0.4553502*h1_17+-0.71959645*h1_18+-0.17320661*h1_19+0.6566257);\n",
        "h2_2 = tanh(-0.021933522*h1_0+-0.017245224*h1_1+0.030417712*h1_2+0.2967218*h1_3+-0.048668377*h1_4+0.029245213*h1_5+0.011756416*h1_6+0.004705658*h1_7+-0.042515088*h1_8+0.010462476*h1_9+0.001331279*h1_10+0.1694541*h1_11+-0.010949079*h1_12+-0.004144526*h1_13+0.04856694*h1_14+-0.010465159*h1_15+0.24588406*h1_16+-0.028521152*h1_17+0.12161568*h1_18+0.1753255*h1_19+-0.09125355);\n",
        "h2_3 = tanh(-0.025546636*h1_0+-3.2702503*h1_1+0.46812135*h1_2+-0.25135994*h1_3+3.0725436*h1_4+0.22513995*h1_5+-1.0327209*h1_6+-0.56274736*h1_7+0.4726107*h1_8+0.38182434*h1_9+0.016403453*h1_10+-0.09128962*h1_11+0.20997792*h1_12+-0.4951615*h1_13+0.026481293*h1_14+0.6085288*h1_15+-0.09078652*h1_16+0.36613584*h1_17+0.26257026*h1_18+0.39794916*h1_19+-0.52920526);\n",
        "h2_4 = tanh(-0.44392154*h1_0+-0.5650475*h1_1+0.91716766*h1_2+0.96703196*h1_3+0.4597048*h1_4+1.452921*h1_5+-0.8115141*h1_6+0.92326456*h1_7+0.2862403*h1_8+-0.9441585*h1_9+0.22188367*h1_10+-1.0092766*h1_11+0.5495966*h1_12+0.44870532*h1_13+0.48705962*h1_14+-0.6957133*h1_15+0.27711377*h1_16+1.0138507*h1_17+-0.34337458*h1_18+-0.21548931*h1_19+0.42685974);\n",
        "h2_5 = tanh(0.0003710325*h1_0+0.5746112*h1_1+0.4144258*h1_2+0.04459103*h1_3+1.2373503*h1_4+0.12786175*h1_5+-0.16099685*h1_6+0.9826207*h1_7+-0.09701193*h1_8+-0.4379135*h1_9+-0.0035542254*h1_10+0.04205593*h1_11+0.65820116*h1_12+0.3810506*h1_13+0.0004259507*h1_14+-0.21782044*h1_15+-0.057544652*h1_16+0.24420041*h1_17+-0.10138292*h1_18+-0.2387106*h1_19+0.867847);\n",
        "h2_6 = tanh(-0.0077049686*h1_0+-0.05349668*h1_1+-0.11536639*h1_2+0.06141029*h1_3+-0.034344573*h1_4+0.21672213*h1_5+-0.09835135*h1_6+-0.25849992*h1_7+0.10576329*h1_8+-0.14288934*h1_9+0.027846925*h1_10+-0.20104973*h1_11+0.24784875*h1_12+0.03396087*h1_13+-0.016039118*h1_14+-0.03147038*h1_15+-0.023109786*h1_16+-0.118931994*h1_17+0.18256636*h1_18+0.09981429*h1_19+0.079372324);\n",
        "h2_7 = tanh(-0.009369999*h1_0+-2.4468672*h1_1+-0.41433397*h1_2+-0.6289744*h1_3+-1.8285819*h1_4+0.12905455*h1_5+0.83588725*h1_6+0.11491322*h1_7+0.3871084*h1_8+0.07153052*h1_9+0.013530584*h1_10+0.40614852*h1_11+0.10340061*h1_12+-0.04473306*h1_13+0.0075287875*h1_14+0.5593612*h1_15+0.34401885*h1_16+-0.5538749*h1_17+0.36615464*h1_18+-0.2666981*h1_19+0.20194086);\n",
        "h2_8 = tanh(-0.0017045025*h1_0+-1.1388719*h1_1+0.25932005*h1_2+-0.6482845*h1_3+2.263395*h1_4+0.115192235*h1_5+-0.40946937*h1_6+-0.038117886*h1_7+-0.03703431*h1_8+0.05965774*h1_9+0.0032721795*h1_10+0.2561857*h1_11+0.47324425*h1_12+-0.25200802*h1_13+0.0017081021*h1_14+0.29819545*h1_15+0.061945435*h1_16+0.13516657*h1_17+0.5409335*h1_18+-0.17400871*h1_19+0.11659722);\n",
        "h2_9 = tanh(0.014866875*h1_0+1.0399909*h1_1+1.0108095*h1_2+-0.0918755*h1_3+-0.7634763*h1_4+-0.49749368*h1_5+-0.7020006*h1_6+-0.16384888*h1_7+-0.07833025*h1_8+-0.23745225*h1_9+-0.023095092*h1_10+-0.29244414*h1_11+0.6255917*h1_12+0.41413808*h1_13+-0.019823061*h1_14+-1.0593235*h1_15+-0.42221433*h1_16+0.98025715*h1_17+-0.71444243*h1_18+0.84347373*h1_19+0.28510216);\n",
        "h2_10 = tanh(-0.013515852*h1_0+0.68840384*h1_1+-0.030184174*h1_2+-0.48098114*h1_3+0.08609854*h1_4+-0.50878614*h1_5+0.26547763*h1_6+-0.7151076*h1_7+0.111288495*h1_8+0.53379977*h1_9+0.022258151*h1_10+0.16971351*h1_11+-1.2486296*h1_12+0.9649942*h1_13+0.0028906881*h1_14+0.31582054*h1_15+0.24213083*h1_16+-0.27242163*h1_17+0.11330636*h1_18+0.17038865*h1_19+-0.42089102);\n",
        "h2_11 = tanh(-0.009949424*h1_0+-0.6704206*h1_1+0.13177924*h1_2+0.07316155*h1_3+-0.28207502*h1_4+-0.32401362*h1_5+0.020746209*h1_6+-0.05517531*h1_7+0.10340257*h1_8+0.29381263*h1_9+0.020497978*h1_10+0.031984854*h1_11+0.079552434*h1_12+-7.7884445*h1_13+0.01224806*h1_14+0.8700812*h1_15+-0.16562358*h1_16+0.14370379*h1_17+0.2389181*h1_18+-0.19771136*h1_19+0.33039534);\n",
        "h2_12 = tanh(-0.002431529*h1_0+0.30154988*h1_1+-0.45072728*h1_2+0.27823612*h1_3+-0.25173753*h1_4+-0.58188903*h1_5+0.39836177*h1_6+-0.060329597*h1_7+0.1667837*h1_8+-0.09419194*h1_9+0.0108116735*h1_10+-0.3581154*h1_11+0.15690842*h1_12+-0.41209573*h1_13+-0.0070331707*h1_14+-0.23976392*h1_15+0.16620094*h1_16+-0.034617994*h1_17+0.32754526*h1_18+0.8490298*h1_19+0.026407415);\n",
        "h2_13 = tanh(-0.7385622*h1_0+-0.25174448*h1_1+2.090295*h1_2+0.67499936*h1_3+0.07392262*h1_4+0.5413692*h1_5+-0.8287433*h1_6+2.2810504*h1_7+1.1917778*h1_8+1.3719273*h1_9+0.25762329*h1_10+-2.1111712*h1_11+1.6987114*h1_12+-0.437825*h1_13+1.7649944*h1_14+-2.2254016*h1_15+-2.3555171*h1_16+-0.7732424*h1_17+-1.0756916*h1_18+-0.1367373*h1_19+1.5085722);\n",
        "h2_14 = tanh(-0.002535408*h1_0+-4.2686224*h1_1+-0.045537975*h1_2+0.08043166*h1_3+1.7274952*h1_4+-0.1260494*h1_5+-0.23011239*h1_6+-0.21025337*h1_7+0.48537356*h1_8+0.39049447*h1_9+-0.006565428*h1_10+-0.293028*h1_11+-1.1253971*h1_12+-0.2890831*h1_13+0.0042421576*h1_14+0.4230598*h1_15+-0.17869289*h1_16+-0.034056116*h1_17+0.61135995*h1_18+0.6345532*h1_19+0.73144835);\n",
        "h2_15 = tanh(-0.5239093*h1_0+0.49806875*h1_1+0.53555894*h1_2+1.2645539*h1_3+1.011548*h1_4+0.39209503*h1_5+-0.78194916*h1_6+0.94610345*h1_7+0.9121405*h1_8+-1.4693716*h1_9+-0.58336824*h1_10+-1.1661471*h1_11+-0.16158457*h1_12+0.058512915*h1_13+1.3358645*h1_14+0.5742125*h1_15+-1.3325212*h1_16+1.1804186*h1_17+-1.4877121*h1_18+0.6357802*h1_19+1.1328585);\n",
        "h2_16 = tanh(0.0095406*h1_0+-0.53232694*h1_1+0.1535685*h1_2+0.26108417*h1_3+-0.08375187*h1_4+0.3462123*h1_5+-0.61755395*h1_6+0.0015145333*h1_7+-0.108780764*h1_8+0.2555477*h1_9+0.00023940884*h1_10+-0.26577523*h1_11+-0.27689674*h1_12+0.43049082*h1_13+-0.025424613*h1_14+0.24634649*h1_15+-0.22579487*h1_16+0.37249807*h1_17+-0.8058662*h1_18+-0.77451354*h1_19+0.5821276);\n",
        "y = -0.17525196*h2_0+-0.22995844*h2_1+0.0026147955*h2_2+0.091129646*h2_3+-0.4534783*h2_4+0.4119419*h2_5+-1.4205361e-05*h2_6+-0.11359831*h2_7+0.1762025*h2_8+-0.1002102*h2_9+-0.37161925*h2_10+0.2683793*h2_11+0.051897053*h2_12+0.19354156*h2_13+-0.10134394*h2_14+0.3937854*h2_15+-0.3296008*h2_16+0.3220947;\n",
        "\n",
        "        Id = pow(10, (y/normI + MinI)) ;\n",
        "\n",
        "if (Id <= 1e-15) begin //limit\n",
        "        Id = 1e-15;\n",
        "        //Id = Id;\n",
        "end\n",
        "else begin\n",
        "        Id = Id;\n",
        "end  //limit end\n",
        "\n",
        "\n",
        "        I(g, d) <+ Cgsd*ddt(Vg-Vd) ;\n",
        "        I(g, s) <+ Cgsd*ddt(Vg-Vs) ;\n",
        "\n",
        "if (Vgsraw >= Vgdraw) begin\n",
        "        I(d, s) <+ dir*Id*W ;\n",
        "\n",
        "end\n",
        "\n",
        "else begin\n",
        "        I(d, s) <+ dir*Id*W ;\n",
        "\n",
        "end\n",
        "\n",
        "end\n",
        "\n",
        "endmodule\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "print(verilog_code)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 524
        },
        "id": "jlLiTNqU9gKw",
        "outputId": "0ea817f0-3097-453a-8ccb-efb54cd7b44b"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAj4AAAGzCAYAAAAv9B03AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABsI0lEQVR4nO3deViU1dsH8O/MKIsCo4gsEolLZWQKoiBuaWGSJvqmoRjivi8pmUu5oSWapVjuCqKiApmVK6moqYlhAqXi8lMpNxYVZZN15nn/IElkcQZmZb6f65qrOHOe57lPk87NWUWCIAggIiIiMgBibQdAREREpClMfIiIiMhgMPEhIiIig8HEh4iIiAwGEx8iIiIyGEx8iIiIyGAw8SEiIiKDwcSHiIiIDAYTHyIiIjIYTHyIiIjIYNTRdgBr1qzB8uXLkZqairZt2+K7776Dm5tbpfWDg4Oxbt063Lp1C1ZWVhg4cCCCgoJgYmKi0PPkcjnu3bsHc3NziEQiVTWDiIiI1EgQBGRnZ6NJkyYQi2vQbyNoUUREhGBkZCSEhoYKly5dEsaMGSM0aNBASEtLq7D+jh07BGNjY2HHjh1CcnKy8Msvvwh2dnbC9OnTFX7m7du3BQB88cUXX3zxxZcevm7fvl2j3EMkCNo7pNTd3R0dOnTA6tWrAZT0xjg4OGDKlCmYPXt2ufqTJ0/G5cuXERMTU1r2ySef4Pfff8fp06cVemZmZiYaNGiA27dvw8LCQjUNISIiIrXKysqCg4MDHj9+DKlUWu37aG2oq7CwEOfPn8ecOXNKy8RiMTw9PREbG1vhNZ06dUJ4eDji4uLg5uaGmzdv4uDBgxg6dGilzykoKEBBQUHpz9nZ2QAACwsLJj5ERER6pqbTVLSW+Dx48AAymQw2NjZlym1sbHDlypUKrxkyZAgePHiALl26QBAEFBcXY/z48fjss88qfU5QUBACAwNVGjsRERHpJ71a1XXixAksWbIEa9euRXx8PPbs2YMDBw5g8eLFlV4zZ84cZGZmlr5u376twYiJiIhIl2itx8fKygoSiQRpaWllytPS0mBra1vhNfPmzcPQoUMxevRoAMCbb76J3NxcjB07Fp9//nmFs7yNjY1hbGys+gYQERGR3tFaj4+RkRFcXV3LTFSWy+WIiYmBh4dHhdc8efKkXHIjkUgAAFqco01ERER6Qqv7+AQEBGDYsGFo37493NzcEBwcjNzcXIwYMQIA4O/vD3t7ewQFBQEA+vbtixUrVsDFxQXu7u64fv065s2bh759+5YmQERERESV0WriM2jQINy/fx/z589HamoqnJ2dER0dXTrh+datW2V6eObOnQuRSIS5c+fi7t27aNy4Mfr27Ysvv/xSW00gIiIiPaLVfXy0ISsrC1KpFJmZmVzOTkREpCdU9f2tV6u6iIiIiGqCiQ8REREZDK0fUkpERFQTMrmAuOQMpGfnw9rcBG7NLCERq+YQamXurc44SHWY+BARkd6KvpiCwH1JSMnMLy2zk5pgQV8neLW209i91RkHqRaHuoiISC9FX0zBhPD4MskGAKRm5mNCeDyiL6Zo5N7qjINUj4kPERHpHZlcQOC+JFS0LPlpWeC+JMjkyi9cVube6oxDX8XGxuL+/fvaDqNSTHyIiEjvxCVnlOtheZYAICUzH3HJGWq9tzrj0DdyuRxfffUVunbtimHDhkEul2s7pApxjg8REemd9OzKk43q1FP3vasThz65f/8+hg0bhkOHDgEApFIpCgoKYGpqquXIymPiQ0REesfa3ESl9dR97+rEoS9OnjwJX19f3Lt3DyYmJvj2228xevRoiES6uaKNQ11ERKR33JpZwk5qgsq+WkUoWVXl1sxSrfdWZxy6TiaT4YsvvkCPHj1w7949vPbaa/j9998xZswYnU16ACY+RESkhyRiERb0dQKAcknH058X9HWq1j46ytxbnXHosrS0NHh5eWHevHmQy+UYOnQo/vjjD7Rp00bbob0QEx8iItJLXq3tsM6vHWylZYeRbKUmWOfXrkb75yhzb3XGoYuOHTsGZ2dnHD16FKamptiyZQu2bdsGMzMzbYemEB5SSkREeo07N2uGTCbDokWLsHjxYgiCgDfeeANRUVFwcnLSyPNV9f3Nyc1ERKTXJGIRPFo00vq91RmHtt27dw8fffQRTpw4AQAYOXIkvvvuO9SrV0+7gVUDEx8iIiKq1OHDh+Hn54f79++jfv36WL9+Pfz8/LQdVrVxjg8RERGVU1xcjM8//xxeXl64f/8+2rRpg/Pnz+t10gOwx4eIiIiec+fOHfj6+uL06dMAgPHjx2PFihU6uSGhspj4EBERUamDBw/C398fDx8+hLm5OTZt2oRBgwZpOyyV4VAXERERoaioCDNnzkSfPn3w8OFDtGvXDvHx8bUq6QHY40NERGTw/vnnHwwePBhnz54FAEyZMgXLly+HsbGxliNTPSY+REREBuznn3/GiBEj8OjRI0ilUoSGhuKDDz7Qdlhqw6EuIiIiA1RYWIhp06ahf//+ePToETp06ICEhIRanfQATHyIiIgMzs2bN9G5c2esWrUKADB9+nScPn0azZo103Jk6sehLiIiIgPyww8/YOTIkcjKykLDhg0RFhYGb29vbYelMezxISIiMgD5+fmYPHkyBg4ciKysLHh4eCAxMdGgkh6AiQ8REVGt97///Q+dOnXCmjVrAAAzZ87Er7/+ipdfflnLkWkeh7qIiIhqsYiICIwdOxbZ2dmwsrLCtm3b8N5772k7LK1hjw8REVEtlJeXh3HjxsHX1xfZ2dno2rUrEhMTDTrpAZj4EBER1TpXrlyBu7s7Nm7cCJFIhLlz5+LYsWOwt7fXdmhax6EuIiKiWmT79u2YMGECcnNzYW1tjfDwcPTs2VPbYekM9vgQERHVArm5uRg5ciT8/f2Rm5uLHj16IDExkUnPc5j4EBER6blLly7Bzc0NW7ZsgUgkwsKFC3HkyBHY2dlpOzSdw6EuIiIiPSUIArZs2YLJkycjLy8Ptra22LlzJ3r06KHt0HQWEx8iIiI9lJOTgwkTJiA8PBwA0LNnT4SHh8Pa2lrLkek2DnURERHpmb/++guurq4IDw+HWCzGl19+iejoaCY9CtCJxGfNmjVwdHSEiYkJ3N3dERcXV2nd7t27QyQSlXv16dNHgxETERFpniAI2LBhA9zc3HDt2jXY29vjxIkT+OyzzyAW68RXus7T+n+lyMhIBAQEYMGCBYiPj0fbtm3Rq1cvpKenV1h/z549SElJKX1dvHgREokEH374oYYjJyIi0pysrCz4+vpi/PjxKCgoQO/evZGYmIiuXbtqOzS9ovXEZ8WKFRgzZgxGjBgBJycnrF+/HvXq1UNoaGiF9S0tLWFra1v6OnLkCOrVq8fEh4iIaq34+Hi4uroiMjISderUwVdffYV9+/bByspK26HpHa0mPoWFhTh//jw8PT1Ly8RiMTw9PREbG6vQPUJCQjB48GDUr1+/wvcLCgqQlZVV5kVERKQPBEHA6tWr4eHhgevXr+Pll1/GyZMn8emnn3Joq5q0+l/twYMHkMlksLGxKVNuY2OD1NTUF14fFxeHixcvYvTo0ZXWCQoKglQqLX05ODjUOG4iIiJ1e/z4MT788ENMmTIFhYWF8Pb2RkJCAjw8PLQdml7T63QxJCQEb775Jtzc3CqtM2fOHGRmZpa+bt++rcEIiYiIlBcXFwcXFxf88MMPqFu3LlauXImffvoJlpaW2g5N72l1Hx8rKytIJBKkpaWVKU9LS4OtrW2V1+bm5iIiIgKLFi2qsp6xsTGMjY1rHCsREZG6CYKA4OBgzJo1C0VFRWjWrBkiIyPRoUMHbYdWa2i1x8fIyAiurq6IiYkpLZPL5YiJiXlhV97333+PgoIC+Pn5qTtMIiIitcvIyED//v0REBCAoqIiDBgwAPHx8Ux6VEzrQ10BAQHYtGkTtm7disuXL5eeKDtixAgAgL+/P+bMmVPuupCQEPTv3x+NGjXSdMhEREQqFRsbC2dnZ+zduxdGRkZYvXo1vv/+ezRo0EDbodU6Wj+yYtCgQbh//z7mz5+P1NRUODs7Izo6unTC861bt8rNXL969SpOnz6Nw4cPayNkIiIilZDL5fj666/x2WefQSaToWXLloiKioKLi4u2Q6u1RIIgCNoOQpOysrIglUqRmZkJCwsLbYdDREQG6sGDB/D398ehQ4cAAIMHD8aGDRv43VQJVX1/a32oi4iIyNCcOnUKzs7OOHToEExMTLBhwwbs3LmTSY8GMPEhIiLSELlcji+//BLdu3fH3bt38dprr+H333/H2LFjIRKJtB2eQdD6HB8iIiJDkJaWhqFDh+LIkSMAgKFDh2Lt2rUwMzPTcmSGhYkPERGRmh0/fhxDhgxBamoqTE1NsWbNGgwfPpy9PFrAxIeIiEhNZDIZvvjiCyxatAhyuRxOTk6IiorCG2+8oe3Q1EMuA/45A+SkAWY2QNNOgFii7ajKYOJDRESkBikpKfjoo49w/PhxAMDIkSPx3XffoV69elqOTE2S9gLRs4Cse/+VWTQBvJYBTt7ai+s5nNxMRESkYkeOHIGzszOOHz+O+vXrY9u2bQgJCandSU+Uf9mkBwCyUkrKk/ZqJ64KMPEhIiJSkeLiYsydOxe9evVCeno62rRpgz/++ANDhw7VdmjqI5eV9PSgom0B/y2Lnl1STwcw8SEiIlKBO3fu4O2338aXX34JQRAwbtw4nD17Fq1atdJ2aOr1z5nyPT1lCEDW3ZJ6OoBzfIiIiGro4MGD8Pf3x8OHD2Fubo6NGzdi8ODB2g5LM3LSVFtPzdjjQ0REVE1FRUWYOXMm+vTpg4cPH8LFxQXx8fGGk/QAJau3VFlPzZj4EBERVcOtW7fw1ltvYfny5QCAyZMn48yZM2jZsqWWI9Owpp1KVm+hsj2JRICFfUk9HcDEh4iISEl79+6Fs7MzYmNjIZVKsXv3bnz33XcwMTHRdmiaJ5aULFkHUD75+fdnr6U6s58PEx8iIiIFFRYWIiAgAP369cOjR4/QoUMHxMfHY8CAAdoOTbucvAGfbYCFXdlyiyYl5Tq0jw8nNxMRESkgOTkZgwYNwrlz5wAA06ZNw7Jly2BkZKTlyHSI8NySdkGunTiqwB4fIiKiF9izZw9cXFxw7tw5NGzYED///DNWrlzJpOeppxsYZqeULc9O5QaGRERE+iI/Px9TpkzBgAEDkJmZCQ8PDyQkJMDbW3eGbrSOGxgSERHpv+vXr6NTp05YvXo1AGDmzJn49ddf0bRpUy1HpmO4gSEREZF+i4yMxJgxY5CdnY1GjRph27Zt6N27t7bD0k3cwJCIiEg/5eXlYfz48Rg8eDCys7PRpUsXJCYmMumpCjcwJCIi0j9Xr15Fx44dsWHDBohEInz++ec4fvw4XnrpJW2Hptu4gSEREZF+CQ8Ph6urK/766y80btwYv/zyC7744gvUqcMZIS/EDQyJiIj0w5MnTzBq1CgMHToUubm56NGjB/7880/07NlT26HpF25gSEREpNuSkpLw4YcfIikpCSKRCPPnz8e8efMgkehGz4TecfIGWvUpWb2Vk1Yyp6dpJ53p6XmKiQ8RERkUQRAQFhaGSZMmIS8vD7a2ttixYwfefvttbYem/8QSoFlXbUdRJSY+RERkMHJycjBx4kRs374dANCzZ09s374dNja6seKI1I9zfIiIyCD89ddf6NChA7Zv3w6xWIwvv/wS0dHRTHoMDHt8iIgIMrmAuOQMpGblIyOnAJb1jWArNYVbM0tIxJUtU9YPgiBg06ZN+Pjjj5Gfnw97e3vs2rULXbvq9pAMqQcTHyIiAxd9MQWB+5KQkplf7j07qQkW9HWCV2u7Cq7UfVlZWRg3bhwiIiIAAO+99x62bdsGKysrLUdG2sKhLiIiAxZ9MQUTwuMrTHoAICUzHxPC4xF9MaXC93VZQkICXF1dERERAYlEgq+++gr79+9n0mPgmPgQERkomVxA4L6kCs/UfpYAIHBfEmTyF9XUDYIgYM2aNejYsSOuX78OBwcHnDp1Cp9++inEYn7tqZVcBiSfAi7sLvmnjpzI/iwOdRERGai45IxKe3qel5KZj7jkDHi0aKTmqGrm8ePHGDNmDHbv3g0A8Pb2xpYtW2BpaanlyAxA0l4gelbZk9otmpTs6qxDGxgy9SUiMlDp2YolPdWtr2nnzp1Du3btsHv3btStWxcrVqzATz/9xKRHE5L2AlH+ZZMeAMhKKSlP2quduCrAxIeIyEBZm5uotb6mCIKA4OBgdO7cGcnJyXB0dMTp06cxffp0iET6vSJNL8hlJT09FQ6a/lsWPVtnhr2Y+BARGSi3Zpawk5pUeqb2s+ykJnBrpns9JxkZGfi///s/TJ8+HUVFRfjggw+QkJAANzc3bYdmOP45U76npwwByLpbUk8HMPEhIjJQErEIC/o6vbCeCMCCvk46t5/P2bNn4eLigp9//hlGRkZYvXo1du/ejQYNGmg7NMOSk6baemqm9cRnzZo1cHR0hImJCdzd3REXF1dl/cePH2PSpEmws7ODsbExXn31VRw8eFBD0RIR1S5ere2wzq8d7KQVD2PZSU2wzq+dTu3jI5fLsXz5cnTt2hW3bt1CixYtEBsbi0mTJnFoSxvMFNz5WtF6aqbVVV2RkZEICAjA+vXr4e7ujuDgYPTq1QtXr16FtbV1ufqFhYXo2bMnrK2tsXv3btjb2+Off/5hdk9EVANere3Q08lWL3ZufvDgAYYPH44DBw4AAAYNGoSNGzfCwsJCy5EZsKadSlZvZaWg4nk+opL3m3bSdGQVEgmCoLWNGdzd3dGhQwesXr0aQEkW7+DggClTpmD27Nnl6q9fvx7Lly/HlStXULduXYWeUVBQgIKCgtKfs7Ky4ODggMzMTP5BISLSI6dOnYKvry/u3r0LY2NjfPvttxgzZgx7eXTB01VdAMomP/9+Nj7barykPSsrC1KptMbf31ob6iosLMT58+fh6en5XzBiMTw9PREbG1vhNXv37oWHhwcmTZoEGxsbtG7dGkuWLIFMVvlM8aCgIEil0tKXg4ODyttCRETqI5fLsWTJEvTo0QN3797Fq6++iri4OIwdO5ZJj65w8i5JbiyeGxK1aKKSpEeVtDbU9eDBA8hksnKn4trY2ODKlSsVXnPz5k0cO3YMH330EQ4ePIjr169j4sSJKCoqwoIFCyq8Zs6cOQgICCj9+WmPDxER6b709HQMHToUhw8fBgD4+flh3bp1MDMz03JkVI6TN9CqT8nqrZy0kjk9TTsBYom2IytDr3ZulsvlsLa2xsaNGyGRSODq6oq7d+9i+fLllSY+xsbGMDY21nCkRERUUydOnMCQIUOQkpICU1NTrF69GiNGjGAvjy4TS4Bmun3qvdYSHysrK0gkEqSllV3elpaWBltb2wqvsbOzQ926dSGR/Jc9vv7660hNTUVhYSGMjIzUGjMREamfTCbDF198gUWLFkEul8PJyQlRUVF44403tB0a1QJam+NjZGQEV1dXxMTElJbJ5XLExMTAw8Ojwms6d+6M69evQy6Xl5Zdu3YNdnZ2THqIiGqB1NRUvPvuu1i4cCHkcjlGjBiBuLg4Jj2kMlrdxycgIACbNm3C1q1bcfnyZUyYMAG5ubkYMWIEAMDf3x9z5swprT9hwgRkZGTg448/xrVr13DgwAEsWbIEkyZN0lYTiIhIRY4ePYq2bdvi2LFjqF+/PrZt24bQ0FDUr19f26FRLaLVOT6DBg3C/fv3MX/+fKSmpsLZ2RnR0dGlE55v3boFsfi/3MzBwQG//PILpk+fjjZt2sDe3h4ff/wxZs2apa0mEBFRDRUXF2PhwoVYsmQJBEHAm2++iaioKLRq1UrboVEtpNV9fLRBVfsAEBFRzd29exdDhgzByZMnAQBjx45FcHAwTE1NtRwZ6RpVfX/r1aouIiKqPQ4dOgR/f388ePAAZmZm2LRpEwYPHqztsKiW0/pZXUREZFiKioowa9Ys9O7dGw8ePICLiwvi4+OZ9JBGsMeHiIg05tatW/D19cWZM2cAAJMmTcLXX38NE5OKD0klUjUmPkREpBH79u3DsGHD8OjRI1hYWCAkJAQDBw7UdlhkYDjURUREalVYWIhPPvkE3t7eePToEdq3b4+EhAQmPaQV7PEhIiK1SU5OxuDBgxEXFwcAmDZtGpYtW8ZNZ0lrmPgQEZFa7NmzByNHjkRmZiYaNGiAsLAw9OvXT9thkYHjUBcREalUQUEBpkyZggEDBiAzMxMdO3ZEYmIikx7SCUx8iIhIZa5fv45OnTph9erVAIBPP/0UJ0+eRNOmTbUcGVEJDnUREZFKREVFYfTo0cjOzkajRo2wdetW9OnTR9thEZXBHh8iIqqRvLw8TJgwAYMGDUJ2dja6dOmCxMREJj2kkxTq8cnKylL6xjwHi4io9rt69Sp8fHzw119/QSQSYc6cOQgMDESdOhxQIN2k0P+ZDRo0gEgkUvimIpEI165dQ/PmzasdGBER6bYdO3Zg3LhxyM3NRePGjREeHo53331X22ERVUnhlHz37t2wtLR8YT1BENC7d+8aBUVERLrryZMnmDp1KkJCQgAA3bt3x44dO9CkSRMtR0b0YgolPk2bNkW3bt3QqFEjhW7avHlz1K1bt0aBERGR7klKSoKPjw8uXboEkUiE+fPnY968eZBIJNoOjUghCiU+ycnJSt304sWL1QqGiIh0V1hYGCZNmoQnT57A1tYWO3bswNtvv63tsIiUwtlnRETVJZcB/5wBctIAMxugaSdAXPt6PnJycjBp0iRs27YNAODp6Ynw8HDY2NhoObIKGMhnQtVXrcTn3LlzOH78ONLT0yGXy8u8t2LFCpUERkSk05L2AtGzgKx7/5VZNAG8lgFO3tqLS8UuXLgAHx8fXLlyBWKxGIsWLcKcOXMgFuvgbigG8plQzSid+CxZsgRz587Fa6+9BhsbmzKrvZRZ+UVEpPMq6z1I2gtE+QMQytbPSikp99mm91+0giBg8+bNmDp1KvLz89GkSRPs2rUL3bp103ZoFav0M7lXaz4TUg2RIAjCi6v9x8bGBsuWLcPw4cPVFJJ6ZWVlQSqVIjMzk3sNEVHlKus96BUE/DKnbHkZopJ60y7o7RBLdnY2xo0bh127dgEAvLy8sG3bNjRu3FjLkVVCLgOCW1fxmQCwsNfrz4RU9/2tdF+lWCxG586dq/1AIiKd97T34Pkv0qwU4PthVX/BQgCy7pb0FOmhhIQEtGvXDrt27YJEIsGyZctw4MAB3U16gJL/1lV+JtDrz4RUS+nEZ/r06VizZo06YiEi0j65rKSn5/khE6CSskrkpKkqIo0QBAFr166Fh4cHrl+/DgcHB5w8eRIzZ87Uzfk8z8pOUW09qtWUnuMzY8YM9OnTBy1atICTk1O5/Xr27NmjsuCIiDROkd4DRZjp4IqnSmRmZmL06NHYvXs3AKBv377YsmWLwnu3aV3ufdXWo1pN6cRn6tSpOH78OHr06IFGjRpxQjMR1S417qn5d45P004qCUfd/vjjD/j4+CA5ORl169bFsmXLMG3aNP36u72+gsNwitajWk3pxGfr1q344YcfeOouEdVOSvXUiFB2+OvfZMFrqc5PohUEAd9++y0+/fRTFBUVwdHREZGRkXBzc9N2aMozt1NtParVlB64tbS0RIsWLdQRCxGR9jXtVNJjg8p6PEQlK4QGbgUsnvsitWiiF8umHz16hA8++ADTpk1DUVERPvjgAyQkJOhn0gM885lVwcJeb3rhSL2UXs6+ZcsWREdHY8uWLahXr5664lIbLmcnohcq3RMGqLBH52lyo4e7BJ89exaDBw/GP//8AyMjI3zzzTeYNGmSfg1tVUTRz4z0lqq+v5VOfFxcXHDjxg0IggBHR8dyk5vj4+OrHYwmMPEhIoVUuI+Pfckwlh5+gcrlcqxYsQJz5sxBcXExWrRogcjISLi6umo7NNWpZZ8ZlaWq72+l5/j079+/2g8jItIbTt5Aqz5616NTkYcPH2LYsGE4cOAAAMDHxwebNm2qfb/81aLPjNRH6R4ffcceHyIyJKdPn4avry/u3LkDY2NjrFq1CmPHjtX/oS0yOFrbuZmIiHSfXC5HUFAQunfvjjt37uDVV1/F77//jnHjxjHpIYOm9FCXWCyu8g+NTCarUUBERFQz6enp8Pf3xy+//AIA+Oijj7Bu3TqYm5trOTIi7VM68fnxxx/L/FxUVISEhARs3boVgYGBKguMiIiU9+uvv8LX1xcpKSkwNTXF6tWrMWLECPbyEP1LZXN8du7cicjISPz888+quJ3acI4PEdVGMpkMX375JQIDAyGXy/H6668jKioKrVu31nZoRCqhc3N8OnbsiJiYGFXdjoiIFJSamop3330XCxYsgFwux4gRI3Du3DkmPUQVUEnik5eXh2+//Rb29vbVun7NmjVwdHSEiYkJ3N3dERcXV2ndsLAwiESiMi8TE5Pqhk5EpNeOHj0KZ2dnHDt2DPXq1cO2bdsQGhqK+vXrazs0Ip2k9Byfhg0blhkrFgQB2dnZqFevHsLDw5UOIDIyEgEBAVi/fj3c3d0RHByMXr164erVq7C2tq7wGgsLC1y9erX0Z45dE5GhKS4uRmBgIL788ksIgoA333wTUVFRaNWqlbZDI9JpSic+K1euLJNoiMViNG7cGO7u7mjYsKHSAaxYsQJjxozBiBEjAADr16/HgQMHEBoaitmzZ1d4jUgkgq2trdLPIiKqDe7evYshQ4bg5MmTAIAxY8Zg1apVMDU11XJkRLpP4cQnNDQU3t7eGD58uMoeXlhYiPPnz2POnDmlZWKxGJ6enoiNja30upycHDRt2hRyuRzt2rXDkiVL8MYbb1RYt6CgAAUFBaU/Z2VlqSx+IiJNi46OxtChQ/HgwQOYmZlh48aN8PX11XZYRHpD4Tk+4eHheOmll9CpUycsW7YMV65cqfHDHzx4AJlMBhsbmzLlNjY2SE1NrfCa1157DaGhofj5558RHh4OuVyOTp064c6dOxXWDwoKglQqLX05ODjUOG4iIk0rKirCnDlz8N577+HBgwdwdnZGfHw8kx4iJSmc+Bw7dgwpKSmYOHEizp8/Dzc3N7zyyiv45JNPcPLkScjlcnXGWcrDwwP+/v5wdnbGW2+9hT179qBx48bYsGFDhfXnzJmDzMzM0tft27c1EicRkarcvn0b3bt3x9KlSwEAEydORGxsLF555RUtR0akf5Sa49OwYUP4+fnBz88PhYWFOHbsGPbu3YuPPvoIeXl56N27N7y9vfHee+8ptKLAysoKEokEaWlpZcrT0tIUnsNTt25duLi44Pr16xW+b2xsDGNjY4XuRUSkSTK5gLjkDKRn58Pa3ARuzSwhEZddrLF//34MGzYMGRkZsLCwQEhICAYOHKiliIn0X7WXsxsZGcHLywtr167F7du3ER0dDUdHRyxevBgrVqxQ+B6urq5l9v+Ry+WIiYmBh4eHQveQyWS4cOEC7OzsqtUOIiJtiL6Ygi7LjsF301l8HJEI301n0WXZMURfTAFQMgfyk08+Qd++fZGRkYH27dsjISGBSQ9RDSm9qqsy9evXx5MnT/Dnn3+iqKhI4esCAgIwbNgwtG/fHm5ubggODkZubm7pKi9/f3/Y29sjKCgIALBo0SJ07NgRLVu2xOPHj7F8+XL8888/GD16tKqaQkSkVtEXUzAhPB7Pb5ufmpmPCeHxWNijMdbOn1K6p9m0adOwdOlS9l4TqUCNEp/c3FxEREQgJCQEZ8+ehZOTE77++mvUrVtX4XsMGjQI9+/fx/z585GamgpnZ2dER0eXTni+desWxOL/OqYePXqEMWPGIDU1FQ0bNoSrqyvOnDkDJyenmjSFiEgjZHIBgfuSyiU9ACCCHE3+txvjV0UhPz8fDRo0QFhYGPr166fxOIlqq2qd1fXbb78hJCQEUVFRyMvLw/Tp0zF69Gi92DiLZ3URkTbF3ngI301ny5V7ys9A+PU7hMZlAwA6viRBxDBHNB2yAnDy1nSYRDpH42d1paen46uvvkKrVq0wcOBANGjQACdOnIBYLMbIkSP1IukhItK29Oz8cmUemdFIDF9amvTM8DDCyeH10LTOAyDKH0jaq+kwiWothYe6mjZtioEDB2LVqlXo2bNnmeEnIiJSjLV52bMF866cxIHoNcguENDIVISt/U3Q59Wn0wUEACIgejbQqg8glmg8XqLaRqnE5/Tp03j55ZfRtGlT9vAQEVWDWzNL2ElNcO9hFjKObUZOwkEAQGcHCSIGmuIli+d/qRSArLvAP2eAZl01HzBRLaNwt82VK1cQHh6OlJQUdOjQAa6urli5ciUAHhJKRKQoiViEUW8aI2X7J6VJz5wuRjgxvF4FSc8zctIqf4+IFKbUeFXnzp0RGhqKlJQUjB8/Ht9//z1kMhkmTpyITZs24f79++qKk4ioVti5cyem+76HovRk1KnfAN0GjcKSd0xQR/yCXyDNbKp+n4gUUq1VXc+6fPkyQkJCsH37dmRkZCi1h482cFUXEWnDkydP8PHHH2Pz5s0AgO7du2Pb9nDcyasL5x+6wCQ/HaJKFrnDogkw7QLn+JBB0/iqrsq8/vrr+Prrr3H37l1ERkbW9HZERLXO5cuX4e7ujs2bN0MkEmH+/Pk4evQoHF6yh8cr1jD1/hol/T3P9/r8+7PXUiY9RCqiUOKTlZX1wjp16tTBBx98AADIzs6uWVRERLXE1q1b0b59e1y8eBE2NjY4evQoAgMDIZE8k8g4eQM+2wCL547esWhSUs59fIhURqFVXQ0bNkRKSgqsra0Vuqm9vT0SExPRvHnzGgVHRKQz5LKSlVU5aSXzbZp2qrIXJjc3FxMnTsS2bdsAAJ6enggPDy/dlb4cJ++SJetKPIOIlKdQ4iMIAjZv3gwzMzOFbqrr83yIiJSStBeIngVk3fuvzKIJ4LWswt6YCxcuwMfHB1euXIFYLEZgYCDmzJlTtpenImIJl6wTqZlCic/LL7+MTZs2KXxTW1tbpc7rIiLSWUl7S3ZPfn7icVZKSfkzQ1GCICAkJARTpkxBfn4+mjRpgp07d+Ktt97SfNzVJJMLiEvOQHp2PqzNTeDWzBKSF604I9IjCiU+f//9t5rDICLSQXJZSU9Phautyu6qnJ37BOPHj8fOnTsBAF5eXti2bRsaN26syYhrJPpiCgL3JSEl879jNeykJljQ1wlere2quJJIf/DcCSKiyvxzpuzwVjkluyonHtoKV1dX7Ny5ExKJBEuXLsWBAwf0LumZEB5fJukBgNTMfEwIj0f0xRQtRUakWkx8iIgq84LdkgVBwLpzhXD/v3H43//+B2vbJjh2/ARmzZqlV+cZyuQCAvclVdqvBQCB+5Igk9do2zcinaA/fzKJiDStit2SM/MFDNqdh4kH81FYVAzTFh1g5PM1Zp0u0LvekbjkjHI9Pc8SAKRk5iMuOUNzQRGpCRMfIqLKNO1UsnrruY0F/7gnQ7uNOfg+qRh1xIBlj5FoPGA+JKYWejk0lJ5dedJTnXpEuoyJDxFRZcSSkiXrAAARBEHAt78XoFNILm4+EtBUKsL7fiNh7vZB6WHN+jg0ZG1uotJ6RLpMoVVdz3v8+DHi4uKQnp4OuVxe5j1/f3+VBEZEpBP+3VX50Z4ZGLUjGT9eKQYAvNeqHqy9JuGEcfml6s8ODXm0aKThgJX3KLcQYhFQWZ4mAmArLVnaTqTvlE589u3bh48++gg5OTmwsLAo/S0HAEQiERMfIqp1fs+2waDNhfjnn2IY1a2DCWOG4WczbySJqt6QUB+GhqIvpmDSzvgKJzY/a0FfJ+7nQ7WC0kNdn3zyCUaOHImcnBw8fvwYjx49Kn1lZHDiGxHVHoIg4JtvvkGXLl3wzz//oHnz5jgTexaDApZBeEHSA+j+0FBVq7meEouANUNcuI8P1RpK9/jcvXsXU6dORb169dQRDxGRTnj48CGGDx+O/fv3AwB8fHywceNGSKVSyOQC7KQmSM3MrzBp0JehoRet5gJKhr8a1jfWUERE6qd0j0+vXr3wxx9/qCMWIiKd8Ntvv8HZ2Rn79++HsbEx1q1bh4iICEilUgCARCzCgr5OAJ5f7/Xfz/owNMTVXGSIlO7x6dOnDz799FMkJSXhzTffLHcml7d3+QP7iIj0gVwux1dffYW5c+dCJpPhlVdeQVRUFJydncvV9Wpth3V+7cod8WCrR0c8cDUXGSKRIAhKrbesajdSkUgEmUxW46DUKSsrC1KpFJmZmbCwsNB2OESkI+7fvw9/f39ER0cDAIYMGYL169fD3Ny8yuv0+VBPmVxAl2XHXjhkd3rW23rTJqq9VPX9rXSPz/PL14mI9N2vv/6KIUOG4N69ezA1NcV3332HkSNHllm1WhmJWKQXS9Yr8nTIbkJ4PEQoexSrPg3ZESmDGxgSkcGSyWRYvHgx3n77bdy7dw+vv/464uLiMGrUKIWSntrg6ZCdrbTscJat1ATr/NrpxZAdkTKqtYHhr7/+iq+//hqXL18GADg5OeHTTz9F165dVRocEZG6pKamws/PDzExMQCA4cOHY/Xq1ahfv76WI9M8r9Z26Olkq7dDdkTKULrHJzw8HJ6enqhXrx6mTp2KqVOnwtTUFO+88w527typjhiJiFQqJiYGzs7OiImJQb169bB161Zs2bLFIJOep54O2fVztodHi0ZMeqjWUnpy8+uvv46xY8di+vTpZcpXrFiBTZs2lfYC6SpObiYyXDKZDIsWLcLixYshCAJat26NqKgovP7669oOjYheQFXf30r3+Ny8eRN9+/YtV+7t7Y3k5ORqB0JEpE737t3DO++8g0WLFkEQBIwZMwZxcXFMeogMjNKJj4ODQ+mY+LOOHj0KBwcHlQRFRKRKv/zyC9q2bYtff/0VZmZm2LFjBzZu3AhTU1Nth0ZEGqb05OZPPvkEU6dORWJiIjp16gSgZJfTsLAwrFq1SuUBEhFVV3FxMebNm4elS5cCANq2bYuoqCi8+uqrWo6MiLRF6cRnwoQJsLW1xTfffIOoqCgAJfN+IiMj0a9fP5UHSERUHbdv34avry9+++03AMDEiRPxzTffwMSEuxATGTKlJzfrO05uJqr9Dhw4AH9/f2RkZMDCwgKbN2/Ghx9+qO2wiKgGtDa5mYhIVxUVFWHGjBl4//33kZGRAVdXV8THxzPpIaJSCiU+lpaWePDgAQCgYcOGsLS0rPRVHWvWrIGjoyNMTEzg7u6OuLg4ha6LiIiASCRC//79q/VcIqo9/v77b3Tt2hXffPMNAODjjz/Gb7/9hhYtWmg5MiLSJQrN8Vm5cmXpQX0rV65U6VbukZGRCAgIwPr16+Hu7o7g4GD06tULV69ehbW1daXX/f3335gxYwZ3iyaqzeQy4J8zQE4aYGYDNO0EiCXlqv30008YMWIEHj9+jAYNGmDLli38hYiIKqT1OT7u7u7o0KEDVq9eDaDkEFQHBwdMmTIFs2fPrvAamUyGbt26YeTIkTh16hQeP36Mn376SaHncY4PkZ5I2gtEzwKy7v1XZtEE8FoGOHkDAAoKCjBr1qzSFaXu7u6IiIiAo6OjFgImInXS2hwfiUSC9PT0cuUPHz6ERFL+N7GqFBYW4vz58/D09PwvILEYnp6eiI2NrfS6RYsWwdraGqNGjXrhMwoKCpCVlVXmRUQ6LmkvEOVfNukBgKyUkvKkvbh58yY6d+5cmvR88sknOHnyJJMeIqqS0svZK+sgKigogJGRkVL3evDgAWQyGWxsbMqU29jY4MqVKxVec/r0aYSEhCAxMVGhZwQFBSEwMFCpuIhIi+Sykp4eVPR3jQABIuxcMh7jfs5Bbk42LC0tsXXrVrz//vuajpSI9JDCic+3334LABCJRNi8eTPMzMxK35PJZDh58iRatWql+gifkZ2djaFDh2LTpk2wsrJS6Jo5c+YgICCg9OesrCzuME2ky/45U76n51/5xQICfsnDuj8yAQDmTd9A8OYwvO/ZXpMREpEeUzjxWblyJYCSHp/169eXGdYyMjKCo6Mj1q9fr9TDraysIJFIkJaWVqY8LS0Ntra25erfuHEDf//9d5mzwuRyeUlD6tTB1atXy63gMDY2hrGxsVJxEZEW5aRVWPy/hzL47M5DYmrJn3lXj0540Hkm5h9NQ2PbFHi1ttNklESkpxROfJ4eQNqjRw/s2bMHDRs2rPHDjYyM4OrqipiYmNIVGHK5HDExMZg8eXK5+q1atcKFCxfKlM2dOxfZ2dlYtWoVe3KIagMzm3JFuy4UYez+POQUAo3ribD9/0yx5eX+eCAv+SsscF8SejrZQiJW3YpTIqqdlJ7jc/z4cZUGEBAQgGHDhqF9+/Zwc3NDcHAwcnNzMWLECACAv78/7O3tERQUBBMTE7Ru3brM9Q0aNACAcuVEpKeadipZvZWVgrwiOaYeysfmhCIAwFtNJQj/wBRi88aIKygZWhcApGTmIy45Ax4tGmkxcCLSB0onPgMGDICbmxtmzZpVpvyrr77CuXPn8P333yt1v0GDBuH+/fuYP38+UlNT4ezsjOjo6NIJz7du3YJYzA2miQyGWAJ4LcPlNR/BZ/cTXEyXQwRgXjcjfN7NGHXEIkwoGgr5c4tS07PztRMvEekVpffxady4MY4dO4Y333yzTPmFCxfg6elZbr6OruE+PkS6b9u2bZgwfiye5BXApr4IOz4wxTvN6+Ce0AiBRUPxi9yt3DW7xnRkjw9RLaaq72+le3xycnIqXLZet25d7pFDRDWSm5uLyZMnIywsDADwzjtvY+uXk7HhyJ/Y9KQ+4uStyvX0iADYSk3g1qx6R+YQkWFRegzpzTffRGRkZLnyiIgIODk5qSQoIjI8Fy9eRIcOHRAWFgaxWIxFixbhl18Ow979/9Cp/zj8LneCUEHSAwAL+jpxYjMRKUTpHp958+bhgw8+wI0bN/D2228DAGJiYrBr1y6l5/cQEQmCgNDQUEyZMgV5eXlo0qQJdu7cibfeequ0jldrO6zza4fAfUlIyfxvLo+t1AQL+jpxKTsRKaxaZ3UdOHAAS5YsQWJiIkxNTdGmTRssWLCgzF9UuopzfIh0R3Z2NiZMmIAdO3YAAHr16oXt27ejcePGFdaXyQXEJWcgPTsf1uYlw1vs6SEyDKr6/tb6IaWaxsSHSI0UPE0dAP7880/4+Pjg2rVrkEgk+OKLLzBz5kyu4iSiCmltcjMRUYWS9gKHZgLZKf+VmdsB731Vepo6UDK0tWHDBkybNg0FBQV46aWXEBERgc6dO2shaCIyNAolPpaWlrh27RqsrKzQsGFDiESVdy1nZGSoLDgi0hNJe4GooeXLs1NKyn22A07eyMzMxNixYxEVFQUAeP/99xEWFoZGjbgMnYg0Q6HEZ+XKlTA3NwcABAcHqzMeItI3chmwb2rVdfZ9jPO5thjkOwQ3btxAnTp1sGzZMkyfPr3KX6SIiFSNc3yIqGZunAC296v0bUEQsDquCAFHi1FcXAxbewf88H0UOnl01FyMRKT3NDrHR5mNCZlMEBmYf05X+tajPAGj9ubhxyvFAADTVzqiTu9pmHgkC0vNeaI6EWmeQolPgwYNFO6OlslkNQqIiPRMJX3GcXdlGLT7Cf5+LKCuGOjr2QF/OH8OkUiEx0+KMD48Huv92jH5ISKNUijxefZE9r///huzZ8/G8OHD4eHhAQCIjY3F1q1bERQUpJ4oiUh3NesKnFpe+qMgCFh5thCzjhagWA40byhC5MB6+MZqAETysr9ALdx7CT2dbLkXDxFpjNJzfN555x2MHj0avr6+Zcp37tyJjRs34sSJE6qMT+U4x4dIxeQyYHlLIC8DGXkChv+Uh33XSoa2PnSqg43vm0JuYo72BevLnbMF8HBRIlKMqr6/ld4pLDY2Fu3bty9X3r59e8TFxVU7ECLSU2IJ0HcVztwuhvP6HOy7VgxjCbC2twkiBphCaiLCnKLRFSY9AJCenV9hORGROiid+Dg4OGDTpk3lyjdv3gwHBweVBEVE+kMul2PZvsvotjUft7MEvGIpxtnR9TGhgxFSRY0wvmgafpG7VXq9tbmJBqMlIkOn9M7NK1euxIABA3Do0CG4u7sDAOLi4vC///0PP/zwg8oDJCLddf/+ffj7+yM6OhoAMMTXF+vnDEN9IQsT995DdE7zSnt6AMDWwhhuzSw1FS4RkfI9Pr1798a1a9fQt29fZGRkICMjA3379sW1a9fQu3dvdcRIRDro5MmTcHZ2RnR0NExMTLB582aE79gB8zd7QdzmQ3j396ky6QGAhd5vcGIzEWkUNzAkIqXIZDIEBQVhwYIFkMvlaNWqFb7//nu0bt26XN3oiymYvecCHj8pKlPeoF5dLP3gTS5lJyKFafWQ0lOnTmHDhg24efMmvv/+e9jb22P79u1o1qwZunTpUu1giEi3paWlwc/PD0ePHgUADBs2DGvWrEH9+vUrrO/V2g49nWxx9uZDxN54CECAR3MrdGzRiD09RKQVSg91/fDDD+jVqxdMTU0RHx+PgoICAEBmZiaWLFmi8gCJSDccO3YMbdu2xdGjR1GvXj2EhYUhLCys0qTnKYlYhM4trTCj12uY0asVOr9ixaSHiLRG6cTniy++wPr167Fp0ybUrVu3tLxz586Ij49XaXBEpH0ymQwLFiyAp6cn0tLS0Lp1a5w7dw7Dhg3TdmhEREpTeqjr6tWr6NatW7lyqVSKx48fqyImItIUuQz45wyQkwaY2QBNO5Xsy/Ove/fu4aOPPirdmHT06NFYtWoV6tWrp6WAiYhqRunEx9bWFtevX4ejo2OZ8tOnT6N58+aqiouI1C1pLxA9C8i691+ZRRPAaxng5I3Dhw/Dz88P9+/fh5mZGTZs2IAhQ4ZoL14iIhVQOvEZM2YMPv74Y4SGhkIkEuHevXuIjY3FjBkzMG/ePHXESEQqJrv0M8TfDwMg4NnZNkLWPRRHDMXkG29h4859AIC2bdsiKioKr776qlZiJSJSJaUTn9mzZ0Mul+Odd97BkydP0K1bNxgbG2PGjBmYMmWKOmIkIhWKvnAHLj9MR2NBwPNzjO9myeG7Ow+nb5ckPX0GDcPusPUwMeHuykRUOyiV+MhkMvz222+YNGkSPv30U1y/fh05OTlwcnKCmZmZumIkIhWJvpiCsF274GX0EHgu6TlwrQjDfsrHwzwBFsbAG+99hEuOH+LE9Ufcb4eIag2lVnVJJBK8++67ePToEYyMjODk5AQ3NzcmPUR6QCYXELgvCTbIKFNeJBPw6eF8vL8rDw/zBLjaiRE/1gwdXneAACBwXxJkcoPa55SIajGll7O3bt0aN2/eVEcsRKRGcckZSMnMRyNRVmnZP4/l6Bb2BF/HFgIAproZ4beR9dHCUlxaLyUzH3HJGRXek4hI31RrH58ZM2Zg//79SElJQVZWVpkXEemm9Ox8AECGYA4A+PlKEZw35ODsHRkamAB7fEyx6j0TGNcRlan37LVERPpO6cnNTw8i9fb2hkj03yQBQRAgEokgk8lUFx0RqYy1eckEZQv5Y0yLzseq30t6edzsxYgcWA+ODcr+HmQpyi53LRGRvlM68Tl+/Lg64iAiNXNrZomGsgxsCP8Z6aklSc8nHkZY8o4xjCTlj5B4KJQcAmgnNYFbM0uNxkpEpC5KJT6CIKBJkyYoLCzEa6+9hjp1qnXGKRFpwY97fsC19ZPwJCcblqYihPUzQd/X6lZaPw2WEAFY0NeJZ2sRUa2h8Byf5ORktGnTBq1atUKbNm3QokUL/PHHH+qMjYhUID8/H5MmTcKHH36IJznZcHJujyPjXkKfVytOeuQCcE9ohNtmbbHOrx2XshNRraJw4vPpp5+iuLgY4eHh2L17N1566SWMGzdOnbERUQ3973//Q6dOnbB27VoAJRuQJsadQdvR30EkEuH5ReoCAJFIhOzui3Fydk8mPURU6yg8VnX69Gns3r0bXbp0AQB07NgRL730EnJzc1G/fn21BUhE1RMREYExY8YgJycHVlZW2L59O7y8vErefKMfINpW7qwukYU94LUUrzl5aylqIiL1UrjHJz09Ha+88krpz3Z2djA1NUV6enqNg1izZg0cHR1hYmICd3d3xMXFVVp3z549aN++PRo0aID69evD2dkZ27dvr3EMRLVFXl4exo0bB19fX+Tk5KBbt25ITEz8L+l5yskbmHYRGLYfGBBS8s9pF0rKiYhqKYV7fEQiEXJycmBqalpaJhaLkZ2dXWb/HgsLC6UCiIyMREBAANavXw93d3cEBwejV69euHr1KqytrcvVt7S0xOeff45WrVrByMgI+/fvx4gRI2BtbY1evXop9Wyi2ubKlSvw8fHBhQsXIBKJMHfuXMyfP7/yhQhiCdCsq2aDJCLSIpEgCArtRS8Wi8vs2wP8t3fPs/+u7D4+7u7u6NChA1avXg0AkMvlcHBwwJQpUzB79myF7tGuXTv06dMHixcvfmHdrKwsSKVSZGZmKp2kEemybdu2YcKECXjy5AlsbGwQHh4OT09PbYdFRKQSqvr+VrjHRx379xQWFuL8+fOYM2dOaZlYLIanpydiY2NfeL0gCDh27BiuXr2KZcuWVVinoKAABQUFpT9zd2mqbXJzczF58mSEhYUBAN5++23s2LEDtra22g2MiEgHKZz4vPXWWyp/+IMHDyCTyWBjY1Om3MbGBleuXKn0uszMTNjb26OgoAASiQRr165Fz549K6wbFBSEwMBAlcZNpCsuXboEHx8fJCUlQSwWY+HChfjss88gkUi0HRoRkU5S+qyuZ/Xp0wcpKSmqikVh5ubmSExMxLlz5/Dll18iICAAJ06cqLDunDlzkJmZWfq6ffu2ZoMlUgNBEBAaGooOHTogKSkJdnZ2iImJwbx585j0EBFVoUZbL588eRJ5eXnVvt7KygoSiQRpaWllytPS0qrspheLxWjZsiUAwNnZGZcvX0ZQUBC6d+9erq6xsTGMjY2rHSORrsnJycH48eOxY8cOAMC7776L7du3V7gYgIiIyqpRj09NGRkZwdXVFTExMaVlcrkcMTEx8PDwUPg+crm8zDweotrqzz//hKurK3bs2AGJRIKgoCAcOnSISQ8RkYJq1OPTtGlT1K1b+Vk/iggICMCwYcPQvn17uLm5ITg4GLm5uRgxYgQAwN/fH/b29ggKCgJQMmenffv2aNGiBQoKCnDw4EFs374d69atq1EcRLpMEARs3LgRH3/8MQoKCvDSSy9h165dpRuKEhGRYmqU+Fy8eLH03+/cuYNFixZh48aNSt1j0KBBuH//PubPn4/U1FQ4OzsjOjq6dMLzrVu3IBb/1zGVm5uLiRMn4s6dOzA1NUWrVq0QHh6OQYMG1aQpRDorKysLY8eORWRkJICSuXVbt25Fo0aNtBwZEZH+UXgfnxf5888/0a5dO6X38dE07uND+iQ+Ph4+Pj64ceMG6tSpg6VLl2L69OllfhkgIjIEGt/Hh4g0RxAErFmzBp988gkKCwvRtGlTREREoGPHjtoOjYhIrzHxIdIxjx8/xqhRo7Bnzx4AQP/+/REaGoqGDRtqOTIiIv3H/nIiHRIXFwcXFxfs2bMHdevWxapVq7Bnzx4mPUREKqJwj88HH3xQ5fuPHz+uaSxEBksQBAQHB2PWrFkoKipC8+bNERkZifbt22s7NCKiWkXhxEcqlb7wfX9//xoHRGRoMjIyMHz4cOzbtw8AMHDgQGzevPmFf+aIiEh5Cic+W7ZsUWccRAbpzJkzGDx4MG7fvg1jY2OsXLkS48ePh0gk0nZoRES1Euf4EGmBXC7HV199hW7duuH27dt45ZVXcPbsWUyYMIFJDxGRGinc4zNy5EiF6oWGhlY7GCJ9I5MLiEvOQHp2PqzNTeDWzBIScdWJy/379zFs2DAcOnQIAODr64sNGzbA3NxcEyETERk0hROfsLAwNG3aFC4uLlDRnodEei36YgoC9yUhJTO/tMxOaoIFfZ3g1dquwmtOnTqFwYMH4969ezAxMcF3332HUaNGsZeHiEhDFE58JkyYgF27diE5ORkjRoyAn58fLC0t1Rkbkc6KvpiCCeHxeP5XgNTMfEwIj8c6v3Zlkh+5XI6goCDMnz8fcrkcrVq1QlRUFN58803NBk5EZOAUnuOzZs0apKSkYObMmdi3bx8cHBzg4+ODX375hT1AZFBkcgGB+5LKJT0ASssC9yVBJi/5KS0tDV5eXpg7dy7kcjn8/f1x7tw5Jj1ERFqg1ORmY2Nj+Pr64siRI0hKSsIbb7yBiRMnwtHRETk5OeqKkUinxCVnlBneep4AICUzH3HJGTh27BicnZ1x5MgR1KtXD1u2bMHWrVthZmamuYCJiKhUtY+sEIvFEIlEEARB5w8mJVKl9OzKk56nBLkMq5Z/iaiNwRAEAW+88QaioqLg5OSkgQiJiKgySvX4FBQUYNeuXejZsydeffVVXLhwAatXr8atW7f4GywZDGtzkyrfL87JQFrkXERuWAlBEDDKpw/iQufAyfQhIOcvCURE2qRwj8/EiRMREREBBwcHjBw5Ert27YKVlZU6YyPSSW7NLGEnNUFqZn65eT55yfF4sP8byJ9kor6pCTb8X0N89Mop4OCpkgoWTQCvZYCTt8bjJiIiQCQoODNZLBbj5ZdfhouLS5VLb5+eKK2rsrKyIJVKkZmZCQsLC22HQ3rq6aouoGROjyCX4fHpHciK/R6AgFebO2Bf70d4tdHznar//tnx2cbkh4hICar6/la4x8ff3597jRD9y6u1Hdb5tUPgviTcvn0HD/YtR8GdSwCA3h/64QfXOJjkZVZwpQBABETPBlr1AcQSjcZNRGTolNrAkIj+49XaDkV/x2Po8gAUPMpAvfpmCNm8GYPdmwBb91ZxpQBk3QX+OQM066qxeImIiGd1EVVLUVERZs6cCe++7yPzUQbatWuHv/5MxODBg4CcNMVuomg9IiJSmWovZycyVLdu3cLgwYMRGxsLAJgyZQqWL18OY2PjkgpmNordSNF6RESkMuzxIVLC3r174ezsjNjYWEilUvzwww/49ttv/0t6AKBpp5LVW6hsTpwIsLAvqUdERBrFxIdIAYWFhZg+fTr69euHR48ewc3NDQkJCfjggw/KVxZLSpasAyif/Pz7s9dSTmwmItICJj5EL5CcnIwuXbogODgYABAQEIBTp06hWbNmlV/k5F2yZN3iuVPaLZpwKTsRkRZxjg9RFX744QeMGjUKmZmZaNiwIbZu3Yq+ffsqdrGTd8mS9X/OlExkNrMpGd5iTw8RkdYw8SGqQH5+PmbMmIE1a9YAADp16oRdu3bh5ZdfVu5GYgmXrBMR6RAOdRE95/r16+jUqVNp0jNr1iycOHFC+aSHiIh0Dnt8iJ4RERGBsWPHIjs7G1ZWVti+fTu8vLy0HRYREakIe3yIAOTl5WHcuHHw9fVFdnY2unXrhsTERCY9RES1DBMfMnhXr15Fx44dsXHjRohEIsydOxcxMTGwt7fXdmhERKRiHOoigxYeHo7x48cjNzcX1tbW2LFjBzw9PbUdFhERqQl7fMggPXnyBCNHjsTQoUORm5uLt99+G4mJiUx6iIhqOSY+ZHAuXbqEDh06YMuWLRCLxQgMDMThw4dhZ2f34ouJiEivcaiLDIYgCAgLC8OkSZOQl5cHOzs77Ny5E927d9d2aEREpCFMfMgg5OTkYMKECQgPDwcAvPvuu9i+fTusra21HBkREWkSEx8dJ5MLiEvOQHp2PqzNTeDWzBIScWWnflNF/vrrL/j4+ODq1auQSCRYvHgxZs2aBbGYI71ERIaGiY8Oi76YgsB9SUjJzC8ts5OaYEFfJ3i15nyUFxEEAZs2bcLUqVNRUFAAe3t7REREoEuXLtoOjYiItEQnfuVds2YNHB0dYWJiAnd3d8TFxVVad9OmTejatSsaNmyIhg0bwtPTs8r6+ir6YgomhMeXSXoAIDUzHxPC4xF9MUVLkemHrKwsDBkyBOPGjUNBQQH69OmDxMREJj1ERAZO64lPZGQkAgICsGDBAsTHx6Nt27bo1asX0tPTK6x/4sQJ+Pr64vjx44iNjYWDgwPeffdd3L17V8ORq49MLiBwXxKECt57Wha4LwkyeUU1KCEhAa6uroiIiECdOnWwfPly7N27F1ZWVtoOjYiItEwkCIJWvz3d3d3RoUMHrF69GgAgl8vh4OCAKVOmYPbs2S+8XiaToWHDhli9ejX8/f3LvV9QUICCgoLSn7OysuDg4IDMzExYWFioriEqFHvjIXw3nX1hvV1jOsKjRSMNRKQfBEHA2rVrERAQgMLCQrz88suIjIxEx44dtR0aERHVUFZWFqRSaY2/v7Xa41NYWIjz58+X2TROLBbD09MTsbGxCt3jyZMnKCoqgqWlZYXvBwUFQSqVlr4cHBxUErs6pWfnv7iSEvUMwePHj/Hhhx9i8uTJKCwsRL9+/ZCQkMCkh4iIytBq4vPgwQPIZDLY2NiUKbexsUFqaqpC95g1axaaNGlS6Y67c+bMQWZmZunr9u3bNY5b3azNTVRar7Y7d+4c2rVrhx9++AF169ZFcHAwfvzxx0qTYSIiMlx6vapr6dKliIiIwIkTJ2BiUnESYGxsDGNjYw1HVjNuzSxhJzVBamZ+hfN8RABspSVL2w2ZIAhYtWoVZs6ciaKiIjRr1gyRkZHo0KGDtkMjIiIdpdUeHysrK0gkEqSlpZUpT0tLg62tbZXXfv3111i6dCkOHz6MNm3aqDNMjZOIRVjQ1wlASZLzrKc/L+jrZND7+WRkZKB///6YPn06ioqKMHDgQCQkJDDpISKiKmk18TEyMoKrqytiYmJKy+RyOWJiYuDh4VHpdV999RUWL16M6OhotG/fXhOhapxXazus82sHW2nZnixbqQnW+bUz6H18YmNj4eLigr1798LIyAhr1qxBVFQUpFKptkMjIiIdp/WhroCAAAwbNgzt27eHm5sbgoODkZubixEjRgAA/P39YW9vj6CgIADAsmXLMH/+fOzcuROOjo6lc4HMzMxgZmamtXaog1drO/R0suXOzf+Sy+X45ptv8Nlnn6G4uBgtW7ZEVFQUXFxctB0aERHpCa0nPoMGDcL9+/cxf/58pKamwtnZGdHR0aUTnm/dulXmaIF169ahsLAQAwcOLHOfBQsWYOHChZoMXSMkYhGXrKNkIvywYcNw8OBBAICvry82bNgAc3NzLUdGRET6ROv7+GiaqvYBIM05deoUfH19cffuXZiYmODbb7/F6NGjIRIZZs8XEZEhqhX7+BBVRS6XY8mSJejRowfu3r2L1157Db///jvGjBnDpIeIiKpF60NdRBVJT0+Hn58fjhw5AgAYOnQo1q5dW+vmcRERkWYx8SGdc/z4cQwZMgSpqakwNTXF2rVrMXz4cG2HRUREtQCHukhnyGQyBAYGwtPTE6mpqXjjjTfwxx9/MOkhIiKVYY8P6YSUlBT4+fnh2LFjAIBRo0bh22+/Rb169bQcGRER1SZMfEjrjhw5Aj8/P6Snp6N+/frYsGEDPvroI22HRUREtRCHukhriouLMXfuXPTq1Qvp6elo06YNzp8/z6SHiIjUhj0+pBV37tzBkCFDcOrUKQDA+PHjsWLFCpiammo5MiIiqs2Y+JDGHTp0CEOHDsXDhw9hbm6OTZs2YdCgQdoOi4iIDACHukhjioqKMGvWLPTu3RsPHz5Eu3btEB8fz6SHiIg0hj0+ukguA/45A+SkAWY2QNNOgFii7ahq5NatWxg8eDBiY2MBAFOmTMHy5cthbGys5ciIiMiQMPHRNUl7gehZQNa9/8osmgBeywAnb+3FVQN79+7F8OHD8ejRI0ilUoSGhuKDDz7QdlhERGSAONSlS5L2AlH+ZZMeAMhKKSlP2quduKqpsLAQAQEB6NevHx49eoQOHTogISGBSQ8REWkNEx9dIZeV9PRAqODNf8uiZ5fU0wPJycno2rUrVq5cCQAICAjA6dOn0axZMy1HRkREhoyJj67450z5np4yBCDrbkk9Hbdnzx64uLggLi4ODRs2xN69e/HNN9/AyMhI26EREZGBY+KjK3LSVFtPCwoKCjBlyhQMGDAAmZmZ8PDwQGJiIvr27avt0IiIiAAw8dEdZjaqradh169fR6dOnbB69WoAwMyZM/Hrr7/i5Zdf1nJkRERE/+GqLl3RtFPJ6q2sFFQ8z0dU8n7TTpqO7IUiIyMxZswYZGdnw8rKCtu2bcN7772n7bCIiIjKYY+PrhBLSpasAwBEz735789eS9W6n49MLiD2xkP8nHgXsTceQiavKAH7T15eHsaPH4/BgwcjOzsbXbt2RWJiIpMeIiLSWezx0SVO3oDPtkr28Vmq1n18oi+mIHBfElIy80vL7KQmWNDXCV6t7crVv3r1Knx8fPDXX39BJBLh888/x4IFC1CnDv+XIiIi3SUSBKHqX+trmaysLEilUmRmZsLCwkLb4VRMwzs3R19MwYTw+HIDbE/7ndb5tSuT/ISHh2P8+PHIzc2FtbU1wsPD0bNnT7XFR0REpKrvb/56rovEEqBZV408SiYXELgvqdLdg0QAAvcloaeTLQry8zBlyhSEhoYCAHr06IEdO3bAzq58jxAREZEu4hwfAxeXnFFmeOt5AoCUzHxEHo6Fm5sbQkNDIRKJsHDhQhw5coRJDxER6RX2+GiDDh1Cmp5dedLzVM6Foxixaj0K8/Nha9UAO1ctRI/Bk/X+4FQiIjI8THw0SS4DTn4N/L4WyHv8X7kWDyG1Njep9D15YR4yjqxD7sVjAIB3W0iw/f+KYX1tPhC8Xq8PTiUiIsPEoS5NSdoLLG8JnFhSNukBtHoIqVszS9hJTcotoC+8/zdStk5H7sVjEIuAL942xqGP6sG6vljrMRMREVUXEx9NeHrqel5GJRW0dwipRCzCgr5OAEomMguCgOzEaKRuC0Bxxh3YmktwfFg9fN7VGGLRs+mR/h2cSkRExMRH3ao8df1Z2juE1Ku1Hdb5tYOVsQwP9i1Hxi+rIRQX4uVXnXBhvCm6Na1sRFR/Dk4lIiICmPio3wtPXX+Olg4htSlKxeOdn+DJ5ZOQ1KmDSbMW4EbkfFjVU+B/ER0+OJWIiOhZTHzUTdmkQMOHkAqCgLVr16Jjx464fv06Xn75ZZw6eRKrly5EHWkTxW6iowenEhERPY+Jj7opkxRY2Gv0ENLMzEz4+Phg0qRJKCwshLe3NxISEuDh4VFS4enBqeWmPj8l0njMRERENcHER91emDw8Q82HkD7r3LlzcHFxwe7du1G3bl2sXLkSP/30EywtLf+rpAMHpxIREakSEx91qzJ5+JepJeCzXSN74giCgFWrVqFz585ITk5Gs2bN8Ntvv2HatGkQiSqI7+nBqRbP7dBs0aSknPv4EBGRHuEhpZqStLf8qeumloD7eKDbDI30mmRkZGDkyJH4+eefAQADBgzA5s2b0aBBgxdfrEO7TRMRkeHhIaX6xskbaNUHuHkS+GsXUJgLvNwRcBtXZQIhkwuIS85AelYuWj65gNfNn0Bsbqt04nH27FkMGjQIt27dgpGREVasWIGJEydW3MtTEQ0enEpERKQuWh/qWrNmDRwdHWFiYgJ3d3fExcVVWvfSpUsYMGAAHB0dIRKJEBwcrLlAVeHoQmDHB8BfkcCV/cDhucCXNsDheRVWj76Ygi7LjiEs5Ft0+OktvHFkCMR7RgNb3weCWyu0a7JcLsfXX3+Nrl274tatW2jZsiXOnj2LSZMmKZ70EBER1RJaTXwiIyMREBCABQsWID4+Hm3btkWvXr2Qnp5eYf0nT56gefPmWLp0KWxtbTUcrYLkMiD5FHBhd8k/n+5qfHgecOZbQJCXrS/IS8qfS36iL6ZgQng82mSfxLq6wbBF2V2fBQWOjHjw4AG8vb3x6aefori4GIMHD8b58+fh4uKikqYSERHpG63O8XF3d0eHDh2wevVqACW9Ew4ODpgyZQpmz55d5bWOjo6YNm0apk2bptQz1TrHp6J5PBZNgHe/BH4YVT7peZZIAnyeCtQxgkwuoMuyY0jLfILTxlNhiwyIK+icESCCyKIJMO1CuWGv06dPw9fXF3fu3IGJiQlWrVqFMWPGsJeHiIj0kqq+v7XW41NYWIjz58/D09Pzv2DEYnh6eiI2NlZlzykoKEBWVlaZl1o8PY/r+V2as1KA3SOqTnoAQJAB5zYBAOKSM5CSmQ838RU0EVWc9ACAqIIjI+RyOYKCgtC9e3fcuXMHr732Gn7//XeMHTuWSQ8RERk8rSU+Dx48gEwmg41N2Q3+bGxskJqaqrLnBAUFQSqVlr4cHBxUdu9SVZ7HpUSH2qO/AQDp2fkAAGs8Vuy6f3eHTk9Px3vvvYfPPvsMMpkMQ4cOxR9//IE2bdooHgMREVEtpvXJzeo2Z84cZGZmlr5u376t+ocoex5XZRo6AgCszU0AAOlooNh1ZjY4ceIEnJ2dcfjwYZiamiI0NBRbt26FmZlZzeMiIiKqJbS2nN3KygoSiQRpaWXPskpLS1PpxGVjY2MYGxur7H4VUsUhnSIJ0GEMAMCtmSXspCY4l9kK9wTLKuf4yM3s8OW2GAQuWgy5XA4nJydERUXhjTfeqHlMREREtYzWenyMjIzg6uqKmJiY0jK5XI6YmJj/zorSF6o4pNNjElDHCAAgEYuwoK8T5BBjUZE/AED+3IiZABHScuR4d7cRFiwMhFwux8iRI3Hu3Dm0et0JsTce4ufEu4i98RCy5y8mIiIyUFrdwDAgIADDhg1D+/bt4ebmhuDgYOTm5mLEiBEAAH9/f9jb2yMoKAhAyYTopKSk0n+/e/cuEhMTYWZmhpYtW2qtHaXncVU63CUqef+N/wPOri070VkkKUl63l1c5gqv1nZY59cOgftMMCEbWFB3G5o8s6Q9JlWKj3ZnIv3hX6hfvz7Wr18PPz8/RF9MweK9p+GQ8yes8RjpaIDbZm0xz/tNeLV+7tiJ2og7TBMRURW0fmTF6tWrsXz5cqSmpsLZ2Rnffvst3N3dAQDdu3eHo6MjwsLCAAB///03mjVrVu4eb731Fk6cOKHQ89SynD1pL7BvKpD3qII3/x2jenquVWEecGQukHETsGwO9PwCMDKt9NbP79z8imk2Fm85hC/XbIcgCGjTpg0iIyPRqlUrRF9MwU8712N+3W1oIvovSbonWGJRkT/6Dxlfu5OfyrYT8FrGM8WIiPScqr6/tZ74aJrKE5+ny9grW71lagn0XVXyxVvDL+a7d+9iyJAhOHnyJABg3LhxWLlyJUxNTSGTC/h8yRIsKfoKAMrMCXo60vVZ3Zn48rPPIKlsfbw+q/RzeC7xJCIivaT3+/jUClUuY/9XHZOSM7qq2ufnBTswA0B0dDScnZ1x8uRJmJubY9euXVi/fj1MTUt6i+Ju3MfUos0AUG4i9NOfpxaFIO7GfWVaWC0yuaDZOUaKbCcQPfu/XbSJiMhg8ZDSmlBkGXv2PeDv0y/4YhaVfDG36lNuPkpRURHmzZuHZcuWAQBcXFwQFRVVbk6T7O/fygxvPU8sAprgIW7+/Rvwyv8p0Ljqib6YgsB9SUjJzC8ts5OaYEFfJ/UNs73wc3hmo0cetEpEZNDY41MTii5jTz6l+BfzM27duoXu3buXJj2TJ0/GmTNnKpzIbS16rFAoitarjqfniz2b9ABAamY+JoTHI/piinoerOjnoIptB4iISK8x8akJRZexZ95SrN4zX8z79u2Di4sLzpw5A6lUit27d+O7776DiYlJhZe2aN5CoUcoWk9ZMrmAwH1JVe5dHbgvST3DXop+DqrYdoCIiPQaE5+aeLqMHVVMFhaJgb8iFbufmQ0KCwvxySefwNvbGxkZGejQoQPi4+MxYMCAKi+VOHZGnqltuf1+npILQJ6pLSSOnRWLRUlPzxerjAAgJTMfccmVD8dV2ws/BxFgYV9Sj4iIDBoTn5oQS0pWZAGo9Ev3RYeTPr3Wwh7Jcjt07doVK1asAABMnz4dp0+fRvPmzRWKxbTvcohEIjz/RDkAkUgE077Lq9zTpiaTkp+eL6aqekqp8nP492evpdzPh4iIOLm5xpy8S5ZKP79MXSRWPOkB8GOdfhjh2h6ZmZlo2LAhwsLC0Of9viV7+GTnw9rcBG7NLKteiu7kDZHPNgiHZpVMqn7Kwh4ir6VVLueu6aTkp+eLqaqe0ir7HCyalCQ9XMpORETgPj6qu/GzOwbnpAG/fKbQZQX17PDpXy3x3Y4DAAAPDw/s2rULl7ONqpWIBB1MQsipG2gvulK6c/MfQiuM6toCc3o7VXjN00nJFf2PIAKwzq/dC5MfmVxAl2XHkJqZX+l9bKUmOD3rbfXuI8Sdm4mIaiXu46NrxJKSpdJvDlR4Eu2NFqPQOcqsNOmZOXMmfv31V1zONqrW6qigg0nYcDIZxYIYZ+VO2CvvhLNyJxQLYmw4mYygg0nlrqlqUjJQMjdHkUnJT88XAyodbMKCvk7q3zzx2c+hWVcmPUREVAYTH3VQIPGJulQEl4kbcP78eUgbWmLut1vRf+xMCCJJtVZHFRbLselUcpXP3HQqGYXFZYffXjQpGVB8UvLT88VspWWHs2ylJgr1GhEREakb5/ioQ+mhpSl4ftPC/GIB06MLsP58IQDAvGlrmPWege13G2H7prOwrG+EjNzCSm/97OoojxaNSsu3x/5d6Yqup+RCSb1RXf+bLJ2amadQkxSt59XaDj2dbJWbm0RERKQhTHzU4ekqoyh/lAz0lGQk1x7K4PN9Hv5Mk0MkEsGiow+kXYZA9MxwTFVJz7OeXx31T8YTha57vp6iz1O0HlAy7PVsUkZERKQrONSlLk9XGVmUDO/s+KsI7Tbk4s80ORpbSvHa8KVo0G1omaRHGc+vjmpqWU+h656vZ2lmrNB1itYjIiLSZUx81MnJG0/GxmH05a7w+zEPuUVAjx7dEbL/N+RZv1GtW4pQsrrLrZllmfIh7k2r2kYRQMl5XUM9HMuU2Vootrxc0XpERES6jImPGiUlJcGtowdCog5AJBJh4cKFOHLkKOSmDap9TwHlV0dFX0zB29+cqOqMeADAmK7NYFSn7Efu1swSdtKqk5qKEi0iIiJ9xMRHTcLCwtChQwdcunQJtra2iImJwYIFCyCRSBTexM/MuPwwWIN6dcv8XNnBoM8Si4Bx3ZpVuI/P02XoIlS8DF0EDS1DJyIi0gAmPiqWk5ODYcOGYcSIEXjy5Al69uyJxMRE9OjRo7TO016WKk6WQoN6dZFbICv3XuaTotK9fF60Bw8AWJjUwaVAr0o3LwS4DJ2IiAwHV3Wp0IULF+Dj44MrV65ALBZj8eLFmD17NsTisvnl016WCeHxz6z5KvHsz5Xt5SNCyV4+5iZ1X7gHT1Z+MRJvP37hKisuQyciIkPAHh8V2bt3L9zc3HDlyhXY29vjxIkT+Oyzz8olPU9V1csy3fMVPH5SVOmznu7lE3vjoUKxKXow6NNl6P2c7eHRohGTHiIiqnXY46Mibdu2hampKXr06IFt27bBysrqhddU1suy/697L7y2hGLHrKntYFAiIiI9w8RHRZo2bYqzZ8+iZcuWlfbyVKSizf4UTVQ8mlvhh/i7LzwYlCuyiIiISnCoS4VeffVVpZKeyigy+dlOaoKOLRrpxsGgREREeoKJjw5S5qRzrsgiIiJSnEgQBMUmitQSWVlZkEqlyMzMhIWFhbbDqVL0xRQE7ksqs3LLTmqCBX2dyiU0MrnAFVlERFRrqer7m4mPjmNCQ0REpLrvb05u1nE86ZyIiEh1OMeHiIiIDAYTHyIiIjIYTHyIiIjIYDDxISIiIoPBxIeIiIgMBhMfIiIiMhhMfIiIiMhgMPEhIiIig8HEh4iIiAyGwe3c/PSEjqysLC1HQkRERIp6+r1d05O2DC7xyc7OBgA4ODhoORIiIiJSVnZ2NqRSabWvN7hDSuVyOe7duwdzc3OIRLX3sM+srCw4ODjg9u3benEYq6qw3Wy3IWC72W5D8Hy7BUFAdnY2mjRpArG4+jN1DK7HRywW46WXXtJ2GBpjYWFhUH9QnmK7DQvbbVjYbsPybLtr0tPzFCc3ExERkcFg4kNEREQGg4lPLWVsbIwFCxbA2NhY26FoFNvNdhsCtpvtNgTqarfBTW4mIiIiw8UeHyIiIjIYTHyIiIjIYDDxISIiIoPBxIeIiIgMBhMfPbZmzRo4OjrCxMQE7u7uiIuLq7TupUuXMGDAADg6OkIkEiE4OFhzgaqYMu3etGkTunbtioYNG6Jhw4bw9PSssr4uU6bde/bsQfv27dGgQQPUr18fzs7O2L59uwajVR1l2v2siIgIiEQi9O/fX70Bqoky7Q4LC4NIJCrzMjEx0WC0qqPs5/348WNMmjQJdnZ2MDY2xquvvoqDBw9qKFrVUabd3bt3L/d5i0Qi9OnTR4MRq4ayn3dwcDBee+01mJqawsHBAdOnT0d+fr5yDxVIL0VERAhGRkZCaGiocOnSJWHMmDFCgwYNhLS0tArrx8XFCTNmzBB27dol2NraCitXrtRswCqibLuHDBkirFmzRkhISBAuX74sDB8+XJBKpcKdO3c0HHnNKNvu48ePC3v27BGSkpKE69evC8HBwYJEIhGio6M1HHnNKNvup5KTkwV7e3uha9euQr9+/TQTrAop2+4tW7YIFhYWQkpKSukrNTVVw1HXnLLtLigoENq3by/07t1bOH36tJCcnCycOHFCSExM1HDkNaNsux8+fFjms7548aIgkUiELVu2aDbwGlK23Tt27BCMjY2FHTt2CMnJycIvv/wi2NnZCdOnT1fquUx89JSbm5swadKk0p9lMpnQpEkTISgo6IXXNm3aVG8Tn5q0WxAEobi4WDA3Nxe2bt2qrhDVoqbtFgRBcHFxEebOnauO8NSmOu0uLi4WOnXqJGzevFkYNmyYXiY+yrZ7y5YtglQq1VB06qNsu9etWyc0b95cKCws1FSIalHTP98rV64UzM3NhZycHHWFqBbKtnvSpEnC22+/XaYsICBA6Ny5s1LP5VCXHiosLMT58+fh6elZWiYWi+Hp6YnY2FgtRqZeqmj3kydPUFRUBEtLS3WFqXI1bbcgCIiJicHVq1fRrVs3dYaqUtVt96JFi2BtbY1Ro0ZpIkyVq267c3Jy0LRpUzg4OKBfv364dOmSJsJVmeq0e+/evfDw8MCkSZNgY2OD1q1bY8mSJZDJZJoKu8ZU8fdaSEgIBg8ejPr166srTJWrTrs7deqE8+fPlw6H3bx5EwcPHkTv3r2VerbBHVJaGzx48AAymQw2NjZlym1sbHDlyhUtRaV+qmj3rFmz0KRJkzJ/2HRdddudmZkJe3t7FBQUQCKRYO3atejZs6e6w1WZ6rT79OnTCAkJQWJiogYiVI/qtPu1115DaGgo2rRpg8zMTHz99dfo1KkTLl26pDeHMlen3Tdv3sSxY8fw0Ucf4eDBg7h+/TomTpyIoqIiLFiwQBNh11hN/16Li4vDxYsXERISoq4Q1aI67R4yZAgePHiALl26QBAEFBcXY/z48fjss8+UejYTHzIYS5cuRUREBE6cOKG3Ez+VYW5ujsTEROTk5CAmJgYBAQFo3rw5unfvru3Q1CI7OxtDhw7Fpk2bYGVlpe1wNMrDwwMeHh6lP3fq1Amvv/46NmzYgMWLF2sxMvWSy+WwtrbGxo0bIZFI4Orqirt372L58uV6k/jUVEhICN588024ublpOxS1O3HiBJYsWYK1a9fC3d0d169fx8cff4zFixdj3rx5Ct+HiY8esrKygkQiQVpaWpnytLQ02Nraaikq9atJu7/++mssXboUR48eRZs2bdQZpspVt91isRgtW7YEADg7O+Py5csICgrSm8RH2XbfuHEDf//9N/r27VtaJpfLAQB16tTB1atX0aJFC/UGrQKq+PNdt25duLi44Pr16+oIUS2q0247OzvUrVsXEomktOz1119HamoqCgsLYWRkpNaYVaEmn3dubi4iIiKwaNEidYaoFtVp97x58zB06FCMHj0aAPDmm28iNzcXY8eOxeeffw6xWLHZO5zjo4eMjIzg6uqKmJiY0jK5XI6YmJgyv/XVNtVt91dffYXFixcjOjoa7du310SoKqWqz1sul6OgoEAdIaqFsu1u1aoVLly4gMTExNKXt7c3evTogcTERDg4OGgy/GpTxectk8lw4cIF2NnZqStMlatOuzt37ozr16+XJrgAcO3aNdjZ2elF0gPU7PP+/vvvUVBQAD8/P3WHqXLVafeTJ0/KJTdPk15BmWNHlZyETToiIiJCMDY2FsLCwoSkpCRh7NixQoMGDUqXsA4dOlSYPXt2af2CggIhISFBSEhIEOzs7IQZM2YICQkJwv/+9z9tNaFalG330qVLBSMjI2H37t1lln9mZ2drqwnVomy7lyxZIhw+fFi4ceOGkJSUJHz99ddCnTp1hE2bNmmrCdWibLufp6+rupRtd2BgoPDLL78IN27cEM6fPy8MHjxYMDExES5duqStJlSLsu2+deuWYG5uLkyePFm4evWqsH//fsHa2lr44osvtNWEaqnu/+ddunQRBg0apOlwVUbZdi9YsEAwNzcXdu3aJdy8eVM4fPiw0KJFC8HHx0ep5zLx0WPfffed8PLLLwtGRkaCm5ubcPbs2dL33nrrLWHYsGGlPycnJwsAyr3eeustzQdeQ8q0u2nTphW2e8GCBZoPvIaUaffnn38utGzZUjAxMREaNmwoeHh4CBEREVqIuuaUaffz9DXxEQTl2j1t2rTSujY2NkLv3r2F+Ph4LURdc8p+3mfOnBHc3d0FY2NjoXnz5sKXX34pFBcXazjqmlO23VeuXBEACIcPH9ZwpKqlTLuLioqEhQsXCi1atBBMTEwEBwcHYeLEicKjR4+UeqZIEJTpHyIiIiLSX5zjQ0RERAaDiQ8REREZDCY+REREZDCY+BAREZHBYOJDREREBoOJDxERERkMJj5ERERkMJj4EBERkcFg4kNEtcbw4cMhEokgEonw008/aTucSulLnES1ERMfIqrS0y/oyl4LFy4EACQkJODDDz+EjY0NTExM8Morr2DMmDG4du1auXv26tULEokE586dK/fes0lB3bp1YWNjg549eyI0NLTMYZSV8fLyQkpKCt57771y740bNw4SiQTff/99pdcHBgaq/dDHVatWISUlRa3PIKKKMfEhoiqlpKSUvoKDg2FhYVGmbMaMGdi/fz86duyIgoIC7NixA5cvX0Z4eDikUinmzZtX5n63bt3CmTNnMHnyZISGhlb4zKfJy99//41Dhw6hR48e+Pjjj/H++++juLi4yniNjY1ha2sLY2PjMuVPnjxBREQEZs6cWelzAeDnn3+Gt7e3gv91qkcqlcLW1latzyCiSqjklDEiMghbtmwRpFJpmbLc3FzByspK6N+/f4XXPH+A4MKFC4XBgwcLly9fFqRSqfDkyZMy71d2sGhMTIwAoMoT5qs6lDQsLEzo2LGj8PjxY6FevXrCrVu3ytW5deuWYGRkJGRmZpYe7JuQkFCmLQCE48ePC4IgCMePHxcACNHR0YKzs7NgYmIi9OjRQ0hLSxMOHjwotGrVSjA3Nxd8fX2F3Nzccs8DIPz444+VtoeIVI89PkRUI7/88gsePHiAmTNnVvh+gwYNSv9dEARs2bIFfn5+aNWqFVq2bIndu3cr9Jy3334bbdu2xZ49e6oVZ0hICPz8/CCVSvHee+8hLCysXJ29e/eie/fusLCwUOreCxcuxOrVq3HmzBncvn0bPj4+CA4Oxs6dO3HgwAEcPnwY3333XbXiJiLVYuJDRDXyv//9DwDQqlWrF9Y9evQonjx5gl69egEA/Pz8EBISovCzWrVqhb///rtaMZ49exaDBg0qfe6WLVsgCEKZetUd5vriiy/QuXNnuLi4YNSoUfj111+xbt06uLi4oGvXrhg4cCCOHz+u9H2JSPWY+BBRjTyfPFQlNDQUgwYNQp06dQAAvr6++O2333Djxg2FnyUSiZSOMTQ0FL169YKVlRUAoHfv3sjMzMSxY8dK62RlZeHXX3+tVuLTpk2b0n+3sbFBvXr10Lx58zJl6enpSt+XiFSPiQ8R1cirr74KALhy5UqV9TIyMvDjjz9i7dq1qFOnDurUqQN7e3sUFxdXOdn4WZcvX0azZs2Uik8mk2Hr1q04cOBA6XPr1auHjIyMMs89dOgQnJyc4ODgAAAQi0v+enw2sSsqKqrwGXXr1i3996er0Z4lEokUWpFGROrHxIeIauTdd9+FlZUVvvrqqwrff/z4MQBgx44deOmll/Dnn38iMTGx9PXNN98gLCwMMpmsyuccO3YMFy5cwIABA5SK7+DBg8jOzkZCQkKZ5+7atQt79uwpje/nn39Gv379Sq9r3LgxAJRZdp6YmKjUs4lI99TRdgBEpN/q16+PzZs348MPP4S3tzemTp2Kli1b4sGDB4iKisKtW7cQERGBkJAQDBw4EK1bty5zvYODA+bMmYPo6Gj06dMHAFBQUIDU1FTIZDKkpaUhOjoaQUFBeP/99+Hv769UfCEhIejTpw/atm1bptzJyQnTp0/Hjh07MG7cOBw6dAgzZswofd/U1BQdO3bE0qVL0axZM6Snp2Pu3LnV/K9ERLqCPT5EVGP9+vXDmTNnULduXQwZMgStWrWCr68vMjMz8cUXX+D8+fP4888/K+ytkUqleOedd8pMco6OjoadnR0cHR3h5eWF48eP49tvv8XPP/8MiUSicFxpaWk4cOBAhc8Vi8X4v//7P4SEhODXX3+FmZkZ2rVrV6ZOaGgoiouL4erqimnTpuGLL75Q4r8KEekikaDMzEQiIh02fPhwPH78WOljIKZOnYri4mKsXbtWPYFVQiQS4ccff0T//v01+lwiQ8YeHyKqVfbv3w8zMzPs379f4Wtat26NCRMmqDGqssaPHw8zMzONPY+I/sMeHyKqNdLT05GVlQUAsLOzQ/369bUcUcX0JU6i2oiJDxERERkMDnURERGRwWDiQ0RERAaDiQ8REREZDCY+REREZDCY+BAREZHBYOJDREREBoOJDxERERkMJj5ERERkMP4fFmR6k7+FepsAAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted delta_V: 0.04247812554240227V\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-19-fefaffd3ea6c>:16: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  print(\"Predicted delta_V: {}V\".format(float(pred_y)))\n"
          ]
        }
      ],
      "source": [
        "output1 = np.array(output)/normdelta_Vth + Mindelta_Vth\n",
        "ytest1 = np.array(y_test)/normdelta_Vth + Mindelta_Vth\n",
        "\n",
        "plt.scatter(output1, ytest1)\n",
        "a = [min(ytest1), max(ytest1)]\n",
        "b = [min(ytest1), max(ytest1)]\n",
        "plt.plot(a,b,'k')\n",
        "plt.scatter(ytest1,output1)\n",
        "plt.xlabel(\"TCAD [A/um]\")\n",
        "plt.ylabel(\"ML-Prediction [A/um]\")\n",
        "plt.show()\n",
        "\n",
        "tp = [0.001, 0.001, 50, 1.7, 25] # [t_st, t_rec, clk_loops, V_ov, temperature]\n",
        "new_var = torch.FloatTensor([(tp[0]-Mint_stress)*normt_stress, (tp[1]-Mint_rec)*normt_rec, (tp[2]-Minclk_loops)*normclk_loops, (tp[3]-MinV_ov)*normV_ov, (tp[4] - Mintemperature)* normtemperature])\n",
        "pred_y=model(new_var).data.numpy()\n",
        "print(\"Predicted delta_V: {}V\".format(float(pred_y)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mCmumHX7By8e"
      },
      "source": [
        "    \n",
        "    ===========================================\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Layer 1**"
      ],
      "metadata": {
        "id": "_svAlxE1RRYo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from re import L\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import random\n",
        "\n",
        "from torch import optim\n",
        "from torch.utils import data\n",
        "from torch.optim import lr_scheduler\n",
        "from torchvision import datasets, models, transforms\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import statistics\n",
        "import datetime\n",
        "import os\n",
        "import csv\n",
        "import math\n",
        "import time\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_tensor = torch.tensor(X, dtype=torch.float32)\n",
        "Y_tensor = torch.tensor(Y, dtype=torch.float32)\n",
        "x_train, x_test, y_train, y_test = train_test_split(X_tensor, Y_tensor, test_size=0.1, random_state=41)\n",
        "dataset = TensorDataset(x_train, y_train)\n",
        "dataloader = DataLoader(dataset, batch_size = 32)\n",
        "testdataloader = DataLoader(TensorDataset(x_test, y_test))\n",
        "\n",
        "n1 = 20\n",
        "#n2 = 10\n",
        "\n",
        "# Define the neural network class\n",
        "class MLP(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MLP, self).__init__()\n",
        "        self.fc1 = torch.nn.Linear(5, n1)\n",
        "        self.fc2 = torch.nn.Linear(n1, n2)\n",
        "        self.fc3 = torch.nn.Linear(n2, 1)\n",
        "        self.fc4 = torch.nn.Linear(n1, 1)\n",
        "        self.dropout = torch.nn.Dropout(0.1)\n",
        "        self.tanh = torch.nn.Tanh()\n",
        "        self.bn1 = torch.nn.BatchNorm1d(n1)\n",
        "        self.bn2 = torch.nn.BatchNorm1d(n2)\n",
        "        self.bn3 = torch.nn.BatchNorm1d(1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        #x = self.bn1(x)\n",
        "        x = self.tanh(x)\n",
        "        #x = self.dropout(x)\n",
        "        #x = self.fc2(x)\n",
        "        #x = self.bn2(x)\n",
        "        #x = self.tanh(x)\n",
        "        #x = self.dropout(x)\n",
        "        #x = self.fc3(x)\n",
        "        #x = self.bn3(x)\n",
        "        x = self.fc4(x)\n",
        "        return x\n",
        "\n",
        "# Create an instance of the MLP class\n",
        "model = MLP()\n",
        "\n",
        "def initialize_weights(m):\n",
        "    if isinstance(m, torch.nn.Linear):\n",
        "        torch.nn.init.xavier_uniform_(m.weight)\n",
        "        if m.bias is not None:\n",
        "            torch.nn.init.constant_(m.bias, 0)\n",
        "\n",
        "model.apply(initialize_weights)\n",
        "\n",
        "loss_function = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5)\n",
        "#torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "nb_epochs = 1000\n",
        "MLoss = []\n",
        "for epoch in range(0, nb_epochs):\n",
        "\n",
        "    current_loss = 0.0\n",
        "    losses = []\n",
        "    # Iterate over the dataloader for training data\n",
        "    for i, data in enumerate(dataloader, 0):\n",
        "        inputs, targets = data\n",
        "        inputs, targets = inputs.float(), targets.float()\n",
        "        targets = targets.reshape((targets.shape[0],1))\n",
        "        #zero the gradients\n",
        "        optimizer.zero_grad()\n",
        "        #perform forward pass\n",
        "        outputs = model(inputs)\n",
        "        L_weight = 3\n",
        "        #compute loss\n",
        "        batch_loss = []\n",
        "        for j in range(inputs.size(0)):\n",
        "            input_j = inputs[j].reshape((1, inputs.shape[1]))\n",
        "            if input_j[0,0]>0.3:\n",
        "                batch_loss.append(L_weight*loss_function(outputs[j], targets[j]))\n",
        "            else:\n",
        "                batch_loss.append(loss_function(outputs[j], targets[j]))\n",
        "        loss = torch.stack(batch_loss).mean()\n",
        "        losses.append(loss.item())\n",
        "        #perform backward pass\n",
        "        loss.backward()\n",
        "        #perform optimization\n",
        "        optimizer.step()\n",
        "        # Print statistics\n",
        "\n",
        "    mean_loss = sum(losses)/len(losses)\n",
        "    scheduler.step(mean_loss)\n",
        "\n",
        "    print('Loss (epoch: %4d): %.8f' %(epoch+1, mean_loss))\n",
        "    current_loss = 0.0\n",
        "    MLoss.append(mean_loss)\n",
        "\n",
        "# Process is complete.\n",
        "#print('Training process has finished.')\n",
        "\n",
        "torch.save(model, 'IWO_idvg.pt')\n",
        "torch.save(model.state_dict(), 'IWO_idvg_state_dict.pt')\n",
        "\n",
        "####### loss vs. epoch #######\n",
        "xloss = list(range(0, nb_epochs))\n",
        "plt.plot(xloss, np.log10(MLoss))\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "_UsVAZBFRUtV",
        "outputId": "b3a3bb8c-3913-46a9-e68e-20a38697af48"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss (epoch:    1): 0.31129827\n",
            "Loss (epoch:    2): 0.15855061\n",
            "Loss (epoch:    3): 0.09820366\n",
            "Loss (epoch:    4): 0.08249971\n",
            "Loss (epoch:    5): 0.07596356\n",
            "Loss (epoch:    6): 0.06827245\n",
            "Loss (epoch:    7): 0.06176361\n",
            "Loss (epoch:    8): 0.05765158\n",
            "Loss (epoch:    9): 0.05494209\n",
            "Loss (epoch:   10): 0.05289087\n",
            "Loss (epoch:   11): 0.05122984\n",
            "Loss (epoch:   12): 0.04974822\n",
            "Loss (epoch:   13): 0.04831939\n",
            "Loss (epoch:   14): 0.04692385\n",
            "Loss (epoch:   15): 0.04557586\n",
            "Loss (epoch:   16): 0.04428807\n",
            "Loss (epoch:   17): 0.04306558\n",
            "Loss (epoch:   18): 0.04190652\n",
            "Loss (epoch:   19): 0.04080790\n",
            "Loss (epoch:   20): 0.03976917\n",
            "Loss (epoch:   21): 0.03879116\n",
            "Loss (epoch:   22): 0.03787402\n",
            "Loss (epoch:   23): 0.03701659\n",
            "Loss (epoch:   24): 0.03621689\n",
            "Loss (epoch:   25): 0.03547283\n",
            "Loss (epoch:   26): 0.03478240\n",
            "Loss (epoch:   27): 0.03414350\n",
            "Loss (epoch:   28): 0.03355382\n",
            "Loss (epoch:   29): 0.03301087\n",
            "Loss (epoch:   30): 0.03251210\n",
            "Loss (epoch:   31): 0.03205499\n",
            "Loss (epoch:   32): 0.03163699\n",
            "Loss (epoch:   33): 0.03125560\n",
            "Loss (epoch:   34): 0.03090829\n",
            "Loss (epoch:   35): 0.03059265\n",
            "Loss (epoch:   36): 0.03030631\n",
            "Loss (epoch:   37): 0.03004698\n",
            "Loss (epoch:   38): 0.02981250\n",
            "Loss (epoch:   39): 0.02960076\n",
            "Loss (epoch:   40): 0.02940980\n",
            "Loss (epoch:   41): 0.02923775\n",
            "Loss (epoch:   42): 0.02908286\n",
            "Loss (epoch:   43): 0.02894350\n",
            "Loss (epoch:   44): 0.02881814\n",
            "Loss (epoch:   45): 0.02870538\n",
            "Loss (epoch:   46): 0.02860391\n",
            "Loss (epoch:   47): 0.02851254\n",
            "Loss (epoch:   48): 0.02843018\n",
            "Loss (epoch:   49): 0.02835583\n",
            "Loss (epoch:   50): 0.02828858\n",
            "Loss (epoch:   51): 0.02822763\n",
            "Loss (epoch:   52): 0.02817222\n",
            "Loss (epoch:   53): 0.02812171\n",
            "Loss (epoch:   54): 0.02807549\n",
            "Loss (epoch:   55): 0.02803303\n",
            "Loss (epoch:   56): 0.02799387\n",
            "Loss (epoch:   57): 0.02795759\n",
            "Loss (epoch:   58): 0.02792381\n",
            "Loss (epoch:   59): 0.02789223\n",
            "Loss (epoch:   60): 0.02786252\n",
            "Loss (epoch:   61): 0.02783447\n",
            "Loss (epoch:   62): 0.02780785\n",
            "Loss (epoch:   63): 0.02778246\n",
            "Loss (epoch:   64): 0.02775814\n",
            "Loss (epoch:   65): 0.02773475\n",
            "Loss (epoch:   66): 0.02771216\n",
            "Loss (epoch:   67): 0.02769027\n",
            "Loss (epoch:   68): 0.02766897\n",
            "Loss (epoch:   69): 0.02764820\n",
            "Loss (epoch:   70): 0.02762788\n",
            "Loss (epoch:   71): 0.02760796\n",
            "Loss (epoch:   72): 0.02758838\n",
            "Loss (epoch:   73): 0.02756912\n",
            "Loss (epoch:   74): 0.02755012\n",
            "Loss (epoch:   75): 0.02753136\n",
            "Loss (epoch:   76): 0.02751280\n",
            "Loss (epoch:   77): 0.02749445\n",
            "Loss (epoch:   78): 0.02747627\n",
            "Loss (epoch:   79): 0.02745824\n",
            "Loss (epoch:   80): 0.02744036\n",
            "Loss (epoch:   81): 0.02742262\n",
            "Loss (epoch:   82): 0.02740500\n",
            "Loss (epoch:   83): 0.02738750\n",
            "Loss (epoch:   84): 0.02737011\n",
            "Loss (epoch:   85): 0.02735283\n",
            "Loss (epoch:   86): 0.02733565\n",
            "Loss (epoch:   87): 0.02731858\n",
            "Loss (epoch:   88): 0.02730159\n",
            "Loss (epoch:   89): 0.02728471\n",
            "Loss (epoch:   90): 0.02726791\n",
            "Loss (epoch:   91): 0.02725120\n",
            "Loss (epoch:   92): 0.02723459\n",
            "Loss (epoch:   93): 0.02721806\n",
            "Loss (epoch:   94): 0.02720162\n",
            "Loss (epoch:   95): 0.02718527\n",
            "Loss (epoch:   96): 0.02716899\n",
            "Loss (epoch:   97): 0.02715281\n",
            "Loss (epoch:   98): 0.02713671\n",
            "Loss (epoch:   99): 0.02712070\n",
            "Loss (epoch:  100): 0.02710477\n",
            "Loss (epoch:  101): 0.02708892\n",
            "Loss (epoch:  102): 0.02707316\n",
            "Loss (epoch:  103): 0.02705749\n",
            "Loss (epoch:  104): 0.02704189\n",
            "Loss (epoch:  105): 0.02702638\n",
            "Loss (epoch:  106): 0.02701095\n",
            "Loss (epoch:  107): 0.02699562\n",
            "Loss (epoch:  108): 0.02698036\n",
            "Loss (epoch:  109): 0.02696518\n",
            "Loss (epoch:  110): 0.02695009\n",
            "Loss (epoch:  111): 0.02693508\n",
            "Loss (epoch:  112): 0.02692016\n",
            "Loss (epoch:  113): 0.02690532\n",
            "Loss (epoch:  114): 0.02689056\n",
            "Loss (epoch:  115): 0.02687588\n",
            "Loss (epoch:  116): 0.02686129\n",
            "Loss (epoch:  117): 0.02684679\n",
            "Loss (epoch:  118): 0.02683236\n",
            "Loss (epoch:  119): 0.02681802\n",
            "Loss (epoch:  120): 0.02680377\n",
            "Loss (epoch:  121): 0.02678960\n",
            "Loss (epoch:  122): 0.02677551\n",
            "Loss (epoch:  123): 0.02676150\n",
            "Loss (epoch:  124): 0.02674759\n",
            "Loss (epoch:  125): 0.02673375\n",
            "Loss (epoch:  126): 0.02672000\n",
            "Loss (epoch:  127): 0.02670633\n",
            "Loss (epoch:  128): 0.02669275\n",
            "Loss (epoch:  129): 0.02667925\n",
            "Loss (epoch:  130): 0.02666584\n",
            "Loss (epoch:  131): 0.02665251\n",
            "Loss (epoch:  132): 0.02663927\n",
            "Loss (epoch:  133): 0.02662611\n",
            "Loss (epoch:  134): 0.02661304\n",
            "Loss (epoch:  135): 0.02660005\n",
            "Loss (epoch:  136): 0.02658714\n",
            "Loss (epoch:  137): 0.02657432\n",
            "Loss (epoch:  138): 0.02656159\n",
            "Loss (epoch:  139): 0.02654894\n",
            "Loss (epoch:  140): 0.02653637\n",
            "Loss (epoch:  141): 0.02652389\n",
            "Loss (epoch:  142): 0.02651150\n",
            "Loss (epoch:  143): 0.02649919\n",
            "Loss (epoch:  144): 0.02648697\n",
            "Loss (epoch:  145): 0.02647483\n",
            "Loss (epoch:  146): 0.02646278\n",
            "Loss (epoch:  147): 0.02645081\n",
            "Loss (epoch:  148): 0.02643893\n",
            "Loss (epoch:  149): 0.02642714\n",
            "Loss (epoch:  150): 0.02641543\n",
            "Loss (epoch:  151): 0.02640380\n",
            "Loss (epoch:  152): 0.02639226\n",
            "Loss (epoch:  153): 0.02638081\n",
            "Loss (epoch:  154): 0.02636944\n",
            "Loss (epoch:  155): 0.02635816\n",
            "Loss (epoch:  156): 0.02634696\n",
            "Loss (epoch:  157): 0.02633585\n",
            "Loss (epoch:  158): 0.02632482\n",
            "Loss (epoch:  159): 0.02631388\n",
            "Loss (epoch:  160): 0.02630302\n",
            "Loss (epoch:  161): 0.02629225\n",
            "Loss (epoch:  162): 0.02628156\n",
            "Loss (epoch:  163): 0.02627096\n",
            "Loss (epoch:  164): 0.02626045\n",
            "Loss (epoch:  165): 0.02625001\n",
            "Loss (epoch:  166): 0.02623967\n",
            "Loss (epoch:  167): 0.02622941\n",
            "Loss (epoch:  168): 0.02621923\n",
            "Loss (epoch:  169): 0.02620913\n",
            "Loss (epoch:  170): 0.02619912\n",
            "Loss (epoch:  171): 0.02618920\n",
            "Loss (epoch:  172): 0.02617935\n",
            "Loss (epoch:  173): 0.02616959\n",
            "Loss (epoch:  174): 0.02615992\n",
            "Loss (epoch:  175): 0.02615032\n",
            "Loss (epoch:  176): 0.02614081\n",
            "Loss (epoch:  177): 0.02613138\n",
            "Loss (epoch:  178): 0.02612204\n",
            "Loss (epoch:  179): 0.02611277\n",
            "Loss (epoch:  180): 0.02610359\n",
            "Loss (epoch:  181): 0.02609449\n",
            "Loss (epoch:  182): 0.02608547\n",
            "Loss (epoch:  183): 0.02607653\n",
            "Loss (epoch:  184): 0.02606767\n",
            "Loss (epoch:  185): 0.02605890\n",
            "Loss (epoch:  186): 0.02605020\n",
            "Loss (epoch:  187): 0.02604159\n",
            "Loss (epoch:  188): 0.02603305\n",
            "Loss (epoch:  189): 0.02602459\n",
            "Loss (epoch:  190): 0.02601621\n",
            "Loss (epoch:  191): 0.02600790\n",
            "Loss (epoch:  192): 0.02599968\n",
            "Loss (epoch:  193): 0.02599153\n",
            "Loss (epoch:  194): 0.02598346\n",
            "Loss (epoch:  195): 0.02597547\n",
            "Loss (epoch:  196): 0.02596756\n",
            "Loss (epoch:  197): 0.02595971\n",
            "Loss (epoch:  198): 0.02595195\n",
            "Loss (epoch:  199): 0.02594426\n",
            "Loss (epoch:  200): 0.02593664\n",
            "Loss (epoch:  201): 0.02592910\n",
            "Loss (epoch:  202): 0.02592164\n",
            "Loss (epoch:  203): 0.02591424\n",
            "Loss (epoch:  204): 0.02590692\n",
            "Loss (epoch:  205): 0.02589967\n",
            "Loss (epoch:  206): 0.02589249\n",
            "Loss (epoch:  207): 0.02588539\n",
            "Loss (epoch:  208): 0.02587836\n",
            "Loss (epoch:  209): 0.02587139\n",
            "Loss (epoch:  210): 0.02586450\n",
            "Loss (epoch:  211): 0.02585768\n",
            "Loss (epoch:  212): 0.02585092\n",
            "Loss (epoch:  213): 0.02584424\n",
            "Loss (epoch:  214): 0.02583762\n",
            "Loss (epoch:  215): 0.02583107\n",
            "Loss (epoch:  216): 0.02582459\n",
            "Loss (epoch:  217): 0.02581817\n",
            "Loss (epoch:  218): 0.02581182\n",
            "Loss (epoch:  219): 0.02580553\n",
            "Loss (epoch:  220): 0.02579932\n",
            "Loss (epoch:  221): 0.02579316\n",
            "Loss (epoch:  222): 0.02578707\n",
            "Loss (epoch:  223): 0.02578104\n",
            "Loss (epoch:  224): 0.02577508\n",
            "Loss (epoch:  225): 0.02576918\n",
            "Loss (epoch:  226): 0.02576334\n",
            "Loss (epoch:  227): 0.02575755\n",
            "Loss (epoch:  228): 0.02575184\n",
            "Loss (epoch:  229): 0.02574618\n",
            "Loss (epoch:  230): 0.02574058\n",
            "Loss (epoch:  231): 0.02573505\n",
            "Loss (epoch:  232): 0.02572956\n",
            "Loss (epoch:  233): 0.02572415\n",
            "Loss (epoch:  234): 0.02571878\n",
            "Loss (epoch:  235): 0.02571348\n",
            "Loss (epoch:  236): 0.02570822\n",
            "Loss (epoch:  237): 0.02570303\n",
            "Loss (epoch:  238): 0.02569789\n",
            "Loss (epoch:  239): 0.02569280\n",
            "Loss (epoch:  240): 0.02568778\n",
            "Loss (epoch:  241): 0.02568280\n",
            "Loss (epoch:  242): 0.02567788\n",
            "Loss (epoch:  243): 0.02567302\n",
            "Loss (epoch:  244): 0.02566819\n",
            "Loss (epoch:  245): 0.02566343\n",
            "Loss (epoch:  246): 0.02565872\n",
            "Loss (epoch:  247): 0.02565406\n",
            "Loss (epoch:  248): 0.02564944\n",
            "Loss (epoch:  249): 0.02564488\n",
            "Loss (epoch:  250): 0.02564037\n",
            "Loss (epoch:  251): 0.02563591\n",
            "Loss (epoch:  252): 0.02563149\n",
            "Loss (epoch:  253): 0.02562712\n",
            "Loss (epoch:  254): 0.02562281\n",
            "Loss (epoch:  255): 0.02561853\n",
            "Loss (epoch:  256): 0.02561431\n",
            "Loss (epoch:  257): 0.02561012\n",
            "Loss (epoch:  258): 0.02560599\n",
            "Loss (epoch:  259): 0.02560190\n",
            "Loss (epoch:  260): 0.02559784\n",
            "Loss (epoch:  261): 0.02559384\n",
            "Loss (epoch:  262): 0.02558988\n",
            "Loss (epoch:  263): 0.02558597\n",
            "Loss (epoch:  264): 0.02558210\n",
            "Loss (epoch:  265): 0.02557826\n",
            "Loss (epoch:  266): 0.02557447\n",
            "Loss (epoch:  267): 0.02557072\n",
            "Loss (epoch:  268): 0.02556702\n",
            "Loss (epoch:  269): 0.02556335\n",
            "Loss (epoch:  270): 0.02555972\n",
            "Loss (epoch:  271): 0.02555613\n",
            "Loss (epoch:  272): 0.02555258\n",
            "Loss (epoch:  273): 0.02554906\n",
            "Loss (epoch:  274): 0.02554559\n",
            "Loss (epoch:  275): 0.02554215\n",
            "Loss (epoch:  276): 0.02553875\n",
            "Loss (epoch:  277): 0.02553539\n",
            "Loss (epoch:  278): 0.02553206\n",
            "Loss (epoch:  279): 0.02552877\n",
            "Loss (epoch:  280): 0.02552552\n",
            "Loss (epoch:  281): 0.02552229\n",
            "Loss (epoch:  282): 0.02551911\n",
            "Loss (epoch:  283): 0.02551596\n",
            "Loss (epoch:  284): 0.02551284\n",
            "Loss (epoch:  285): 0.02550976\n",
            "Loss (epoch:  286): 0.02550670\n",
            "Loss (epoch:  287): 0.02550368\n",
            "Loss (epoch:  288): 0.02550070\n",
            "Loss (epoch:  289): 0.02549774\n",
            "Loss (epoch:  290): 0.02549482\n",
            "Loss (epoch:  291): 0.02549193\n",
            "Loss (epoch:  292): 0.02548907\n",
            "Loss (epoch:  293): 0.02548623\n",
            "Loss (epoch:  294): 0.02548344\n",
            "Loss (epoch:  295): 0.02548067\n",
            "Loss (epoch:  296): 0.02547792\n",
            "Loss (epoch:  297): 0.02547522\n",
            "Loss (epoch:  298): 0.02547253\n",
            "Loss (epoch:  299): 0.02546987\n",
            "Loss (epoch:  300): 0.02546724\n",
            "Loss (epoch:  301): 0.02546465\n",
            "Loss (epoch:  302): 0.02546207\n",
            "Loss (epoch:  303): 0.02545953\n",
            "Loss (epoch:  304): 0.02545700\n",
            "Loss (epoch:  305): 0.02545451\n",
            "Loss (epoch:  306): 0.02545205\n",
            "Loss (epoch:  307): 0.02544961\n",
            "Loss (epoch:  308): 0.02544719\n",
            "Loss (epoch:  309): 0.02544480\n",
            "Loss (epoch:  310): 0.02544243\n",
            "Loss (epoch:  311): 0.02544009\n",
            "Loss (epoch:  312): 0.02543778\n",
            "Loss (epoch:  313): 0.02543548\n",
            "Loss (epoch:  314): 0.02543321\n",
            "Loss (epoch:  315): 0.02543097\n",
            "Loss (epoch:  316): 0.02542874\n",
            "Loss (epoch:  317): 0.02542654\n",
            "Loss (epoch:  318): 0.02542436\n",
            "Loss (epoch:  319): 0.02542220\n",
            "Loss (epoch:  320): 0.02542007\n",
            "Loss (epoch:  321): 0.02541795\n",
            "Loss (epoch:  322): 0.02541587\n",
            "Loss (epoch:  323): 0.02541380\n",
            "Loss (epoch:  324): 0.02541175\n",
            "Loss (epoch:  325): 0.02540972\n",
            "Loss (epoch:  326): 0.02540771\n",
            "Loss (epoch:  327): 0.02540573\n",
            "Loss (epoch:  328): 0.02540376\n",
            "Loss (epoch:  329): 0.02540181\n",
            "Loss (epoch:  330): 0.02539988\n",
            "Loss (epoch:  331): 0.02539797\n",
            "Loss (epoch:  332): 0.02539608\n",
            "Loss (epoch:  333): 0.02539420\n",
            "Loss (epoch:  334): 0.02539235\n",
            "Loss (epoch:  335): 0.02539051\n",
            "Loss (epoch:  336): 0.02538870\n",
            "Loss (epoch:  337): 0.02538690\n",
            "Loss (epoch:  338): 0.02538512\n",
            "Loss (epoch:  339): 0.02538335\n",
            "Loss (epoch:  340): 0.02538160\n",
            "Loss (epoch:  341): 0.02537987\n",
            "Loss (epoch:  342): 0.02537815\n",
            "Loss (epoch:  343): 0.02537646\n",
            "Loss (epoch:  344): 0.02537477\n",
            "Loss (epoch:  345): 0.02537311\n",
            "Loss (epoch:  346): 0.02537146\n",
            "Loss (epoch:  347): 0.02536982\n",
            "Loss (epoch:  348): 0.02536821\n",
            "Loss (epoch:  349): 0.02536660\n",
            "Loss (epoch:  350): 0.02536502\n",
            "Loss (epoch:  351): 0.02536345\n",
            "Loss (epoch:  352): 0.02536189\n",
            "Loss (epoch:  353): 0.02536035\n",
            "Loss (epoch:  354): 0.02535881\n",
            "Loss (epoch:  355): 0.02535730\n",
            "Loss (epoch:  356): 0.02535580\n",
            "Loss (epoch:  357): 0.02535431\n",
            "Loss (epoch:  358): 0.02535284\n",
            "Loss (epoch:  359): 0.02535138\n",
            "Loss (epoch:  360): 0.02534993\n",
            "Loss (epoch:  361): 0.02534850\n",
            "Loss (epoch:  362): 0.02534708\n",
            "Loss (epoch:  363): 0.02534567\n",
            "Loss (epoch:  364): 0.02534428\n",
            "Loss (epoch:  365): 0.02534290\n",
            "Loss (epoch:  366): 0.02534153\n",
            "Loss (epoch:  367): 0.02534017\n",
            "Loss (epoch:  368): 0.02533882\n",
            "Loss (epoch:  369): 0.02533749\n",
            "Loss (epoch:  370): 0.02533617\n",
            "Loss (epoch:  371): 0.02533486\n",
            "Loss (epoch:  372): 0.02533356\n",
            "Loss (epoch:  373): 0.02533228\n",
            "Loss (epoch:  374): 0.02533100\n",
            "Loss (epoch:  375): 0.02532973\n",
            "Loss (epoch:  376): 0.02532848\n",
            "Loss (epoch:  377): 0.02532724\n",
            "Loss (epoch:  378): 0.02532601\n",
            "Loss (epoch:  379): 0.02532479\n",
            "Loss (epoch:  380): 0.02532358\n",
            "Loss (epoch:  381): 0.02532237\n",
            "Loss (epoch:  382): 0.02532118\n",
            "Loss (epoch:  383): 0.02532000\n",
            "Loss (epoch:  384): 0.02531884\n",
            "Loss (epoch:  385): 0.02531767\n",
            "Loss (epoch:  386): 0.02531652\n",
            "Loss (epoch:  387): 0.02531539\n",
            "Loss (epoch:  388): 0.02531425\n",
            "Loss (epoch:  389): 0.02531313\n",
            "Loss (epoch:  390): 0.02531202\n",
            "Loss (epoch:  391): 0.02531091\n",
            "Loss (epoch:  392): 0.02530982\n",
            "Loss (epoch:  393): 0.02530873\n",
            "Loss (epoch:  394): 0.02530765\n",
            "Loss (epoch:  395): 0.02530659\n",
            "Loss (epoch:  396): 0.02530552\n",
            "Loss (epoch:  397): 0.02530447\n",
            "Loss (epoch:  398): 0.02530343\n",
            "Loss (epoch:  399): 0.02530240\n",
            "Loss (epoch:  400): 0.02530137\n",
            "Loss (epoch:  401): 0.02530035\n",
            "Loss (epoch:  402): 0.02529934\n",
            "Loss (epoch:  403): 0.02529834\n",
            "Loss (epoch:  404): 0.02529735\n",
            "Loss (epoch:  405): 0.02529636\n",
            "Loss (epoch:  406): 0.02529538\n",
            "Loss (epoch:  407): 0.02529441\n",
            "Loss (epoch:  408): 0.02529344\n",
            "Loss (epoch:  409): 0.02529249\n",
            "Loss (epoch:  410): 0.02529154\n",
            "Loss (epoch:  411): 0.02529059\n",
            "Loss (epoch:  412): 0.02528966\n",
            "Loss (epoch:  413): 0.02528873\n",
            "Loss (epoch:  414): 0.02528781\n",
            "Loss (epoch:  415): 0.02528689\n",
            "Loss (epoch:  416): 0.02528599\n",
            "Loss (epoch:  417): 0.02528509\n",
            "Loss (epoch:  418): 0.02528419\n",
            "Loss (epoch:  419): 0.02528330\n",
            "Loss (epoch:  420): 0.02528242\n",
            "Loss (epoch:  421): 0.02528154\n",
            "Loss (epoch:  422): 0.02528068\n",
            "Loss (epoch:  423): 0.02527981\n",
            "Loss (epoch:  424): 0.02527896\n",
            "Loss (epoch:  425): 0.02527810\n",
            "Loss (epoch:  426): 0.02527726\n",
            "Loss (epoch:  427): 0.02527642\n",
            "Loss (epoch:  428): 0.02527559\n",
            "Loss (epoch:  429): 0.02527476\n",
            "Loss (epoch:  430): 0.02527394\n",
            "Loss (epoch:  431): 0.02527312\n",
            "Loss (epoch:  432): 0.02527231\n",
            "Loss (epoch:  433): 0.02527152\n",
            "Loss (epoch:  434): 0.02527071\n",
            "Loss (epoch:  435): 0.02526991\n",
            "Loss (epoch:  436): 0.02526914\n",
            "Loss (epoch:  437): 0.02526834\n",
            "Loss (epoch:  438): 0.02526757\n",
            "Loss (epoch:  439): 0.02526679\n",
            "Loss (epoch:  440): 0.02526603\n",
            "Loss (epoch:  441): 0.02526527\n",
            "Loss (epoch:  442): 0.02526451\n",
            "Loss (epoch:  443): 0.02526375\n",
            "Loss (epoch:  444): 0.02526301\n",
            "Loss (epoch:  445): 0.02526226\n",
            "Loss (epoch:  446): 0.02526152\n",
            "Loss (epoch:  447): 0.02526079\n",
            "Loss (epoch:  448): 0.02526006\n",
            "Loss (epoch:  449): 0.02525934\n",
            "Loss (epoch:  450): 0.02525861\n",
            "Loss (epoch:  451): 0.02525790\n",
            "Loss (epoch:  452): 0.02525719\n",
            "Loss (epoch:  453): 0.02525648\n",
            "Loss (epoch:  454): 0.02525578\n",
            "Loss (epoch:  455): 0.02525508\n",
            "Loss (epoch:  456): 0.02525439\n",
            "Loss (epoch:  457): 0.02525370\n",
            "Loss (epoch:  458): 0.02525301\n",
            "Loss (epoch:  459): 0.02525233\n",
            "Loss (epoch:  460): 0.02525165\n",
            "Loss (epoch:  461): 0.02525098\n",
            "Loss (epoch:  462): 0.02525031\n",
            "Loss (epoch:  463): 0.02524965\n",
            "Loss (epoch:  464): 0.02524898\n",
            "Loss (epoch:  465): 0.02524833\n",
            "Loss (epoch:  466): 0.02524767\n",
            "Loss (epoch:  467): 0.02524702\n",
            "Loss (epoch:  468): 0.02524638\n",
            "Loss (epoch:  469): 0.02524573\n",
            "Loss (epoch:  470): 0.02524509\n",
            "Loss (epoch:  471): 0.02524445\n",
            "Loss (epoch:  472): 0.02524382\n",
            "Loss (epoch:  473): 0.02524319\n",
            "Loss (epoch:  474): 0.02524257\n",
            "Loss (epoch:  475): 0.02524194\n",
            "Loss (epoch:  476): 0.02524133\n",
            "Loss (epoch:  477): 0.02524071\n",
            "Loss (epoch:  478): 0.02524010\n",
            "Loss (epoch:  479): 0.02523949\n",
            "Loss (epoch:  480): 0.02523889\n",
            "Loss (epoch:  481): 0.02523828\n",
            "Loss (epoch:  482): 0.02523768\n",
            "Loss (epoch:  483): 0.02523709\n",
            "Loss (epoch:  484): 0.02523649\n",
            "Loss (epoch:  485): 0.02523590\n",
            "Loss (epoch:  486): 0.02523531\n",
            "Loss (epoch:  487): 0.02523473\n",
            "Loss (epoch:  488): 0.02523415\n",
            "Loss (epoch:  489): 0.02523357\n",
            "Loss (epoch:  490): 0.02523300\n",
            "Loss (epoch:  491): 0.02523242\n",
            "Loss (epoch:  492): 0.02523185\n",
            "Loss (epoch:  493): 0.02523128\n",
            "Loss (epoch:  494): 0.02523072\n",
            "Loss (epoch:  495): 0.02523017\n",
            "Loss (epoch:  496): 0.02522960\n",
            "Loss (epoch:  497): 0.02522904\n",
            "Loss (epoch:  498): 0.02522849\n",
            "Loss (epoch:  499): 0.02522794\n",
            "Loss (epoch:  500): 0.02522739\n",
            "Loss (epoch:  501): 0.02522684\n",
            "Loss (epoch:  502): 0.02522631\n",
            "Loss (epoch:  503): 0.02522576\n",
            "Loss (epoch:  504): 0.02522523\n",
            "Loss (epoch:  505): 0.02522469\n",
            "Loss (epoch:  506): 0.02522415\n",
            "Loss (epoch:  507): 0.02522362\n",
            "Loss (epoch:  508): 0.02522310\n",
            "Loss (epoch:  509): 0.02522257\n",
            "Loss (epoch:  510): 0.02522204\n",
            "Loss (epoch:  511): 0.02522152\n",
            "Loss (epoch:  512): 0.02522101\n",
            "Loss (epoch:  513): 0.02522049\n",
            "Loss (epoch:  514): 0.02521997\n",
            "Loss (epoch:  515): 0.02521946\n",
            "Loss (epoch:  516): 0.02521895\n",
            "Loss (epoch:  517): 0.02521844\n",
            "Loss (epoch:  518): 0.02521793\n",
            "Loss (epoch:  519): 0.02521743\n",
            "Loss (epoch:  520): 0.02521692\n",
            "Loss (epoch:  521): 0.02521643\n",
            "Loss (epoch:  522): 0.02521593\n",
            "Loss (epoch:  523): 0.02521544\n",
            "Loss (epoch:  524): 0.02521494\n",
            "Loss (epoch:  525): 0.02521444\n",
            "Loss (epoch:  526): 0.02521396\n",
            "Loss (epoch:  527): 0.02521346\n",
            "Loss (epoch:  528): 0.02521298\n",
            "Loss (epoch:  529): 0.02521250\n",
            "Loss (epoch:  530): 0.02521201\n",
            "Loss (epoch:  531): 0.02521153\n",
            "Loss (epoch:  532): 0.02521105\n",
            "Loss (epoch:  533): 0.02521058\n",
            "Loss (epoch:  534): 0.02521010\n",
            "Loss (epoch:  535): 0.02520963\n",
            "Loss (epoch:  536): 0.02520916\n",
            "Loss (epoch:  537): 0.02520869\n",
            "Loss (epoch:  538): 0.02520822\n",
            "Loss (epoch:  539): 0.02520775\n",
            "Loss (epoch:  540): 0.02520729\n",
            "Loss (epoch:  541): 0.02520682\n",
            "Loss (epoch:  542): 0.02520636\n",
            "Loss (epoch:  543): 0.02520590\n",
            "Loss (epoch:  544): 0.02520544\n",
            "Loss (epoch:  545): 0.02520499\n",
            "Loss (epoch:  546): 0.02520453\n",
            "Loss (epoch:  547): 0.02520408\n",
            "Loss (epoch:  548): 0.02520363\n",
            "Loss (epoch:  549): 0.02520317\n",
            "Loss (epoch:  550): 0.02520273\n",
            "Loss (epoch:  551): 0.02520228\n",
            "Loss (epoch:  552): 0.02520183\n",
            "Loss (epoch:  553): 0.02520139\n",
            "Loss (epoch:  554): 0.02520094\n",
            "Loss (epoch:  555): 0.02520050\n",
            "Loss (epoch:  556): 0.02520006\n",
            "Loss (epoch:  557): 0.02519963\n",
            "Loss (epoch:  558): 0.02519919\n",
            "Loss (epoch:  559): 0.02519875\n",
            "Loss (epoch:  560): 0.02519832\n",
            "Loss (epoch:  561): 0.02519788\n",
            "Loss (epoch:  562): 0.02519745\n",
            "Loss (epoch:  563): 0.02519702\n",
            "Loss (epoch:  564): 0.02519659\n",
            "Loss (epoch:  565): 0.02519616\n",
            "Loss (epoch:  566): 0.02519574\n",
            "Loss (epoch:  567): 0.02519531\n",
            "Loss (epoch:  568): 0.02519488\n",
            "Loss (epoch:  569): 0.02519446\n",
            "Loss (epoch:  570): 0.02519404\n",
            "Loss (epoch:  571): 0.02519362\n",
            "Loss (epoch:  572): 0.02519320\n",
            "Loss (epoch:  573): 0.02519278\n",
            "Loss (epoch:  574): 0.02519236\n",
            "Loss (epoch:  575): 0.02519195\n",
            "Loss (epoch:  576): 0.02519153\n",
            "Loss (epoch:  577): 0.02519112\n",
            "Loss (epoch:  578): 0.02519070\n",
            "Loss (epoch:  579): 0.02502655\n",
            "Loss (epoch:  580): 0.02503124\n",
            "Loss (epoch:  581): 0.02502926\n",
            "Loss (epoch:  582): 0.02502577\n",
            "Loss (epoch:  583): 0.02502274\n",
            "Loss (epoch:  584): 0.02502072\n",
            "Loss (epoch:  585): 0.02501966\n",
            "Loss (epoch:  586): 0.02501928\n",
            "Loss (epoch:  587): 0.02501925\n",
            "Loss (epoch:  588): 0.02501933\n",
            "Loss (epoch:  589): 0.02501938\n",
            "Loss (epoch:  590): 0.02501937\n",
            "Loss (epoch:  591): 0.02501932\n",
            "Loss (epoch:  592): 0.02500309\n",
            "Loss (epoch:  593): 0.02500315\n",
            "Loss (epoch:  594): 0.02500316\n",
            "Loss (epoch:  595): 0.02500316\n",
            "Loss (epoch:  596): 0.02500315\n",
            "Loss (epoch:  597): 0.02500314\n",
            "Loss (epoch:  598): 0.02500312\n",
            "Loss (epoch:  599): 0.02500141\n",
            "Loss (epoch:  600): 0.02500141\n",
            "Loss (epoch:  601): 0.02500141\n",
            "Loss (epoch:  602): 0.02500141\n",
            "Loss (epoch:  603): 0.02500141\n",
            "Loss (epoch:  604): 0.02500141\n",
            "Loss (epoch:  605): 0.02500124\n",
            "Loss (epoch:  606): 0.02500124\n",
            "Loss (epoch:  607): 0.02500124\n",
            "Loss (epoch:  608): 0.02500124\n",
            "Loss (epoch:  609): 0.02500124\n",
            "Loss (epoch:  610): 0.02500124\n",
            "Loss (epoch:  611): 0.02500122\n",
            "Loss (epoch:  612): 0.02500122\n",
            "Loss (epoch:  613): 0.02500122\n",
            "Loss (epoch:  614): 0.02500122\n",
            "Loss (epoch:  615): 0.02500122\n",
            "Loss (epoch:  616): 0.02500122\n",
            "Loss (epoch:  617): 0.02500122\n",
            "Loss (epoch:  618): 0.02500122\n",
            "Loss (epoch:  619): 0.02500122\n",
            "Loss (epoch:  620): 0.02500122\n",
            "Loss (epoch:  621): 0.02500122\n",
            "Loss (epoch:  622): 0.02500122\n",
            "Loss (epoch:  623): 0.02500122\n",
            "Loss (epoch:  624): 0.02500122\n",
            "Loss (epoch:  625): 0.02500122\n",
            "Loss (epoch:  626): 0.02500122\n",
            "Loss (epoch:  627): 0.02500122\n",
            "Loss (epoch:  628): 0.02500122\n",
            "Loss (epoch:  629): 0.02500122\n",
            "Loss (epoch:  630): 0.02500122\n",
            "Loss (epoch:  631): 0.02500122\n",
            "Loss (epoch:  632): 0.02500122\n",
            "Loss (epoch:  633): 0.02500122\n",
            "Loss (epoch:  634): 0.02500122\n",
            "Loss (epoch:  635): 0.02500122\n",
            "Loss (epoch:  636): 0.02500122\n",
            "Loss (epoch:  637): 0.02500122\n",
            "Loss (epoch:  638): 0.02500122\n",
            "Loss (epoch:  639): 0.02500122\n",
            "Loss (epoch:  640): 0.02500122\n",
            "Loss (epoch:  641): 0.02500122\n",
            "Loss (epoch:  642): 0.02500122\n",
            "Loss (epoch:  643): 0.02500122\n",
            "Loss (epoch:  644): 0.02500122\n",
            "Loss (epoch:  645): 0.02500122\n",
            "Loss (epoch:  646): 0.02500122\n",
            "Loss (epoch:  647): 0.02500122\n",
            "Loss (epoch:  648): 0.02500122\n",
            "Loss (epoch:  649): 0.02500122\n",
            "Loss (epoch:  650): 0.02500122\n",
            "Loss (epoch:  651): 0.02500122\n",
            "Loss (epoch:  652): 0.02500122\n",
            "Loss (epoch:  653): 0.02500122\n",
            "Loss (epoch:  654): 0.02500122\n",
            "Loss (epoch:  655): 0.02500122\n",
            "Loss (epoch:  656): 0.02500122\n",
            "Loss (epoch:  657): 0.02500122\n",
            "Loss (epoch:  658): 0.02500122\n",
            "Loss (epoch:  659): 0.02500122\n",
            "Loss (epoch:  660): 0.02500122\n",
            "Loss (epoch:  661): 0.02500122\n",
            "Loss (epoch:  662): 0.02500122\n",
            "Loss (epoch:  663): 0.02500122\n",
            "Loss (epoch:  664): 0.02500122\n",
            "Loss (epoch:  665): 0.02500122\n",
            "Loss (epoch:  666): 0.02500122\n",
            "Loss (epoch:  667): 0.02500122\n",
            "Loss (epoch:  668): 0.02500122\n",
            "Loss (epoch:  669): 0.02500122\n",
            "Loss (epoch:  670): 0.02500122\n",
            "Loss (epoch:  671): 0.02500122\n",
            "Loss (epoch:  672): 0.02500122\n",
            "Loss (epoch:  673): 0.02500122\n",
            "Loss (epoch:  674): 0.02500122\n",
            "Loss (epoch:  675): 0.02500122\n",
            "Loss (epoch:  676): 0.02500122\n",
            "Loss (epoch:  677): 0.02500122\n",
            "Loss (epoch:  678): 0.02500122\n",
            "Loss (epoch:  679): 0.02500122\n",
            "Loss (epoch:  680): 0.02500122\n",
            "Loss (epoch:  681): 0.02500122\n",
            "Loss (epoch:  682): 0.02500122\n",
            "Loss (epoch:  683): 0.02500122\n",
            "Loss (epoch:  684): 0.02500122\n",
            "Loss (epoch:  685): 0.02500122\n",
            "Loss (epoch:  686): 0.02500122\n",
            "Loss (epoch:  687): 0.02500122\n",
            "Loss (epoch:  688): 0.02500122\n",
            "Loss (epoch:  689): 0.02500122\n",
            "Loss (epoch:  690): 0.02500122\n",
            "Loss (epoch:  691): 0.02500122\n",
            "Loss (epoch:  692): 0.02500122\n",
            "Loss (epoch:  693): 0.02500122\n",
            "Loss (epoch:  694): 0.02500122\n",
            "Loss (epoch:  695): 0.02500122\n",
            "Loss (epoch:  696): 0.02500122\n",
            "Loss (epoch:  697): 0.02500122\n",
            "Loss (epoch:  698): 0.02500122\n",
            "Loss (epoch:  699): 0.02500122\n",
            "Loss (epoch:  700): 0.02500122\n",
            "Loss (epoch:  701): 0.02500122\n",
            "Loss (epoch:  702): 0.02500122\n",
            "Loss (epoch:  703): 0.02500122\n",
            "Loss (epoch:  704): 0.02500122\n",
            "Loss (epoch:  705): 0.02500122\n",
            "Loss (epoch:  706): 0.02500122\n",
            "Loss (epoch:  707): 0.02500122\n",
            "Loss (epoch:  708): 0.02500122\n",
            "Loss (epoch:  709): 0.02500122\n",
            "Loss (epoch:  710): 0.02500122\n",
            "Loss (epoch:  711): 0.02500122\n",
            "Loss (epoch:  712): 0.02500122\n",
            "Loss (epoch:  713): 0.02500122\n",
            "Loss (epoch:  714): 0.02500122\n",
            "Loss (epoch:  715): 0.02500122\n",
            "Loss (epoch:  716): 0.02500122\n",
            "Loss (epoch:  717): 0.02500122\n",
            "Loss (epoch:  718): 0.02500122\n",
            "Loss (epoch:  719): 0.02500122\n",
            "Loss (epoch:  720): 0.02500122\n",
            "Loss (epoch:  721): 0.02500122\n",
            "Loss (epoch:  722): 0.02500122\n",
            "Loss (epoch:  723): 0.02500122\n",
            "Loss (epoch:  724): 0.02500122\n",
            "Loss (epoch:  725): 0.02500122\n",
            "Loss (epoch:  726): 0.02500122\n",
            "Loss (epoch:  727): 0.02500122\n",
            "Loss (epoch:  728): 0.02500122\n",
            "Loss (epoch:  729): 0.02500122\n",
            "Loss (epoch:  730): 0.02500122\n",
            "Loss (epoch:  731): 0.02500122\n",
            "Loss (epoch:  732): 0.02500122\n",
            "Loss (epoch:  733): 0.02500122\n",
            "Loss (epoch:  734): 0.02500122\n",
            "Loss (epoch:  735): 0.02500122\n",
            "Loss (epoch:  736): 0.02500122\n",
            "Loss (epoch:  737): 0.02500122\n",
            "Loss (epoch:  738): 0.02500122\n",
            "Loss (epoch:  739): 0.02500122\n",
            "Loss (epoch:  740): 0.02500122\n",
            "Loss (epoch:  741): 0.02500122\n",
            "Loss (epoch:  742): 0.02500122\n",
            "Loss (epoch:  743): 0.02500122\n",
            "Loss (epoch:  744): 0.02500122\n",
            "Loss (epoch:  745): 0.02500122\n",
            "Loss (epoch:  746): 0.02500122\n",
            "Loss (epoch:  747): 0.02500122\n",
            "Loss (epoch:  748): 0.02500122\n",
            "Loss (epoch:  749): 0.02500122\n",
            "Loss (epoch:  750): 0.02500122\n",
            "Loss (epoch:  751): 0.02500122\n",
            "Loss (epoch:  752): 0.02500122\n",
            "Loss (epoch:  753): 0.02500122\n",
            "Loss (epoch:  754): 0.02500122\n",
            "Loss (epoch:  755): 0.02500122\n",
            "Loss (epoch:  756): 0.02500122\n",
            "Loss (epoch:  757): 0.02500122\n",
            "Loss (epoch:  758): 0.02500122\n",
            "Loss (epoch:  759): 0.02500122\n",
            "Loss (epoch:  760): 0.02500122\n",
            "Loss (epoch:  761): 0.02500122\n",
            "Loss (epoch:  762): 0.02500122\n",
            "Loss (epoch:  763): 0.02500122\n",
            "Loss (epoch:  764): 0.02500122\n",
            "Loss (epoch:  765): 0.02500122\n",
            "Loss (epoch:  766): 0.02500122\n",
            "Loss (epoch:  767): 0.02500122\n",
            "Loss (epoch:  768): 0.02500122\n",
            "Loss (epoch:  769): 0.02500122\n",
            "Loss (epoch:  770): 0.02500122\n",
            "Loss (epoch:  771): 0.02500122\n",
            "Loss (epoch:  772): 0.02500122\n",
            "Loss (epoch:  773): 0.02500122\n",
            "Loss (epoch:  774): 0.02500122\n",
            "Loss (epoch:  775): 0.02500122\n",
            "Loss (epoch:  776): 0.02500122\n",
            "Loss (epoch:  777): 0.02500122\n",
            "Loss (epoch:  778): 0.02500122\n",
            "Loss (epoch:  779): 0.02500122\n",
            "Loss (epoch:  780): 0.02500122\n",
            "Loss (epoch:  781): 0.02500122\n",
            "Loss (epoch:  782): 0.02500122\n",
            "Loss (epoch:  783): 0.02500122\n",
            "Loss (epoch:  784): 0.02500122\n",
            "Loss (epoch:  785): 0.02500122\n",
            "Loss (epoch:  786): 0.02500122\n",
            "Loss (epoch:  787): 0.02500122\n",
            "Loss (epoch:  788): 0.02500122\n",
            "Loss (epoch:  789): 0.02500122\n",
            "Loss (epoch:  790): 0.02500122\n",
            "Loss (epoch:  791): 0.02500122\n",
            "Loss (epoch:  792): 0.02500122\n",
            "Loss (epoch:  793): 0.02500122\n",
            "Loss (epoch:  794): 0.02500122\n",
            "Loss (epoch:  795): 0.02500122\n",
            "Loss (epoch:  796): 0.02500122\n",
            "Loss (epoch:  797): 0.02500122\n",
            "Loss (epoch:  798): 0.02500122\n",
            "Loss (epoch:  799): 0.02500122\n",
            "Loss (epoch:  800): 0.02500122\n",
            "Loss (epoch:  801): 0.02500122\n",
            "Loss (epoch:  802): 0.02500122\n",
            "Loss (epoch:  803): 0.02500122\n",
            "Loss (epoch:  804): 0.02500122\n",
            "Loss (epoch:  805): 0.02500122\n",
            "Loss (epoch:  806): 0.02500122\n",
            "Loss (epoch:  807): 0.02500122\n",
            "Loss (epoch:  808): 0.02500122\n",
            "Loss (epoch:  809): 0.02500122\n",
            "Loss (epoch:  810): 0.02500122\n",
            "Loss (epoch:  811): 0.02500122\n",
            "Loss (epoch:  812): 0.02500122\n",
            "Loss (epoch:  813): 0.02500122\n",
            "Loss (epoch:  814): 0.02500122\n",
            "Loss (epoch:  815): 0.02500122\n",
            "Loss (epoch:  816): 0.02500122\n",
            "Loss (epoch:  817): 0.02500122\n",
            "Loss (epoch:  818): 0.02500122\n",
            "Loss (epoch:  819): 0.02500122\n",
            "Loss (epoch:  820): 0.02500122\n",
            "Loss (epoch:  821): 0.02500122\n",
            "Loss (epoch:  822): 0.02500122\n",
            "Loss (epoch:  823): 0.02500122\n",
            "Loss (epoch:  824): 0.02500122\n",
            "Loss (epoch:  825): 0.02500122\n",
            "Loss (epoch:  826): 0.02500122\n",
            "Loss (epoch:  827): 0.02500122\n",
            "Loss (epoch:  828): 0.02500122\n",
            "Loss (epoch:  829): 0.02500122\n",
            "Loss (epoch:  830): 0.02500122\n",
            "Loss (epoch:  831): 0.02500122\n",
            "Loss (epoch:  832): 0.02500122\n",
            "Loss (epoch:  833): 0.02500122\n",
            "Loss (epoch:  834): 0.02500122\n",
            "Loss (epoch:  835): 0.02500122\n",
            "Loss (epoch:  836): 0.02500122\n",
            "Loss (epoch:  837): 0.02500122\n",
            "Loss (epoch:  838): 0.02500122\n",
            "Loss (epoch:  839): 0.02500122\n",
            "Loss (epoch:  840): 0.02500122\n",
            "Loss (epoch:  841): 0.02500122\n",
            "Loss (epoch:  842): 0.02500122\n",
            "Loss (epoch:  843): 0.02500122\n",
            "Loss (epoch:  844): 0.02500122\n",
            "Loss (epoch:  845): 0.02500122\n",
            "Loss (epoch:  846): 0.02500122\n",
            "Loss (epoch:  847): 0.02500122\n",
            "Loss (epoch:  848): 0.02500122\n",
            "Loss (epoch:  849): 0.02500122\n",
            "Loss (epoch:  850): 0.02500122\n",
            "Loss (epoch:  851): 0.02500122\n",
            "Loss (epoch:  852): 0.02500122\n",
            "Loss (epoch:  853): 0.02500122\n",
            "Loss (epoch:  854): 0.02500122\n",
            "Loss (epoch:  855): 0.02500122\n",
            "Loss (epoch:  856): 0.02500122\n",
            "Loss (epoch:  857): 0.02500122\n",
            "Loss (epoch:  858): 0.02500122\n",
            "Loss (epoch:  859): 0.02500122\n",
            "Loss (epoch:  860): 0.02500122\n",
            "Loss (epoch:  861): 0.02500122\n",
            "Loss (epoch:  862): 0.02500122\n",
            "Loss (epoch:  863): 0.02500122\n",
            "Loss (epoch:  864): 0.02500122\n",
            "Loss (epoch:  865): 0.02500122\n",
            "Loss (epoch:  866): 0.02500122\n",
            "Loss (epoch:  867): 0.02500122\n",
            "Loss (epoch:  868): 0.02500122\n",
            "Loss (epoch:  869): 0.02500122\n",
            "Loss (epoch:  870): 0.02500122\n",
            "Loss (epoch:  871): 0.02500122\n",
            "Loss (epoch:  872): 0.02500122\n",
            "Loss (epoch:  873): 0.02500122\n",
            "Loss (epoch:  874): 0.02500122\n",
            "Loss (epoch:  875): 0.02500122\n",
            "Loss (epoch:  876): 0.02500122\n",
            "Loss (epoch:  877): 0.02500122\n",
            "Loss (epoch:  878): 0.02500122\n",
            "Loss (epoch:  879): 0.02500122\n",
            "Loss (epoch:  880): 0.02500122\n",
            "Loss (epoch:  881): 0.02500122\n",
            "Loss (epoch:  882): 0.02500122\n",
            "Loss (epoch:  883): 0.02500122\n",
            "Loss (epoch:  884): 0.02500122\n",
            "Loss (epoch:  885): 0.02500122\n",
            "Loss (epoch:  886): 0.02500122\n",
            "Loss (epoch:  887): 0.02500122\n",
            "Loss (epoch:  888): 0.02500122\n",
            "Loss (epoch:  889): 0.02500122\n",
            "Loss (epoch:  890): 0.02500122\n",
            "Loss (epoch:  891): 0.02500122\n",
            "Loss (epoch:  892): 0.02500122\n",
            "Loss (epoch:  893): 0.02500122\n",
            "Loss (epoch:  894): 0.02500122\n",
            "Loss (epoch:  895): 0.02500122\n",
            "Loss (epoch:  896): 0.02500122\n",
            "Loss (epoch:  897): 0.02500122\n",
            "Loss (epoch:  898): 0.02500122\n",
            "Loss (epoch:  899): 0.02500122\n",
            "Loss (epoch:  900): 0.02500122\n",
            "Loss (epoch:  901): 0.02500122\n",
            "Loss (epoch:  902): 0.02500122\n",
            "Loss (epoch:  903): 0.02500122\n",
            "Loss (epoch:  904): 0.02500122\n",
            "Loss (epoch:  905): 0.02500122\n",
            "Loss (epoch:  906): 0.02500122\n",
            "Loss (epoch:  907): 0.02500122\n",
            "Loss (epoch:  908): 0.02500122\n",
            "Loss (epoch:  909): 0.02500122\n",
            "Loss (epoch:  910): 0.02500122\n",
            "Loss (epoch:  911): 0.02500122\n",
            "Loss (epoch:  912): 0.02500122\n",
            "Loss (epoch:  913): 0.02500122\n",
            "Loss (epoch:  914): 0.02500122\n",
            "Loss (epoch:  915): 0.02500122\n",
            "Loss (epoch:  916): 0.02500122\n",
            "Loss (epoch:  917): 0.02500122\n",
            "Loss (epoch:  918): 0.02500122\n",
            "Loss (epoch:  919): 0.02500122\n",
            "Loss (epoch:  920): 0.02500122\n",
            "Loss (epoch:  921): 0.02500122\n",
            "Loss (epoch:  922): 0.02500122\n",
            "Loss (epoch:  923): 0.02500122\n",
            "Loss (epoch:  924): 0.02500122\n",
            "Loss (epoch:  925): 0.02500122\n",
            "Loss (epoch:  926): 0.02500122\n",
            "Loss (epoch:  927): 0.02500122\n",
            "Loss (epoch:  928): 0.02500122\n",
            "Loss (epoch:  929): 0.02500122\n",
            "Loss (epoch:  930): 0.02500122\n",
            "Loss (epoch:  931): 0.02500122\n",
            "Loss (epoch:  932): 0.02500122\n",
            "Loss (epoch:  933): 0.02500122\n",
            "Loss (epoch:  934): 0.02500122\n",
            "Loss (epoch:  935): 0.02500122\n",
            "Loss (epoch:  936): 0.02500122\n",
            "Loss (epoch:  937): 0.02500122\n",
            "Loss (epoch:  938): 0.02500122\n",
            "Loss (epoch:  939): 0.02500122\n",
            "Loss (epoch:  940): 0.02500122\n",
            "Loss (epoch:  941): 0.02500122\n",
            "Loss (epoch:  942): 0.02500122\n",
            "Loss (epoch:  943): 0.02500122\n",
            "Loss (epoch:  944): 0.02500122\n",
            "Loss (epoch:  945): 0.02500122\n",
            "Loss (epoch:  946): 0.02500122\n",
            "Loss (epoch:  947): 0.02500122\n",
            "Loss (epoch:  948): 0.02500122\n",
            "Loss (epoch:  949): 0.02500122\n",
            "Loss (epoch:  950): 0.02500122\n",
            "Loss (epoch:  951): 0.02500122\n",
            "Loss (epoch:  952): 0.02500122\n",
            "Loss (epoch:  953): 0.02500122\n",
            "Loss (epoch:  954): 0.02500122\n",
            "Loss (epoch:  955): 0.02500122\n",
            "Loss (epoch:  956): 0.02500122\n",
            "Loss (epoch:  957): 0.02500122\n",
            "Loss (epoch:  958): 0.02500122\n",
            "Loss (epoch:  959): 0.02500122\n",
            "Loss (epoch:  960): 0.02500122\n",
            "Loss (epoch:  961): 0.02500122\n",
            "Loss (epoch:  962): 0.02500122\n",
            "Loss (epoch:  963): 0.02500122\n",
            "Loss (epoch:  964): 0.02500122\n",
            "Loss (epoch:  965): 0.02500122\n",
            "Loss (epoch:  966): 0.02500122\n",
            "Loss (epoch:  967): 0.02500122\n",
            "Loss (epoch:  968): 0.02500122\n",
            "Loss (epoch:  969): 0.02500122\n",
            "Loss (epoch:  970): 0.02500122\n",
            "Loss (epoch:  971): 0.02500122\n",
            "Loss (epoch:  972): 0.02500122\n",
            "Loss (epoch:  973): 0.02500122\n",
            "Loss (epoch:  974): 0.02500122\n",
            "Loss (epoch:  975): 0.02500122\n",
            "Loss (epoch:  976): 0.02500122\n",
            "Loss (epoch:  977): 0.02500122\n",
            "Loss (epoch:  978): 0.02500122\n",
            "Loss (epoch:  979): 0.02500122\n",
            "Loss (epoch:  980): 0.02500122\n",
            "Loss (epoch:  981): 0.02500122\n",
            "Loss (epoch:  982): 0.02500122\n",
            "Loss (epoch:  983): 0.02500122\n",
            "Loss (epoch:  984): 0.02500122\n",
            "Loss (epoch:  985): 0.02500122\n",
            "Loss (epoch:  986): 0.02500122\n",
            "Loss (epoch:  987): 0.02500122\n",
            "Loss (epoch:  988): 0.02500122\n",
            "Loss (epoch:  989): 0.02500122\n",
            "Loss (epoch:  990): 0.02500122\n",
            "Loss (epoch:  991): 0.02500122\n",
            "Loss (epoch:  992): 0.02500122\n",
            "Loss (epoch:  993): 0.02500122\n",
            "Loss (epoch:  994): 0.02500122\n",
            "Loss (epoch:  995): 0.02500122\n",
            "Loss (epoch:  996): 0.02500122\n",
            "Loss (epoch:  997): 0.02500122\n",
            "Loss (epoch:  998): 0.02500122\n",
            "Loss (epoch:  999): 0.02500122\n",
            "Loss (epoch: 1000): 0.02500122\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "\n",
        "    output = []\n",
        "    # Iterate over the dataloader for training data\n",
        "    for i, data in enumerate(testdataloader, 0):\n",
        "        inputs, targets = data\n",
        "        inputs, targets = inputs.float(), targets.float()\n",
        "        targets = targets.reshape((targets.shape[0],1))\n",
        "\n",
        "        #zero the gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        #perform forward pass\n",
        "        outputs = model(inputs)\n",
        "        output.append(outputs)\n",
        "# Process is complete.\n",
        "#print('Training process has finished.')\n",
        "\n",
        "# Extract the weights and biases from the model\n",
        "weights_1 = model.fc1.weight.detach().numpy()\n",
        "bias_1 = model.fc1.bias.detach().numpy()\n",
        "#weights_2 = model.fc2.weight.detach().numpy()\n",
        "#bias_2 = model.fc2.bias.detach().numpy()\n",
        "#weights_3 = model.fc3.weight.detach().numpy()\n",
        "#bias_3 = model.fc3.bias.detach().numpy()\n",
        "weights_4 = model.fc4.weight.detach().numpy()\n",
        "bias_4 = model.fc4.bias.detach().numpy()\n",
        "\n",
        "def generate_variable_declarations(weights_shape, layer_prefix):\n",
        "    declarations = \"\"\n",
        "    num_neurons = weights_shape  # Number of neurons is determined by the first dimension of the weights matrix\n",
        "    layer_declarations = \", \".join([f\"{layer_prefix}_{i}\" for i in range(num_neurons)]) + \";\"\n",
        "    declarations += layer_declarations\n",
        "    return declarations\n",
        "\n",
        "# Use the function to generate declarations for each layer\n",
        "h1_declarations = generate_variable_declarations(weights_1.shape[0], \"hvth1\")\n",
        "#2_declarations = generate_variable_declarations(weights_2.shape[0], \"hvth2\")\n",
        "\n",
        "verilog_code = \"\"\"\n",
        "// VerilogA for GB_lib, IWO_verliogA, veriloga\n",
        "//*******************************************************************************\n",
        "//* * School of Electrical and Computer Engineering, Georgia Institute of Technology\n",
        "//* PI: Prof. Shimeng Yu\n",
        "//* All rights reserved.\n",
        "//*\n",
        "//* Copyright of the model is maintained by the developers, and the model is distributed under\n",
        "//* the terms of the Creative Commons Attribution-NonCommercial 4.0 International Public License\n",
        "//* http://creativecommons.org/licenses/by-nc/4.0/legalcode.\n",
        "//*\n",
        "//* THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND\n",
        "//* ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\n",
        "//* WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n",
        "//* DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\n",
        "//* FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\n",
        "//* DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\n",
        "//* SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\n",
        "//* CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\n",
        "//* OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n",
        "//* OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n",
        "//*\n",
        "//* Developer:\n",
        "//*  Gihun Choe gchoe6@gatech.edu\n",
        "//********************************************************************************/\n",
        "\n",
        "`include \"constants.vams\"\n",
        "`include \"disciplines.vams\"\n",
        "\n",
        "\n",
        "module IWO_verliogA(d, g, s);\n",
        "        inout d, g, s;\n",
        "        electrical d, g, s;\n",
        "\n",
        "        //***** parameters L and W ******//\n",
        "        parameter real W = 0.1; //get parameter fom spectre\n",
        "        parameter real L = 0.05; //get parameter fom spectre\n",
        "        parameter real T_stress = 0.0001; //set on cadence as variable\n",
        "        parameter real T_rec = 0.001;     //set on cadence as variable\n",
        "        parameter real Clk_loops = 50;    //set on cadence as variable\n",
        "        parameter real V_ov = 1.7;        //set on cadence as variable\n",
        "        parameter real Temp = 25;  //set on cadence as variable\n",
        "\n",
        "        parameter MinVg = -1.0 ;\n",
        "        parameter normVg = 0.2222222222222222 ;\n",
        "        parameter MinVd = 0.01 ;\n",
        "        parameter normVd = 0.2949852507374631 ;\n",
        "        parameter MinLg = 0.05 ;\n",
        "        parameter normLg = 1.4285714285714286 ;\n",
        "        parameter MinO = 8.15e-15 ;\n",
        "        parameter normO =33613445378151.26;\n",
        "        parameter MinI = -23.98798356587402 ;\n",
        "        parameter normI =0.04615548498417793;\n",
        "\n",
        "        parameter Mint_stress = {} ;\n",
        "        parameter normt_stress = {} ;\n",
        "        parameter Mint_rec = {} ;\n",
        "        parameter normt_rec = {} ;\n",
        "        parameter Minclk_loops = {} ;\n",
        "        parameter normclk_loops = {} ;\n",
        "        parameter Minv_ov = {} ;\n",
        "        parameter normv_ov = {} ;\n",
        "        parameter Mintemperature = {} ;\n",
        "        parameter normtemperature = {} ;\n",
        "        parameter Mindelta_Vth = {} ;\n",
        "        parameter normdelta_Vth = {} ;\n",
        "\n",
        "        real {}\n",
        "        real {}\n",
        "\n",
        "        real Vg, Vd, Vs, Vgs, Vds, Lg, Id, Cgg, Cgsd, Vgd;\n",
        "        real Vgsraw, Vgdraw, dir;\n",
        "        real t_stress, v_ov, t_rec, clk_loops, temp, delta_Vth;\n",
        "\n",
        "        real h1_0, h1_1, h1_2, h1_3, h1_4, h1_5, h1_6, h1_7, h1_8, h1_9, h1_10, h1_11, h1_12, h1_13, h1_14, h1_15, h1_16, h1_17, h1_18, h1_19, h1_20, h1_21, h1_22, h1_23, h1_24;\n",
        "        real h2_0, h2_1, h2_2, h2_3, h2_4, h2_5, h2_6, h2_7, h2_8, h2_9, h2_10, h2_11, h2_12, h2_13, h2_14, h2_15, h2_16, y;\n",
        "        real hc1_0, hc1_1, hc1_2, hc1_3, hc1_4, hc1_5, hc1_6, hc1_7, hc1_8, hc1_9;\n",
        "        real hc1_10, hc1_11, hc1_12, hc1_13, hc1_14, hc1_15, hc1_16;\n",
        "        real hc2_0, hc2_1, hc2_2, hc2_3, hc2_4, hc2_5, hc2_6, hc2_7, hc2_8, hc2_9;\n",
        "        real hc2_10, hc2_11, hc2_12, hc2_13, hc2_14, hc2_15, hc2_16, yc, yvth;\n",
        "\n",
        "analog begin\n",
        "\n",
        "t_stress = (T_stress - Mint_stress)*normt_stress;\n",
        "v_ov = (V_ov - Minv_ov)*normv_ov;\n",
        "t_rec = (T_rec - Mint_rec)*normt_rec ;\n",
        "clk_loops = (Clk_loops - Minclk_loops)*normclk_loops ;\n",
        "temp = (Temp - Mintemperature)*normtemperature ;\n",
        "\n",
        "//******************** delta_Vth NN **********************************//\n",
        "\n",
        "\"\"\".format(Mint_stress, normt_stress, Mint_rec, normt_rec, Minclk_loops, normclk_loops, MinV_ov, normV_ov, Mintemperature, normtemperature, Mindelta_Vth, normdelta_Vth, h1_declarations, h2_declarations)\n",
        "# V_ov = (V_ov - MinV_ov)*normV_ov ;\n",
        "# t_stress = (T_stress - Mint_stress)*normt_stress ;\n",
        "\n",
        "# Create the Verilog-A code for the 1st hidden layer\n",
        "for i in range(n1):\n",
        "    inputs = [\"t_stress\", \"t_rec\", \"clk_loops\", \"v_ov\", \"temp\"]\n",
        "    inputs = [\"*\".join([str(weights_1[i][j]), inp]) for j, inp in enumerate(inputs)]\n",
        "    inputs = \"+\".join(inputs)\n",
        "    inputs = \"+\".join([inputs, str(bias_1[i])])\n",
        "    verilog_code += \"hvth1_{} = tanh({});\\n\".format(i, inputs)\n",
        "verilog_code += \"\\n\"\n",
        "\n",
        "\"\"\"\n",
        "# Create the Verilog-A code for the 2nd hidden layer\n",
        "for i in range(n2):\n",
        "    inputs = [\"h1_{}\".format(j) for j in range(n1)]\n",
        "    inputs = [\"*\".join([str(weights_2[i][j]), inp]) for j, inp in enumerate(inputs)]\n",
        "    inputs = \"+\".join(inputs)\n",
        "    inputs = \"+\".join([inputs, str(bias_2[i])])\n",
        "    verilog_code += \"hvth2_{} = tanh({});\\n\".format(i, inputs)\n",
        "verilog_code += \"\\n\"\n",
        "\"\"\"\n",
        "# Create the Verilog-A code for the output layer\n",
        "inputs = [\"hvth1_{}\".format(i) for i in range(n1)]\n",
        "inputs = [\"*\".join([str(weights_4[0][i]), inp]) for i, inp in enumerate(inputs)]\n",
        "inputs = \"+\".join(inputs)\n",
        "inputs = \"+\".join([inputs, str(bias_4[0])])\n",
        "verilog_code += \"yvth = {};\\n\\n\".format(inputs)\n",
        "verilog_code += \"delta_Vth = (yvth - Mindelta_Vth) * normdelta_Vth;\"\n",
        "verilog_code += \"\"\"\n",
        "\n",
        "\n",
        "        Vg = V(g) ;\n",
        "        Vs = V(s) ;\n",
        "        Vd = V(d) ;\n",
        "        Vgsraw = Vg-Vs ;\n",
        "        Vgdraw = Vg-Vd ;\n",
        "\n",
        "if (Vgsraw >= Vgdraw) begin\n",
        "        Vgs = ((Vg-Vs) - MinVg) * normVg ;\n",
        "        dir = 1;\n",
        "end\n",
        "\n",
        "else begin\n",
        "        Vgs = ((Vg-Vd) - MinVg) * normVg ;\n",
        "        dir = -1;\n",
        "end\n",
        "\n",
        "        Vds = (abs(Vd-Vs) - MinVd) * normVd ;\n",
        "        Vgs = Vgs - delta_Vth ; // BTI Vth variation\n",
        "        Lg = (L -MinLg)*normLg ;\n",
        "\n",
        "\n",
        "\n",
        "//******************** C-V NN **********************************//\n",
        "hc1_0 = tanh(-0.99871427*Vgs+-0.16952373*Vds+0.32118186*Lg+0.41485423);\n",
        "hc1_1 = tanh(0.31587568*Vgs+0.13397887*Vds+-0.4541538*Lg+-0.3630942);\n",
        "hc1_2 = tanh(-0.76281804*Vgs+0.09352969*Vds+1.1961353*Lg+0.3904977);\n",
        "hc1_3 = tanh(-1.115087*Vgs+0.85752946*Vds+-0.11746484*Lg+0.5500279);\n",
        "hc1_4 = tanh(1.0741386*Vgs+0.82041687*Vds+0.19092631*Lg+-0.4009425);\n",
        "hc1_5 = tanh(-0.47921795*Vgs+-0.8749933*Vds+-0.054768667*Lg+0.62785167);\n",
        "hc1_6 = tanh(0.5449184*Vgs+-4.409165*Vds+-0.072947875*Lg+-0.31324536);\n",
        "hc1_7 = tanh(-2.9224303*Vgs+2.7675478*Vds+0.08862238*Lg+0.6493558);\n",
        "hc1_8 = tanh(0.65050656*Vgs+-0.29751927*Vds+0.1571876*Lg+-0.38088372);\n",
        "hc1_9 = tanh(-0.30384183*Vgs+0.5649165*Vds+2.6806898*Lg+0.3197917);\n",
        "hc1_10 = tanh(-0.095988505*Vgs+2.0158541*Vds+-0.42972717*Lg+-0.30388466);\n",
        "hc1_11 = tanh(6.7699738*Vgs+-0.07234483*Vds+-0.013545353*Lg+-1.3694142);\n",
        "hc1_12 = tanh(-0.3404029*Vgs+0.0443459*Vds+0.89597*Lg+0.069993004);\n",
        "hc1_13 = tanh(0.62300175*Vgs+-0.29515797*Vds+1.6753465*Lg+-0.6520838);\n",
        "hc1_14 = tanh(0.37957156*Vgs+0.2237372*Vds+0.08591952*Lg+0.13126835);\n",
        "hc1_15 = tanh(0.19949242*Vgs+-0.26481664*Vds+-0.41059187*Lg+-0.40832308);\n",
        "hc1_16 = tanh(0.98966587*Vgs+-0.24259183*Vds+0.36584845*Lg+-0.8024042);\n",
        "\n",
        "hc2_0 = tanh(-0.91327864*hc1_0+0.4696781*hc1_1+0.3302202*hc1_2+0.11393868*hc1_3+0.45070222*hc1_4+-0.2894044*hc1_5+0.55066*hc1_6+-1.6242687*hc1_7+-0.38140613*hc1_8+0.032771554*hc1_9+0.17647126);\n",
        "hc2_1 = tanh(-1.1663305*hc1_0+-0.523984*hc1_1+-0.90804136*hc1_2+-0.7418044*hc1_3+1.456171*hc1_4+-0.16802542*hc1_5+0.8235596*hc1_6+-2.2246246*hc1_7+-0.40805355*hc1_8+0.7207601*hc1_9+0.4729169);\n",
        "hc2_2 = tanh(-0.69065374*hc1_0+0.40205315*hc1_1+-0.49410668*hc1_2+0.8681325*hc1_3+0.471351*hc1_4+0.46939445*hc1_5+0.45568785*hc1_6+-0.92935294*hc1_7+-0.8209646*hc1_8+0.1158967*hc1_9+-0.075798474);\n",
        "hc2_3 = tanh(0.11535446*hc1_0+-0.06296927*hc1_1+-0.6740435*hc1_2+0.7428315*hc1_3+0.05890677*hc1_4+0.9579441*hc1_5+-0.037319*hc1_6+-0.18491426*hc1_7+-0.02981994*hc1_8+0.038347963*hc1_9+0.039531134);\n",
        "hc2_4 = tanh(0.37817463*hc1_0+-0.6811279*hc1_1+1.1388369*hc1_2+0.19983096*hc1_3+-0.20415118*hc1_4+1.3022176*hc1_5+0.22571652*hc1_6+0.1690611*hc1_7+-0.56475276*hc1_8+-0.4069731*hc1_9+0.99962974);\n",
        "hc2_5 = tanh(0.032873478*hc1_0+-0.05209407*hc1_1+-0.010839908*hc1_2+-0.13892579*hc1_3+-0.050480977*hc1_4+0.0127089145*hc1_5+0.0052771433*hc1_6+0.02029242*hc1_7+-0.08705659*hc1_8+0.0254766*hc1_9+0.025135752);\n",
        "hc2_6 = tanh(-0.34639204*hc1_0+0.06937975*hc1_1+0.18671949*hc1_2+-0.18912783*hc1_3+0.1312504*hc1_4+0.4627272*hc1_5+-0.42590702*hc1_6+-0.10966313*hc1_7+0.66083515*hc1_8+-0.050718334*hc1_9+0.08234678);\n",
        "hc2_7 = tanh(0.90656275*hc1_0+-0.037281644*hc1_1+0.77237594*hc1_2+1.4710428*hc1_3+0.13597831*hc1_4+-0.059844542*hc1_5+-0.7801535*hc1_6+3.7814677*hc1_7+-0.5976644*hc1_8+0.2721995*hc1_9+0.023777716);\n",
        "hc2_8 = tanh(0.39720625*hc1_0+-0.45262313*hc1_1+0.19873238*hc1_2+0.9750888*hc1_3+-0.9427992*hc1_4+0.4487432*hc1_5+-0.3372945*hc1_6+0.33729544*hc1_7+-0.1667088*hc1_8+-0.5707525*hc1_9+0.27954483);\n",
        "hc2_9 = tanh(0.28551984*hc1_0+-0.68350387*hc1_1+0.9916423*hc1_2+-0.8254094*hc1_3+0.09875706*hc1_4+0.47609732*hc1_5+-0.058662917*hc1_6+0.09181381*hc1_7+0.09592329*hc1_8+1.3940467*hc1_9+0.3865768);\n",
        "hc2_10 = tanh(0.098737516*hc1_0+0.060473576*hc1_1+0.42824662*hc1_2+0.15018038*hc1_3+0.082621895*hc1_4+-0.00019039502*hc1_5+-0.3321634*hc1_6+0.7936295*hc1_7+-0.041197542*hc1_8+0.6530619*hc1_9+0.1338804);\n",
        "hc2_11 = tanh(-0.3585284*hc1_0+-0.09956017*hc1_1+0.17224246*hc1_2+-0.016925728*hc1_3+-0.46462816*hc1_4+-0.5649022*hc1_5+1.251695*hc1_6+-0.4303161*hc1_7+0.48546878*hc1_8+0.22958975*hc1_9+-0.27899802);\n",
        "hc2_12 = tanh(0.8565631*hc1_0+-0.7622999*hc1_1+0.32367912*hc1_2+1.4776785*hc1_3+0.2712369*hc1_4+0.2275511*hc1_5+0.39908803*hc1_6+4.2305493*hc1_7+-0.3467536*hc1_8+0.41231114*hc1_9+0.47123823);\n",
        "hc2_13 = tanh(-0.26411077*hc1_0+-0.17583853*hc1_1+0.045439184*hc1_2+-0.13801138*hc1_3+0.03278085*hc1_4+-0.45625108*hc1_5+-0.17905861*hc1_6+0.3060186*hc1_7+-0.3361926*hc1_8+-0.050055273*hc1_9+0.17996444);\n",
        "hc2_14 = tanh(0.016094366*hc1_0+-0.013196736*hc1_1+0.4124856*hc1_2+0.22926371*hc1_3+-0.35071182*hc1_4+-0.34217188*hc1_5+-0.69466996*hc1_6+1.0563152*hc1_7+-0.18019852*hc1_8+0.061871335*hc1_9+0.09762555);\n",
        "hc2_15 = tanh(-0.18970782*hc1_0+0.3624813*hc1_1+-1.3419824*hc1_2+0.103635244*hc1_3+-0.14595217*hc1_4+-0.1899393*hc1_5+0.176524*hc1_6+-0.4428012*hc1_7+-0.39544868*hc1_8+-0.45783517*hc1_9+0.1884755);\n",
        "hc2_16 = tanh(-0.1585831*hc1_0+0.035894837*hc1_1+-0.14261873*hc1_2+0.25914294*hc1_3+0.040607046*hc1_4+0.11555795*hc1_5+0.0022548323*hc1_6+-0.002149359*hc1_7+0.07067297*hc1_8+0.019578662*hc1_9+0.16657573);\n",
        "yc = 0.17503543*hc2_0+-0.15556754*hc2_1+-0.125455*hc2_2+-0.07502612*hc2_3+-0.16671115*hc2_4+0.29854503;\n",
        "\n",
        "Cgg = (yc / normO + MinO)*W;\n",
        "Cgsd = Cgg/2 ;\n",
        "\n",
        "//******************** I-V NN **********************************//\n",
        "h1_0 = tanh(0.0023826673*Vgs+-0.0009636134*Vds+0.006639037*Lg+-0.0004692053);\n",
        "h1_1 = tanh(-7.8007555*Vgs+2.639791*Vds+-0.025273597*Lg+-1.1747514);\n",
        "h1_2 = tanh(1.9884652*Vgs+0.62111*Vds+-0.11525345*Lg+-0.14575638);\n",
        "h1_3 = tanh(0.10625471*Vgs+0.09442053*Vds+-0.052119628*Lg+1.1568379);\n",
        "h1_4 = tanh(4.058087*Vgs+-0.7568158*Vds+0.008070099*Lg+-0.4820452);\n",
        "h1_5 = tanh(-1.3065948*Vgs+-0.0492497*Vds+-0.081622526*Lg+0.20351954);\n",
        "h1_6 = tanh(-2.9507525*Vgs+-0.91150546*Vds+-0.06477873*Lg+0.09646383);\n",
        "h1_7 = tanh(-0.5886177*Vgs+0.63788587*Vds+-0.13710928*Lg+0.30260038);\n",
        "h1_8 = tanh(-0.4311161*Vgs+-0.7250705*Vds+-0.06134645*Lg+0.44054285);\n",
        "h1_9 = tanh(0.012132638*Vgs+-0.42545322*Vds+-0.66478956*Lg+0.13182367);\n",
        "h1_10 = tanh(3.289041e-05*Vgs+0.0011367714*Vds+-0.0051904405*Lg+5.4112927e-05);\n",
        "h1_11 = tanh(-0.6007774*Vgs+0.09259224*Vds+0.2170182*Lg+-0.2908748);\n",
        "h1_12 = tanh(0.28018302*Vgs+1.3014064*Vds+-0.13490067*Lg+-0.22006753);\n",
        "h1_13 = tanh(0.01864017*Vgs+-13.0928755*Vds+-0.0037254083*Lg+-0.10935691);\n",
        "h1_14 = tanh(-0.0049708197*Vgs+0.0022613218*Vds+-0.014408149*Lg+0.0011340647);\n",
        "h1_15 = tanh(-0.094654046*Vgs+-0.4174114*Vds+0.18892045*Lg+0.05248958);\n",
        "h1_16 = tanh(-0.25090316*Vgs+-0.11111481*Vds+-0.009133225*Lg+0.08377026);\n",
        "h1_17 = tanh(0.9275306*Vgs+0.40686756*Vds+0.14127344*Lg+-0.059246097);\n",
        "h1_18 = tanh(-0.27084363*Vgs+0.07915911*Vds+-10.836302*Lg+-1.1535788);\n",
        "h1_19 = tanh(1.6852072*Vgs+-0.24846223*Vds+0.049734037*Lg+-0.19625053);\n",
        "\n",
        "h2_0 = tanh(1.1139417*h1_0+-0.40033522*h1_1+0.920759*h1_2+0.47607598*h1_3+0.3978683*h1_4+0.8008105*h1_5+-0.40445566*h1_6+0.8668027*h1_7+0.23365597*h1_8+-0.28254375*h1_9+-0.63985884*h1_10+-0.62523574*h1_11+0.8338383*h1_12+-2.0540943*h1_13+1.0749483*h1_14+-1.1075479*h1_15+0.60926706*h1_16+1.1118286*h1_17+-0.8917559*h1_18+0.029025732*h1_19+0.6622382);\n",
        "h2_1 = tanh(0.37195024*h1_0+-0.761445*h1_1+0.6514153*h1_2+0.6211995*h1_3+-0.063539445*h1_4+0.31260127*h1_5+-0.3529579*h1_6+0.50422627*h1_7+0.18943883*h1_8+-0.38029787*h1_9+-0.3464503*h1_10+-0.48301366*h1_11+0.081978925*h1_12+-0.08305682*h1_13+-0.08420117*h1_14+-0.8029313*h1_15+-0.37052512*h1_16+0.4553502*h1_17+-0.71959645*h1_18+-0.17320661*h1_19+0.6566257);\n",
        "h2_2 = tanh(-0.021933522*h1_0+-0.017245224*h1_1+0.030417712*h1_2+0.2967218*h1_3+-0.048668377*h1_4+0.029245213*h1_5+0.011756416*h1_6+0.004705658*h1_7+-0.042515088*h1_8+0.010462476*h1_9+0.001331279*h1_10+0.1694541*h1_11+-0.010949079*h1_12+-0.004144526*h1_13+0.04856694*h1_14+-0.010465159*h1_15+0.24588406*h1_16+-0.028521152*h1_17+0.12161568*h1_18+0.1753255*h1_19+-0.09125355);\n",
        "h2_3 = tanh(-0.025546636*h1_0+-3.2702503*h1_1+0.46812135*h1_2+-0.25135994*h1_3+3.0725436*h1_4+0.22513995*h1_5+-1.0327209*h1_6+-0.56274736*h1_7+0.4726107*h1_8+0.38182434*h1_9+0.016403453*h1_10+-0.09128962*h1_11+0.20997792*h1_12+-0.4951615*h1_13+0.026481293*h1_14+0.6085288*h1_15+-0.09078652*h1_16+0.36613584*h1_17+0.26257026*h1_18+0.39794916*h1_19+-0.52920526);\n",
        "h2_4 = tanh(-0.44392154*h1_0+-0.5650475*h1_1+0.91716766*h1_2+0.96703196*h1_3+0.4597048*h1_4+1.452921*h1_5+-0.8115141*h1_6+0.92326456*h1_7+0.2862403*h1_8+-0.9441585*h1_9+0.22188367*h1_10+-1.0092766*h1_11+0.5495966*h1_12+0.44870532*h1_13+0.48705962*h1_14+-0.6957133*h1_15+0.27711377*h1_16+1.0138507*h1_17+-0.34337458*h1_18+-0.21548931*h1_19+0.42685974);\n",
        "h2_5 = tanh(0.0003710325*h1_0+0.5746112*h1_1+0.4144258*h1_2+0.04459103*h1_3+1.2373503*h1_4+0.12786175*h1_5+-0.16099685*h1_6+0.9826207*h1_7+-0.09701193*h1_8+-0.4379135*h1_9+-0.0035542254*h1_10+0.04205593*h1_11+0.65820116*h1_12+0.3810506*h1_13+0.0004259507*h1_14+-0.21782044*h1_15+-0.057544652*h1_16+0.24420041*h1_17+-0.10138292*h1_18+-0.2387106*h1_19+0.867847);\n",
        "h2_6 = tanh(-0.0077049686*h1_0+-0.05349668*h1_1+-0.11536639*h1_2+0.06141029*h1_3+-0.034344573*h1_4+0.21672213*h1_5+-0.09835135*h1_6+-0.25849992*h1_7+0.10576329*h1_8+-0.14288934*h1_9+0.027846925*h1_10+-0.20104973*h1_11+0.24784875*h1_12+0.03396087*h1_13+-0.016039118*h1_14+-0.03147038*h1_15+-0.023109786*h1_16+-0.118931994*h1_17+0.18256636*h1_18+0.09981429*h1_19+0.079372324);\n",
        "h2_7 = tanh(-0.009369999*h1_0+-2.4468672*h1_1+-0.41433397*h1_2+-0.6289744*h1_3+-1.8285819*h1_4+0.12905455*h1_5+0.83588725*h1_6+0.11491322*h1_7+0.3871084*h1_8+0.07153052*h1_9+0.013530584*h1_10+0.40614852*h1_11+0.10340061*h1_12+-0.04473306*h1_13+0.0075287875*h1_14+0.5593612*h1_15+0.34401885*h1_16+-0.5538749*h1_17+0.36615464*h1_18+-0.2666981*h1_19+0.20194086);\n",
        "h2_8 = tanh(-0.0017045025*h1_0+-1.1388719*h1_1+0.25932005*h1_2+-0.6482845*h1_3+2.263395*h1_4+0.115192235*h1_5+-0.40946937*h1_6+-0.038117886*h1_7+-0.03703431*h1_8+0.05965774*h1_9+0.0032721795*h1_10+0.2561857*h1_11+0.47324425*h1_12+-0.25200802*h1_13+0.0017081021*h1_14+0.29819545*h1_15+0.061945435*h1_16+0.13516657*h1_17+0.5409335*h1_18+-0.17400871*h1_19+0.11659722);\n",
        "h2_9 = tanh(0.014866875*h1_0+1.0399909*h1_1+1.0108095*h1_2+-0.0918755*h1_3+-0.7634763*h1_4+-0.49749368*h1_5+-0.7020006*h1_6+-0.16384888*h1_7+-0.07833025*h1_8+-0.23745225*h1_9+-0.023095092*h1_10+-0.29244414*h1_11+0.6255917*h1_12+0.41413808*h1_13+-0.019823061*h1_14+-1.0593235*h1_15+-0.42221433*h1_16+0.98025715*h1_17+-0.71444243*h1_18+0.84347373*h1_19+0.28510216);\n",
        "h2_10 = tanh(-0.013515852*h1_0+0.68840384*h1_1+-0.030184174*h1_2+-0.48098114*h1_3+0.08609854*h1_4+-0.50878614*h1_5+0.26547763*h1_6+-0.7151076*h1_7+0.111288495*h1_8+0.53379977*h1_9+0.022258151*h1_10+0.16971351*h1_11+-1.2486296*h1_12+0.9649942*h1_13+0.0028906881*h1_14+0.31582054*h1_15+0.24213083*h1_16+-0.27242163*h1_17+0.11330636*h1_18+0.17038865*h1_19+-0.42089102);\n",
        "h2_11 = tanh(-0.009949424*h1_0+-0.6704206*h1_1+0.13177924*h1_2+0.07316155*h1_3+-0.28207502*h1_4+-0.32401362*h1_5+0.020746209*h1_6+-0.05517531*h1_7+0.10340257*h1_8+0.29381263*h1_9+0.020497978*h1_10+0.031984854*h1_11+0.079552434*h1_12+-7.7884445*h1_13+0.01224806*h1_14+0.8700812*h1_15+-0.16562358*h1_16+0.14370379*h1_17+0.2389181*h1_18+-0.19771136*h1_19+0.33039534);\n",
        "h2_12 = tanh(-0.002431529*h1_0+0.30154988*h1_1+-0.45072728*h1_2+0.27823612*h1_3+-0.25173753*h1_4+-0.58188903*h1_5+0.39836177*h1_6+-0.060329597*h1_7+0.1667837*h1_8+-0.09419194*h1_9+0.0108116735*h1_10+-0.3581154*h1_11+0.15690842*h1_12+-0.41209573*h1_13+-0.0070331707*h1_14+-0.23976392*h1_15+0.16620094*h1_16+-0.034617994*h1_17+0.32754526*h1_18+0.8490298*h1_19+0.026407415);\n",
        "h2_13 = tanh(-0.7385622*h1_0+-0.25174448*h1_1+2.090295*h1_2+0.67499936*h1_3+0.07392262*h1_4+0.5413692*h1_5+-0.8287433*h1_6+2.2810504*h1_7+1.1917778*h1_8+1.3719273*h1_9+0.25762329*h1_10+-2.1111712*h1_11+1.6987114*h1_12+-0.437825*h1_13+1.7649944*h1_14+-2.2254016*h1_15+-2.3555171*h1_16+-0.7732424*h1_17+-1.0756916*h1_18+-0.1367373*h1_19+1.5085722);\n",
        "h2_14 = tanh(-0.002535408*h1_0+-4.2686224*h1_1+-0.045537975*h1_2+0.08043166*h1_3+1.7274952*h1_4+-0.1260494*h1_5+-0.23011239*h1_6+-0.21025337*h1_7+0.48537356*h1_8+0.39049447*h1_9+-0.006565428*h1_10+-0.293028*h1_11+-1.1253971*h1_12+-0.2890831*h1_13+0.0042421576*h1_14+0.4230598*h1_15+-0.17869289*h1_16+-0.034056116*h1_17+0.61135995*h1_18+0.6345532*h1_19+0.73144835);\n",
        "h2_15 = tanh(-0.5239093*h1_0+0.49806875*h1_1+0.53555894*h1_2+1.2645539*h1_3+1.011548*h1_4+0.39209503*h1_5+-0.78194916*h1_6+0.94610345*h1_7+0.9121405*h1_8+-1.4693716*h1_9+-0.58336824*h1_10+-1.1661471*h1_11+-0.16158457*h1_12+0.058512915*h1_13+1.3358645*h1_14+0.5742125*h1_15+-1.3325212*h1_16+1.1804186*h1_17+-1.4877121*h1_18+0.6357802*h1_19+1.1328585);\n",
        "h2_16 = tanh(0.0095406*h1_0+-0.53232694*h1_1+0.1535685*h1_2+0.26108417*h1_3+-0.08375187*h1_4+0.3462123*h1_5+-0.61755395*h1_6+0.0015145333*h1_7+-0.108780764*h1_8+0.2555477*h1_9+0.00023940884*h1_10+-0.26577523*h1_11+-0.27689674*h1_12+0.43049082*h1_13+-0.025424613*h1_14+0.24634649*h1_15+-0.22579487*h1_16+0.37249807*h1_17+-0.8058662*h1_18+-0.77451354*h1_19+0.5821276);\n",
        "y = -0.17525196*h2_0+-0.22995844*h2_1+0.0026147955*h2_2+0.091129646*h2_3+-0.4534783*h2_4+0.4119419*h2_5+-1.4205361e-05*h2_6+-0.11359831*h2_7+0.1762025*h2_8+-0.1002102*h2_9+-0.37161925*h2_10+0.2683793*h2_11+0.051897053*h2_12+0.19354156*h2_13+-0.10134394*h2_14+0.3937854*h2_15+-0.3296008*h2_16+0.3220947;\n",
        "\n",
        "        Id = pow(10, (y/normI + MinI)) ;\n",
        "\n",
        "if (Id <= 1e-15) begin //limit\n",
        "        Id = 1e-15;\n",
        "        //Id = Id;\n",
        "end\n",
        "else begin\n",
        "        Id = Id;\n",
        "end  //limit end\n",
        "\n",
        "\n",
        "        I(g, d) <+ Cgsd*ddt(Vg-Vd) ;\n",
        "        I(g, s) <+ Cgsd*ddt(Vg-Vs) ;\n",
        "\n",
        "if (Vgsraw >= Vgdraw) begin\n",
        "        I(d, s) <+ dir*Id*W ;\n",
        "\n",
        "end\n",
        "\n",
        "else begin\n",
        "        I(d, s) <+ dir*Id*W ;\n",
        "\n",
        "end\n",
        "\n",
        "end\n",
        "\n",
        "endmodule\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "print(verilog_code)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bbMxJW3-Ras7",
        "outputId": "3819a774-e383-4ad4-9ee8-c3f6dbd0eb80"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "// VerilogA for GB_lib, IWO_verliogA, veriloga\n",
            "//*******************************************************************************\n",
            "//* * School of Electrical and Computer Engineering, Georgia Institute of Technology\n",
            "//* PI: Prof. Shimeng Yu\n",
            "//* All rights reserved.\n",
            "//*\n",
            "//* Copyright of the model is maintained by the developers, and the model is distributed under\n",
            "//* the terms of the Creative Commons Attribution-NonCommercial 4.0 International Public License\n",
            "//* http://creativecommons.org/licenses/by-nc/4.0/legalcode.\n",
            "//*\n",
            "//* THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND\n",
            "//* ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\n",
            "//* WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n",
            "//* DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\n",
            "//* FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\n",
            "//* DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\n",
            "//* SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\n",
            "//* CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\n",
            "//* OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n",
            "//* OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n",
            "//*\n",
            "//* Developer:\n",
            "//*  Gihun Choe gchoe6@gatech.edu\n",
            "//********************************************************************************/\n",
            "\n",
            "`include \"constants.vams\"\n",
            "`include \"disciplines.vams\"\n",
            "\n",
            "\n",
            "module IWO_verliogA(d, g, s);\n",
            "        inout d, g, s;\n",
            "        electrical d, g, s;\n",
            "\n",
            "        //***** parameters L and W ******//\n",
            "        parameter real W = 0.1; //get parameter fom spectre\n",
            "        parameter real L = 0.05; //get parameter fom spectre\n",
            "        parameter real T_stress = 0.0001; //set on cadence as variable\n",
            "        parameter real T_rec = 0.001;     //set on cadence as variable\n",
            "        parameter real Clk_loops = 50;    //set on cadence as variable\n",
            "        parameter real V_ov = 1.7;        //set on cadence as variable\n",
            "        parameter real Temp = 25;  //set on cadence as variable\n",
            "\n",
            "        parameter MinVg = -1.0 ;\n",
            "        parameter normVg = 0.2222222222222222 ;\n",
            "        parameter MinVd = 0.01 ;\n",
            "        parameter normVd = 0.2949852507374631 ;\n",
            "        parameter MinLg = 0.05 ;\n",
            "        parameter normLg = 1.4285714285714286 ;\n",
            "        parameter MinO = 8.15e-15 ;\n",
            "        parameter normO =33613445378151.26;\n",
            "        parameter MinI = -23.98798356587402 ;\n",
            "        parameter normI =0.04615548498417793;\n",
            "\n",
            "        parameter Mint_stress = 0.0001 ;\n",
            "        parameter normt_stress = 1111.111111111111 ;\n",
            "        parameter Mint_rec = 0.0001 ;\n",
            "        parameter normt_rec = 0.07629452739355005 ;\n",
            "        parameter Minclk_loops = 100.0 ;\n",
            "        parameter normclk_loops = 0.0011111111111111111 ;\n",
            "        parameter Minv_ov = 1.0 ;\n",
            "        parameter normv_ov = 1.4285714285714286 ;\n",
            "        parameter Mintemperature = 25.0 ;\n",
            "        parameter normtemperature = 0.016666666666666666 ;\n",
            "        parameter Mindelta_Vth = 0.04321379542061602 ;\n",
            "        parameter normdelta_Vth = 1.10036881607686 ;\n",
            "\n",
            "        real hvth1_0, hvth1_1, hvth1_2, hvth1_3, hvth1_4, hvth1_5, hvth1_6, hvth1_7, hvth1_8, hvth1_9, hvth1_10, hvth1_11, hvth1_12, hvth1_13, hvth1_14, hvth1_15, hvth1_16, hvth1_17, hvth1_18, hvth1_19;\n",
            "        real hvth2_0, hvth2_1, hvth2_2, hvth2_3, hvth2_4, hvth2_5, hvth2_6, hvth2_7, hvth2_8, hvth2_9;\n",
            "\n",
            "        real Vg, Vd, Vs, Vgs, Vds, Lg, Id, Cgg, Cgsd, Vgd;\n",
            "        real Vgsraw, Vgdraw, dir;\n",
            "        real t_stress, v_ov, t_rec, clk_loops, temp, delta_Vth;\n",
            "\n",
            "        real h1_0, h1_1, h1_2, h1_3, h1_4, h1_5, h1_6, h1_7, h1_8, h1_9, h1_10, h1_11, h1_12, h1_13, h1_14, h1_15, h1_16, h1_17, h1_18, h1_19, h1_20, h1_21, h1_22, h1_23, h1_24;\n",
            "        real h2_0, h2_1, h2_2, h2_3, h2_4, h2_5, h2_6, h2_7, h2_8, h2_9, h2_10, h2_11, h2_12, h2_13, h2_14, h2_15, h2_16, y;\n",
            "        real hc1_0, hc1_1, hc1_2, hc1_3, hc1_4, hc1_5, hc1_6, hc1_7, hc1_8, hc1_9;\n",
            "        real hc1_10, hc1_11, hc1_12, hc1_13, hc1_14, hc1_15, hc1_16;\n",
            "        real hc2_0, hc2_1, hc2_2, hc2_3, hc2_4, hc2_5, hc2_6, hc2_7, hc2_8, hc2_9;\n",
            "        real hc2_10, hc2_11, hc2_12, hc2_13, hc2_14, hc2_15, hc2_16, yc, yvth;\n",
            "\n",
            "analog begin\n",
            "\n",
            "t_stress = (T_stress - Mint_stress)*normt_stress;\n",
            "v_ov = (V_ov - Minv_ov)*normv_ov;\n",
            "t_rec = (T_rec - Mint_rec)*normt_rec ;\n",
            "clk_loops = (Clk_loops - Minclk_loops)*normclk_loops ;\n",
            "temp = (Temp - Mintemperature)*normtemperature ;\n",
            "\n",
            "//******************** delta_Vth NN **********************************//\n",
            "\n",
            "hvth1_0 = tanh(0.2500816*t_stress+0.3494927*t_rec+-0.027227858*clk_loops+0.19615911*v_ov+-0.12638812*temp+-0.10495603);\n",
            "hvth1_1 = tanh(0.6339284*t_stress+-0.09373678*t_rec+-0.04278805*clk_loops+-0.12487813*v_ov+-0.121251374*temp+-0.0003817874);\n",
            "hvth1_2 = tanh(0.22474506*t_stress+0.28950128*t_rec+0.30563113*clk_loops+-0.15888163*v_ov+0.05780247*temp+0.05209613);\n",
            "hvth1_3 = tanh(0.4275834*t_stress+0.20436187*t_rec+-0.12034748*clk_loops+-0.12077719*v_ov+0.11329896*temp+-0.096425034);\n",
            "hvth1_4 = tanh(0.42832723*t_stress+-0.23254706*t_rec+-0.05830425*clk_loops+-0.2999877*v_ov+0.052400496*temp+0.063853726);\n",
            "hvth1_5 = tanh(0.11975818*t_stress+-0.095110685*t_rec+0.3201763*clk_loops+-0.331158*v_ov+-0.2563358*temp+0.013856548);\n",
            "hvth1_6 = tanh(-0.27721095*t_stress+-0.0615577*t_rec+0.47190538*clk_loops+0.16667077*v_ov+-0.17281374*temp+-0.032367684);\n",
            "hvth1_7 = tanh(-0.08809367*t_stress+-0.43541688*t_rec+-0.08767599*clk_loops+0.2334363*v_ov+-0.11216702*temp+0.04557218);\n",
            "hvth1_8 = tanh(0.11537128*t_stress+0.062496774*t_rec+-0.53504914*clk_loops+-0.51772684*v_ov+-0.6574143*temp+-0.01823639);\n",
            "hvth1_9 = tanh(-0.15813704*t_stress+-0.35906208*t_rec+-0.37548852*clk_loops+0.22874445*v_ov+0.2433172*temp+0.060471945);\n",
            "hvth1_10 = tanh(-0.12911297*t_stress+0.026989972*t_rec+0.37391135*clk_loops+-0.19842574*v_ov+0.19676876*temp+-0.02631906);\n",
            "hvth1_11 = tanh(0.5649682*t_stress+0.13145167*t_rec+-0.37169528*clk_loops+-0.26134253*v_ov+-0.045833785*temp+0.056586515);\n",
            "hvth1_12 = tanh(-0.35522515*t_stress+0.12236696*t_rec+0.21175715*clk_loops+0.30376297*v_ov+-0.16240065*temp+0.005025978);\n",
            "hvth1_13 = tanh(0.18278329*t_stress+-0.05047196*t_rec+-0.0859669*clk_loops+0.08623643*v_ov+-0.29378998*temp+0.024062997);\n",
            "hvth1_14 = tanh(0.32751706*t_stress+0.18366009*t_rec+0.096448384*clk_loops+-0.010355521*v_ov+0.37254447*temp+0.073224336);\n",
            "hvth1_15 = tanh(-0.51555645*t_stress+-0.020809859*t_rec+0.15948933*clk_loops+-0.27451771*v_ov+0.39268005*temp+0.058021);\n",
            "hvth1_16 = tanh(0.26681328*t_stress+-0.09101257*t_rec+0.14451417*clk_loops+-0.072999075*v_ov+0.23352014*temp+0.041466154);\n",
            "hvth1_17 = tanh(-0.2325035*t_stress+-0.12786983*t_rec+-0.33691865*clk_loops+-0.22249557*v_ov+0.455452*temp+-0.08384482);\n",
            "hvth1_18 = tanh(-0.5573306*t_stress+0.4057994*t_rec+0.29177016*clk_loops+0.0020979329*v_ov+-0.20111765*temp+0.027436657);\n",
            "hvth1_19 = tanh(0.6195811*t_stress+0.14640138*t_rec+-0.12379404*clk_loops+0.3302253*v_ov+0.6246317*temp+0.09103866);\n",
            "\n",
            "yvth = -0.12529083*hvth1_0+-0.12773834*hvth1_1+0.1842808*hvth1_2+-0.42540267*hvth1_3+-0.23190032*hvth1_4+0.023249157*hvth1_5+-0.41356125*hvth1_6+0.2147577*hvth1_7+0.23298194*hvth1_8+0.18823026*hvth1_9+-0.06883408*hvth1_10+-0.329176*hvth1_11+-0.25085244*hvth1_12+0.30608433*hvth1_13+0.2231641*hvth1_14+0.5466542*hvth1_15+0.33310744*hvth1_16+-0.48210886*hvth1_17+0.2697012*hvth1_18+0.3399677*hvth1_19+0.039679233;\n",
            "\n",
            "delta_Vth = (yvth - Mindelta_Vth) * normdelta_Vth;\n",
            "\n",
            "\n",
            "        Vg = V(g) ;\n",
            "        Vs = V(s) ;\n",
            "        Vd = V(d) ;\n",
            "        Vgsraw = Vg-Vs ;\n",
            "        Vgdraw = Vg-Vd ;\n",
            "\n",
            "if (Vgsraw >= Vgdraw) begin\n",
            "        Vgs = ((Vg-Vs) - MinVg) * normVg ;\n",
            "        dir = 1;\n",
            "end\n",
            "\n",
            "else begin\n",
            "        Vgs = ((Vg-Vd) - MinVg) * normVg ;\n",
            "        dir = -1;\n",
            "end\n",
            "\n",
            "        Vds = (abs(Vd-Vs) - MinVd) * normVd ;\n",
            "        Vgs = Vgs - delta_Vth ; // BTI Vth variation\n",
            "        Lg = (L -MinLg)*normLg ;\n",
            "\n",
            "\n",
            "\n",
            "//******************** C-V NN **********************************//\n",
            "hc1_0 = tanh(-0.99871427*Vgs+-0.16952373*Vds+0.32118186*Lg+0.41485423);\n",
            "hc1_1 = tanh(0.31587568*Vgs+0.13397887*Vds+-0.4541538*Lg+-0.3630942);\n",
            "hc1_2 = tanh(-0.76281804*Vgs+0.09352969*Vds+1.1961353*Lg+0.3904977);\n",
            "hc1_3 = tanh(-1.115087*Vgs+0.85752946*Vds+-0.11746484*Lg+0.5500279);\n",
            "hc1_4 = tanh(1.0741386*Vgs+0.82041687*Vds+0.19092631*Lg+-0.4009425);\n",
            "hc1_5 = tanh(-0.47921795*Vgs+-0.8749933*Vds+-0.054768667*Lg+0.62785167);\n",
            "hc1_6 = tanh(0.5449184*Vgs+-4.409165*Vds+-0.072947875*Lg+-0.31324536);\n",
            "hc1_7 = tanh(-2.9224303*Vgs+2.7675478*Vds+0.08862238*Lg+0.6493558);\n",
            "hc1_8 = tanh(0.65050656*Vgs+-0.29751927*Vds+0.1571876*Lg+-0.38088372);\n",
            "hc1_9 = tanh(-0.30384183*Vgs+0.5649165*Vds+2.6806898*Lg+0.3197917);\n",
            "hc1_10 = tanh(-0.095988505*Vgs+2.0158541*Vds+-0.42972717*Lg+-0.30388466);\n",
            "hc1_11 = tanh(6.7699738*Vgs+-0.07234483*Vds+-0.013545353*Lg+-1.3694142);\n",
            "hc1_12 = tanh(-0.3404029*Vgs+0.0443459*Vds+0.89597*Lg+0.069993004);\n",
            "hc1_13 = tanh(0.62300175*Vgs+-0.29515797*Vds+1.6753465*Lg+-0.6520838);\n",
            "hc1_14 = tanh(0.37957156*Vgs+0.2237372*Vds+0.08591952*Lg+0.13126835);\n",
            "hc1_15 = tanh(0.19949242*Vgs+-0.26481664*Vds+-0.41059187*Lg+-0.40832308);\n",
            "hc1_16 = tanh(0.98966587*Vgs+-0.24259183*Vds+0.36584845*Lg+-0.8024042);\n",
            "\n",
            "hc2_0 = tanh(-0.91327864*hc1_0+0.4696781*hc1_1+0.3302202*hc1_2+0.11393868*hc1_3+0.45070222*hc1_4+-0.2894044*hc1_5+0.55066*hc1_6+-1.6242687*hc1_7+-0.38140613*hc1_8+0.032771554*hc1_9+0.17647126);\n",
            "hc2_1 = tanh(-1.1663305*hc1_0+-0.523984*hc1_1+-0.90804136*hc1_2+-0.7418044*hc1_3+1.456171*hc1_4+-0.16802542*hc1_5+0.8235596*hc1_6+-2.2246246*hc1_7+-0.40805355*hc1_8+0.7207601*hc1_9+0.4729169);\n",
            "hc2_2 = tanh(-0.69065374*hc1_0+0.40205315*hc1_1+-0.49410668*hc1_2+0.8681325*hc1_3+0.471351*hc1_4+0.46939445*hc1_5+0.45568785*hc1_6+-0.92935294*hc1_7+-0.8209646*hc1_8+0.1158967*hc1_9+-0.075798474);\n",
            "hc2_3 = tanh(0.11535446*hc1_0+-0.06296927*hc1_1+-0.6740435*hc1_2+0.7428315*hc1_3+0.05890677*hc1_4+0.9579441*hc1_5+-0.037319*hc1_6+-0.18491426*hc1_7+-0.02981994*hc1_8+0.038347963*hc1_9+0.039531134);\n",
            "hc2_4 = tanh(0.37817463*hc1_0+-0.6811279*hc1_1+1.1388369*hc1_2+0.19983096*hc1_3+-0.20415118*hc1_4+1.3022176*hc1_5+0.22571652*hc1_6+0.1690611*hc1_7+-0.56475276*hc1_8+-0.4069731*hc1_9+0.99962974);\n",
            "hc2_5 = tanh(0.032873478*hc1_0+-0.05209407*hc1_1+-0.010839908*hc1_2+-0.13892579*hc1_3+-0.050480977*hc1_4+0.0127089145*hc1_5+0.0052771433*hc1_6+0.02029242*hc1_7+-0.08705659*hc1_8+0.0254766*hc1_9+0.025135752);\n",
            "hc2_6 = tanh(-0.34639204*hc1_0+0.06937975*hc1_1+0.18671949*hc1_2+-0.18912783*hc1_3+0.1312504*hc1_4+0.4627272*hc1_5+-0.42590702*hc1_6+-0.10966313*hc1_7+0.66083515*hc1_8+-0.050718334*hc1_9+0.08234678);\n",
            "hc2_7 = tanh(0.90656275*hc1_0+-0.037281644*hc1_1+0.77237594*hc1_2+1.4710428*hc1_3+0.13597831*hc1_4+-0.059844542*hc1_5+-0.7801535*hc1_6+3.7814677*hc1_7+-0.5976644*hc1_8+0.2721995*hc1_9+0.023777716);\n",
            "hc2_8 = tanh(0.39720625*hc1_0+-0.45262313*hc1_1+0.19873238*hc1_2+0.9750888*hc1_3+-0.9427992*hc1_4+0.4487432*hc1_5+-0.3372945*hc1_6+0.33729544*hc1_7+-0.1667088*hc1_8+-0.5707525*hc1_9+0.27954483);\n",
            "hc2_9 = tanh(0.28551984*hc1_0+-0.68350387*hc1_1+0.9916423*hc1_2+-0.8254094*hc1_3+0.09875706*hc1_4+0.47609732*hc1_5+-0.058662917*hc1_6+0.09181381*hc1_7+0.09592329*hc1_8+1.3940467*hc1_9+0.3865768);\n",
            "hc2_10 = tanh(0.098737516*hc1_0+0.060473576*hc1_1+0.42824662*hc1_2+0.15018038*hc1_3+0.082621895*hc1_4+-0.00019039502*hc1_5+-0.3321634*hc1_6+0.7936295*hc1_7+-0.041197542*hc1_8+0.6530619*hc1_9+0.1338804);\n",
            "hc2_11 = tanh(-0.3585284*hc1_0+-0.09956017*hc1_1+0.17224246*hc1_2+-0.016925728*hc1_3+-0.46462816*hc1_4+-0.5649022*hc1_5+1.251695*hc1_6+-0.4303161*hc1_7+0.48546878*hc1_8+0.22958975*hc1_9+-0.27899802);\n",
            "hc2_12 = tanh(0.8565631*hc1_0+-0.7622999*hc1_1+0.32367912*hc1_2+1.4776785*hc1_3+0.2712369*hc1_4+0.2275511*hc1_5+0.39908803*hc1_6+4.2305493*hc1_7+-0.3467536*hc1_8+0.41231114*hc1_9+0.47123823);\n",
            "hc2_13 = tanh(-0.26411077*hc1_0+-0.17583853*hc1_1+0.045439184*hc1_2+-0.13801138*hc1_3+0.03278085*hc1_4+-0.45625108*hc1_5+-0.17905861*hc1_6+0.3060186*hc1_7+-0.3361926*hc1_8+-0.050055273*hc1_9+0.17996444);\n",
            "hc2_14 = tanh(0.016094366*hc1_0+-0.013196736*hc1_1+0.4124856*hc1_2+0.22926371*hc1_3+-0.35071182*hc1_4+-0.34217188*hc1_5+-0.69466996*hc1_6+1.0563152*hc1_7+-0.18019852*hc1_8+0.061871335*hc1_9+0.09762555);\n",
            "hc2_15 = tanh(-0.18970782*hc1_0+0.3624813*hc1_1+-1.3419824*hc1_2+0.103635244*hc1_3+-0.14595217*hc1_4+-0.1899393*hc1_5+0.176524*hc1_6+-0.4428012*hc1_7+-0.39544868*hc1_8+-0.45783517*hc1_9+0.1884755);\n",
            "hc2_16 = tanh(-0.1585831*hc1_0+0.035894837*hc1_1+-0.14261873*hc1_2+0.25914294*hc1_3+0.040607046*hc1_4+0.11555795*hc1_5+0.0022548323*hc1_6+-0.002149359*hc1_7+0.07067297*hc1_8+0.019578662*hc1_9+0.16657573);\n",
            "yc = 0.17503543*hc2_0+-0.15556754*hc2_1+-0.125455*hc2_2+-0.07502612*hc2_3+-0.16671115*hc2_4+0.29854503;\n",
            "\n",
            "Cgg = (yc / normO + MinO)*W;\n",
            "Cgsd = Cgg/2 ;\n",
            "\n",
            "//******************** I-V NN **********************************//\n",
            "h1_0 = tanh(0.0023826673*Vgs+-0.0009636134*Vds+0.006639037*Lg+-0.0004692053);\n",
            "h1_1 = tanh(-7.8007555*Vgs+2.639791*Vds+-0.025273597*Lg+-1.1747514);\n",
            "h1_2 = tanh(1.9884652*Vgs+0.62111*Vds+-0.11525345*Lg+-0.14575638);\n",
            "h1_3 = tanh(0.10625471*Vgs+0.09442053*Vds+-0.052119628*Lg+1.1568379);\n",
            "h1_4 = tanh(4.058087*Vgs+-0.7568158*Vds+0.008070099*Lg+-0.4820452);\n",
            "h1_5 = tanh(-1.3065948*Vgs+-0.0492497*Vds+-0.081622526*Lg+0.20351954);\n",
            "h1_6 = tanh(-2.9507525*Vgs+-0.91150546*Vds+-0.06477873*Lg+0.09646383);\n",
            "h1_7 = tanh(-0.5886177*Vgs+0.63788587*Vds+-0.13710928*Lg+0.30260038);\n",
            "h1_8 = tanh(-0.4311161*Vgs+-0.7250705*Vds+-0.06134645*Lg+0.44054285);\n",
            "h1_9 = tanh(0.012132638*Vgs+-0.42545322*Vds+-0.66478956*Lg+0.13182367);\n",
            "h1_10 = tanh(3.289041e-05*Vgs+0.0011367714*Vds+-0.0051904405*Lg+5.4112927e-05);\n",
            "h1_11 = tanh(-0.6007774*Vgs+0.09259224*Vds+0.2170182*Lg+-0.2908748);\n",
            "h1_12 = tanh(0.28018302*Vgs+1.3014064*Vds+-0.13490067*Lg+-0.22006753);\n",
            "h1_13 = tanh(0.01864017*Vgs+-13.0928755*Vds+-0.0037254083*Lg+-0.10935691);\n",
            "h1_14 = tanh(-0.0049708197*Vgs+0.0022613218*Vds+-0.014408149*Lg+0.0011340647);\n",
            "h1_15 = tanh(-0.094654046*Vgs+-0.4174114*Vds+0.18892045*Lg+0.05248958);\n",
            "h1_16 = tanh(-0.25090316*Vgs+-0.11111481*Vds+-0.009133225*Lg+0.08377026);\n",
            "h1_17 = tanh(0.9275306*Vgs+0.40686756*Vds+0.14127344*Lg+-0.059246097);\n",
            "h1_18 = tanh(-0.27084363*Vgs+0.07915911*Vds+-10.836302*Lg+-1.1535788);\n",
            "h1_19 = tanh(1.6852072*Vgs+-0.24846223*Vds+0.049734037*Lg+-0.19625053);\n",
            "\n",
            "h2_0 = tanh(1.1139417*h1_0+-0.40033522*h1_1+0.920759*h1_2+0.47607598*h1_3+0.3978683*h1_4+0.8008105*h1_5+-0.40445566*h1_6+0.8668027*h1_7+0.23365597*h1_8+-0.28254375*h1_9+-0.63985884*h1_10+-0.62523574*h1_11+0.8338383*h1_12+-2.0540943*h1_13+1.0749483*h1_14+-1.1075479*h1_15+0.60926706*h1_16+1.1118286*h1_17+-0.8917559*h1_18+0.029025732*h1_19+0.6622382);\n",
            "h2_1 = tanh(0.37195024*h1_0+-0.761445*h1_1+0.6514153*h1_2+0.6211995*h1_3+-0.063539445*h1_4+0.31260127*h1_5+-0.3529579*h1_6+0.50422627*h1_7+0.18943883*h1_8+-0.38029787*h1_9+-0.3464503*h1_10+-0.48301366*h1_11+0.081978925*h1_12+-0.08305682*h1_13+-0.08420117*h1_14+-0.8029313*h1_15+-0.37052512*h1_16+0.4553502*h1_17+-0.71959645*h1_18+-0.17320661*h1_19+0.6566257);\n",
            "h2_2 = tanh(-0.021933522*h1_0+-0.017245224*h1_1+0.030417712*h1_2+0.2967218*h1_3+-0.048668377*h1_4+0.029245213*h1_5+0.011756416*h1_6+0.004705658*h1_7+-0.042515088*h1_8+0.010462476*h1_9+0.001331279*h1_10+0.1694541*h1_11+-0.010949079*h1_12+-0.004144526*h1_13+0.04856694*h1_14+-0.010465159*h1_15+0.24588406*h1_16+-0.028521152*h1_17+0.12161568*h1_18+0.1753255*h1_19+-0.09125355);\n",
            "h2_3 = tanh(-0.025546636*h1_0+-3.2702503*h1_1+0.46812135*h1_2+-0.25135994*h1_3+3.0725436*h1_4+0.22513995*h1_5+-1.0327209*h1_6+-0.56274736*h1_7+0.4726107*h1_8+0.38182434*h1_9+0.016403453*h1_10+-0.09128962*h1_11+0.20997792*h1_12+-0.4951615*h1_13+0.026481293*h1_14+0.6085288*h1_15+-0.09078652*h1_16+0.36613584*h1_17+0.26257026*h1_18+0.39794916*h1_19+-0.52920526);\n",
            "h2_4 = tanh(-0.44392154*h1_0+-0.5650475*h1_1+0.91716766*h1_2+0.96703196*h1_3+0.4597048*h1_4+1.452921*h1_5+-0.8115141*h1_6+0.92326456*h1_7+0.2862403*h1_8+-0.9441585*h1_9+0.22188367*h1_10+-1.0092766*h1_11+0.5495966*h1_12+0.44870532*h1_13+0.48705962*h1_14+-0.6957133*h1_15+0.27711377*h1_16+1.0138507*h1_17+-0.34337458*h1_18+-0.21548931*h1_19+0.42685974);\n",
            "h2_5 = tanh(0.0003710325*h1_0+0.5746112*h1_1+0.4144258*h1_2+0.04459103*h1_3+1.2373503*h1_4+0.12786175*h1_5+-0.16099685*h1_6+0.9826207*h1_7+-0.09701193*h1_8+-0.4379135*h1_9+-0.0035542254*h1_10+0.04205593*h1_11+0.65820116*h1_12+0.3810506*h1_13+0.0004259507*h1_14+-0.21782044*h1_15+-0.057544652*h1_16+0.24420041*h1_17+-0.10138292*h1_18+-0.2387106*h1_19+0.867847);\n",
            "h2_6 = tanh(-0.0077049686*h1_0+-0.05349668*h1_1+-0.11536639*h1_2+0.06141029*h1_3+-0.034344573*h1_4+0.21672213*h1_5+-0.09835135*h1_6+-0.25849992*h1_7+0.10576329*h1_8+-0.14288934*h1_9+0.027846925*h1_10+-0.20104973*h1_11+0.24784875*h1_12+0.03396087*h1_13+-0.016039118*h1_14+-0.03147038*h1_15+-0.023109786*h1_16+-0.118931994*h1_17+0.18256636*h1_18+0.09981429*h1_19+0.079372324);\n",
            "h2_7 = tanh(-0.009369999*h1_0+-2.4468672*h1_1+-0.41433397*h1_2+-0.6289744*h1_3+-1.8285819*h1_4+0.12905455*h1_5+0.83588725*h1_6+0.11491322*h1_7+0.3871084*h1_8+0.07153052*h1_9+0.013530584*h1_10+0.40614852*h1_11+0.10340061*h1_12+-0.04473306*h1_13+0.0075287875*h1_14+0.5593612*h1_15+0.34401885*h1_16+-0.5538749*h1_17+0.36615464*h1_18+-0.2666981*h1_19+0.20194086);\n",
            "h2_8 = tanh(-0.0017045025*h1_0+-1.1388719*h1_1+0.25932005*h1_2+-0.6482845*h1_3+2.263395*h1_4+0.115192235*h1_5+-0.40946937*h1_6+-0.038117886*h1_7+-0.03703431*h1_8+0.05965774*h1_9+0.0032721795*h1_10+0.2561857*h1_11+0.47324425*h1_12+-0.25200802*h1_13+0.0017081021*h1_14+0.29819545*h1_15+0.061945435*h1_16+0.13516657*h1_17+0.5409335*h1_18+-0.17400871*h1_19+0.11659722);\n",
            "h2_9 = tanh(0.014866875*h1_0+1.0399909*h1_1+1.0108095*h1_2+-0.0918755*h1_3+-0.7634763*h1_4+-0.49749368*h1_5+-0.7020006*h1_6+-0.16384888*h1_7+-0.07833025*h1_8+-0.23745225*h1_9+-0.023095092*h1_10+-0.29244414*h1_11+0.6255917*h1_12+0.41413808*h1_13+-0.019823061*h1_14+-1.0593235*h1_15+-0.42221433*h1_16+0.98025715*h1_17+-0.71444243*h1_18+0.84347373*h1_19+0.28510216);\n",
            "h2_10 = tanh(-0.013515852*h1_0+0.68840384*h1_1+-0.030184174*h1_2+-0.48098114*h1_3+0.08609854*h1_4+-0.50878614*h1_5+0.26547763*h1_6+-0.7151076*h1_7+0.111288495*h1_8+0.53379977*h1_9+0.022258151*h1_10+0.16971351*h1_11+-1.2486296*h1_12+0.9649942*h1_13+0.0028906881*h1_14+0.31582054*h1_15+0.24213083*h1_16+-0.27242163*h1_17+0.11330636*h1_18+0.17038865*h1_19+-0.42089102);\n",
            "h2_11 = tanh(-0.009949424*h1_0+-0.6704206*h1_1+0.13177924*h1_2+0.07316155*h1_3+-0.28207502*h1_4+-0.32401362*h1_5+0.020746209*h1_6+-0.05517531*h1_7+0.10340257*h1_8+0.29381263*h1_9+0.020497978*h1_10+0.031984854*h1_11+0.079552434*h1_12+-7.7884445*h1_13+0.01224806*h1_14+0.8700812*h1_15+-0.16562358*h1_16+0.14370379*h1_17+0.2389181*h1_18+-0.19771136*h1_19+0.33039534);\n",
            "h2_12 = tanh(-0.002431529*h1_0+0.30154988*h1_1+-0.45072728*h1_2+0.27823612*h1_3+-0.25173753*h1_4+-0.58188903*h1_5+0.39836177*h1_6+-0.060329597*h1_7+0.1667837*h1_8+-0.09419194*h1_9+0.0108116735*h1_10+-0.3581154*h1_11+0.15690842*h1_12+-0.41209573*h1_13+-0.0070331707*h1_14+-0.23976392*h1_15+0.16620094*h1_16+-0.034617994*h1_17+0.32754526*h1_18+0.8490298*h1_19+0.026407415);\n",
            "h2_13 = tanh(-0.7385622*h1_0+-0.25174448*h1_1+2.090295*h1_2+0.67499936*h1_3+0.07392262*h1_4+0.5413692*h1_5+-0.8287433*h1_6+2.2810504*h1_7+1.1917778*h1_8+1.3719273*h1_9+0.25762329*h1_10+-2.1111712*h1_11+1.6987114*h1_12+-0.437825*h1_13+1.7649944*h1_14+-2.2254016*h1_15+-2.3555171*h1_16+-0.7732424*h1_17+-1.0756916*h1_18+-0.1367373*h1_19+1.5085722);\n",
            "h2_14 = tanh(-0.002535408*h1_0+-4.2686224*h1_1+-0.045537975*h1_2+0.08043166*h1_3+1.7274952*h1_4+-0.1260494*h1_5+-0.23011239*h1_6+-0.21025337*h1_7+0.48537356*h1_8+0.39049447*h1_9+-0.006565428*h1_10+-0.293028*h1_11+-1.1253971*h1_12+-0.2890831*h1_13+0.0042421576*h1_14+0.4230598*h1_15+-0.17869289*h1_16+-0.034056116*h1_17+0.61135995*h1_18+0.6345532*h1_19+0.73144835);\n",
            "h2_15 = tanh(-0.5239093*h1_0+0.49806875*h1_1+0.53555894*h1_2+1.2645539*h1_3+1.011548*h1_4+0.39209503*h1_5+-0.78194916*h1_6+0.94610345*h1_7+0.9121405*h1_8+-1.4693716*h1_9+-0.58336824*h1_10+-1.1661471*h1_11+-0.16158457*h1_12+0.058512915*h1_13+1.3358645*h1_14+0.5742125*h1_15+-1.3325212*h1_16+1.1804186*h1_17+-1.4877121*h1_18+0.6357802*h1_19+1.1328585);\n",
            "h2_16 = tanh(0.0095406*h1_0+-0.53232694*h1_1+0.1535685*h1_2+0.26108417*h1_3+-0.08375187*h1_4+0.3462123*h1_5+-0.61755395*h1_6+0.0015145333*h1_7+-0.108780764*h1_8+0.2555477*h1_9+0.00023940884*h1_10+-0.26577523*h1_11+-0.27689674*h1_12+0.43049082*h1_13+-0.025424613*h1_14+0.24634649*h1_15+-0.22579487*h1_16+0.37249807*h1_17+-0.8058662*h1_18+-0.77451354*h1_19+0.5821276);\n",
            "y = -0.17525196*h2_0+-0.22995844*h2_1+0.0026147955*h2_2+0.091129646*h2_3+-0.4534783*h2_4+0.4119419*h2_5+-1.4205361e-05*h2_6+-0.11359831*h2_7+0.1762025*h2_8+-0.1002102*h2_9+-0.37161925*h2_10+0.2683793*h2_11+0.051897053*h2_12+0.19354156*h2_13+-0.10134394*h2_14+0.3937854*h2_15+-0.3296008*h2_16+0.3220947;\n",
            "\n",
            "        Id = pow(10, (y/normI + MinI)) ;\n",
            "\n",
            "if (Id <= 1e-15) begin //limit\n",
            "        Id = 1e-15;\n",
            "        //Id = Id;\n",
            "end\n",
            "else begin\n",
            "        Id = Id;\n",
            "end  //limit end\n",
            "\n",
            "\n",
            "        I(g, d) <+ Cgsd*ddt(Vg-Vd) ;\n",
            "        I(g, s) <+ Cgsd*ddt(Vg-Vs) ;\n",
            "\n",
            "if (Vgsraw >= Vgdraw) begin\n",
            "        I(d, s) <+ dir*Id*W ;\n",
            "\n",
            "end\n",
            "\n",
            "else begin\n",
            "        I(d, s) <+ dir*Id*W ;\n",
            "\n",
            "end\n",
            "\n",
            "end\n",
            "\n",
            "endmodule\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O8ZOvUesBszP"
      },
      "source": [
        "**Deprecated Version (Linear Regression)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ASPvpASs1kUi",
        "outputId": "c5c7ddd0-a242-42d6-a36f-32f8004863b4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [50/5000], Loss: 216.5890\n",
            "Epoch [100/5000], Loss: 35.0012\n",
            "Epoch [150/5000], Loss: 14.2757\n",
            "Epoch [200/5000], Loss: 5.4854\n",
            "Epoch [250/5000], Loss: 2.1224\n",
            "Epoch [300/5000], Loss: 1.0208\n",
            "Epoch [350/5000], Loss: 0.6775\n",
            "Epoch [400/5000], Loss: 0.5532\n",
            "Epoch [450/5000], Loss: 0.4914\n",
            "Epoch [500/5000], Loss: 0.4525\n",
            "Epoch [550/5000], Loss: 0.4252\n",
            "Epoch [600/5000], Loss: 0.4048\n",
            "Epoch [650/5000], Loss: 0.3887\n",
            "Epoch [700/5000], Loss: 0.3751\n",
            "Epoch [750/5000], Loss: 0.3628\n",
            "Epoch [800/5000], Loss: 0.3512\n",
            "Epoch [850/5000], Loss: 0.3399\n",
            "Epoch [900/5000], Loss: 0.3286\n",
            "Epoch [950/5000], Loss: 0.3174\n",
            "Epoch [1000/5000], Loss: 0.3062\n",
            "Epoch [1050/5000], Loss: 0.2950\n",
            "Epoch [1100/5000], Loss: 0.2838\n",
            "Epoch [1150/5000], Loss: 0.2727\n",
            "Epoch [1200/5000], Loss: 0.2616\n",
            "Epoch [1250/5000], Loss: 0.2506\n",
            "Epoch [1300/5000], Loss: 0.2397\n",
            "Epoch [1350/5000], Loss: 0.2289\n",
            "Epoch [1400/5000], Loss: 0.2183\n",
            "Epoch [1450/5000], Loss: 0.2078\n",
            "Epoch [1500/5000], Loss: 0.1976\n",
            "Epoch [1550/5000], Loss: 0.1875\n",
            "Epoch [1600/5000], Loss: 0.1777\n",
            "Epoch [1650/5000], Loss: 0.1682\n",
            "Epoch [1700/5000], Loss: 0.1589\n",
            "Epoch [1750/5000], Loss: 0.1499\n",
            "Epoch [1800/5000], Loss: 0.1411\n",
            "Epoch [1850/5000], Loss: 0.1327\n",
            "Epoch [1900/5000], Loss: 0.1245\n",
            "Epoch [1950/5000], Loss: 0.1167\n",
            "Epoch [2000/5000], Loss: 0.1092\n",
            "Epoch [2050/5000], Loss: 0.1020\n",
            "Epoch [2100/5000], Loss: 0.0951\n",
            "Epoch [2150/5000], Loss: 0.0886\n",
            "Epoch [2200/5000], Loss: 0.0823\n",
            "Epoch [2250/5000], Loss: 0.0764\n",
            "Epoch [2300/5000], Loss: 0.0708\n",
            "Epoch [2350/5000], Loss: 0.0655\n",
            "Epoch [2400/5000], Loss: 0.0606\n",
            "Epoch [2450/5000], Loss: 0.0559\n",
            "Epoch [2500/5000], Loss: 0.0515\n",
            "Epoch [2550/5000], Loss: 0.0475\n",
            "Epoch [2600/5000], Loss: 0.0437\n",
            "Epoch [2650/5000], Loss: 0.0401\n",
            "Epoch [2700/5000], Loss: 0.0369\n",
            "Epoch [2750/5000], Loss: 0.0339\n",
            "Epoch [2800/5000], Loss: 0.0311\n",
            "Epoch [2850/5000], Loss: 0.0286\n",
            "Epoch [2900/5000], Loss: 0.0262\n",
            "Epoch [2950/5000], Loss: 0.0241\n",
            "Epoch [3000/5000], Loss: 0.0222\n",
            "Epoch [3050/5000], Loss: 0.0204\n",
            "Epoch [3100/5000], Loss: 0.0189\n",
            "Epoch [3150/5000], Loss: 0.0175\n",
            "Epoch [3200/5000], Loss: 0.0162\n",
            "Epoch [3250/5000], Loss: 0.0151\n",
            "Epoch [3300/5000], Loss: 0.0141\n",
            "Epoch [3350/5000], Loss: 0.0132\n",
            "Epoch [3400/5000], Loss: 0.0124\n",
            "Epoch [3450/5000], Loss: 0.0117\n",
            "Epoch [3500/5000], Loss: 0.0111\n",
            "Epoch [3550/5000], Loss: 0.0105\n",
            "Epoch [3600/5000], Loss: 0.0101\n",
            "Epoch [3650/5000], Loss: 0.0097\n",
            "Epoch [3700/5000], Loss: 0.0094\n",
            "Epoch [3750/5000], Loss: 0.0091\n",
            "Epoch [3800/5000], Loss: 0.0088\n",
            "Epoch [3850/5000], Loss: 0.0086\n",
            "Epoch [3900/5000], Loss: 0.0084\n",
            "Epoch [3950/5000], Loss: 0.0083\n",
            "Epoch [4000/5000], Loss: 0.0082\n",
            "Epoch [4050/5000], Loss: 0.0081\n",
            "Epoch [4100/5000], Loss: 0.0080\n",
            "Epoch [4150/5000], Loss: 0.0079\n",
            "Epoch [4200/5000], Loss: 0.0078\n",
            "Epoch [4250/5000], Loss: 0.0078\n",
            "Epoch [4300/5000], Loss: 0.0078\n",
            "Epoch [4350/5000], Loss: 0.0077\n",
            "Epoch [4400/5000], Loss: 0.0077\n",
            "Epoch [4450/5000], Loss: 0.0077\n",
            "Epoch [4500/5000], Loss: 0.0077\n",
            "Epoch [4550/5000], Loss: 0.0077\n",
            "Epoch [4600/5000], Loss: 0.0077\n",
            "Epoch [4650/5000], Loss: 0.0077\n",
            "Epoch [4700/5000], Loss: 0.0076\n",
            "Epoch [4750/5000], Loss: 0.0076\n",
            "Epoch [4800/5000], Loss: 0.0076\n",
            "Epoch [4850/5000], Loss: 0.0076\n",
            "Epoch [4900/5000], Loss: 0.0076\n",
            "Epoch [4950/5000], Loss: 0.0076\n",
            "Epoch [5000/5000], Loss: 0.0076\n",
            "Learned linear coefficients (h1, h2, h3, h4, h5, h6): [[3.2403252e-01 1.2588283e-04 1.4926398e-01 1.6216135e-06 5.8017345e-03\n",
            "  3.4838660e-05]]\n",
            "Learned bias: [0.369416]\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "X_tensor = torch.tensor(X, dtype=torch.float32)\n",
        "Y_tensor = torch.tensor(Y, dtype=torch.float32)\n",
        "\n",
        "# Define the model\n",
        "class LinearModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LinearModel, self).__init__()\n",
        "        self.linear = nn.Linear(6, 1)  # 6 inputs (t_stress, t_rec, etc.), 1 output (deltaV)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linear(x)\n",
        "\n",
        "# Instantiate the model\n",
        "model = LinearModel()\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 5000\n",
        "for epoch in range(num_epochs):\n",
        "    # Forward pass\n",
        "    outputs = model(X_tensor)\n",
        "    loss = criterion(outputs, Y_tensor)\n",
        "\n",
        "    # Backward and optimize\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if (epoch+1) % 50 == 0:\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "# Print learned weights and biases\n",
        "print(\"Learned linear coefficients (h1, h2, h3, h4, h5, h6):\", model.linear.weight.data.numpy())\n",
        "print(\"Learned bias:\", model.linear.bias.data.numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rjD077d3381W",
        "outputId": "ead20c1e-027c-46e5-ac60-530137e872f3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Estimated Output delta_V: 0.45497649908065796\n"
          ]
        }
      ],
      "source": [
        "input_array = np.array([[0.0001, 0.0002, 0.5, 100, 1.7, 25]])  # Sample input\n",
        "\n",
        "# Convert input data to a tensor\n",
        "input_tensor = torch.tensor(input_array, dtype=torch.float32)\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# No need to track gradients for making predictions\n",
        "with torch.no_grad():\n",
        "    # Predict the output using the model\n",
        "    predicted_output = model(input_tensor)\n",
        "\n",
        "# Print the predicted delta_V value\n",
        "print(\"Estimated Output delta_V:\", predicted_output.item())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9OZZWfblrhuz",
        "outputId": "3b31f421-93f6-4151-b35e-2e8776ffba7b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 10/100, Loss: 0.048087798058986664\n",
            "Epoch 20/100, Loss: 0.07116486877202988\n",
            "Epoch 30/100, Loss: 0.08174236863851547\n",
            "Epoch 40/100, Loss: 0.10612807422876358\n",
            "Epoch 50/100, Loss: 0.05975770205259323\n",
            "Epoch 60/100, Loss: 0.04767672345042229\n",
            "Epoch 70/100, Loss: 0.06521065533161163\n",
            "Epoch 80/100, Loss: 0.04492233693599701\n",
            "Epoch 90/100, Loss: 0.09233981370925903\n",
            "Epoch 100/100, Loss: 0.052101973444223404\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import numpy as np\n",
        "\n",
        "# Define the MLP neural network class\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MLP, self).__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(5, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)\n",
        "\n",
        "# Example data\n",
        "np.random.seed(42)\n",
        "x_data = np.random.rand(1000, 5)  # 5D Input (t_st, t_rec, t_cycle, Vov, temp)\n",
        "y_data = np.random.rand(1000, 1)  # 1D Output(deltaV)\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "x_tensor = torch.FloatTensor(x_data)\n",
        "y_tensor = torch.FloatTensor(y_data)\n",
        "\n",
        "# Create dataset and dataloader\n",
        "dataset = TensorDataset(x_tensor, y_tensor)\n",
        "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "# Initialize the MLP\n",
        "model = MLP()\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 100\n",
        "for epoch in range(num_epochs):\n",
        "    for inputs, targets in dataloader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}')\n",
        "\n",
        "# Save the trained model\n",
        "torch.save(model.state_dict(), 'mlp_regression_model.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vGO6_LmkvEpO",
        "outputId": "10734133-2889-4575-d0c0-d57ccc05b958"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predicted delta_V: 26.089859008789062V\n"
          ]
        }
      ],
      "source": [
        "input_features = [0.01, 0.01, 0.001, 1.7, 300] # 5D Input (t_st, t_rec, t_cycle, Vov, temp)\n",
        "input_tensor = torch.FloatTensor([input_features])\n",
        "\n",
        "# Inference\n",
        "with torch.no_grad():\n",
        "    output = model(input_tensor)\n",
        "\n",
        "print(\"Predicted delta_V: {}V\".format(output.item()))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "197YwTf3VfAoIKLyiMET0e8Bp9tePnvKt",
      "authorship_tag": "ABX9TyO2T1wETVciW3wEwOzTg3+H",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}