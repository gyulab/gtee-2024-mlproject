{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "197YwTf3VfAoIKLyiMET0e8Bp9tePnvKt",
      "authorship_tag": "ABX9TyPlgRzksLaVfLR08mmlHLhI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gyulab/gtee-bti-mlproject/blob/main/IWO_ML_dVth_inference.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Call\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "dVth_xls_data = pd.read_excel(r'/content/drive/MyDrive/Colab_ML_ProfYu/csv_data/ML_BTI_Vth_dataset_MOCKUP.xlsx')\n",
        "t_stress, t_rec, t_ratio, duty_cycle, clk_loops, V_ov, temperature, delta_Vth = dVth_xls_data.iloc[1:, [2, 3, 4, 5, 6, 7, 8, 9]].T.values\n",
        "# t_stress, t_rec, clk_loops, V_ov, temperature\n",
        "#print((np.vstack((t_stress, t_rec, t_ratio)).T))\n",
        "#print(t_stress)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WeS55towxECg",
        "outputId": "95d851d9-a3d2-46cf-beb5-82284f912002"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.0001 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001\n",
            " 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001\n",
            " 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001\n",
            " 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001\n",
            " 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001\n",
            " 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001\n",
            " 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001\n",
            " 0.0001 0.0001]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def normaliz(target): #Minmax normalization\n",
        "    Min = min(target)\n",
        "    Val = target-Min\n",
        "    Val = Val\n",
        "    Max = max(Val)\n",
        "    if Max == 0:\n",
        "        Norm = 1\n",
        "        Val = target\n",
        "    else:\n",
        "        Norm = 1/Max\n",
        "    return (Norm, Val, Min)\n",
        "\n",
        "(normt_stress, t_stress_1, Mint_stress) = normaliz(t_stress)\n",
        "(normt_rec, t_rec_1, Mint_rec) = normaliz(t_rec)\n",
        "(normclk_loops, clk_loops_1, Minclk_loops) = normaliz(clk_loops)\n",
        "(normV_ov, V_ov_1, MinV_ov) = normaliz(V_ov)\n",
        "(normtemperature, temperature_1, Mintemperature) = normaliz(temperature)\n",
        "(normdelta_Vth, delta_Vth_1, Mindelta_Vth) = normaliz(delta_Vth)\n",
        "\n",
        "T_stress = normt_stress * t_stress_1\n",
        "T_rec = normt_rec * t_rec_1\n",
        "Clk_loops = normclk_loops * clk_loops_1\n",
        "Vov = normV_ov * V_ov_1\n",
        "Temperature = normtemperature * temperature_1\n",
        "Delta_Vth = normdelta_Vth * delta_Vth_1\n",
        "\n",
        "X = np.vstack((T_stress.astype(float), T_rec.astype(float), Clk_loops.astype(float), Vov.astype(float), Temperature.astype(float))).T\n",
        "Y = Delta_Vth.astype(float)"
      ],
      "metadata": {
        "id": "GIMnZZ9yy7jz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from re import L\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import random\n",
        "\n",
        "from torch import optim\n",
        "from torch.utils import data\n",
        "from torch.optim import lr_scheduler\n",
        "from torchvision import datasets, models, transforms\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import statistics\n",
        "import datetime\n",
        "import os\n",
        "import csv\n",
        "import math\n",
        "import time\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_tensor = torch.tensor(X, dtype=torch.float32)\n",
        "Y_tensor = torch.tensor(Y, dtype=torch.float32)\n",
        "x_train, x_test, y_train, y_test = train_test_split(X_tensor, Y_tensor, test_size=0.1, random_state=41)\n",
        "dataset = TensorDataset(x_train, y_train)\n",
        "dataloader = DataLoader(dataset, batch_size = 16)\n",
        "testdataloader = DataLoader(TensorDataset(x_test, y_test))\n",
        "\n",
        "n1 = 20\n",
        "n2 = 10\n",
        "\n",
        "# Define the neural network class\n",
        "class MLP(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MLP, self).__init__()\n",
        "        self.fc1 = torch.nn.Linear(5, n1)\n",
        "        self.fc2 = torch.nn.Linear(n1, n2)\n",
        "        self.fc3 = torch.nn.Linear(n2, 1)\n",
        "        self.dropout = torch.nn.Dropout(0.1)\n",
        "        self.tanh = torch.nn.Tanh()\n",
        "        self.bn1 = torch.nn.BatchNorm1d(n1)\n",
        "        self.bn2 = torch.nn.BatchNorm1d(n2)\n",
        "        self.bn3 = torch.nn.BatchNorm1d(1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        #x = self.bn1(x)\n",
        "        x = self.tanh(x)\n",
        "        #x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        #x = self.bn2(x)\n",
        "        x = self.tanh(x)\n",
        "        #x = self.dropout(x)\n",
        "        x = self.fc3(x)\n",
        "        #x = self.bn3(x)\n",
        "        return x\n",
        "\n",
        "# Create an instance of the MLP class\n",
        "model = MLP()\n",
        "\n",
        "def initialize_weights(m):\n",
        "    if isinstance(m, torch.nn.Linear):\n",
        "        torch.nn.init.xavier_uniform_(m.weight)\n",
        "        if m.bias is not None:\n",
        "            torch.nn.init.constant_(m.bias, 0)\n",
        "\n",
        "model.apply(initialize_weights)\n",
        "\n",
        "loss_function = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5)\n",
        "torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "\n",
        "nb_epochs = 1000\n",
        "MLoss = []\n",
        "for epoch in range(0, nb_epochs):\n",
        "\n",
        "    current_loss = 0.0\n",
        "    losses = []\n",
        "    # Iterate over the dataloader for training data\n",
        "    for i, data in enumerate(dataloader, 0):\n",
        "        inputs, targets = data\n",
        "        inputs, targets = inputs.float(), targets.float()\n",
        "        targets = targets.reshape((targets.shape[0],1))\n",
        "        #zero the gradients\n",
        "        optimizer.zero_grad()\n",
        "        #perform forward pass\n",
        "        outputs = model(inputs)\n",
        "        L_weight = 3\n",
        "        #compute loss\n",
        "        batch_loss = []\n",
        "        for j in range(inputs.size(0)):\n",
        "            input_j = inputs[j].reshape((1, inputs.shape[1]))\n",
        "            if input_j[0,0]>0.3:\n",
        "                batch_loss.append(L_weight*loss_function(outputs[j], targets[j]))\n",
        "            else:\n",
        "                batch_loss.append(loss_function(outputs[j], targets[j]))\n",
        "        loss = torch.stack(batch_loss).mean()\n",
        "        losses.append(loss.item())\n",
        "        #perform backward pass\n",
        "        loss.backward()\n",
        "        #perform optimization\n",
        "        optimizer.step()\n",
        "        # Print statistics\n",
        "\n",
        "    mean_loss = sum(losses)/len(losses)\n",
        "    scheduler.step(mean_loss)\n",
        "\n",
        "    print('Loss (epoch: %4d): %.8f' %(epoch+1, mean_loss))\n",
        "    current_loss = 0.0\n",
        "    MLoss.append(mean_loss)\n",
        "\n",
        "# Process is complete.\n",
        "print('Training process has finished.')\n",
        "\n",
        "torch.save(model, 'IWO_idvg.pt')\n",
        "torch.save(model.state_dict(), 'IWO_idvg_state_dict.pt')\n",
        "\n",
        "####### loss vs. epoch #######\n",
        "xloss = list(range(0, nb_epochs))\n",
        "plt.plot(xloss, np.log10(MLoss))\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "gLtzD7eS3ICV",
        "outputId": "b5f7505e-c9e3-42ce-8e3c-e24658195837"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss (epoch:    1): 0.16867169\n",
            "Loss (epoch:    2): 0.12973921\n",
            "Loss (epoch:    3): 0.10878845\n",
            "Loss (epoch:    4): 0.08936301\n",
            "Loss (epoch:    5): 0.07109062\n",
            "Loss (epoch:    6): 0.05691054\n",
            "Loss (epoch:    7): 0.04599139\n",
            "Loss (epoch:    8): 0.03651176\n",
            "Loss (epoch:    9): 0.02852925\n",
            "Loss (epoch:   10): 0.02248033\n",
            "Loss (epoch:   11): 0.01789047\n",
            "Loss (epoch:   12): 0.01433745\n",
            "Loss (epoch:   13): 0.01178624\n",
            "Loss (epoch:   14): 0.01000870\n",
            "Loss (epoch:   15): 0.00875676\n",
            "Loss (epoch:   16): 0.00797980\n",
            "Loss (epoch:   17): 0.00755480\n",
            "Loss (epoch:   18): 0.00730753\n",
            "Loss (epoch:   19): 0.00715258\n",
            "Loss (epoch:   20): 0.00704218\n",
            "Loss (epoch:   21): 0.00696066\n",
            "Loss (epoch:   22): 0.00690879\n",
            "Loss (epoch:   23): 0.00686380\n",
            "Loss (epoch:   24): 0.00680661\n",
            "Loss (epoch:   25): 0.00673823\n",
            "Loss (epoch:   26): 0.00666894\n",
            "Loss (epoch:   27): 0.00660832\n",
            "Loss (epoch:   28): 0.00655537\n",
            "Loss (epoch:   29): 0.00650345\n",
            "Loss (epoch:   30): 0.00645011\n",
            "Loss (epoch:   31): 0.00639768\n",
            "Loss (epoch:   32): 0.00634906\n",
            "Loss (epoch:   33): 0.00630389\n",
            "Loss (epoch:   34): 0.00625958\n",
            "Loss (epoch:   35): 0.00621477\n",
            "Loss (epoch:   36): 0.00617025\n",
            "Loss (epoch:   37): 0.00612735\n",
            "Loss (epoch:   38): 0.00608613\n",
            "Loss (epoch:   39): 0.00604575\n",
            "Loss (epoch:   40): 0.00600575\n",
            "Loss (epoch:   41): 0.00596647\n",
            "Loss (epoch:   42): 0.00592839\n",
            "Loss (epoch:   43): 0.00589149\n",
            "Loss (epoch:   44): 0.00585544\n",
            "Loss (epoch:   45): 0.00582007\n",
            "Loss (epoch:   46): 0.00578554\n",
            "Loss (epoch:   47): 0.00575199\n",
            "Loss (epoch:   48): 0.00571939\n",
            "Loss (epoch:   49): 0.00568757\n",
            "Loss (epoch:   50): 0.00565652\n",
            "Loss (epoch:   51): 0.00562629\n",
            "Loss (epoch:   52): 0.00559690\n",
            "Loss (epoch:   53): 0.00556832\n",
            "Loss (epoch:   54): 0.00554049\n",
            "Loss (epoch:   55): 0.00551340\n",
            "Loss (epoch:   56): 0.00548707\n",
            "Loss (epoch:   57): 0.00546148\n",
            "Loss (epoch:   58): 0.00543663\n",
            "Loss (epoch:   59): 0.00541247\n",
            "Loss (epoch:   60): 0.00538900\n",
            "Loss (epoch:   61): 0.00536623\n",
            "Loss (epoch:   62): 0.00534413\n",
            "Loss (epoch:   63): 0.00532269\n",
            "Loss (epoch:   64): 0.00530190\n",
            "Loss (epoch:   65): 0.00528173\n",
            "Loss (epoch:   66): 0.00526220\n",
            "Loss (epoch:   67): 0.00524327\n",
            "Loss (epoch:   68): 0.00522494\n",
            "Loss (epoch:   69): 0.00520719\n",
            "Loss (epoch:   70): 0.00519002\n",
            "Loss (epoch:   71): 0.00517340\n",
            "Loss (epoch:   72): 0.00515733\n",
            "Loss (epoch:   73): 0.00514180\n",
            "Loss (epoch:   74): 0.00512678\n",
            "Loss (epoch:   75): 0.00511228\n",
            "Loss (epoch:   76): 0.00509827\n",
            "Loss (epoch:   77): 0.00508475\n",
            "Loss (epoch:   78): 0.00507170\n",
            "Loss (epoch:   79): 0.00505911\n",
            "Loss (epoch:   80): 0.00504697\n",
            "Loss (epoch:   81): 0.00503527\n",
            "Loss (epoch:   82): 0.00502399\n",
            "Loss (epoch:   83): 0.00501313\n",
            "Loss (epoch:   84): 0.00500267\n",
            "Loss (epoch:   85): 0.00499260\n",
            "Loss (epoch:   86): 0.00498290\n",
            "Loss (epoch:   87): 0.00497357\n",
            "Loss (epoch:   88): 0.00496460\n",
            "Loss (epoch:   89): 0.00495599\n",
            "Loss (epoch:   90): 0.00494769\n",
            "Loss (epoch:   91): 0.00493973\n",
            "Loss (epoch:   92): 0.00493208\n",
            "Loss (epoch:   93): 0.00492474\n",
            "Loss (epoch:   94): 0.00491768\n",
            "Loss (epoch:   95): 0.00491091\n",
            "Loss (epoch:   96): 0.00490442\n",
            "Loss (epoch:   97): 0.00489819\n",
            "Loss (epoch:   98): 0.00489221\n",
            "Loss (epoch:   99): 0.00488648\n",
            "Loss (epoch:  100): 0.00488099\n",
            "Loss (epoch:  101): 0.00487572\n",
            "Loss (epoch:  102): 0.00487067\n",
            "Loss (epoch:  103): 0.00486584\n",
            "Loss (epoch:  104): 0.00486120\n",
            "Loss (epoch:  105): 0.00485676\n",
            "Loss (epoch:  106): 0.00485251\n",
            "Loss (epoch:  107): 0.00484844\n",
            "Loss (epoch:  108): 0.00484453\n",
            "Loss (epoch:  109): 0.00484080\n",
            "Loss (epoch:  110): 0.00483722\n",
            "Loss (epoch:  111): 0.00483379\n",
            "Loss (epoch:  112): 0.00483050\n",
            "Loss (epoch:  113): 0.00482736\n",
            "Loss (epoch:  114): 0.00482434\n",
            "Loss (epoch:  115): 0.00482146\n",
            "Loss (epoch:  116): 0.00481869\n",
            "Loss (epoch:  117): 0.00481603\n",
            "Loss (epoch:  118): 0.00481349\n",
            "Loss (epoch:  119): 0.00481105\n",
            "Loss (epoch:  120): 0.00480871\n",
            "Loss (epoch:  121): 0.00480647\n",
            "Loss (epoch:  122): 0.00480431\n",
            "Loss (epoch:  123): 0.00480224\n",
            "Loss (epoch:  124): 0.00480026\n",
            "Loss (epoch:  125): 0.00479835\n",
            "Loss (epoch:  126): 0.00479651\n",
            "Loss (epoch:  127): 0.00479475\n",
            "Loss (epoch:  128): 0.00479305\n",
            "Loss (epoch:  129): 0.00479140\n",
            "Loss (epoch:  130): 0.00478983\n",
            "Loss (epoch:  131): 0.00478831\n",
            "Loss (epoch:  132): 0.00478684\n",
            "Loss (epoch:  133): 0.00478542\n",
            "Loss (epoch:  134): 0.00478405\n",
            "Loss (epoch:  135): 0.00478272\n",
            "Loss (epoch:  136): 0.00478145\n",
            "Loss (epoch:  137): 0.00478021\n",
            "Loss (epoch:  138): 0.00477900\n",
            "Loss (epoch:  139): 0.00477784\n",
            "Loss (epoch:  140): 0.00477671\n",
            "Loss (epoch:  141): 0.00477561\n",
            "Loss (epoch:  142): 0.00477454\n",
            "Loss (epoch:  143): 0.00477350\n",
            "Loss (epoch:  144): 0.00477249\n",
            "Loss (epoch:  145): 0.00477150\n",
            "Loss (epoch:  146): 0.00477054\n",
            "Loss (epoch:  147): 0.00476960\n",
            "Loss (epoch:  148): 0.00476868\n",
            "Loss (epoch:  149): 0.00476778\n",
            "Loss (epoch:  150): 0.00476691\n",
            "Loss (epoch:  151): 0.00476605\n",
            "Loss (epoch:  152): 0.00476520\n",
            "Loss (epoch:  153): 0.00476438\n",
            "Loss (epoch:  154): 0.00476356\n",
            "Loss (epoch:  155): 0.00476277\n",
            "Loss (epoch:  156): 0.00476198\n",
            "Loss (epoch:  157): 0.00476121\n",
            "Loss (epoch:  158): 0.00476045\n",
            "Loss (epoch:  159): 0.00475970\n",
            "Loss (epoch:  160): 0.00475897\n",
            "Loss (epoch:  161): 0.00475824\n",
            "Loss (epoch:  162): 0.00475752\n",
            "Loss (epoch:  163): 0.00475681\n",
            "Loss (epoch:  164): 0.00475611\n",
            "Loss (epoch:  165): 0.00475542\n",
            "Loss (epoch:  166): 0.00475474\n",
            "Loss (epoch:  167): 0.00475406\n",
            "Loss (epoch:  168): 0.00475339\n",
            "Loss (epoch:  169): 0.00475272\n",
            "Loss (epoch:  170): 0.00475206\n",
            "Loss (epoch:  171): 0.00475141\n",
            "Loss (epoch:  172): 0.00475076\n",
            "Loss (epoch:  173): 0.00475012\n",
            "Loss (epoch:  174): 0.00474949\n",
            "Loss (epoch:  175): 0.00474885\n",
            "Loss (epoch:  176): 0.00474823\n",
            "Loss (epoch:  177): 0.00474760\n",
            "Loss (epoch:  178): 0.00474698\n",
            "Loss (epoch:  179): 0.00474637\n",
            "Loss (epoch:  180): 0.00474576\n",
            "Loss (epoch:  181): 0.00474515\n",
            "Loss (epoch:  182): 0.00474454\n",
            "Loss (epoch:  183): 0.00474394\n",
            "Loss (epoch:  184): 0.00474334\n",
            "Loss (epoch:  185): 0.00474275\n",
            "Loss (epoch:  186): 0.00474216\n",
            "Loss (epoch:  187): 0.00474157\n",
            "Loss (epoch:  188): 0.00474098\n",
            "Loss (epoch:  189): 0.00474040\n",
            "Loss (epoch:  190): 0.00473981\n",
            "Loss (epoch:  191): 0.00473923\n",
            "Loss (epoch:  192): 0.00473866\n",
            "Loss (epoch:  193): 0.00473808\n",
            "Loss (epoch:  194): 0.00473751\n",
            "Loss (epoch:  195): 0.00473694\n",
            "Loss (epoch:  196): 0.00473637\n",
            "Loss (epoch:  197): 0.00473581\n",
            "Loss (epoch:  198): 0.00473524\n",
            "Loss (epoch:  199): 0.00473468\n",
            "Loss (epoch:  200): 0.00473412\n",
            "Loss (epoch:  201): 0.00473356\n",
            "Loss (epoch:  202): 0.00473300\n",
            "Loss (epoch:  203): 0.00473245\n",
            "Loss (epoch:  204): 0.00473190\n",
            "Loss (epoch:  205): 0.00473135\n",
            "Loss (epoch:  206): 0.00473080\n",
            "Loss (epoch:  207): 0.00473025\n",
            "Loss (epoch:  208): 0.00472970\n",
            "Loss (epoch:  209): 0.00472916\n",
            "Loss (epoch:  210): 0.00472861\n",
            "Loss (epoch:  211): 0.00472807\n",
            "Loss (epoch:  212): 0.00472753\n",
            "Loss (epoch:  213): 0.00472700\n",
            "Loss (epoch:  214): 0.00472646\n",
            "Loss (epoch:  215): 0.00472592\n",
            "Loss (epoch:  216): 0.00472539\n",
            "Loss (epoch:  217): 0.00472486\n",
            "Loss (epoch:  218): 0.00472433\n",
            "Loss (epoch:  219): 0.00472380\n",
            "Loss (epoch:  220): 0.00472327\n",
            "Loss (epoch:  221): 0.00472275\n",
            "Loss (epoch:  222): 0.00472222\n",
            "Loss (epoch:  223): 0.00472169\n",
            "Loss (epoch:  224): 0.00472117\n",
            "Loss (epoch:  225): 0.00472066\n",
            "Loss (epoch:  226): 0.00472014\n",
            "Loss (epoch:  227): 0.00471962\n",
            "Loss (epoch:  228): 0.00471910\n",
            "Loss (epoch:  229): 0.00471859\n",
            "Loss (epoch:  230): 0.00471808\n",
            "Loss (epoch:  231): 0.00471756\n",
            "Loss (epoch:  232): 0.00471705\n",
            "Loss (epoch:  233): 0.00471654\n",
            "Loss (epoch:  234): 0.00471604\n",
            "Loss (epoch:  235): 0.00471553\n",
            "Loss (epoch:  236): 0.00471502\n",
            "Loss (epoch:  237): 0.00471452\n",
            "Loss (epoch:  238): 0.00471402\n",
            "Loss (epoch:  239): 0.00471352\n",
            "Loss (epoch:  240): 0.00471302\n",
            "Loss (epoch:  241): 0.00471252\n",
            "Loss (epoch:  242): 0.00471203\n",
            "Loss (epoch:  243): 0.00471153\n",
            "Loss (epoch:  244): 0.00471104\n",
            "Loss (epoch:  245): 0.00471054\n",
            "Loss (epoch:  246): 0.00471005\n",
            "Loss (epoch:  247): 0.00470956\n",
            "Loss (epoch:  248): 0.00470908\n",
            "Loss (epoch:  249): 0.00470859\n",
            "Loss (epoch:  250): 0.00470810\n",
            "Loss (epoch:  251): 0.00470762\n",
            "Loss (epoch:  252): 0.00470713\n",
            "Loss (epoch:  253): 0.00470665\n",
            "Loss (epoch:  254): 0.00470617\n",
            "Loss (epoch:  255): 0.00470569\n",
            "Loss (epoch:  256): 0.00470522\n",
            "Loss (epoch:  257): 0.00470474\n",
            "Loss (epoch:  258): 0.00470427\n",
            "Loss (epoch:  259): 0.00470379\n",
            "Loss (epoch:  260): 0.00470332\n",
            "Loss (epoch:  261): 0.00470285\n",
            "Loss (epoch:  262): 0.00470238\n",
            "Loss (epoch:  263): 0.00470191\n",
            "Loss (epoch:  264): 0.00470144\n",
            "Loss (epoch:  265): 0.00470098\n",
            "Loss (epoch:  266): 0.00470051\n",
            "Loss (epoch:  267): 0.00470005\n",
            "Loss (epoch:  268): 0.00469959\n",
            "Loss (epoch:  269): 0.00469913\n",
            "Loss (epoch:  270): 0.00469867\n",
            "Loss (epoch:  271): 0.00469822\n",
            "Loss (epoch:  272): 0.00469776\n",
            "Loss (epoch:  273): 0.00469731\n",
            "Loss (epoch:  274): 0.00469685\n",
            "Loss (epoch:  275): 0.00469640\n",
            "Loss (epoch:  276): 0.00469595\n",
            "Loss (epoch:  277): 0.00469550\n",
            "Loss (epoch:  278): 0.00469505\n",
            "Loss (epoch:  279): 0.00469461\n",
            "Loss (epoch:  280): 0.00469416\n",
            "Loss (epoch:  281): 0.00469372\n",
            "Loss (epoch:  282): 0.00469328\n",
            "Loss (epoch:  283): 0.00469284\n",
            "Loss (epoch:  284): 0.00469240\n",
            "Loss (epoch:  285): 0.00469196\n",
            "Loss (epoch:  286): 0.00469152\n",
            "Loss (epoch:  287): 0.00469108\n",
            "Loss (epoch:  288): 0.00469065\n",
            "Loss (epoch:  289): 0.00469022\n",
            "Loss (epoch:  290): 0.00468979\n",
            "Loss (epoch:  291): 0.00468936\n",
            "Loss (epoch:  292): 0.00468893\n",
            "Loss (epoch:  293): 0.00468850\n",
            "Loss (epoch:  294): 0.00468808\n",
            "Loss (epoch:  295): 0.00468765\n",
            "Loss (epoch:  296): 0.00468723\n",
            "Loss (epoch:  297): 0.00468681\n",
            "Loss (epoch:  298): 0.00468638\n",
            "Loss (epoch:  299): 0.00468596\n",
            "Loss (epoch:  300): 0.00468555\n",
            "Loss (epoch:  301): 0.00468513\n",
            "Loss (epoch:  302): 0.00468471\n",
            "Loss (epoch:  303): 0.00468430\n",
            "Loss (epoch:  304): 0.00468388\n",
            "Loss (epoch:  305): 0.00468347\n",
            "Loss (epoch:  306): 0.00468306\n",
            "Loss (epoch:  307): 0.00468265\n",
            "Loss (epoch:  308): 0.00468224\n",
            "Loss (epoch:  309): 0.00468184\n",
            "Loss (epoch:  310): 0.00468143\n",
            "Loss (epoch:  311): 0.00468103\n",
            "Loss (epoch:  312): 0.00468063\n",
            "Loss (epoch:  313): 0.00468023\n",
            "Loss (epoch:  314): 0.00467983\n",
            "Loss (epoch:  315): 0.00467943\n",
            "Loss (epoch:  316): 0.00467903\n",
            "Loss (epoch:  317): 0.00467863\n",
            "Loss (epoch:  318): 0.00467824\n",
            "Loss (epoch:  319): 0.00467784\n",
            "Loss (epoch:  320): 0.00467745\n",
            "Loss (epoch:  321): 0.00467706\n",
            "Loss (epoch:  322): 0.00467667\n",
            "Loss (epoch:  323): 0.00467628\n",
            "Loss (epoch:  324): 0.00467590\n",
            "Loss (epoch:  325): 0.00467551\n",
            "Loss (epoch:  326): 0.00467512\n",
            "Loss (epoch:  327): 0.00467474\n",
            "Loss (epoch:  328): 0.00467436\n",
            "Loss (epoch:  329): 0.00467397\n",
            "Loss (epoch:  330): 0.00467360\n",
            "Loss (epoch:  331): 0.00467322\n",
            "Loss (epoch:  332): 0.00467284\n",
            "Loss (epoch:  333): 0.00467246\n",
            "Loss (epoch:  334): 0.00467209\n",
            "Loss (epoch:  335): 0.00467172\n",
            "Loss (epoch:  336): 0.00467134\n",
            "Loss (epoch:  337): 0.00467097\n",
            "Loss (epoch:  338): 0.00467060\n",
            "Loss (epoch:  339): 0.00467023\n",
            "Loss (epoch:  340): 0.00466986\n",
            "Loss (epoch:  341): 0.00466950\n",
            "Loss (epoch:  342): 0.00466913\n",
            "Loss (epoch:  343): 0.00466876\n",
            "Loss (epoch:  344): 0.00466840\n",
            "Loss (epoch:  345): 0.00466804\n",
            "Loss (epoch:  346): 0.00466768\n",
            "Loss (epoch:  347): 0.00466732\n",
            "Loss (epoch:  348): 0.00466696\n",
            "Loss (epoch:  349): 0.00466660\n",
            "Loss (epoch:  350): 0.00466624\n",
            "Loss (epoch:  351): 0.00466589\n",
            "Loss (epoch:  352): 0.00466553\n",
            "Loss (epoch:  353): 0.00466518\n",
            "Loss (epoch:  354): 0.00466483\n",
            "Loss (epoch:  355): 0.00466447\n",
            "Loss (epoch:  356): 0.00466412\n",
            "Loss (epoch:  357): 0.00466377\n",
            "Loss (epoch:  358): 0.00466342\n",
            "Loss (epoch:  359): 0.00466308\n",
            "Loss (epoch:  360): 0.00466273\n",
            "Loss (epoch:  361): 0.00466239\n",
            "Loss (epoch:  362): 0.00466204\n",
            "Loss (epoch:  363): 0.00466170\n",
            "Loss (epoch:  364): 0.00466135\n",
            "Loss (epoch:  365): 0.00466102\n",
            "Loss (epoch:  366): 0.00466067\n",
            "Loss (epoch:  367): 0.00466033\n",
            "Loss (epoch:  368): 0.00466000\n",
            "Loss (epoch:  369): 0.00465966\n",
            "Loss (epoch:  370): 0.00465932\n",
            "Loss (epoch:  371): 0.00465899\n",
            "Loss (epoch:  372): 0.00465865\n",
            "Loss (epoch:  373): 0.00465832\n",
            "Loss (epoch:  374): 0.00465799\n",
            "Loss (epoch:  375): 0.00465766\n",
            "Loss (epoch:  376): 0.00465732\n",
            "Loss (epoch:  377): 0.00465699\n",
            "Loss (epoch:  378): 0.00465667\n",
            "Loss (epoch:  379): 0.00465634\n",
            "Loss (epoch:  380): 0.00465601\n",
            "Loss (epoch:  381): 0.00465568\n",
            "Loss (epoch:  382): 0.00465536\n",
            "Loss (epoch:  383): 0.00465503\n",
            "Loss (epoch:  384): 0.00465471\n",
            "Loss (epoch:  385): 0.00465438\n",
            "Loss (epoch:  386): 0.00465406\n",
            "Loss (epoch:  387): 0.00465374\n",
            "Loss (epoch:  388): 0.00465342\n",
            "Loss (epoch:  389): 0.00465310\n",
            "Loss (epoch:  390): 0.00465278\n",
            "Loss (epoch:  391): 0.00465246\n",
            "Loss (epoch:  392): 0.00465214\n",
            "Loss (epoch:  393): 0.00465183\n",
            "Loss (epoch:  394): 0.00465151\n",
            "Loss (epoch:  395): 0.00465120\n",
            "Loss (epoch:  396): 0.00465088\n",
            "Loss (epoch:  397): 0.00465057\n",
            "Loss (epoch:  398): 0.00465025\n",
            "Loss (epoch:  399): 0.00464994\n",
            "Loss (epoch:  400): 0.00464963\n",
            "Loss (epoch:  401): 0.00464932\n",
            "Loss (epoch:  402): 0.00464901\n",
            "Loss (epoch:  403): 0.00464869\n",
            "Loss (epoch:  404): 0.00464839\n",
            "Loss (epoch:  405): 0.00464808\n",
            "Loss (epoch:  406): 0.00464777\n",
            "Loss (epoch:  407): 0.00464746\n",
            "Loss (epoch:  408): 0.00464716\n",
            "Loss (epoch:  409): 0.00464685\n",
            "Loss (epoch:  410): 0.00464654\n",
            "Loss (epoch:  411): 0.00464624\n",
            "Loss (epoch:  412): 0.00464593\n",
            "Loss (epoch:  413): 0.00464563\n",
            "Loss (epoch:  414): 0.00464533\n",
            "Loss (epoch:  415): 0.00464503\n",
            "Loss (epoch:  416): 0.00464472\n",
            "Loss (epoch:  417): 0.00464442\n",
            "Loss (epoch:  418): 0.00464412\n",
            "Loss (epoch:  419): 0.00464382\n",
            "Loss (epoch:  420): 0.00464352\n",
            "Loss (epoch:  421): 0.00464322\n",
            "Loss (epoch:  422): 0.00464292\n",
            "Loss (epoch:  423): 0.00464262\n",
            "Loss (epoch:  424): 0.00464232\n",
            "Loss (epoch:  425): 0.00464202\n",
            "Loss (epoch:  426): 0.00464173\n",
            "Loss (epoch:  427): 0.00464143\n",
            "Loss (epoch:  428): 0.00464114\n",
            "Loss (epoch:  429): 0.00464084\n",
            "Loss (epoch:  430): 0.00464054\n",
            "Loss (epoch:  431): 0.00464025\n",
            "Loss (epoch:  432): 0.00463995\n",
            "Loss (epoch:  433): 0.00463966\n",
            "Loss (epoch:  434): 0.00463936\n",
            "Loss (epoch:  435): 0.00463907\n",
            "Loss (epoch:  436): 0.00463878\n",
            "Loss (epoch:  437): 0.00463848\n",
            "Loss (epoch:  438): 0.00463819\n",
            "Loss (epoch:  439): 0.00463790\n",
            "Loss (epoch:  440): 0.00463761\n",
            "Loss (epoch:  441): 0.00463731\n",
            "Loss (epoch:  442): 0.00463702\n",
            "Loss (epoch:  443): 0.00463673\n",
            "Loss (epoch:  444): 0.00463644\n",
            "Loss (epoch:  445): 0.00463615\n",
            "Loss (epoch:  446): 0.00463586\n",
            "Loss (epoch:  447): 0.00463557\n",
            "Loss (epoch:  448): 0.00463528\n",
            "Loss (epoch:  449): 0.00463499\n",
            "Loss (epoch:  450): 0.00463470\n",
            "Loss (epoch:  451): 0.00463441\n",
            "Loss (epoch:  452): 0.00463412\n",
            "Loss (epoch:  453): 0.00463383\n",
            "Loss (epoch:  454): 0.00463354\n",
            "Loss (epoch:  455): 0.00463325\n",
            "Loss (epoch:  456): 0.00463296\n",
            "Loss (epoch:  457): 0.00463267\n",
            "Loss (epoch:  458): 0.00463238\n",
            "Loss (epoch:  459): 0.00463210\n",
            "Loss (epoch:  460): 0.00463181\n",
            "Loss (epoch:  461): 0.00463152\n",
            "Loss (epoch:  462): 0.00463123\n",
            "Loss (epoch:  463): 0.00463094\n",
            "Loss (epoch:  464): 0.00463065\n",
            "Loss (epoch:  465): 0.00463037\n",
            "Loss (epoch:  466): 0.00463008\n",
            "Loss (epoch:  467): 0.00462979\n",
            "Loss (epoch:  468): 0.00462951\n",
            "Loss (epoch:  469): 0.00462922\n",
            "Loss (epoch:  470): 0.00462893\n",
            "Loss (epoch:  471): 0.00462865\n",
            "Loss (epoch:  472): 0.00462836\n",
            "Loss (epoch:  473): 0.00462807\n",
            "Loss (epoch:  474): 0.00462778\n",
            "Loss (epoch:  475): 0.00462749\n",
            "Loss (epoch:  476): 0.00462721\n",
            "Loss (epoch:  477): 0.00462692\n",
            "Loss (epoch:  478): 0.00462663\n",
            "Loss (epoch:  479): 0.00462634\n",
            "Loss (epoch:  480): 0.00462606\n",
            "Loss (epoch:  481): 0.00462577\n",
            "Loss (epoch:  482): 0.00462548\n",
            "Loss (epoch:  483): 0.00462519\n",
            "Loss (epoch:  484): 0.00462491\n",
            "Loss (epoch:  485): 0.00462462\n",
            "Loss (epoch:  486): 0.00462433\n",
            "Loss (epoch:  487): 0.00462404\n",
            "Loss (epoch:  488): 0.00462375\n",
            "Loss (epoch:  489): 0.00462347\n",
            "Loss (epoch:  490): 0.00462317\n",
            "Loss (epoch:  491): 0.00462289\n",
            "Loss (epoch:  492): 0.00462260\n",
            "Loss (epoch:  493): 0.00462231\n",
            "Loss (epoch:  494): 0.00462202\n",
            "Loss (epoch:  495): 0.00462173\n",
            "Loss (epoch:  496): 0.00462144\n",
            "Loss (epoch:  497): 0.00462115\n",
            "Loss (epoch:  498): 0.00462086\n",
            "Loss (epoch:  499): 0.00462058\n",
            "Loss (epoch:  500): 0.00462028\n",
            "Loss (epoch:  501): 0.00462000\n",
            "Loss (epoch:  502): 0.00461970\n",
            "Loss (epoch:  503): 0.00461941\n",
            "Loss (epoch:  504): 0.00461912\n",
            "Loss (epoch:  505): 0.00461883\n",
            "Loss (epoch:  506): 0.00461854\n",
            "Loss (epoch:  507): 0.00461825\n",
            "Loss (epoch:  508): 0.00461795\n",
            "Loss (epoch:  509): 0.00461766\n",
            "Loss (epoch:  510): 0.00461737\n",
            "Loss (epoch:  511): 0.00461708\n",
            "Loss (epoch:  512): 0.00461678\n",
            "Loss (epoch:  513): 0.00461649\n",
            "Loss (epoch:  514): 0.00461620\n",
            "Loss (epoch:  515): 0.00461590\n",
            "Loss (epoch:  516): 0.00461561\n",
            "Loss (epoch:  517): 0.00461531\n",
            "Loss (epoch:  518): 0.00461502\n",
            "Loss (epoch:  519): 0.00461472\n",
            "Loss (epoch:  520): 0.00461443\n",
            "Loss (epoch:  521): 0.00461413\n",
            "Loss (epoch:  522): 0.00461383\n",
            "Loss (epoch:  523): 0.00461354\n",
            "Loss (epoch:  524): 0.00461324\n",
            "Loss (epoch:  525): 0.00461294\n",
            "Loss (epoch:  526): 0.00461264\n",
            "Loss (epoch:  527): 0.00461235\n",
            "Loss (epoch:  528): 0.00461205\n",
            "Loss (epoch:  529): 0.00461175\n",
            "Loss (epoch:  530): 0.00461145\n",
            "Loss (epoch:  531): 0.00461115\n",
            "Loss (epoch:  532): 0.00461085\n",
            "Loss (epoch:  533): 0.00461055\n",
            "Loss (epoch:  534): 0.00461025\n",
            "Loss (epoch:  535): 0.00460994\n",
            "Loss (epoch:  536): 0.00460964\n",
            "Loss (epoch:  537): 0.00460934\n",
            "Loss (epoch:  538): 0.00460904\n",
            "Loss (epoch:  539): 0.00460874\n",
            "Loss (epoch:  540): 0.00460843\n",
            "Loss (epoch:  541): 0.00460812\n",
            "Loss (epoch:  542): 0.00460783\n",
            "Loss (epoch:  543): 0.00460752\n",
            "Loss (epoch:  544): 0.00460721\n",
            "Loss (epoch:  545): 0.00460691\n",
            "Loss (epoch:  546): 0.00460660\n",
            "Loss (epoch:  547): 0.00460629\n",
            "Loss (epoch:  548): 0.00460598\n",
            "Loss (epoch:  549): 0.00460568\n",
            "Loss (epoch:  550): 0.00460537\n",
            "Loss (epoch:  551): 0.00460506\n",
            "Loss (epoch:  552): 0.00460475\n",
            "Loss (epoch:  553): 0.00460444\n",
            "Loss (epoch:  554): 0.00460413\n",
            "Loss (epoch:  555): 0.00460382\n",
            "Loss (epoch:  556): 0.00460351\n",
            "Loss (epoch:  557): 0.00460319\n",
            "Loss (epoch:  558): 0.00460288\n",
            "Loss (epoch:  559): 0.00460257\n",
            "Loss (epoch:  560): 0.00460225\n",
            "Loss (epoch:  561): 0.00460194\n",
            "Loss (epoch:  562): 0.00460162\n",
            "Loss (epoch:  563): 0.00460131\n",
            "Loss (epoch:  564): 0.00460099\n",
            "Loss (epoch:  565): 0.00460068\n",
            "Loss (epoch:  566): 0.00460036\n",
            "Loss (epoch:  567): 0.00460004\n",
            "Loss (epoch:  568): 0.00459972\n",
            "Loss (epoch:  569): 0.00459940\n",
            "Loss (epoch:  570): 0.00459909\n",
            "Loss (epoch:  571): 0.00459876\n",
            "Loss (epoch:  572): 0.00459844\n",
            "Loss (epoch:  573): 0.00459812\n",
            "Loss (epoch:  574): 0.00459780\n",
            "Loss (epoch:  575): 0.00459748\n",
            "Loss (epoch:  576): 0.00459715\n",
            "Loss (epoch:  577): 0.00459683\n",
            "Loss (epoch:  578): 0.00459651\n",
            "Loss (epoch:  579): 0.00459618\n",
            "Loss (epoch:  580): 0.00459585\n",
            "Loss (epoch:  581): 0.00459553\n",
            "Loss (epoch:  582): 0.00459520\n",
            "Loss (epoch:  583): 0.00459487\n",
            "Loss (epoch:  584): 0.00459454\n",
            "Loss (epoch:  585): 0.00459421\n",
            "Loss (epoch:  586): 0.00459388\n",
            "Loss (epoch:  587): 0.00459356\n",
            "Loss (epoch:  588): 0.00459322\n",
            "Loss (epoch:  589): 0.00459289\n",
            "Loss (epoch:  590): 0.00459256\n",
            "Loss (epoch:  591): 0.00459223\n",
            "Loss (epoch:  592): 0.00459189\n",
            "Loss (epoch:  593): 0.00459156\n",
            "Loss (epoch:  594): 0.00459122\n",
            "Loss (epoch:  595): 0.00459089\n",
            "Loss (epoch:  596): 0.00459055\n",
            "Loss (epoch:  597): 0.00459022\n",
            "Loss (epoch:  598): 0.00458988\n",
            "Loss (epoch:  599): 0.00458954\n",
            "Loss (epoch:  600): 0.00458920\n",
            "Loss (epoch:  601): 0.00458886\n",
            "Loss (epoch:  602): 0.00458852\n",
            "Loss (epoch:  603): 0.00458818\n",
            "Loss (epoch:  604): 0.00458784\n",
            "Loss (epoch:  605): 0.00458749\n",
            "Loss (epoch:  606): 0.00458715\n",
            "Loss (epoch:  607): 0.00458680\n",
            "Loss (epoch:  608): 0.00458646\n",
            "Loss (epoch:  609): 0.00458612\n",
            "Loss (epoch:  610): 0.00458577\n",
            "Loss (epoch:  611): 0.00458542\n",
            "Loss (epoch:  612): 0.00458507\n",
            "Loss (epoch:  613): 0.00458472\n",
            "Loss (epoch:  614): 0.00458438\n",
            "Loss (epoch:  615): 0.00458402\n",
            "Loss (epoch:  616): 0.00458368\n",
            "Loss (epoch:  617): 0.00458332\n",
            "Loss (epoch:  618): 0.00458297\n",
            "Loss (epoch:  619): 0.00458262\n",
            "Loss (epoch:  620): 0.00458226\n",
            "Loss (epoch:  621): 0.00458191\n",
            "Loss (epoch:  622): 0.00458156\n",
            "Loss (epoch:  623): 0.00458120\n",
            "Loss (epoch:  624): 0.00458084\n",
            "Loss (epoch:  625): 0.00458048\n",
            "Loss (epoch:  626): 0.00458013\n",
            "Loss (epoch:  627): 0.00457977\n",
            "Loss (epoch:  628): 0.00457941\n",
            "Loss (epoch:  629): 0.00457905\n",
            "Loss (epoch:  630): 0.00457868\n",
            "Loss (epoch:  631): 0.00457832\n",
            "Loss (epoch:  632): 0.00457796\n",
            "Loss (epoch:  633): 0.00457760\n",
            "Loss (epoch:  634): 0.00457723\n",
            "Loss (epoch:  635): 0.00457687\n",
            "Loss (epoch:  636): 0.00457650\n",
            "Loss (epoch:  637): 0.00457614\n",
            "Loss (epoch:  638): 0.00457577\n",
            "Loss (epoch:  639): 0.00457540\n",
            "Loss (epoch:  640): 0.00457503\n",
            "Loss (epoch:  641): 0.00457466\n",
            "Loss (epoch:  642): 0.00457429\n",
            "Loss (epoch:  643): 0.00457392\n",
            "Loss (epoch:  644): 0.00457355\n",
            "Loss (epoch:  645): 0.00457317\n",
            "Loss (epoch:  646): 0.00457280\n",
            "Loss (epoch:  647): 0.00457243\n",
            "Loss (epoch:  648): 0.00457205\n",
            "Loss (epoch:  649): 0.00457168\n",
            "Loss (epoch:  650): 0.00457130\n",
            "Loss (epoch:  651): 0.00457092\n",
            "Loss (epoch:  652): 0.00457054\n",
            "Loss (epoch:  653): 0.00457016\n",
            "Loss (epoch:  654): 0.00456978\n",
            "Loss (epoch:  655): 0.00456940\n",
            "Loss (epoch:  656): 0.00456902\n",
            "Loss (epoch:  657): 0.00456864\n",
            "Loss (epoch:  658): 0.00456826\n",
            "Loss (epoch:  659): 0.00456787\n",
            "Loss (epoch:  660): 0.00456749\n",
            "Loss (epoch:  661): 0.00456710\n",
            "Loss (epoch:  662): 0.00456672\n",
            "Loss (epoch:  663): 0.00456633\n",
            "Loss (epoch:  664): 0.00456594\n",
            "Loss (epoch:  665): 0.00456555\n",
            "Loss (epoch:  666): 0.00456516\n",
            "Loss (epoch:  667): 0.00456477\n",
            "Loss (epoch:  668): 0.00456438\n",
            "Loss (epoch:  669): 0.00456399\n",
            "Loss (epoch:  670): 0.00456359\n",
            "Loss (epoch:  671): 0.00456320\n",
            "Loss (epoch:  672): 0.00456281\n",
            "Loss (epoch:  673): 0.00456241\n",
            "Loss (epoch:  674): 0.00456202\n",
            "Loss (epoch:  675): 0.00456162\n",
            "Loss (epoch:  676): 0.00456122\n",
            "Loss (epoch:  677): 0.00456082\n",
            "Loss (epoch:  678): 0.00456042\n",
            "Loss (epoch:  679): 0.00456003\n",
            "Loss (epoch:  680): 0.00455962\n",
            "Loss (epoch:  681): 0.00455923\n",
            "Loss (epoch:  682): 0.00455882\n",
            "Loss (epoch:  683): 0.00455841\n",
            "Loss (epoch:  684): 0.00455802\n",
            "Loss (epoch:  685): 0.00455761\n",
            "Loss (epoch:  686): 0.00455720\n",
            "Loss (epoch:  687): 0.00455680\n",
            "Loss (epoch:  688): 0.00455639\n",
            "Loss (epoch:  689): 0.00455598\n",
            "Loss (epoch:  690): 0.00455557\n",
            "Loss (epoch:  691): 0.00455517\n",
            "Loss (epoch:  692): 0.00455476\n",
            "Loss (epoch:  693): 0.00455434\n",
            "Loss (epoch:  694): 0.00455394\n",
            "Loss (epoch:  695): 0.00455352\n",
            "Loss (epoch:  696): 0.00455311\n",
            "Loss (epoch:  697): 0.00455270\n",
            "Loss (epoch:  698): 0.00455228\n",
            "Loss (epoch:  699): 0.00455186\n",
            "Loss (epoch:  700): 0.00455145\n",
            "Loss (epoch:  701): 0.00455103\n",
            "Loss (epoch:  702): 0.00455061\n",
            "Loss (epoch:  703): 0.00455019\n",
            "Loss (epoch:  704): 0.00454978\n",
            "Loss (epoch:  705): 0.00454936\n",
            "Loss (epoch:  706): 0.00454893\n",
            "Loss (epoch:  707): 0.00454851\n",
            "Loss (epoch:  708): 0.00454809\n",
            "Loss (epoch:  709): 0.00454767\n",
            "Loss (epoch:  710): 0.00454724\n",
            "Loss (epoch:  711): 0.00454682\n",
            "Loss (epoch:  712): 0.00454639\n",
            "Loss (epoch:  713): 0.00454597\n",
            "Loss (epoch:  714): 0.00454554\n",
            "Loss (epoch:  715): 0.00454511\n",
            "Loss (epoch:  716): 0.00454468\n",
            "Loss (epoch:  717): 0.00454425\n",
            "Loss (epoch:  718): 0.00454383\n",
            "Loss (epoch:  719): 0.00454340\n",
            "Loss (epoch:  720): 0.00454296\n",
            "Loss (epoch:  721): 0.00454253\n",
            "Loss (epoch:  722): 0.00454210\n",
            "Loss (epoch:  723): 0.00454166\n",
            "Loss (epoch:  724): 0.00454124\n",
            "Loss (epoch:  725): 0.00454080\n",
            "Loss (epoch:  726): 0.00454037\n",
            "Loss (epoch:  727): 0.00453993\n",
            "Loss (epoch:  728): 0.00453949\n",
            "Loss (epoch:  729): 0.00453905\n",
            "Loss (epoch:  730): 0.00453862\n",
            "Loss (epoch:  731): 0.00453818\n",
            "Loss (epoch:  732): 0.00453773\n",
            "Loss (epoch:  733): 0.00453730\n",
            "Loss (epoch:  734): 0.00453686\n",
            "Loss (epoch:  735): 0.00453641\n",
            "Loss (epoch:  736): 0.00453597\n",
            "Loss (epoch:  737): 0.00453553\n",
            "Loss (epoch:  738): 0.00453508\n",
            "Loss (epoch:  739): 0.00453464\n",
            "Loss (epoch:  740): 0.00453419\n",
            "Loss (epoch:  741): 0.00453375\n",
            "Loss (epoch:  742): 0.00453330\n",
            "Loss (epoch:  743): 0.00453285\n",
            "Loss (epoch:  744): 0.00453241\n",
            "Loss (epoch:  745): 0.00453196\n",
            "Loss (epoch:  746): 0.00453151\n",
            "Loss (epoch:  747): 0.00453106\n",
            "Loss (epoch:  748): 0.00453061\n",
            "Loss (epoch:  749): 0.00453016\n",
            "Loss (epoch:  750): 0.00452971\n",
            "Loss (epoch:  751): 0.00452926\n",
            "Loss (epoch:  752): 0.00452881\n",
            "Loss (epoch:  753): 0.00452835\n",
            "Loss (epoch:  754): 0.00452790\n",
            "Loss (epoch:  755): 0.00452745\n",
            "Loss (epoch:  756): 0.00452699\n",
            "Loss (epoch:  757): 0.00452653\n",
            "Loss (epoch:  758): 0.00452608\n",
            "Loss (epoch:  759): 0.00452562\n",
            "Loss (epoch:  760): 0.00452517\n",
            "Loss (epoch:  761): 0.00452471\n",
            "Loss (epoch:  762): 0.00452425\n",
            "Loss (epoch:  763): 0.00452379\n",
            "Loss (epoch:  764): 0.00452333\n",
            "Loss (epoch:  765): 0.00452287\n",
            "Loss (epoch:  766): 0.00452241\n",
            "Loss (epoch:  767): 0.00452195\n",
            "Loss (epoch:  768): 0.00452149\n",
            "Loss (epoch:  769): 0.00452102\n",
            "Loss (epoch:  770): 0.00452056\n",
            "Loss (epoch:  771): 0.00452010\n",
            "Loss (epoch:  772): 0.00451964\n",
            "Loss (epoch:  773): 0.00451917\n",
            "Loss (epoch:  774): 0.00451871\n",
            "Loss (epoch:  775): 0.00451824\n",
            "Loss (epoch:  776): 0.00451777\n",
            "Loss (epoch:  777): 0.00451731\n",
            "Loss (epoch:  778): 0.00451685\n",
            "Loss (epoch:  779): 0.00451638\n",
            "Loss (epoch:  780): 0.00451591\n",
            "Loss (epoch:  781): 0.00451544\n",
            "Loss (epoch:  782): 0.00451497\n",
            "Loss (epoch:  783): 0.00451450\n",
            "Loss (epoch:  784): 0.00451404\n",
            "Loss (epoch:  785): 0.00451357\n",
            "Loss (epoch:  786): 0.00451310\n",
            "Loss (epoch:  787): 0.00451263\n",
            "Loss (epoch:  788): 0.00451216\n",
            "Loss (epoch:  789): 0.00451169\n",
            "Loss (epoch:  790): 0.00451121\n",
            "Loss (epoch:  791): 0.00451074\n",
            "Loss (epoch:  792): 0.00451027\n",
            "Loss (epoch:  793): 0.00450980\n",
            "Loss (epoch:  794): 0.00450933\n",
            "Loss (epoch:  795): 0.00450885\n",
            "Loss (epoch:  796): 0.00450838\n",
            "Loss (epoch:  797): 0.00450790\n",
            "Loss (epoch:  798): 0.00450743\n",
            "Loss (epoch:  799): 0.00450695\n",
            "Loss (epoch:  800): 0.00450648\n",
            "Loss (epoch:  801): 0.00450601\n",
            "Loss (epoch:  802): 0.00450553\n",
            "Loss (epoch:  803): 0.00450505\n",
            "Loss (epoch:  804): 0.00450458\n",
            "Loss (epoch:  805): 0.00450410\n",
            "Loss (epoch:  806): 0.00450362\n",
            "Loss (epoch:  807): 0.00450315\n",
            "Loss (epoch:  808): 0.00450267\n",
            "Loss (epoch:  809): 0.00450219\n",
            "Loss (epoch:  810): 0.00450171\n",
            "Loss (epoch:  811): 0.00450124\n",
            "Loss (epoch:  812): 0.00450076\n",
            "Loss (epoch:  813): 0.00450028\n",
            "Loss (epoch:  814): 0.00449980\n",
            "Loss (epoch:  815): 0.00449932\n",
            "Loss (epoch:  816): 0.00449885\n",
            "Loss (epoch:  817): 0.00449837\n",
            "Loss (epoch:  818): 0.00449789\n",
            "Loss (epoch:  819): 0.00449741\n",
            "Loss (epoch:  820): 0.00449693\n",
            "Loss (epoch:  821): 0.00449645\n",
            "Loss (epoch:  822): 0.00449597\n",
            "Loss (epoch:  823): 0.00449549\n",
            "Loss (epoch:  824): 0.00449501\n",
            "Loss (epoch:  825): 0.00449453\n",
            "Loss (epoch:  826): 0.00449405\n",
            "Loss (epoch:  827): 0.00449357\n",
            "Loss (epoch:  828): 0.00449309\n",
            "Loss (epoch:  829): 0.00449261\n",
            "Loss (epoch:  830): 0.00449213\n",
            "Loss (epoch:  831): 0.00449165\n",
            "Loss (epoch:  832): 0.00449117\n",
            "Loss (epoch:  833): 0.00449068\n",
            "Loss (epoch:  834): 0.00449021\n",
            "Loss (epoch:  835): 0.00448973\n",
            "Loss (epoch:  836): 0.00448925\n",
            "Loss (epoch:  837): 0.00448877\n",
            "Loss (epoch:  838): 0.00448828\n",
            "Loss (epoch:  839): 0.00448780\n",
            "Loss (epoch:  840): 0.00448732\n",
            "Loss (epoch:  841): 0.00448684\n",
            "Loss (epoch:  842): 0.00448636\n",
            "Loss (epoch:  843): 0.00448588\n",
            "Loss (epoch:  844): 0.00448540\n",
            "Loss (epoch:  845): 0.00448492\n",
            "Loss (epoch:  846): 0.00448444\n",
            "Loss (epoch:  847): 0.00448396\n",
            "Loss (epoch:  848): 0.00448348\n",
            "Loss (epoch:  849): 0.00448300\n",
            "Loss (epoch:  850): 0.00448252\n",
            "Loss (epoch:  851): 0.00448204\n",
            "Loss (epoch:  852): 0.00448156\n",
            "Loss (epoch:  853): 0.00448108\n",
            "Loss (epoch:  854): 0.00448061\n",
            "Loss (epoch:  855): 0.00448012\n",
            "Loss (epoch:  856): 0.00447964\n",
            "Loss (epoch:  857): 0.00447917\n",
            "Loss (epoch:  858): 0.00447869\n",
            "Loss (epoch:  859): 0.00447821\n",
            "Loss (epoch:  860): 0.00447773\n",
            "Loss (epoch:  861): 0.00447726\n",
            "Loss (epoch:  862): 0.00447678\n",
            "Loss (epoch:  863): 0.00447630\n",
            "Loss (epoch:  864): 0.00447582\n",
            "Loss (epoch:  865): 0.00447535\n",
            "Loss (epoch:  866): 0.00447487\n",
            "Loss (epoch:  867): 0.00447440\n",
            "Loss (epoch:  868): 0.00447392\n",
            "Loss (epoch:  869): 0.00447345\n",
            "Loss (epoch:  870): 0.00447297\n",
            "Loss (epoch:  871): 0.00447249\n",
            "Loss (epoch:  872): 0.00447202\n",
            "Loss (epoch:  873): 0.00447154\n",
            "Loss (epoch:  874): 0.00447107\n",
            "Loss (epoch:  875): 0.00447060\n",
            "Loss (epoch:  876): 0.00447013\n",
            "Loss (epoch:  877): 0.00446966\n",
            "Loss (epoch:  878): 0.00446918\n",
            "Loss (epoch:  879): 0.00446871\n",
            "Loss (epoch:  880): 0.00446824\n",
            "Loss (epoch:  881): 0.00446777\n",
            "Loss (epoch:  882): 0.00446730\n",
            "Loss (epoch:  883): 0.00446683\n",
            "Loss (epoch:  884): 0.00446636\n",
            "Loss (epoch:  885): 0.00446589\n",
            "Loss (epoch:  886): 0.00446542\n",
            "Loss (epoch:  887): 0.00446496\n",
            "Loss (epoch:  888): 0.00446449\n",
            "Loss (epoch:  889): 0.00446402\n",
            "Loss (epoch:  890): 0.00446355\n",
            "Loss (epoch:  891): 0.00446309\n",
            "Loss (epoch:  892): 0.00446262\n",
            "Loss (epoch:  893): 0.00446216\n",
            "Loss (epoch:  894): 0.00446170\n",
            "Loss (epoch:  895): 0.00446123\n",
            "Loss (epoch:  896): 0.00446077\n",
            "Loss (epoch:  897): 0.00446031\n",
            "Loss (epoch:  898): 0.00445985\n",
            "Loss (epoch:  899): 0.00445939\n",
            "Loss (epoch:  900): 0.00445893\n",
            "Loss (epoch:  901): 0.00445847\n",
            "Loss (epoch:  902): 0.00445801\n",
            "Loss (epoch:  903): 0.00445755\n",
            "Loss (epoch:  904): 0.00445709\n",
            "Loss (epoch:  905): 0.00445664\n",
            "Loss (epoch:  906): 0.00445618\n",
            "Loss (epoch:  907): 0.00445572\n",
            "Loss (epoch:  908): 0.00445527\n",
            "Loss (epoch:  909): 0.00445482\n",
            "Loss (epoch:  910): 0.00445436\n",
            "Loss (epoch:  911): 0.00445391\n",
            "Loss (epoch:  912): 0.00445346\n",
            "Loss (epoch:  913): 0.00445301\n",
            "Loss (epoch:  914): 0.00445256\n",
            "Loss (epoch:  915): 0.00445211\n",
            "Loss (epoch:  916): 0.00445166\n",
            "Loss (epoch:  917): 0.00445121\n",
            "Loss (epoch:  918): 0.00445077\n",
            "Loss (epoch:  919): 0.00445032\n",
            "Loss (epoch:  920): 0.00444988\n",
            "Loss (epoch:  921): 0.00444943\n",
            "Loss (epoch:  922): 0.00444899\n",
            "Loss (epoch:  923): 0.00444855\n",
            "Loss (epoch:  924): 0.00444810\n",
            "Loss (epoch:  925): 0.00444766\n",
            "Loss (epoch:  926): 0.00444722\n",
            "Loss (epoch:  927): 0.00444679\n",
            "Loss (epoch:  928): 0.00444635\n",
            "Loss (epoch:  929): 0.00444591\n",
            "Loss (epoch:  930): 0.00444547\n",
            "Loss (epoch:  931): 0.00444504\n",
            "Loss (epoch:  932): 0.00444460\n",
            "Loss (epoch:  933): 0.00444417\n",
            "Loss (epoch:  934): 0.00444374\n",
            "Loss (epoch:  935): 0.00444331\n",
            "Loss (epoch:  936): 0.00444288\n",
            "Loss (epoch:  937): 0.00444245\n",
            "Loss (epoch:  938): 0.00444202\n",
            "Loss (epoch:  939): 0.00444159\n",
            "Loss (epoch:  940): 0.00444117\n",
            "Loss (epoch:  941): 0.00444074\n",
            "Loss (epoch:  942): 0.00444032\n",
            "Loss (epoch:  943): 0.00443990\n",
            "Loss (epoch:  944): 0.00443947\n",
            "Loss (epoch:  945): 0.00443905\n",
            "Loss (epoch:  946): 0.00443863\n",
            "Loss (epoch:  947): 0.00443821\n",
            "Loss (epoch:  948): 0.00443780\n",
            "Loss (epoch:  949): 0.00443738\n",
            "Loss (epoch:  950): 0.00443697\n",
            "Loss (epoch:  951): 0.00443655\n",
            "Loss (epoch:  952): 0.00443614\n",
            "Loss (epoch:  953): 0.00443573\n",
            "Loss (epoch:  954): 0.00443531\n",
            "Loss (epoch:  955): 0.00443491\n",
            "Loss (epoch:  956): 0.00443450\n",
            "Loss (epoch:  957): 0.00443409\n",
            "Loss (epoch:  958): 0.00443369\n",
            "Loss (epoch:  959): 0.00443328\n",
            "Loss (epoch:  960): 0.00443287\n",
            "Loss (epoch:  961): 0.00443247\n",
            "Loss (epoch:  962): 0.00443207\n",
            "Loss (epoch:  963): 0.00443167\n",
            "Loss (epoch:  964): 0.00443127\n",
            "Loss (epoch:  965): 0.00443088\n",
            "Loss (epoch:  966): 0.00443048\n",
            "Loss (epoch:  967): 0.00443008\n",
            "Loss (epoch:  968): 0.00442969\n",
            "Loss (epoch:  969): 0.00442930\n",
            "Loss (epoch:  970): 0.00442891\n",
            "Loss (epoch:  971): 0.00442852\n",
            "Loss (epoch:  972): 0.00442813\n",
            "Loss (epoch:  973): 0.00442774\n",
            "Loss (epoch:  974): 0.00442735\n",
            "Loss (epoch:  975): 0.00442697\n",
            "Loss (epoch:  976): 0.00442659\n",
            "Loss (epoch:  977): 0.00442620\n",
            "Loss (epoch:  978): 0.00442582\n",
            "Loss (epoch:  979): 0.00442545\n",
            "Loss (epoch:  980): 0.00442507\n",
            "Loss (epoch:  981): 0.00442469\n",
            "Loss (epoch:  982): 0.00442431\n",
            "Loss (epoch:  983): 0.00442394\n",
            "Loss (epoch:  984): 0.00442357\n",
            "Loss (epoch:  985): 0.00442320\n",
            "Loss (epoch:  986): 0.00442282\n",
            "Loss (epoch:  987): 0.00442245\n",
            "Loss (epoch:  988): 0.00442209\n",
            "Loss (epoch:  989): 0.00442172\n",
            "Loss (epoch:  990): 0.00442136\n",
            "Loss (epoch:  991): 0.00442100\n",
            "Loss (epoch:  992): 0.00442063\n",
            "Loss (epoch:  993): 0.00442027\n",
            "Loss (epoch:  994): 0.00441991\n",
            "Loss (epoch:  995): 0.00441956\n",
            "Loss (epoch:  996): 0.00441920\n",
            "Loss (epoch:  997): 0.00441884\n",
            "Loss (epoch:  998): 0.00441849\n",
            "Loss (epoch:  999): 0.00441814\n",
            "Loss (epoch: 1000): 0.00441779\n",
            "Training process has finished.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "\n",
        "    output = []\n",
        "    # Iterate over the dataloader for training data\n",
        "    for i, data in enumerate(testdataloader, 0):\n",
        "        inputs, targets = data\n",
        "        inputs, targets = inputs.float(), targets.float()\n",
        "        targets = targets.reshape((targets.shape[0],1))\n",
        "\n",
        "        #zero the gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        #perform forward pass\n",
        "        outputs = model(inputs)\n",
        "        output.append(outputs)\n",
        "# Process is complete.\n",
        "#print('Training process has finished.')\n",
        "\n",
        "# Extract the weights and biases from the model\n",
        "weights_1 = model.fc1.weight.detach().numpy()\n",
        "bias_1 = model.fc1.bias.detach().numpy()\n",
        "weights_2 = model.fc2.weight.detach().numpy()\n",
        "bias_2 = model.fc2.bias.detach().numpy()\n",
        "weights_3 = model.fc3.weight.detach().numpy()\n",
        "bias_3 = model.fc3.bias.detach().numpy()\n",
        "\n",
        "def generate_variable_declarations(weights_shape, layer_prefix):\n",
        "    declarations = \"\"\n",
        "    num_neurons = weights_shape  # Number of neurons is determined by the first dimension of the weights matrix\n",
        "    layer_declarations = \", \".join([f\"{layer_prefix}_{i}\" for i in range(num_neurons)]) + \";\"\n",
        "    declarations += layer_declarations\n",
        "    return declarations\n",
        "\n",
        "# Use the function to generate declarations for each layer\n",
        "h1_declarations = generate_variable_declarations(weights_1.shape[0], \"hvth1\")\n",
        "h2_declarations = generate_variable_declarations(weights_2.shape[0], \"hvth2\")\n",
        "\n",
        "verilog_code = \"\"\"\n",
        "//*******************************************************************************\n",
        "//* * School of Electrical and Computer Engineering, Georgia Institute of Technology\n",
        "//* PI: Prof. Shimeng Yu\n",
        "//* All rights reserved.\n",
        "//*\n",
        "//* Copyright of the model is maintained by the developers, and the model is distributed under\n",
        "//* the terms of the Creative Commons Attribution-NonCommercial 4.0 International Public License\n",
        "//* http://creativecommons.org/licenses/by-nc/4.0/legalcode.\n",
        "//*\n",
        "//* THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND\n",
        "//* ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\n",
        "//* WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n",
        "//* DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\n",
        "//* FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\n",
        "//* DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\n",
        "//* SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\n",
        "//* CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\n",
        "//* OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n",
        "//* OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n",
        "//*\n",
        "//* Developer:\n",
        "//*  Gihun Choe gchoe6@gatech.edu\n",
        "//********************************************************************************/\n",
        "\n",
        "`include \"constants.vams\"\n",
        "`include \"disciplines.vams\"\n",
        "\n",
        "\n",
        "module IWO_verliogA(d, g, s);\n",
        "        inout d, g, s;\n",
        "        electrical d, g, s;\n",
        "\n",
        "        //***** parameters L and W ******//\n",
        "        parameter real W = 0.1; //get parameter fom spectre\n",
        "        parameter real L = 0.007; //get parameter fom spectre\n",
        "        parameter real T_stress = 0.0001; //set on cadence as variable\n",
        "        parameter real T_rec = 0.001;     //set on cadence as variable\n",
        "        parameter real Clk_loops = 50;    //set on cadence as variable\n",
        "        parameter real V_ov = 1.7;        //set on cadence as variable\n",
        "        parameter real Temp = 25;  //set on cadence as variable\n",
        "\n",
        "        parameter MinVg = -1.0 ;\n",
        "        parameter normVg = 0.2222222222222222 ;\n",
        "        parameter MinVd = 0.01 ;\n",
        "        parameter normVd = 0.2949852507374631 ;\n",
        "        parameter MinLg = 0.05 ;\n",
        "        parameter normLg = 1.4285714285714286 ;\n",
        "        parameter MinO = 8.15e-15 ;\n",
        "        parameter normO =33613445378151.26;\n",
        "        parameter MinI = -23.98798356587402 ;\n",
        "        parameter normI =0.04615548498417793;\n",
        "\n",
        "        parameter Mint_stress = {} ;\n",
        "        parameter normt_stress = {} ;\n",
        "        parameter Mint_rec = {} ;\n",
        "        parameter normt_rec = {} ;\n",
        "        parameter Minclk_loops = {} ;\n",
        "        parameter normclk_loops = {} ;\n",
        "        parameter MinV_ov = {} ;\n",
        "        parameter normV_ov = {} ;\n",
        "        parameter Mintemperature = {} ;\n",
        "        parameter normtemperature = {} ;\n",
        "        parameter Mindelta_Vth = {} ;\n",
        "        parameter normdelta_Vth = {} ;\n",
        "\n",
        "        real {}\n",
        "        real {}\n",
        "\n",
        "        real Vg, Vd, Vs, Vgs, Vds, Lg, Id, Cgg, Cgsd, Vgd;\n",
        "        real Vgsraw, Vgdraw, dir;\n",
        "        real t_stress, v_ov, t_rec, clk_loops, temp, delta_Vth;\n",
        "\n",
        "        real h1_0, h1_1, h1_2, h1_3, h1_4, h1_5, h1_6, h1_7, h1_8, h1_9, h1_10, h1_11, h1_12, h1_13, h1_14, h1_15, h1_16, h1_17, h1_18, h1_19, h1_20, h1_21, h1_22, h1_23, h1_24;\n",
        "        real h2_0, h2_1, h2_2, h2_3, h2_4, h2_5, h2_6, h2_7, h2_8, h2_9, h2_10, h2_11, h2_12, h2_13, h2_14, h2_15, h2_16, y;\n",
        "        real hc1_0, hc1_1, hc1_2, hc1_3, hc1_4, hc1_5, hc1_6, hc1_7, hc1_8, hc1_9;\n",
        "        real hc1_10, hc1_11, hc1_12, hc1_13, hc1_14, hc1_15, hc1_16;\n",
        "        real hc2_0, hc2_1, hc2_2, hc2_3, hc2_4, hc2_5, hc2_6, hc2_7, hc2_8, hc2_9;\n",
        "        real hc2_10, hc2_11, hc2_12, hc2_13, hc2_14, hc2_15, hc2_16, yc, yvth;\n",
        "\n",
        "analog begin\n",
        "\n",
        "t_stress = T_stress;\n",
        "v_ov = V_ov;\n",
        "t_rec = (T_rec - Mint_rec)*normt_rec ;\n",
        "clk_loops = (Clk_loops - Minclk_loops)*normclk_loops ;\n",
        "temp = (Temp - Mintemperature)*normtemperature ;\n",
        "\n",
        "//******************** delta_Vth NN **********************************//\n",
        "\n",
        "\"\"\".format(Mint_stress, normt_stress, Mint_rec, normt_rec, Minclk_loops, normclk_loops, MinV_ov, normV_ov, Mintemperature, normtemperature, Mindelta_Vth, normdelta_Vth, h1_declarations, h2_declarations)\n",
        "# V_ov = (V_ov - MinV_ov)*normV_ov ;\n",
        "# t_stress = (T_stress - Mint_stress)*normt_stress ;\n",
        "\n",
        "# Create the Verilog-A code for the 1st hidden layer\n",
        "for i in range(n1):\n",
        "    inputs = [\"t_stress\", \"t_rec\", \"clk_loops\", \"v_ov\", \"temp\"]\n",
        "    inputs = [\"*\".join([str(weights_1[i][j]), inp]) for j, inp in enumerate(inputs)]\n",
        "    inputs = \"+\".join(inputs)\n",
        "    inputs = \"+\".join([inputs, str(bias_1[i])])\n",
        "    verilog_code += \"hvth1_{} = tanh({});\\n\".format(i, inputs)\n",
        "verilog_code += \"\\n\"\n",
        "\n",
        "# Create the Verilog-A code for the 2nd hidden layer\n",
        "for i in range(n2):\n",
        "    inputs = [\"h1_{}\".format(j) for j in range(n1)]\n",
        "    inputs = [\"*\".join([str(weights_2[i][j]), inp]) for j, inp in enumerate(inputs)]\n",
        "    inputs = \"+\".join(inputs)\n",
        "    inputs = \"+\".join([inputs, str(bias_2[i])])\n",
        "    verilog_code += \"hvth2_{} = tanh({});\\n\".format(i, inputs)\n",
        "verilog_code += \"\\n\"\n",
        "\n",
        "# Create the Verilog-A code for the output layer\n",
        "inputs = [\"hvth2_{}\".format(i) for i in range(n2)]\n",
        "inputs = [\"*\".join([str(weights_3[0][i]), inp]) for i, inp in enumerate(inputs)]\n",
        "inputs = \"+\".join(inputs)\n",
        "inputs = \"+\".join([inputs, str(bias_3[0])])\n",
        "verilog_code += \"yvth = {};\\n\\n\".format(inputs)\n",
        "verilog_code += \"delta_Vth = (yvth - Mindelta_Vth) * normdelta_Vth;\"\n",
        "verilog_code += \"\"\"\n",
        "\n",
        "\n",
        "        Vg = V(g) ;\n",
        "        Vs = V(s) ;\n",
        "        Vd = V(d) ;\n",
        "        Vgsraw = Vg-Vs ;\n",
        "        Vgdraw = Vg-Vd ;\n",
        "\n",
        "if (Vgsraw >= Vgdraw) begin\n",
        "        Vgs = ((Vg-Vs) - MinVg) * normVg ;\n",
        "        dir = 1;\n",
        "end\n",
        "\n",
        "else begin\n",
        "        Vgs = ((Vg-Vd) - MinVg) * normVg ;\n",
        "        dir = -1;\n",
        "end\n",
        "\n",
        "        Vds = (abs(Vd-Vs) - MinVd) * normVd ;\n",
        "        Vgs = Vgs - delta_Vth ; // BTI Vth variation\n",
        "        Lg = (L -MinLg)*normLg ;\n",
        "\n",
        "\n",
        "\n",
        "//******************** C-V NN **********************************//\n",
        "hc1_0 = tanh(-0.99871427*Vgs+-0.16952373*Vds+0.32118186*Lg+0.41485423);\n",
        "hc1_1 = tanh(0.31587568*Vgs+0.13397887*Vds+-0.4541538*Lg+-0.3630942);\n",
        "hc1_2 = tanh(-0.76281804*Vgs+0.09352969*Vds+1.1961353*Lg+0.3904977);\n",
        "hc1_3 = tanh(-1.115087*Vgs+0.85752946*Vds+-0.11746484*Lg+0.5500279);\n",
        "hc1_4 = tanh(1.0741386*Vgs+0.82041687*Vds+0.19092631*Lg+-0.4009425);\n",
        "hc1_5 = tanh(-0.47921795*Vgs+-0.8749933*Vds+-0.054768667*Lg+0.62785167);\n",
        "hc1_6 = tanh(0.5449184*Vgs+-4.409165*Vds+-0.072947875*Lg+-0.31324536);\n",
        "hc1_7 = tanh(-2.9224303*Vgs+2.7675478*Vds+0.08862238*Lg+0.6493558);\n",
        "hc1_8 = tanh(0.65050656*Vgs+-0.29751927*Vds+0.1571876*Lg+-0.38088372);\n",
        "hc1_9 = tanh(-0.30384183*Vgs+0.5649165*Vds+2.6806898*Lg+0.3197917);\n",
        "hc1_10 = tanh(-0.095988505*Vgs+2.0158541*Vds+-0.42972717*Lg+-0.30388466);\n",
        "hc1_11 = tanh(6.7699738*Vgs+-0.07234483*Vds+-0.013545353*Lg+-1.3694142);\n",
        "hc1_12 = tanh(-0.3404029*Vgs+0.0443459*Vds+0.89597*Lg+0.069993004);\n",
        "hc1_13 = tanh(0.62300175*Vgs+-0.29515797*Vds+1.6753465*Lg+-0.6520838);\n",
        "hc1_14 = tanh(0.37957156*Vgs+0.2237372*Vds+0.08591952*Lg+0.13126835);\n",
        "hc1_15 = tanh(0.19949242*Vgs+-0.26481664*Vds+-0.41059187*Lg+-0.40832308);\n",
        "hc1_16 = tanh(0.98966587*Vgs+-0.24259183*Vds+0.36584845*Lg+-0.8024042);\n",
        "\n",
        "hc2_0 = tanh(-0.91327864*hc1_0+0.4696781*hc1_1+0.3302202*hc1_2+0.11393868*hc1_3+0.45070222*hc1_4+-0.2894044*hc1_5+0.55066*hc1_6+-1.6242687*hc1_7+-0.38140613*hc1_8+0.032771554*hc1_9+0.17647126);\n",
        "hc2_1 = tanh(-1.1663305*hc1_0+-0.523984*hc1_1+-0.90804136*hc1_2+-0.7418044*hc1_3+1.456171*hc1_4+-0.16802542*hc1_5+0.8235596*hc1_6+-2.2246246*hc1_7+-0.40805355*hc1_8+0.7207601*hc1_9+0.4729169);\n",
        "hc2_2 = tanh(-0.69065374*hc1_0+0.40205315*hc1_1+-0.49410668*hc1_2+0.8681325*hc1_3+0.471351*hc1_4+0.46939445*hc1_5+0.45568785*hc1_6+-0.92935294*hc1_7+-0.8209646*hc1_8+0.1158967*hc1_9+-0.075798474);\n",
        "hc2_3 = tanh(0.11535446*hc1_0+-0.06296927*hc1_1+-0.6740435*hc1_2+0.7428315*hc1_3+0.05890677*hc1_4+0.9579441*hc1_5+-0.037319*hc1_6+-0.18491426*hc1_7+-0.02981994*hc1_8+0.038347963*hc1_9+0.039531134);\n",
        "hc2_4 = tanh(0.37817463*hc1_0+-0.6811279*hc1_1+1.1388369*hc1_2+0.19983096*hc1_3+-0.20415118*hc1_4+1.3022176*hc1_5+0.22571652*hc1_6+0.1690611*hc1_7+-0.56475276*hc1_8+-0.4069731*hc1_9+0.99962974);\n",
        "hc2_5 = tanh(0.032873478*hc1_0+-0.05209407*hc1_1+-0.010839908*hc1_2+-0.13892579*hc1_3+-0.050480977*hc1_4+0.0127089145*hc1_5+0.0052771433*hc1_6+0.02029242*hc1_7+-0.08705659*hc1_8+0.0254766*hc1_9+0.025135752);\n",
        "hc2_6 = tanh(-0.34639204*hc1_0+0.06937975*hc1_1+0.18671949*hc1_2+-0.18912783*hc1_3+0.1312504*hc1_4+0.4627272*hc1_5+-0.42590702*hc1_6+-0.10966313*hc1_7+0.66083515*hc1_8+-0.050718334*hc1_9+0.08234678);\n",
        "hc2_7 = tanh(0.90656275*hc1_0+-0.037281644*hc1_1+0.77237594*hc1_2+1.4710428*hc1_3+0.13597831*hc1_4+-0.059844542*hc1_5+-0.7801535*hc1_6+3.7814677*hc1_7+-0.5976644*hc1_8+0.2721995*hc1_9+0.023777716);\n",
        "hc2_8 = tanh(0.39720625*hc1_0+-0.45262313*hc1_1+0.19873238*hc1_2+0.9750888*hc1_3+-0.9427992*hc1_4+0.4487432*hc1_5+-0.3372945*hc1_6+0.33729544*hc1_7+-0.1667088*hc1_8+-0.5707525*hc1_9+0.27954483);\n",
        "hc2_9 = tanh(0.28551984*hc1_0+-0.68350387*hc1_1+0.9916423*hc1_2+-0.8254094*hc1_3+0.09875706*hc1_4+0.47609732*hc1_5+-0.058662917*hc1_6+0.09181381*hc1_7+0.09592329*hc1_8+1.3940467*hc1_9+0.3865768);\n",
        "hc2_10 = tanh(0.098737516*hc1_0+0.060473576*hc1_1+0.42824662*hc1_2+0.15018038*hc1_3+0.082621895*hc1_4+-0.00019039502*hc1_5+-0.3321634*hc1_6+0.7936295*hc1_7+-0.041197542*hc1_8+0.6530619*hc1_9+0.1338804);\n",
        "hc2_11 = tanh(-0.3585284*hc1_0+-0.09956017*hc1_1+0.17224246*hc1_2+-0.016925728*hc1_3+-0.46462816*hc1_4+-0.5649022*hc1_5+1.251695*hc1_6+-0.4303161*hc1_7+0.48546878*hc1_8+0.22958975*hc1_9+-0.27899802);\n",
        "hc2_12 = tanh(0.8565631*hc1_0+-0.7622999*hc1_1+0.32367912*hc1_2+1.4776785*hc1_3+0.2712369*hc1_4+0.2275511*hc1_5+0.39908803*hc1_6+4.2305493*hc1_7+-0.3467536*hc1_8+0.41231114*hc1_9+0.47123823);\n",
        "hc2_13 = tanh(-0.26411077*hc1_0+-0.17583853*hc1_1+0.045439184*hc1_2+-0.13801138*hc1_3+0.03278085*hc1_4+-0.45625108*hc1_5+-0.17905861*hc1_6+0.3060186*hc1_7+-0.3361926*hc1_8+-0.050055273*hc1_9+0.17996444);\n",
        "hc2_14 = tanh(0.016094366*hc1_0+-0.013196736*hc1_1+0.4124856*hc1_2+0.22926371*hc1_3+-0.35071182*hc1_4+-0.34217188*hc1_5+-0.69466996*hc1_6+1.0563152*hc1_7+-0.18019852*hc1_8+0.061871335*hc1_9+0.09762555);\n",
        "hc2_15 = tanh(-0.18970782*hc1_0+0.3624813*hc1_1+-1.3419824*hc1_2+0.103635244*hc1_3+-0.14595217*hc1_4+-0.1899393*hc1_5+0.176524*hc1_6+-0.4428012*hc1_7+-0.39544868*hc1_8+-0.45783517*hc1_9+0.1884755);\n",
        "hc2_16 = tanh(-0.1585831*hc1_0+0.035894837*hc1_1+-0.14261873*hc1_2+0.25914294*hc1_3+0.040607046*hc1_4+0.11555795*hc1_5+0.0022548323*hc1_6+-0.002149359*hc1_7+0.07067297*hc1_8+0.019578662*hc1_9+0.16657573);\n",
        "yc = 0.17503543*hc2_0+-0.15556754*hc2_1+-0.125455*hc2_2+-0.07502612*hc2_3+-0.16671115*hc2_4+0.29854503;\n",
        "\n",
        "Cgg = (yc / normO + MinO)*W;\n",
        "Cgsd = Cgg/2 ;\n",
        "\n",
        "//******************** I-V NN **********************************//\n",
        "h1_0 = tanh(0.4313499*Vgs+0.39184842*Vds+-0.28709298*Lg+0.20619453);\n",
        "h1_1 = tanh(0.530215*Vgs+-0.276051*Vds+0.3802653*Lg+-0.49267054);\n",
        "h1_2 = tanh(-0.33458906*Vgs+-0.38640633*Vds+-0.11866613*Lg+-0.14151593);\n",
        "h1_3 = tanh(-0.50232273*Vgs+-0.41781282*Vds+0.040914852*Lg+0.40752855);\n",
        "h1_4 = tanh(-0.2738816*Vgs+0.15826645*Vds+0.3726163*Lg+0.43347502);\n",
        "h1_5 = tanh(-0.89140266*Vgs+0.19422686*Vds+0.25828648*Lg+0.029750101);\n",
        "h1_6 = tanh(0.8155016*Vgs+-0.768934*Vds+-0.27971813*Lg+-0.29921618);\n",
        "h1_7 = tanh(-0.27932712*Vgs+1.3111871*Vds+0.053733792*Lg+0.032400735);\n",
        "h1_8 = tanh(0.0949421*Vgs+0.25981736*Vds+-0.30941275*Lg+-0.011010548);\n",
        "h1_9 = tanh(-1.2044673*Vgs+0.20209628*Vds+0.09035153*Lg+-0.23299748);\n",
        "h1_10 = tanh(0.17326604*Vgs+0.4927804*Vds+-0.37197837*Lg+0.48652422);\n",
        "h1_11 = tanh(0.048797052*Vgs+-0.86918294*Vds+0.049685556*Lg+-0.5611843);\n",
        "h1_12 = tanh(-0.44728053*Vgs+0.4416018*Vds+0.43623027*Lg+-0.39887246);\n",
        "h1_13 = tanh(0.02768353*Vgs+0.013126534*Vds+0.45135143*Lg+-0.05909225);\n",
        "h1_14 = tanh(0.4728052*Vgs+-0.35722205*Vds+-0.41991743*Lg+-0.1526044);\n",
        "h1_15 = tanh(-0.5687236*Vgs+0.07312218*Vds+-0.33128312*Lg+0.45431995);\n",
        "h1_16 = tanh(1.9482948*Vgs+-0.52973956*Vds+-0.15129352*Lg+-0.11858447);\n",
        "h1_17 = tanh(0.48852378*Vgs+0.21598034*Vds+0.24090554*Lg+-0.16491982);\n",
        "h1_18 = tanh(-0.3446666*Vgs+-1.177434*Vds+-0.28390405*Lg+-0.038540863);\n",
        "h1_19 = tanh(-2.3029919*Vgs+0.49877584*Vds+-0.22563547*Lg+-0.27407128);\n",
        "h1_20 = tanh(-0.732954*Vgs+0.23922421*Vds+0.075503394*Lg+-0.5665179);\n",
        "h1_21 = tanh(-0.17605503*Vgs+-0.07067414*Vds+-0.18359897*Lg+0.15867902);\n",
        "h1_22 = tanh(0.7302054*Vgs+-0.25757748*Vds+0.33953145*Lg+0.35818258);\n",
        "h1_23 = tanh(-0.43780586*Vgs+0.19977026*Vds+-0.17486832*Lg+-0.5687275);\n",
        "h1_24 = tanh(-1.3637807*Vgs+-0.15194818*Vds+-0.044956356*Lg+0.3245934);\n",
        "\n",
        "h2_0 = tanh(0.14783205*h1_0+-0.02431098*h1_1+-0.09715118*h1_2+-0.2474231*h1_3+0.32432914*h1_4+1.521693*h1_5+-0.10406853*h1_6+0.29371944*h1_7+-0.13358027*h1_8+1.0691675*h1_9+-0.19404113*h1_10+0.10453362*h1_11+1.6107924*h1_12+0.1920968*h1_13+-0.14785244*h1_14+-0.028951993*h1_15+-1.1547928*h1_16+-0.6376489*h1_17+-0.021565331*h1_18+2.606584*h1_19+0.97442293*h1_20+0.034297176*h1_21+-0.50504786*h1_22+0.8020577*h1_23+0.41248432*h1_24+-0.06995442);\n",
        "h2_1 = tanh(0.30681616*h1_0+0.36377275*h1_1+-0.046106532*h1_2+-0.2534532*h1_3+-0.037327003*h1_4+-0.35939386*h1_5+-0.071234025*h1_6+-0.9586675*h1_7+-0.06324141*h1_8+-0.2969973*h1_9+-0.18527432*h1_10+0.44116643*h1_11+-0.23221572*h1_12+-0.37151667*h1_13+0.15607928*h1_14+-0.6138509*h1_15+0.20915431*h1_16+0.66309977*h1_17+0.36671144*h1_18+-0.527578*h1_19+-0.10497286*h1_20+-0.19303346*h1_21+-0.14065515*h1_22+-0.13561434*h1_23+-0.4676026*h1_24+-0.14758746);\n",
        "h2_2 = tanh(-0.1950115*h1_0+-0.10721866*h1_1+0.23511808*h1_2+-0.13570116*h1_3+0.24990053*h1_4+0.25373626*h1_5+0.268683*h1_6+0.14307122*h1_7+0.055941034*h1_8+-0.113093115*h1_9+0.27305043*h1_10+0.23472951*h1_11+-0.020495743*h1_12+-0.30221954*h1_13+-0.15749171*h1_14+-0.3461544*h1_15+0.11443546*h1_16+-0.16509052*h1_17+-0.139441*h1_18+-0.06326206*h1_19+0.17607576*h1_20+-0.4709045*h1_21+0.2544463*h1_22+0.10170589*h1_23+0.29272288*h1_24+0.12412181);\n",
        "h2_3 = tanh(0.0003776102*h1_0+0.6797598*h1_1+-0.1299236*h1_2+-0.70903736*h1_3+0.028675593*h1_4+-0.38929847*h1_5+0.668139*h1_6+-0.09140271*h1_7+-0.16577661*h1_8+-0.0372596*h1_9+-0.06973977*h1_10+0.33586663*h1_11+0.17698736*h1_12+0.12853792*h1_13+0.75535774*h1_14+-0.7189986*h1_15+0.48737425*h1_16+0.5964495*h1_17+-0.30607405*h1_18+-0.012350458*h1_19+0.12677537*h1_20+-0.1537175*h1_21+0.15927425*h1_22+0.23570293*h1_23+-0.5400177*h1_24+-0.10571702);\n",
        "h2_4 = tanh(0.31527975*h1_0+-0.029269403*h1_1+-0.055212982*h1_2+-0.062310033*h1_3+0.08560037*h1_4+-0.72141176*h1_5+0.2020669*h1_6+0.025841974*h1_7+0.5646926*h1_8+-0.30292585*h1_9+0.00063811685*h1_10+-0.40466273*h1_11+-0.073278934*h1_12+0.34705442*h1_13+-0.05348122*h1_14+0.3436614*h1_15+0.04067239*h1_16+0.784844*h1_17+-0.28573394*h1_18+-0.88371974*h1_19+-0.26338458*h1_20+-0.013300428*h1_21+0.13801576*h1_22+-0.46081108*h1_23+-0.7637864*h1_24+0.12703106);\n",
        "h2_5 = tanh(-0.24617016*h1_0+0.26517266*h1_1+-0.21811938*h1_2+-0.2553703*h1_3+0.14267647*h1_4+0.4159367*h1_5+0.28696465*h1_6+-0.07311365*h1_7+0.2970628*h1_8+0.17417435*h1_9+0.18269496*h1_10+0.24361488*h1_11+0.18268564*h1_12+0.36019236*h1_13+-0.29943472*h1_14+0.39711836*h1_15+0.37918466*h1_16+0.064785175*h1_17+0.26349717*h1_18+-0.0071219024*h1_19+0.22553043*h1_20+0.17283064*h1_21+0.11927425*h1_22+0.10645739*h1_23+-0.34909987*h1_24+-0.15780911);\n",
        "h2_6 = tanh(0.19807929*h1_0+0.15743051*h1_1+-0.20436394*h1_2+-0.5772219*h1_3+0.0328607*h1_4+-0.3125246*h1_5+0.2093223*h1_6+-0.06814246*h1_7+0.10145798*h1_8+-0.3017911*h1_9+0.27700877*h1_10+-0.140296*h1_11+0.40272304*h1_12+-0.14553767*h1_13+0.42105338*h1_14+0.12200454*h1_15+0.19187498*h1_16+0.73878366*h1_17+-0.4901547*h1_18+-0.33965933*h1_19+0.17176971*h1_20+0.08609857*h1_21+0.045344174*h1_22+0.2393916*h1_23+0.17985432*h1_24+0.064894326);\n",
        "h2_7 = tanh(0.2932835*h1_0+0.075287476*h1_1+0.19162141*h1_2+0.2823231*h1_3+0.4070056*h1_4+-0.004358482*h1_5+-0.16040435*h1_6+-0.13784154*h1_7+0.19380908*h1_8+-0.22312845*h1_9+0.3159539*h1_10+0.11714408*h1_11+0.119634695*h1_12+0.32135013*h1_13+0.22685558*h1_14+-0.14774461*h1_15+-0.017703133*h1_16+0.17009398*h1_17+-0.29736042*h1_18+0.14056131*h1_19+0.0013223822*h1_20+0.1552641*h1_21+-0.38140768*h1_22+-0.2736542*h1_23+0.16030753*h1_24+-0.0041591506);\n",
        "h2_8 = tanh(0.008927864*h1_0+-0.43404117*h1_1+-0.19440226*h1_2+-0.35489237*h1_3+0.27103144*h1_4+0.17225082*h1_5+0.11498338*h1_6+-0.28340003*h1_7+0.104762554*h1_8+0.014969947*h1_9+0.14705667*h1_10+-0.20537286*h1_11+0.20507301*h1_12+-0.10717296*h1_13+-0.111048244*h1_14+0.33200043*h1_15+-0.1562027*h1_16+0.20519872*h1_17+-0.20386828*h1_18+0.02051007*h1_19+-0.3751417*h1_20+-0.15634306*h1_21+0.026576566*h1_22+-0.17094427*h1_23+0.35532066*h1_24+0.17647013);\n",
        "h2_9 = tanh(-0.21666996*h1_0+-0.15263206*h1_1+0.15567815*h1_2+-0.3728095*h1_3+0.12017157*h1_4+-0.20643075*h1_5+0.0302251*h1_6+-0.12570679*h1_7+0.16961968*h1_8+0.2556213*h1_9+0.2596346*h1_10+-0.30635038*h1_11+-0.09991426*h1_12+0.4181103*h1_13+-0.11822739*h1_14+-0.3151758*h1_15+-0.38472104*h1_16+-0.10185704*h1_17+0.2863558*h1_18+-0.28567168*h1_19+-0.32558963*h1_20+0.074245796*h1_21+0.26830047*h1_22+0.2782703*h1_23+-0.024281876*h1_24+-0.20702761);\n",
        "h2_10 = tanh(0.53977937*h1_0+-0.6007246*h1_1+0.15633743*h1_2+-0.011934443*h1_3+-0.06411174*h1_4+0.1675366*h1_5+-0.29056197*h1_6+0.11674184*h1_7+0.6260091*h1_8+-0.24048136*h1_9+0.46767548*h1_10+-0.33846152*h1_11+0.44860038*h1_12+0.06362737*h1_13+-0.15254202*h1_14+-0.06285797*h1_15+-0.1342283*h1_16+0.47763664*h1_17+-0.4302196*h1_18+0.052443057*h1_19+-0.2024076*h1_20+-0.19599809*h1_21+0.09704292*h1_22+-0.25679743*h1_23+-0.3169567*h1_24+0.0032167227);\n",
        "h2_11 = tanh(0.08802601*h1_0+0.1090933*h1_1+-0.20088398*h1_2+-0.541398*h1_3+0.17238691*h1_4+-0.46073893*h1_5+0.38645232*h1_6+-0.5844877*h1_7+0.2209515*h1_8+-0.556651*h1_9+-0.08148958*h1_10+-0.008918388*h1_11+-0.6262004*h1_12+-0.021886617*h1_13+0.020536698*h1_14+-0.17836894*h1_15+-0.16673599*h1_16+0.73209023*h1_17+0.15059452*h1_18+-0.824861*h1_19+-0.51423067*h1_20+0.25489765*h1_21+0.48204798*h1_22+-0.41236183*h1_23+-0.39586252*h1_24+0.1301827);\n",
        "y = 0.4150653*h2_0+0.2426105*h2_1+0.000543866*h2_2+-0.17961907*h2_3+-0.13537759*h2_4+0.30538464*h2_5+0.59418905*h2_6+0.09844006*h2_7+0.47166157*h2_8+0.0002579412*h2_9+0.28843826*h2_10+0.52615386*h2_11+-0.051029854;\n",
        "\n",
        "        Id = pow(10, (y/normI + MinI)) ;\n",
        "\n",
        "if (Id <= 1e-15) begin //limit\n",
        "        Id = 1e-15;\n",
        "        //Id = Id;\n",
        "end\n",
        "else begin\n",
        "        Id = Id;\n",
        "end  //limit end\n",
        "\n",
        "\n",
        "        I(g, d) <+ Cgsd*ddt(Vg-Vd) ;\n",
        "        I(g, s) <+ Cgsd*ddt(Vg-Vs) ;\n",
        "\n",
        "if (Vgsraw >= Vgdraw) begin\n",
        "        I(d, s) <+ dir*Id*W ;\n",
        "\n",
        "end\n",
        "\n",
        "else begin\n",
        "        I(d, s) <+ dir*Id*W ;\n",
        "\n",
        "end\n",
        "\n",
        "end\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "print(verilog_code)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Tw_20-o48MH",
        "outputId": "1e4dda3e-3360-49c9-a69b-a701fc98c5d4"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "//*******************************************************************************\n",
            "//* * School of Electrical and Computer Engineering, Georgia Institute of Technology\n",
            "//* PI: Prof. Shimeng Yu\n",
            "//* All rights reserved.\n",
            "//*\n",
            "//* Copyright of the model is maintained by the developers, and the model is distributed under\n",
            "//* the terms of the Creative Commons Attribution-NonCommercial 4.0 International Public License\n",
            "//* http://creativecommons.org/licenses/by-nc/4.0/legalcode.\n",
            "//*\n",
            "//* THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND\n",
            "//* ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\n",
            "//* WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n",
            "//* DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\n",
            "//* FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\n",
            "//* DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\n",
            "//* SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\n",
            "//* CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\n",
            "//* OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n",
            "//* OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n",
            "//*\n",
            "//* Developer:\n",
            "//*  Gihun Choe gchoe6@gatech.edu\n",
            "//********************************************************************************/\n",
            "\n",
            "`include \"constants.vams\"\n",
            "`include \"disciplines.vams\"\n",
            "\n",
            "\n",
            "module IWO_verliogA(d, g, s);\n",
            "        inout d, g, s;\n",
            "        electrical d, g, s;\n",
            "\n",
            "        //***** parameters L and W ******//\n",
            "        parameter real W = 0.1; //get parameter fom spectre\n",
            "        parameter real L = 0.007; //get parameter fom spectre\n",
            "        parameter real T_stress = 0.0001; //set on cadence as variable\n",
            "        parameter real T_rec = 0.001;     //set on cadence as variable\n",
            "        parameter real Clk_loops = 50;    //set on cadence as variable\n",
            "        parameter real V_ov = 1.7;        //set on cadence as variable\n",
            "        parameter real Temp = 25;  //set on cadence as variable\n",
            "\n",
            "        parameter MinVg = -1.0 ;\n",
            "        parameter normVg = 0.2222222222222222 ;\n",
            "        parameter MinVd = 0.01 ;\n",
            "        parameter normVd = 0.2949852507374631 ;\n",
            "        parameter MinLg = 0.05 ;\n",
            "        parameter normLg = 1.4285714285714286 ;\n",
            "        parameter MinO = 8.15e-15 ;\n",
            "        parameter normO =33613445378151.26;\n",
            "        parameter MinI = -23.98798356587402 ;\n",
            "        parameter normI =0.04615548498417793;\n",
            "\n",
            "        parameter Mint_stress = 0.0001 ;\n",
            "        parameter normt_stress = 1 ;\n",
            "        parameter Mint_rec = 0.0001 ;\n",
            "        parameter normt_rec = 0.07629452739355005 ;\n",
            "        parameter Minclk_loops = 100 ;\n",
            "        parameter normclk_loops = 0.0011111111111111111 ;\n",
            "        parameter MinV_ov = 1.7 ;\n",
            "        parameter normV_ov = 1 ;\n",
            "        parameter Mintemperature = 25 ;\n",
            "        parameter normtemperature = 0.016666666666666666 ;\n",
            "        parameter Mindelta_Vth = 0.3234477566859917 ;\n",
            "        parameter normdelta_Vth = 2.8046380825020365 ;\n",
            "\n",
            "        real hvth1_0, hvth1_1, hvth1_2, hvth1_3, hvth1_4, hvth1_5, hvth1_6, hvth1_7, hvth1_8, hvth1_9, hvth1_10, hvth1_11, hvth1_12, hvth1_13, hvth1_14, hvth1_15, hvth1_16, hvth1_17, hvth1_18, hvth1_19;\n",
            "        real hvth2_0, hvth2_1, hvth2_2, hvth2_3, hvth2_4, hvth2_5, hvth2_6, hvth2_7, hvth2_8, hvth2_9;\n",
            "\n",
            "        real Vg, Vd, Vs, Vgs, Vds, Lg, Id, Cgg, Cgsd, Vgd;\n",
            "        real Vgsraw, Vgdraw, dir;\n",
            "        real t_stress, v_ov, t_rec, clk_loops, temp, delta_Vth;\n",
            "\n",
            "        real h1_0, h1_1, h1_2, h1_3, h1_4, h1_5, h1_6, h1_7, h1_8, h1_9, h1_10, h1_11, h1_12, h1_13, h1_14, h1_15, h1_16, h1_17, h1_18, h1_19, h1_20, h1_21, h1_22, h1_23, h1_24;\n",
            "        real h2_0, h2_1, h2_2, h2_3, h2_4, h2_5, h2_6, h2_7, h2_8, h2_9, h2_10, h2_11, h2_12, h2_13, h2_14, h2_15, h2_16, y;\n",
            "        real hc1_0, hc1_1, hc1_2, hc1_3, hc1_4, hc1_5, hc1_6, hc1_7, hc1_8, hc1_9;\n",
            "        real hc1_10, hc1_11, hc1_12, hc1_13, hc1_14, hc1_15, hc1_16;\n",
            "        real hc2_0, hc2_1, hc2_2, hc2_3, hc2_4, hc2_5, hc2_6, hc2_7, hc2_8, hc2_9;\n",
            "        real hc2_10, hc2_11, hc2_12, hc2_13, hc2_14, hc2_15, hc2_16, yc, yvth;\n",
            "\n",
            "analog begin\n",
            "\n",
            "t_stress = T_stress;\n",
            "v_ov = V_ov;\n",
            "t_rec = (T_rec - Mint_rec)*normt_rec ;\n",
            "clk_loops = (Clk_loops - Minclk_loops)*normclk_loops ;\n",
            "temp = (Temp - Mintemperature)*normtemperature ;\n",
            "\n",
            "//******************** delta_Vth NN **********************************//\n",
            "\n",
            "hvth1_0 = tanh(0.3223776*t_stress+-0.009236608*t_rec+0.5051236*clk_loops+0.10228214*v_ov+0.39094886*temp+-0.004476342);\n",
            "hvth1_1 = tanh(-0.26919153*t_stress+0.42856473*t_rec+-0.51785666*clk_loops+0.50790864*v_ov+0.30144733*temp+0.038409557);\n",
            "hvth1_2 = tanh(-0.27568778*t_stress+0.29239145*t_rec+0.28378052*clk_loops+-0.49817294*v_ov+0.27950928*temp+-0.06323713);\n",
            "hvth1_3 = tanh(0.26166126*t_stress+0.45323014*t_rec+-0.30770457*clk_loops+-0.007546696*v_ov+0.4123593*temp+-0.09595003);\n",
            "hvth1_4 = tanh(-0.29875568*t_stress+-0.296921*t_rec+-0.32425153*clk_loops+0.30120564*v_ov+0.20167196*temp+0.022581829);\n",
            "hvth1_5 = tanh(0.0223378*t_stress+-0.3208645*t_rec+-0.22506504*clk_loops+-0.28898084*v_ov+-0.15478039*temp+-0.030084377);\n",
            "hvth1_6 = tanh(-0.046423417*t_stress+-0.14601615*t_rec+0.34470084*clk_loops+-0.15141207*v_ov+0.3558162*temp+0.024532255);\n",
            "hvth1_7 = tanh(0.19075172*t_stress+0.055014424*t_rec+0.35975635*clk_loops+-0.12828624*v_ov+0.40981045*temp+0.057388254);\n",
            "hvth1_8 = tanh(0.18227698*t_stress+0.06842184*t_rec+-0.36610776*clk_loops+0.32031906*v_ov+0.42157808*temp+-0.0365752);\n",
            "hvth1_9 = tanh(-0.08153005*t_stress+-0.2320476*t_rec+0.28360367*clk_loops+0.34609947*v_ov+0.44897982*temp+-0.03935484);\n",
            "hvth1_10 = tanh(0.10328659*t_stress+0.3299492*t_rec+0.19185404*clk_loops+0.3654491*v_ov+0.58321667*temp+-0.0793196);\n",
            "hvth1_11 = tanh(0.29659915*t_stress+0.32946065*t_rec+0.4372847*clk_loops+-0.3653445*v_ov+0.368609*temp+0.04584916);\n",
            "hvth1_12 = tanh(0.006500926*t_stress+-0.8111394*t_rec+0.28204525*clk_loops+-0.10054121*v_ov+-0.34056407*temp+-0.014893414);\n",
            "hvth1_13 = tanh(0.04455428*t_stress+-0.51062423*t_rec+0.05996494*clk_loops+0.15708725*v_ov+0.05633228*temp+-0.0029830076);\n",
            "hvth1_14 = tanh(0.19857658*t_stress+-0.41077852*t_rec+-0.27126473*clk_loops+0.44573805*v_ov+-0.31406307*temp+-0.009999316);\n",
            "hvth1_15 = tanh(-0.35551268*t_stress+0.021231495*t_rec+-0.014926149*clk_loops+0.3960556*v_ov+0.13410321*temp+-0.040791057);\n",
            "hvth1_16 = tanh(0.38708287*t_stress+0.047177732*t_rec+-0.3480148*clk_loops+-0.15815456*v_ov+0.38881767*temp+0.04284011);\n",
            "hvth1_17 = tanh(-0.17926794*t_stress+0.33374506*t_rec+0.2260682*clk_loops+0.39101097*v_ov+-0.038775437*temp+-0.096534364);\n",
            "hvth1_18 = tanh(0.05815248*t_stress+0.2405776*t_rec+0.025204133*clk_loops+-0.36966237*v_ov+-0.25295833*temp+0.029073289);\n",
            "hvth1_19 = tanh(0.12217234*t_stress+0.1992351*t_rec+0.117544994*clk_loops+0.4055724*v_ov+0.5246246*temp+-0.056632224);\n",
            "\n",
            "hvth2_0 = tanh(-0.1126267*h1_0+0.018733028*h1_1+0.27745116*h1_2+0.30211142*h1_3+0.037246503*h1_4+-0.106538855*h1_5+-0.36017728*h1_6+0.32041612*h1_7+-0.12941499*h1_8+-0.14103034*h1_9+-0.4222683*h1_10+-0.29583675*h1_11+-0.32895955*h1_12+-0.40159148*h1_13+-0.342134*h1_14+-0.044357847*h1_15+-0.17254557*h1_16+0.21903218*h1_17+0.42835772*h1_18+-0.25742406*h1_19+-0.023892434);\n",
            "hvth2_1 = tanh(0.15681028*h1_0+-0.07867864*h1_1+-0.40253854*h1_2+0.06082303*h1_3+-0.27505505*h1_4+-0.3735702*h1_5+0.089757524*h1_6+0.0991592*h1_7+0.22539242*h1_8+0.3404443*h1_9+-0.34285003*h1_10+-0.29145664*h1_11+-0.16876285*h1_12+0.29162794*h1_13+0.01892729*h1_14+-0.3050296*h1_15+-0.14232339*h1_16+0.07028284*h1_17+0.38673154*h1_18+-0.40204418*h1_19+0.017297592);\n",
            "hvth2_2 = tanh(0.4846515*h1_0+-0.44938245*h1_1+-0.048292704*h1_2+0.25875154*h1_3+-0.08208744*h1_4+0.22207928*h1_5+0.25389993*h1_6+-0.0385822*h1_7+0.3566071*h1_8+0.116147585*h1_9+0.41702586*h1_10+0.046347965*h1_11+0.12240622*h1_12+0.31878206*h1_13+-0.4031527*h1_14+0.39128438*h1_15+-0.0695142*h1_16+0.19940229*h1_17+0.113380514*h1_18+0.21297237*h1_19+-0.026250547);\n",
            "hvth2_3 = tanh(0.16176273*h1_0+-0.35854438*h1_1+0.3271254*h1_2+-0.34649882*h1_3+-0.37899113*h1_4+-0.14281903*h1_5+0.401555*h1_6+-0.20664756*h1_7+0.36973375*h1_8+0.45274162*h1_9+-0.021110289*h1_10+-0.2618116*h1_11+-0.008064394*h1_12+0.1204612*h1_13+0.08443626*h1_14+-0.34491843*h1_15+-0.14593655*h1_16+-0.37558198*h1_17+-0.09083278*h1_18+0.1280824*h1_19+-0.010780956);\n",
            "hvth2_4 = tanh(-0.37564084*h1_0+0.19973701*h1_1+0.23181707*h1_2+-0.41888586*h1_3+-0.35413125*h1_4+0.22651628*h1_5+0.31310564*h1_6+0.33794057*h1_7+0.3336928*h1_8+0.40895706*h1_9+-0.42692414*h1_10+-0.14731333*h1_11+0.38194773*h1_12+0.24266145*h1_13+0.34808505*h1_14+-0.4094712*h1_15+0.48929894*h1_16+-0.35524476*h1_17+-0.2524605*h1_18+-0.27888805*h1_19+0.001787657);\n",
            "hvth2_5 = tanh(0.10109025*h1_0+0.3804848*h1_1+-0.3605982*h1_2+0.14508636*h1_3+-0.063886*h1_4+-0.06694758*h1_5+-0.31588903*h1_6+-0.15721983*h1_7+0.13281684*h1_8+-0.05755029*h1_9+-0.11051908*h1_10+0.22696814*h1_11+0.288669*h1_12+-0.37777147*h1_13+0.33420274*h1_14+0.083158135*h1_15+-0.047204733*h1_16+0.120564654*h1_17+-0.010811394*h1_18+0.13039613*h1_19+0.010660106);\n",
            "hvth2_6 = tanh(-0.028523734*h1_0+-0.045524627*h1_1+0.15399756*h1_2+0.47487494*h1_3+-0.29458568*h1_4+0.3601318*h1_5+-0.2865457*h1_6+0.37569886*h1_7+-0.37573344*h1_8+0.35157132*h1_9+-0.22426347*h1_10+0.45694694*h1_11+-0.10245604*h1_12+-0.2679713*h1_13+-0.004440246*h1_14+0.26024827*h1_15+0.055387855*h1_16+0.31531838*h1_17+-0.34307414*h1_18+-0.0513394*h1_19+-0.011568692);\n",
            "hvth2_7 = tanh(0.07178947*h1_0+0.29038185*h1_1+-0.4048809*h1_2+0.15022789*h1_3+-0.17210807*h1_4+-0.30970755*h1_5+-0.18594807*h1_6+0.36586866*h1_7+0.42198172*h1_8+-0.2282859*h1_9+0.19313374*h1_10+0.19753653*h1_11+0.007884695*h1_12+-0.028606815*h1_13+0.118019246*h1_14+0.35934886*h1_15+-0.21254441*h1_16+-0.10283318*h1_17+-0.17357902*h1_18+-0.20331915*h1_19+-0.0032071325);\n",
            "hvth2_8 = tanh(-0.057615217*h1_0+-0.4330024*h1_1+-0.34763557*h1_2+-0.10355299*h1_3+-0.13468981*h1_4+-0.42206457*h1_5+-0.2546902*h1_6+0.15848969*h1_7+0.41446975*h1_8+0.22665685*h1_9+-0.024474425*h1_10+0.20189627*h1_11+0.034226373*h1_12+0.0898148*h1_13+0.2435945*h1_14+-0.29249853*h1_15+0.3604111*h1_16+-0.030686846*h1_17+-0.02909773*h1_18+0.42092732*h1_19+-0.01029598);\n",
            "hvth2_9 = tanh(-0.38095903*h1_0+-0.44173273*h1_1+0.11166626*h1_2+0.41693294*h1_3+-0.0057686446*h1_4+0.18925878*h1_5+0.18379767*h1_6+-0.3078482*h1_7+-0.42551604*h1_8+-0.27807727*h1_9+-0.10444791*h1_10+0.104933165*h1_11+0.18249038*h1_12+-0.15064536*h1_13+0.3067071*h1_14+0.18976805*h1_15+-0.34351367*h1_16+-0.05009204*h1_17+-0.13444091*h1_18+0.16898519*h1_19+-0.027474554);\n",
            "\n",
            "yvth = 0.44205672*hvth2_0+-0.38789365*hvth2_1+0.7139153*hvth2_2+0.41243505*hvth2_3+0.52885765*hvth2_4+0.40824538*hvth2_5+0.2521593*hvth2_6+0.5549843*hvth2_7+0.558677*hvth2_8+0.26107034*hvth2_9+-0.006939304;\n",
            "\n",
            "delta_Vth = (yvth - Mindelta_Vth) * normdelta_Vth;\n",
            "\n",
            "\n",
            "        Vg = V(g) ;\n",
            "        Vs = V(s) ;\n",
            "        Vd = V(d) ;\n",
            "        Vgsraw = Vg-Vs ;\n",
            "        Vgdraw = Vg-Vd ;\n",
            "\n",
            "if (Vgsraw >= Vgdraw) begin\n",
            "        Vgs = ((Vg-Vs) - MinVg) * normVg ;\n",
            "        dir = 1;\n",
            "end\n",
            "\n",
            "else begin\n",
            "        Vgs = ((Vg-Vd) - MinVg) * normVg ;\n",
            "        dir = -1;\n",
            "end\n",
            "\n",
            "        Vds = (abs(Vd-Vs) - MinVd) * normVd ;\n",
            "        Vgs = Vgs - delta_Vth ; // BTI Vth variation\n",
            "        Lg = (L -MinLg)*normLg ;\n",
            "\n",
            "\n",
            "\n",
            "//******************** C-V NN **********************************//\n",
            "hc1_0 = tanh(-0.99871427*Vgs+-0.16952373*Vds+0.32118186*Lg+0.41485423);\n",
            "hc1_1 = tanh(0.31587568*Vgs+0.13397887*Vds+-0.4541538*Lg+-0.3630942);\n",
            "hc1_2 = tanh(-0.76281804*Vgs+0.09352969*Vds+1.1961353*Lg+0.3904977);\n",
            "hc1_3 = tanh(-1.115087*Vgs+0.85752946*Vds+-0.11746484*Lg+0.5500279);\n",
            "hc1_4 = tanh(1.0741386*Vgs+0.82041687*Vds+0.19092631*Lg+-0.4009425);\n",
            "hc1_5 = tanh(-0.47921795*Vgs+-0.8749933*Vds+-0.054768667*Lg+0.62785167);\n",
            "hc1_6 = tanh(0.5449184*Vgs+-4.409165*Vds+-0.072947875*Lg+-0.31324536);\n",
            "hc1_7 = tanh(-2.9224303*Vgs+2.7675478*Vds+0.08862238*Lg+0.6493558);\n",
            "hc1_8 = tanh(0.65050656*Vgs+-0.29751927*Vds+0.1571876*Lg+-0.38088372);\n",
            "hc1_9 = tanh(-0.30384183*Vgs+0.5649165*Vds+2.6806898*Lg+0.3197917);\n",
            "hc1_10 = tanh(-0.095988505*Vgs+2.0158541*Vds+-0.42972717*Lg+-0.30388466);\n",
            "hc1_11 = tanh(6.7699738*Vgs+-0.07234483*Vds+-0.013545353*Lg+-1.3694142);\n",
            "hc1_12 = tanh(-0.3404029*Vgs+0.0443459*Vds+0.89597*Lg+0.069993004);\n",
            "hc1_13 = tanh(0.62300175*Vgs+-0.29515797*Vds+1.6753465*Lg+-0.6520838);\n",
            "hc1_14 = tanh(0.37957156*Vgs+0.2237372*Vds+0.08591952*Lg+0.13126835);\n",
            "hc1_15 = tanh(0.19949242*Vgs+-0.26481664*Vds+-0.41059187*Lg+-0.40832308);\n",
            "hc1_16 = tanh(0.98966587*Vgs+-0.24259183*Vds+0.36584845*Lg+-0.8024042);\n",
            "\n",
            "hc2_0 = tanh(-0.91327864*hc1_0+0.4696781*hc1_1+0.3302202*hc1_2+0.11393868*hc1_3+0.45070222*hc1_4+-0.2894044*hc1_5+0.55066*hc1_6+-1.6242687*hc1_7+-0.38140613*hc1_8+0.032771554*hc1_9+0.17647126);\n",
            "hc2_1 = tanh(-1.1663305*hc1_0+-0.523984*hc1_1+-0.90804136*hc1_2+-0.7418044*hc1_3+1.456171*hc1_4+-0.16802542*hc1_5+0.8235596*hc1_6+-2.2246246*hc1_7+-0.40805355*hc1_8+0.7207601*hc1_9+0.4729169);\n",
            "hc2_2 = tanh(-0.69065374*hc1_0+0.40205315*hc1_1+-0.49410668*hc1_2+0.8681325*hc1_3+0.471351*hc1_4+0.46939445*hc1_5+0.45568785*hc1_6+-0.92935294*hc1_7+-0.8209646*hc1_8+0.1158967*hc1_9+-0.075798474);\n",
            "hc2_3 = tanh(0.11535446*hc1_0+-0.06296927*hc1_1+-0.6740435*hc1_2+0.7428315*hc1_3+0.05890677*hc1_4+0.9579441*hc1_5+-0.037319*hc1_6+-0.18491426*hc1_7+-0.02981994*hc1_8+0.038347963*hc1_9+0.039531134);\n",
            "hc2_4 = tanh(0.37817463*hc1_0+-0.6811279*hc1_1+1.1388369*hc1_2+0.19983096*hc1_3+-0.20415118*hc1_4+1.3022176*hc1_5+0.22571652*hc1_6+0.1690611*hc1_7+-0.56475276*hc1_8+-0.4069731*hc1_9+0.99962974);\n",
            "hc2_5 = tanh(0.032873478*hc1_0+-0.05209407*hc1_1+-0.010839908*hc1_2+-0.13892579*hc1_3+-0.050480977*hc1_4+0.0127089145*hc1_5+0.0052771433*hc1_6+0.02029242*hc1_7+-0.08705659*hc1_8+0.0254766*hc1_9+0.025135752);\n",
            "hc2_6 = tanh(-0.34639204*hc1_0+0.06937975*hc1_1+0.18671949*hc1_2+-0.18912783*hc1_3+0.1312504*hc1_4+0.4627272*hc1_5+-0.42590702*hc1_6+-0.10966313*hc1_7+0.66083515*hc1_8+-0.050718334*hc1_9+0.08234678);\n",
            "hc2_7 = tanh(0.90656275*hc1_0+-0.037281644*hc1_1+0.77237594*hc1_2+1.4710428*hc1_3+0.13597831*hc1_4+-0.059844542*hc1_5+-0.7801535*hc1_6+3.7814677*hc1_7+-0.5976644*hc1_8+0.2721995*hc1_9+0.023777716);\n",
            "hc2_8 = tanh(0.39720625*hc1_0+-0.45262313*hc1_1+0.19873238*hc1_2+0.9750888*hc1_3+-0.9427992*hc1_4+0.4487432*hc1_5+-0.3372945*hc1_6+0.33729544*hc1_7+-0.1667088*hc1_8+-0.5707525*hc1_9+0.27954483);\n",
            "hc2_9 = tanh(0.28551984*hc1_0+-0.68350387*hc1_1+0.9916423*hc1_2+-0.8254094*hc1_3+0.09875706*hc1_4+0.47609732*hc1_5+-0.058662917*hc1_6+0.09181381*hc1_7+0.09592329*hc1_8+1.3940467*hc1_9+0.3865768);\n",
            "hc2_10 = tanh(0.098737516*hc1_0+0.060473576*hc1_1+0.42824662*hc1_2+0.15018038*hc1_3+0.082621895*hc1_4+-0.00019039502*hc1_5+-0.3321634*hc1_6+0.7936295*hc1_7+-0.041197542*hc1_8+0.6530619*hc1_9+0.1338804);\n",
            "hc2_11 = tanh(-0.3585284*hc1_0+-0.09956017*hc1_1+0.17224246*hc1_2+-0.016925728*hc1_3+-0.46462816*hc1_4+-0.5649022*hc1_5+1.251695*hc1_6+-0.4303161*hc1_7+0.48546878*hc1_8+0.22958975*hc1_9+-0.27899802);\n",
            "hc2_12 = tanh(0.8565631*hc1_0+-0.7622999*hc1_1+0.32367912*hc1_2+1.4776785*hc1_3+0.2712369*hc1_4+0.2275511*hc1_5+0.39908803*hc1_6+4.2305493*hc1_7+-0.3467536*hc1_8+0.41231114*hc1_9+0.47123823);\n",
            "hc2_13 = tanh(-0.26411077*hc1_0+-0.17583853*hc1_1+0.045439184*hc1_2+-0.13801138*hc1_3+0.03278085*hc1_4+-0.45625108*hc1_5+-0.17905861*hc1_6+0.3060186*hc1_7+-0.3361926*hc1_8+-0.050055273*hc1_9+0.17996444);\n",
            "hc2_14 = tanh(0.016094366*hc1_0+-0.013196736*hc1_1+0.4124856*hc1_2+0.22926371*hc1_3+-0.35071182*hc1_4+-0.34217188*hc1_5+-0.69466996*hc1_6+1.0563152*hc1_7+-0.18019852*hc1_8+0.061871335*hc1_9+0.09762555);\n",
            "hc2_15 = tanh(-0.18970782*hc1_0+0.3624813*hc1_1+-1.3419824*hc1_2+0.103635244*hc1_3+-0.14595217*hc1_4+-0.1899393*hc1_5+0.176524*hc1_6+-0.4428012*hc1_7+-0.39544868*hc1_8+-0.45783517*hc1_9+0.1884755);\n",
            "hc2_16 = tanh(-0.1585831*hc1_0+0.035894837*hc1_1+-0.14261873*hc1_2+0.25914294*hc1_3+0.040607046*hc1_4+0.11555795*hc1_5+0.0022548323*hc1_6+-0.002149359*hc1_7+0.07067297*hc1_8+0.019578662*hc1_9+0.16657573);\n",
            "yc = 0.17503543*hc2_0+-0.15556754*hc2_1+-0.125455*hc2_2+-0.07502612*hc2_3+-0.16671115*hc2_4+0.29854503;\n",
            "\n",
            "Cgg = (yc / normO + MinO)*W;\n",
            "Cgsd = Cgg/2 ;\n",
            "\n",
            "//******************** I-V NN **********************************//\n",
            "h1_0 = tanh(0.4313499*Vgs+0.39184842*Vds+-0.28709298*Lg+0.20619453);\n",
            "h1_1 = tanh(0.530215*Vgs+-0.276051*Vds+0.3802653*Lg+-0.49267054);\n",
            "h1_2 = tanh(-0.33458906*Vgs+-0.38640633*Vds+-0.11866613*Lg+-0.14151593);\n",
            "h1_3 = tanh(-0.50232273*Vgs+-0.41781282*Vds+0.040914852*Lg+0.40752855);\n",
            "h1_4 = tanh(-0.2738816*Vgs+0.15826645*Vds+0.3726163*Lg+0.43347502);\n",
            "h1_5 = tanh(-0.89140266*Vgs+0.19422686*Vds+0.25828648*Lg+0.029750101);\n",
            "h1_6 = tanh(0.8155016*Vgs+-0.768934*Vds+-0.27971813*Lg+-0.29921618);\n",
            "h1_7 = tanh(-0.27932712*Vgs+1.3111871*Vds+0.053733792*Lg+0.032400735);\n",
            "h1_8 = tanh(0.0949421*Vgs+0.25981736*Vds+-0.30941275*Lg+-0.011010548);\n",
            "h1_9 = tanh(-1.2044673*Vgs+0.20209628*Vds+0.09035153*Lg+-0.23299748);\n",
            "h1_10 = tanh(0.17326604*Vgs+0.4927804*Vds+-0.37197837*Lg+0.48652422);\n",
            "h1_11 = tanh(0.048797052*Vgs+-0.86918294*Vds+0.049685556*Lg+-0.5611843);\n",
            "h1_12 = tanh(-0.44728053*Vgs+0.4416018*Vds+0.43623027*Lg+-0.39887246);\n",
            "h1_13 = tanh(0.02768353*Vgs+0.013126534*Vds+0.45135143*Lg+-0.05909225);\n",
            "h1_14 = tanh(0.4728052*Vgs+-0.35722205*Vds+-0.41991743*Lg+-0.1526044);\n",
            "h1_15 = tanh(-0.5687236*Vgs+0.07312218*Vds+-0.33128312*Lg+0.45431995);\n",
            "h1_16 = tanh(1.9482948*Vgs+-0.52973956*Vds+-0.15129352*Lg+-0.11858447);\n",
            "h1_17 = tanh(0.48852378*Vgs+0.21598034*Vds+0.24090554*Lg+-0.16491982);\n",
            "h1_18 = tanh(-0.3446666*Vgs+-1.177434*Vds+-0.28390405*Lg+-0.038540863);\n",
            "h1_19 = tanh(-2.3029919*Vgs+0.49877584*Vds+-0.22563547*Lg+-0.27407128);\n",
            "h1_20 = tanh(-0.732954*Vgs+0.23922421*Vds+0.075503394*Lg+-0.5665179);\n",
            "h1_21 = tanh(-0.17605503*Vgs+-0.07067414*Vds+-0.18359897*Lg+0.15867902);\n",
            "h1_22 = tanh(0.7302054*Vgs+-0.25757748*Vds+0.33953145*Lg+0.35818258);\n",
            "h1_23 = tanh(-0.43780586*Vgs+0.19977026*Vds+-0.17486832*Lg+-0.5687275);\n",
            "h1_24 = tanh(-1.3637807*Vgs+-0.15194818*Vds+-0.044956356*Lg+0.3245934);\n",
            "\n",
            "h2_0 = tanh(0.14783205*h1_0+-0.02431098*h1_1+-0.09715118*h1_2+-0.2474231*h1_3+0.32432914*h1_4+1.521693*h1_5+-0.10406853*h1_6+0.29371944*h1_7+-0.13358027*h1_8+1.0691675*h1_9+-0.19404113*h1_10+0.10453362*h1_11+1.6107924*h1_12+0.1920968*h1_13+-0.14785244*h1_14+-0.028951993*h1_15+-1.1547928*h1_16+-0.6376489*h1_17+-0.021565331*h1_18+2.606584*h1_19+0.97442293*h1_20+0.034297176*h1_21+-0.50504786*h1_22+0.8020577*h1_23+0.41248432*h1_24+-0.06995442);\n",
            "h2_1 = tanh(0.30681616*h1_0+0.36377275*h1_1+-0.046106532*h1_2+-0.2534532*h1_3+-0.037327003*h1_4+-0.35939386*h1_5+-0.071234025*h1_6+-0.9586675*h1_7+-0.06324141*h1_8+-0.2969973*h1_9+-0.18527432*h1_10+0.44116643*h1_11+-0.23221572*h1_12+-0.37151667*h1_13+0.15607928*h1_14+-0.6138509*h1_15+0.20915431*h1_16+0.66309977*h1_17+0.36671144*h1_18+-0.527578*h1_19+-0.10497286*h1_20+-0.19303346*h1_21+-0.14065515*h1_22+-0.13561434*h1_23+-0.4676026*h1_24+-0.14758746);\n",
            "h2_2 = tanh(-0.1950115*h1_0+-0.10721866*h1_1+0.23511808*h1_2+-0.13570116*h1_3+0.24990053*h1_4+0.25373626*h1_5+0.268683*h1_6+0.14307122*h1_7+0.055941034*h1_8+-0.113093115*h1_9+0.27305043*h1_10+0.23472951*h1_11+-0.020495743*h1_12+-0.30221954*h1_13+-0.15749171*h1_14+-0.3461544*h1_15+0.11443546*h1_16+-0.16509052*h1_17+-0.139441*h1_18+-0.06326206*h1_19+0.17607576*h1_20+-0.4709045*h1_21+0.2544463*h1_22+0.10170589*h1_23+0.29272288*h1_24+0.12412181);\n",
            "h2_3 = tanh(0.0003776102*h1_0+0.6797598*h1_1+-0.1299236*h1_2+-0.70903736*h1_3+0.028675593*h1_4+-0.38929847*h1_5+0.668139*h1_6+-0.09140271*h1_7+-0.16577661*h1_8+-0.0372596*h1_9+-0.06973977*h1_10+0.33586663*h1_11+0.17698736*h1_12+0.12853792*h1_13+0.75535774*h1_14+-0.7189986*h1_15+0.48737425*h1_16+0.5964495*h1_17+-0.30607405*h1_18+-0.012350458*h1_19+0.12677537*h1_20+-0.1537175*h1_21+0.15927425*h1_22+0.23570293*h1_23+-0.5400177*h1_24+-0.10571702);\n",
            "h2_4 = tanh(0.31527975*h1_0+-0.029269403*h1_1+-0.055212982*h1_2+-0.062310033*h1_3+0.08560037*h1_4+-0.72141176*h1_5+0.2020669*h1_6+0.025841974*h1_7+0.5646926*h1_8+-0.30292585*h1_9+0.00063811685*h1_10+-0.40466273*h1_11+-0.073278934*h1_12+0.34705442*h1_13+-0.05348122*h1_14+0.3436614*h1_15+0.04067239*h1_16+0.784844*h1_17+-0.28573394*h1_18+-0.88371974*h1_19+-0.26338458*h1_20+-0.013300428*h1_21+0.13801576*h1_22+-0.46081108*h1_23+-0.7637864*h1_24+0.12703106);\n",
            "h2_5 = tanh(-0.24617016*h1_0+0.26517266*h1_1+-0.21811938*h1_2+-0.2553703*h1_3+0.14267647*h1_4+0.4159367*h1_5+0.28696465*h1_6+-0.07311365*h1_7+0.2970628*h1_8+0.17417435*h1_9+0.18269496*h1_10+0.24361488*h1_11+0.18268564*h1_12+0.36019236*h1_13+-0.29943472*h1_14+0.39711836*h1_15+0.37918466*h1_16+0.064785175*h1_17+0.26349717*h1_18+-0.0071219024*h1_19+0.22553043*h1_20+0.17283064*h1_21+0.11927425*h1_22+0.10645739*h1_23+-0.34909987*h1_24+-0.15780911);\n",
            "h2_6 = tanh(0.19807929*h1_0+0.15743051*h1_1+-0.20436394*h1_2+-0.5772219*h1_3+0.0328607*h1_4+-0.3125246*h1_5+0.2093223*h1_6+-0.06814246*h1_7+0.10145798*h1_8+-0.3017911*h1_9+0.27700877*h1_10+-0.140296*h1_11+0.40272304*h1_12+-0.14553767*h1_13+0.42105338*h1_14+0.12200454*h1_15+0.19187498*h1_16+0.73878366*h1_17+-0.4901547*h1_18+-0.33965933*h1_19+0.17176971*h1_20+0.08609857*h1_21+0.045344174*h1_22+0.2393916*h1_23+0.17985432*h1_24+0.064894326);\n",
            "h2_7 = tanh(0.2932835*h1_0+0.075287476*h1_1+0.19162141*h1_2+0.2823231*h1_3+0.4070056*h1_4+-0.004358482*h1_5+-0.16040435*h1_6+-0.13784154*h1_7+0.19380908*h1_8+-0.22312845*h1_9+0.3159539*h1_10+0.11714408*h1_11+0.119634695*h1_12+0.32135013*h1_13+0.22685558*h1_14+-0.14774461*h1_15+-0.017703133*h1_16+0.17009398*h1_17+-0.29736042*h1_18+0.14056131*h1_19+0.0013223822*h1_20+0.1552641*h1_21+-0.38140768*h1_22+-0.2736542*h1_23+0.16030753*h1_24+-0.0041591506);\n",
            "h2_8 = tanh(0.008927864*h1_0+-0.43404117*h1_1+-0.19440226*h1_2+-0.35489237*h1_3+0.27103144*h1_4+0.17225082*h1_5+0.11498338*h1_6+-0.28340003*h1_7+0.104762554*h1_8+0.014969947*h1_9+0.14705667*h1_10+-0.20537286*h1_11+0.20507301*h1_12+-0.10717296*h1_13+-0.111048244*h1_14+0.33200043*h1_15+-0.1562027*h1_16+0.20519872*h1_17+-0.20386828*h1_18+0.02051007*h1_19+-0.3751417*h1_20+-0.15634306*h1_21+0.026576566*h1_22+-0.17094427*h1_23+0.35532066*h1_24+0.17647013);\n",
            "h2_9 = tanh(-0.21666996*h1_0+-0.15263206*h1_1+0.15567815*h1_2+-0.3728095*h1_3+0.12017157*h1_4+-0.20643075*h1_5+0.0302251*h1_6+-0.12570679*h1_7+0.16961968*h1_8+0.2556213*h1_9+0.2596346*h1_10+-0.30635038*h1_11+-0.09991426*h1_12+0.4181103*h1_13+-0.11822739*h1_14+-0.3151758*h1_15+-0.38472104*h1_16+-0.10185704*h1_17+0.2863558*h1_18+-0.28567168*h1_19+-0.32558963*h1_20+0.074245796*h1_21+0.26830047*h1_22+0.2782703*h1_23+-0.024281876*h1_24+-0.20702761);\n",
            "h2_10 = tanh(0.53977937*h1_0+-0.6007246*h1_1+0.15633743*h1_2+-0.011934443*h1_3+-0.06411174*h1_4+0.1675366*h1_5+-0.29056197*h1_6+0.11674184*h1_7+0.6260091*h1_8+-0.24048136*h1_9+0.46767548*h1_10+-0.33846152*h1_11+0.44860038*h1_12+0.06362737*h1_13+-0.15254202*h1_14+-0.06285797*h1_15+-0.1342283*h1_16+0.47763664*h1_17+-0.4302196*h1_18+0.052443057*h1_19+-0.2024076*h1_20+-0.19599809*h1_21+0.09704292*h1_22+-0.25679743*h1_23+-0.3169567*h1_24+0.0032167227);\n",
            "h2_11 = tanh(0.08802601*h1_0+0.1090933*h1_1+-0.20088398*h1_2+-0.541398*h1_3+0.17238691*h1_4+-0.46073893*h1_5+0.38645232*h1_6+-0.5844877*h1_7+0.2209515*h1_8+-0.556651*h1_9+-0.08148958*h1_10+-0.008918388*h1_11+-0.6262004*h1_12+-0.021886617*h1_13+0.020536698*h1_14+-0.17836894*h1_15+-0.16673599*h1_16+0.73209023*h1_17+0.15059452*h1_18+-0.824861*h1_19+-0.51423067*h1_20+0.25489765*h1_21+0.48204798*h1_22+-0.41236183*h1_23+-0.39586252*h1_24+0.1301827);\n",
            "y = 0.4150653*h2_0+0.2426105*h2_1+0.000543866*h2_2+-0.17961907*h2_3+-0.13537759*h2_4+0.30538464*h2_5+0.59418905*h2_6+0.09844006*h2_7+0.47166157*h2_8+0.0002579412*h2_9+0.28843826*h2_10+0.52615386*h2_11+-0.051029854;\n",
            "\n",
            "        Id = pow(10, (y/normI + MinI)) ;\n",
            "\n",
            "if (Id <= 1e-15) begin //limit\n",
            "        Id = 1e-15;\n",
            "        //Id = Id;\n",
            "end\n",
            "else begin\n",
            "        Id = Id;\n",
            "end  //limit end\n",
            "\n",
            "\n",
            "        I(g, d) <+ Cgsd*ddt(Vg-Vd) ;\n",
            "        I(g, s) <+ Cgsd*ddt(Vg-Vs) ;\n",
            "\n",
            "if (Vgsraw >= Vgdraw) begin\n",
            "        I(d, s) <+ dir*Id*W ;\n",
            "\n",
            "end\n",
            "\n",
            "else begin\n",
            "        I(d, s) <+ dir*Id*W ;\n",
            "\n",
            "end\n",
            "\n",
            "end\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "    \n",
        "    ===========================================\n"
      ],
      "metadata": {
        "id": "mCmumHX7By8e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Deprecated Version (Linear Regression)**"
      ],
      "metadata": {
        "id": "O8ZOvUesBszP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "X_tensor = torch.tensor(X, dtype=torch.float32)\n",
        "Y_tensor = torch.tensor(Y, dtype=torch.float32)\n",
        "\n",
        "# Define the model\n",
        "class LinearModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LinearModel, self).__init__()\n",
        "        self.linear = nn.Linear(6, 1)  # 6 inputs (t_stress, t_rec, etc.), 1 output (deltaV)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linear(x)\n",
        "\n",
        "# Instantiate the model\n",
        "model = LinearModel()\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 5000\n",
        "for epoch in range(num_epochs):\n",
        "    # Forward pass\n",
        "    outputs = model(X_tensor)\n",
        "    loss = criterion(outputs, Y_tensor)\n",
        "\n",
        "    # Backward and optimize\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if (epoch+1) % 50 == 0:\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "# Print learned weights and biases\n",
        "print(\"Learned linear coefficients (h1, h2, h3, h4, h5, h6):\", model.linear.weight.data.numpy())\n",
        "print(\"Learned bias:\", model.linear.bias.data.numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ASPvpASs1kUi",
        "outputId": "c5c7ddd0-a242-42d6-a36f-32f8004863b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [50/5000], Loss: 216.5890\n",
            "Epoch [100/5000], Loss: 35.0012\n",
            "Epoch [150/5000], Loss: 14.2757\n",
            "Epoch [200/5000], Loss: 5.4854\n",
            "Epoch [250/5000], Loss: 2.1224\n",
            "Epoch [300/5000], Loss: 1.0208\n",
            "Epoch [350/5000], Loss: 0.6775\n",
            "Epoch [400/5000], Loss: 0.5532\n",
            "Epoch [450/5000], Loss: 0.4914\n",
            "Epoch [500/5000], Loss: 0.4525\n",
            "Epoch [550/5000], Loss: 0.4252\n",
            "Epoch [600/5000], Loss: 0.4048\n",
            "Epoch [650/5000], Loss: 0.3887\n",
            "Epoch [700/5000], Loss: 0.3751\n",
            "Epoch [750/5000], Loss: 0.3628\n",
            "Epoch [800/5000], Loss: 0.3512\n",
            "Epoch [850/5000], Loss: 0.3399\n",
            "Epoch [900/5000], Loss: 0.3286\n",
            "Epoch [950/5000], Loss: 0.3174\n",
            "Epoch [1000/5000], Loss: 0.3062\n",
            "Epoch [1050/5000], Loss: 0.2950\n",
            "Epoch [1100/5000], Loss: 0.2838\n",
            "Epoch [1150/5000], Loss: 0.2727\n",
            "Epoch [1200/5000], Loss: 0.2616\n",
            "Epoch [1250/5000], Loss: 0.2506\n",
            "Epoch [1300/5000], Loss: 0.2397\n",
            "Epoch [1350/5000], Loss: 0.2289\n",
            "Epoch [1400/5000], Loss: 0.2183\n",
            "Epoch [1450/5000], Loss: 0.2078\n",
            "Epoch [1500/5000], Loss: 0.1976\n",
            "Epoch [1550/5000], Loss: 0.1875\n",
            "Epoch [1600/5000], Loss: 0.1777\n",
            "Epoch [1650/5000], Loss: 0.1682\n",
            "Epoch [1700/5000], Loss: 0.1589\n",
            "Epoch [1750/5000], Loss: 0.1499\n",
            "Epoch [1800/5000], Loss: 0.1411\n",
            "Epoch [1850/5000], Loss: 0.1327\n",
            "Epoch [1900/5000], Loss: 0.1245\n",
            "Epoch [1950/5000], Loss: 0.1167\n",
            "Epoch [2000/5000], Loss: 0.1092\n",
            "Epoch [2050/5000], Loss: 0.1020\n",
            "Epoch [2100/5000], Loss: 0.0951\n",
            "Epoch [2150/5000], Loss: 0.0886\n",
            "Epoch [2200/5000], Loss: 0.0823\n",
            "Epoch [2250/5000], Loss: 0.0764\n",
            "Epoch [2300/5000], Loss: 0.0708\n",
            "Epoch [2350/5000], Loss: 0.0655\n",
            "Epoch [2400/5000], Loss: 0.0606\n",
            "Epoch [2450/5000], Loss: 0.0559\n",
            "Epoch [2500/5000], Loss: 0.0515\n",
            "Epoch [2550/5000], Loss: 0.0475\n",
            "Epoch [2600/5000], Loss: 0.0437\n",
            "Epoch [2650/5000], Loss: 0.0401\n",
            "Epoch [2700/5000], Loss: 0.0369\n",
            "Epoch [2750/5000], Loss: 0.0339\n",
            "Epoch [2800/5000], Loss: 0.0311\n",
            "Epoch [2850/5000], Loss: 0.0286\n",
            "Epoch [2900/5000], Loss: 0.0262\n",
            "Epoch [2950/5000], Loss: 0.0241\n",
            "Epoch [3000/5000], Loss: 0.0222\n",
            "Epoch [3050/5000], Loss: 0.0204\n",
            "Epoch [3100/5000], Loss: 0.0189\n",
            "Epoch [3150/5000], Loss: 0.0175\n",
            "Epoch [3200/5000], Loss: 0.0162\n",
            "Epoch [3250/5000], Loss: 0.0151\n",
            "Epoch [3300/5000], Loss: 0.0141\n",
            "Epoch [3350/5000], Loss: 0.0132\n",
            "Epoch [3400/5000], Loss: 0.0124\n",
            "Epoch [3450/5000], Loss: 0.0117\n",
            "Epoch [3500/5000], Loss: 0.0111\n",
            "Epoch [3550/5000], Loss: 0.0105\n",
            "Epoch [3600/5000], Loss: 0.0101\n",
            "Epoch [3650/5000], Loss: 0.0097\n",
            "Epoch [3700/5000], Loss: 0.0094\n",
            "Epoch [3750/5000], Loss: 0.0091\n",
            "Epoch [3800/5000], Loss: 0.0088\n",
            "Epoch [3850/5000], Loss: 0.0086\n",
            "Epoch [3900/5000], Loss: 0.0084\n",
            "Epoch [3950/5000], Loss: 0.0083\n",
            "Epoch [4000/5000], Loss: 0.0082\n",
            "Epoch [4050/5000], Loss: 0.0081\n",
            "Epoch [4100/5000], Loss: 0.0080\n",
            "Epoch [4150/5000], Loss: 0.0079\n",
            "Epoch [4200/5000], Loss: 0.0078\n",
            "Epoch [4250/5000], Loss: 0.0078\n",
            "Epoch [4300/5000], Loss: 0.0078\n",
            "Epoch [4350/5000], Loss: 0.0077\n",
            "Epoch [4400/5000], Loss: 0.0077\n",
            "Epoch [4450/5000], Loss: 0.0077\n",
            "Epoch [4500/5000], Loss: 0.0077\n",
            "Epoch [4550/5000], Loss: 0.0077\n",
            "Epoch [4600/5000], Loss: 0.0077\n",
            "Epoch [4650/5000], Loss: 0.0077\n",
            "Epoch [4700/5000], Loss: 0.0076\n",
            "Epoch [4750/5000], Loss: 0.0076\n",
            "Epoch [4800/5000], Loss: 0.0076\n",
            "Epoch [4850/5000], Loss: 0.0076\n",
            "Epoch [4900/5000], Loss: 0.0076\n",
            "Epoch [4950/5000], Loss: 0.0076\n",
            "Epoch [5000/5000], Loss: 0.0076\n",
            "Learned linear coefficients (h1, h2, h3, h4, h5, h6): [[3.2403252e-01 1.2588283e-04 1.4926398e-01 1.6216135e-06 5.8017345e-03\n",
            "  3.4838660e-05]]\n",
            "Learned bias: [0.369416]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_array = np.array([[0.0001, 0.0002, 0.5, 100, 1.7, 25]])  # Sample input\n",
        "\n",
        "# Convert input data to a tensor\n",
        "input_tensor = torch.tensor(input_array, dtype=torch.float32)\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# No need to track gradients for making predictions\n",
        "with torch.no_grad():\n",
        "    # Predict the output using the model\n",
        "    predicted_output = model(input_tensor)\n",
        "\n",
        "# Print the predicted delta_V value\n",
        "print(\"Estimated Output delta_V:\", predicted_output.item())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rjD077d3381W",
        "outputId": "ead20c1e-027c-46e5-ac60-530137e872f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Estimated Output delta_V: 0.45497649908065796\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9OZZWfblrhuz",
        "outputId": "3b31f421-93f6-4151-b35e-2e8776ffba7b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10/100, Loss: 0.048087798058986664\n",
            "Epoch 20/100, Loss: 0.07116486877202988\n",
            "Epoch 30/100, Loss: 0.08174236863851547\n",
            "Epoch 40/100, Loss: 0.10612807422876358\n",
            "Epoch 50/100, Loss: 0.05975770205259323\n",
            "Epoch 60/100, Loss: 0.04767672345042229\n",
            "Epoch 70/100, Loss: 0.06521065533161163\n",
            "Epoch 80/100, Loss: 0.04492233693599701\n",
            "Epoch 90/100, Loss: 0.09233981370925903\n",
            "Epoch 100/100, Loss: 0.052101973444223404\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import numpy as np\n",
        "\n",
        "# Define the MLP neural network class\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MLP, self).__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(5, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)\n",
        "\n",
        "# Example data\n",
        "np.random.seed(42)\n",
        "x_data = np.random.rand(1000, 5)  # 5D Input (t_st, t_rec, t_cycle, Vov, temp)\n",
        "y_data = np.random.rand(1000, 1)  # 1D Output(deltaV)\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "x_tensor = torch.FloatTensor(x_data)\n",
        "y_tensor = torch.FloatTensor(y_data)\n",
        "\n",
        "# Create dataset and dataloader\n",
        "dataset = TensorDataset(x_tensor, y_tensor)\n",
        "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "# Initialize the MLP\n",
        "model = MLP()\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 100\n",
        "for epoch in range(num_epochs):\n",
        "    for inputs, targets in dataloader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}')\n",
        "\n",
        "# Save the trained model\n",
        "torch.save(model.state_dict(), 'mlp_regression_model.pth')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_features = [0.01, 0.01, 0.001, 1.7, 300] # 5D Input (t_st, t_rec, t_cycle, Vov, temp)\n",
        "input_tensor = torch.FloatTensor([input_features])\n",
        "\n",
        "# Inference\n",
        "with torch.no_grad():\n",
        "    output = model(input_tensor)\n",
        "\n",
        "print(\"Predicted delta_V: {}V\".format(output.item()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vGO6_LmkvEpO",
        "outputId": "10734133-2889-4575-d0c0-d57ccc05b958"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted delta_V: 26.089859008789062V\n"
          ]
        }
      ]
    }
  ]
}
