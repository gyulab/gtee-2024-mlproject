{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WeS55towxECg",
        "outputId": "b1adc065-0114-4e04-98f8-a89a52bf72fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[   1.    4.    7.   10.   40.   70.  100.  400.  700. 1000.]\n",
            "[25 55 85]\n",
            "[0.00614    0.00836    0.00937    0.01       0.01279359 0.01399969\n",
            " 0.0147909  0.01800496 0.0193501  0.02021849 0.00793    0.0107\n",
            " 0.0119     0.0126     0.01587101 0.01724442 0.01813702 0.02167537\n",
            " 0.02312108 0.02403969 0.00992    0.0131     0.0145     0.0154\n",
            " 0.01898129 0.02047152 0.02142934 0.02513192 0.02660211 0.02752889]\n"
          ]
        }
      ],
      "source": [
        "# Dataset Call\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.interpolate import interp1d\n",
        "\n",
        "dVth_xls_data = pd.read_csv(r'/content/drive/MyDrive/Colab_ML_ProfYu/ML_model_save/bti_comphy.csv')\n",
        "#data = dVth_xls_data.iloc[1:, 2:].T.values\n",
        "\n",
        "#[t_stress, t_rec, t_ratio, duty_cycle, clk_loops, V_ov, temperature, delta_Vth]\n",
        "# Function to clean and convert data to only numeric, ignoring non-numeric values\n",
        "def clean_numeric_data(data):\n",
        "    cleaned_data = []\n",
        "    for item in data:\n",
        "        try:\n",
        "            # Convert to float and check if it is not NaN\n",
        "            numeric_value = float(item)\n",
        "            if not np.isnan(numeric_value):\n",
        "                cleaned_data.append(numeric_value)\n",
        "        except ValueError:\n",
        "            continue\n",
        "\n",
        "    return np.array(cleaned_data)\n",
        "\n",
        "#t_stress_BG_sp5 = []\n",
        "t_stress = clean_numeric_data(dVth_xls_data.iloc[0:, 0].values)\n",
        "temperature = np.array([25, 55, 85])\n",
        "delta_Vth = clean_numeric_data(dVth_xls_data.iloc[0:, 1].values)\n",
        "\n",
        "print(t_stress)\n",
        "print(temperature)\n",
        "print(delta_Vth)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "GIMnZZ9yy7jz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91e6ac8a-38b5-4c05-ae98-2bf8b8086c95"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.         0.1037922  0.15101297 0.18046751 0.31107704 0.36746613\n",
            " 0.40445739 0.55472525 0.61761486 0.65821505 0.0836883  0.21319478\n",
            " 0.26929867 0.30202594 0.45495645 0.51916764 0.56089972 0.72632887\n",
            " 0.79392031 0.8368684  0.17672726 0.32540257 0.3908571  0.43293502\n",
            " 0.60037194 0.67004507 0.71482612 0.88793393 0.95667008 1.        ]\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "def normaliz(target): #Minmax normalization\n",
        "    Min = min(target)\n",
        "    Val = target-Min\n",
        "    Val = Val\n",
        "    Max = max(Val)\n",
        "    Norm = 1/Max\n",
        "    return (Norm, Val, Min)\n",
        "\n",
        "\n",
        "(normt_stress, t_stress_1, Mint_stress) = normaliz(t_stress)\n",
        "#(normt_rec, t_rec_1, Mint_rec) = normaliz(t_rec)\n",
        "#(normclk_loops, clk_loops_1, Minclk_loops) = normaliz(clk_loops)\n",
        "#(normV_ov, V_ov_1, MinV_ov) = normaliz(V_ov)\n",
        "(normtemperature, temperature_1, Mintemperature) = normaliz(temperature)\n",
        "(normdelta_Vth, delta_Vth_1, Mindelta_Vth) = normaliz(delta_Vth)\n",
        "\n",
        "T_stress = normt_stress * t_stress_1\n",
        "#T_rec = normt_rec * t_rec_1\n",
        "#Clk_loops = normclk_loops * clk_loops_1\n",
        "#Vov = normV_ov * V_ov_1\n",
        "Temperature = normtemperature * temperature_1\n",
        "Delta_Vth = normdelta_Vth * delta_Vth_1\n",
        "\n",
        "print(Delta_Vth)\n",
        "datasets = []\n",
        "for i in list(range(len(Temperature))):\n",
        "    for j in list(range(len(T_stress))):\n",
        "        temp=[T_stress[j], Temperature[i], Delta_Vth[j+len(T_stress)]]\n",
        "        datasets.append(temp)\n",
        "\n",
        "V = []\n",
        "for i in list(range(len(datasets))):\n",
        "    temp = [datasets[i][0], datasets[i][1]]\n",
        "    V.append(temp)\n",
        "\n",
        "I = []\n",
        "for i in list(range(len(datasets))):\n",
        "    temp = [datasets[i][2]]\n",
        "    I.append(temp)\n",
        "\n",
        "V = torch.tensor(V)\n",
        "I = torch.tensor(I)\n",
        "\n",
        "#X = np.vstack(T_stress.astype(float), Temperature.astype(float)).T #, T_rec.astype(float), Clk_loops.astype(float), Vov.astype(float),\n",
        "#Y = np.vstack(Delta_Vth.astype(float))\n",
        "#X_tensor = torch.tensor(X, dtype=torch.float64)\n",
        "#Y_tensor = torch.tensor(Y, dtype=torch.float64)\n",
        "#print(np.shape(X))\n",
        "#print(np.shape(Y))\n",
        "#print(X_tensor)\n",
        "#print(Y_tensor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "gLtzD7eS3ICV",
        "outputId": "aa661eff-5730-4e73-dd74-7741bbcb6551"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-20-8151015df420>:30: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  X_tensor = torch.tensor(V, dtype=torch.float32)\n",
            "<ipython-input-20-8151015df420>:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  Y_tensor = torch.tensor(I, dtype=torch.float32)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss (epoch:    1): 0.74318567\n",
            "Loss (epoch:    2): 0.67262185\n",
            "Loss (epoch:    3): 0.60708243\n",
            "Loss (epoch:    4): 0.54606794\n",
            "Loss (epoch:    5): 0.48952977\n",
            "Loss (epoch:    6): 0.43740319\n",
            "Loss (epoch:    7): 0.38960794\n",
            "Loss (epoch:    8): 0.34604651\n",
            "Loss (epoch:    9): 0.30660021\n",
            "Loss (epoch:   10): 0.27112642\n",
            "Loss (epoch:   11): 0.23945791\n",
            "Loss (epoch:   12): 0.21140320\n",
            "Loss (epoch:   13): 0.18674800\n",
            "Loss (epoch:   14): 0.16525810\n",
            "Loss (epoch:   15): 0.14668265\n",
            "Loss (epoch:   16): 0.13075830\n",
            "Loss (epoch:   17): 0.11721406\n",
            "Loss (epoch:   18): 0.10577619\n",
            "Loss (epoch:   19): 0.09617367\n",
            "Loss (epoch:   20): 0.08814347\n",
            "Loss (epoch:   21): 0.08143595\n",
            "Loss (epoch:   22): 0.07581940\n",
            "Loss (epoch:   23): 0.07108424\n",
            "Loss (epoch:   24): 0.06704595\n",
            "Loss (epoch:   25): 0.06354690\n",
            "Loss (epoch:   26): 0.06045697\n",
            "Loss (epoch:   27): 0.05767279\n",
            "Loss (epoch:   28): 0.05511596\n",
            "Loss (epoch:   29): 0.05273028\n",
            "Loss (epoch:   30): 0.05047835\n",
            "Loss (epoch:   31): 0.04833782\n",
            "Loss (epoch:   32): 0.04629770\n",
            "Loss (epoch:   33): 0.04435479\n",
            "Loss (epoch:   34): 0.04251061\n",
            "Loss (epoch:   35): 0.04076881\n",
            "Loss (epoch:   36): 0.03913331\n",
            "Loss (epoch:   37): 0.03760694\n",
            "Loss (epoch:   38): 0.03619066\n",
            "Loss (epoch:   39): 0.03488327\n",
            "Loss (epoch:   40): 0.03368146\n",
            "Loss (epoch:   41): 0.03258009\n",
            "Loss (epoch:   42): 0.03157267\n",
            "Loss (epoch:   43): 0.03065180\n",
            "Loss (epoch:   44): 0.02980975\n",
            "Loss (epoch:   45): 0.02903882\n",
            "Loss (epoch:   46): 0.02833165\n",
            "Loss (epoch:   47): 0.02768155\n",
            "Loss (epoch:   48): 0.02708256\n",
            "Loss (epoch:   49): 0.02652951\n",
            "Loss (epoch:   50): 0.02601801\n",
            "Loss (epoch:   51): 0.02554435\n",
            "Loss (epoch:   52): 0.02510542\n",
            "Loss (epoch:   53): 0.02469857\n",
            "Loss (epoch:   54): 0.02432152\n",
            "Loss (epoch:   55): 0.02397225\n",
            "Loss (epoch:   56): 0.02364890\n",
            "Loss (epoch:   57): 0.02334975\n",
            "Loss (epoch:   58): 0.02307314\n",
            "Loss (epoch:   59): 0.02281750\n",
            "Loss (epoch:   60): 0.02258128\n",
            "Loss (epoch:   61): 0.02236303\n",
            "Loss (epoch:   62): 0.02216132\n",
            "Loss (epoch:   63): 0.02197484\n",
            "Loss (epoch:   64): 0.02180231\n",
            "Loss (epoch:   65): 0.02164260\n",
            "Loss (epoch:   66): 0.02149461\n",
            "Loss (epoch:   67): 0.02135737\n",
            "Loss (epoch:   68): 0.02123000\n",
            "Loss (epoch:   69): 0.02111166\n",
            "Loss (epoch:   70): 0.02100162\n",
            "Loss (epoch:   71): 0.02089922\n",
            "Loss (epoch:   72): 0.02080381\n",
            "Loss (epoch:   73): 0.02071484\n",
            "Loss (epoch:   74): 0.02063177\n",
            "Loss (epoch:   75): 0.02055412\n",
            "Loss (epoch:   76): 0.02048145\n",
            "Loss (epoch:   77): 0.02041335\n",
            "Loss (epoch:   78): 0.02034943\n",
            "Loss (epoch:   79): 0.02028933\n",
            "Loss (epoch:   80): 0.02023275\n",
            "Loss (epoch:   81): 0.02017937\n",
            "Loss (epoch:   82): 0.02012893\n",
            "Loss (epoch:   83): 0.02008118\n",
            "Loss (epoch:   84): 0.02003590\n",
            "Loss (epoch:   85): 0.01999289\n",
            "Loss (epoch:   86): 0.01995196\n",
            "Loss (epoch:   87): 0.01991294\n",
            "Loss (epoch:   88): 0.01987569\n",
            "Loss (epoch:   89): 0.01984007\n",
            "Loss (epoch:   90): 0.01980596\n",
            "Loss (epoch:   91): 0.01977325\n",
            "Loss (epoch:   92): 0.01974182\n",
            "Loss (epoch:   93): 0.01971161\n",
            "Loss (epoch:   94): 0.01968251\n",
            "Loss (epoch:   95): 0.01965446\n",
            "Loss (epoch:   96): 0.01962739\n",
            "Loss (epoch:   97): 0.01960124\n",
            "Loss (epoch:   98): 0.01957596\n",
            "Loss (epoch:   99): 0.01955149\n",
            "Loss (epoch:  100): 0.01952779\n",
            "Loss (epoch:  101): 0.01950482\n",
            "Loss (epoch:  102): 0.01948253\n",
            "Loss (epoch:  103): 0.01946090\n",
            "Loss (epoch:  104): 0.01943989\n",
            "Loss (epoch:  105): 0.01941949\n",
            "Loss (epoch:  106): 0.01939964\n",
            "Loss (epoch:  107): 0.01938034\n",
            "Loss (epoch:  108): 0.01936156\n",
            "Loss (epoch:  109): 0.01934327\n",
            "Loss (epoch:  110): 0.01932546\n",
            "Loss (epoch:  111): 0.01930811\n",
            "Loss (epoch:  112): 0.01929120\n",
            "Loss (epoch:  113): 0.01927472\n",
            "Loss (epoch:  114): 0.01925865\n",
            "Loss (epoch:  115): 0.01924297\n",
            "Loss (epoch:  116): 0.01922767\n",
            "Loss (epoch:  117): 0.01921274\n",
            "Loss (epoch:  118): 0.01919817\n",
            "Loss (epoch:  119): 0.01918394\n",
            "Loss (epoch:  120): 0.01917004\n",
            "Loss (epoch:  121): 0.01915646\n",
            "Loss (epoch:  122): 0.01914319\n",
            "Loss (epoch:  123): 0.01913022\n",
            "Loss (epoch:  124): 0.01911754\n",
            "Loss (epoch:  125): 0.01910513\n",
            "Loss (epoch:  126): 0.01909300\n",
            "Loss (epoch:  127): 0.01908112\n",
            "Loss (epoch:  128): 0.01906950\n",
            "Loss (epoch:  129): 0.01905812\n",
            "Loss (epoch:  130): 0.01904697\n",
            "Loss (epoch:  131): 0.01903605\n",
            "Loss (epoch:  132): 0.01902534\n",
            "Loss (epoch:  133): 0.01901485\n",
            "Loss (epoch:  134): 0.01900456\n",
            "Loss (epoch:  135): 0.01899446\n",
            "Loss (epoch:  136): 0.01898455\n",
            "Loss (epoch:  137): 0.01897482\n",
            "Loss (epoch:  138): 0.01896525\n",
            "Loss (epoch:  139): 0.01895586\n",
            "Loss (epoch:  140): 0.01894662\n",
            "Loss (epoch:  141): 0.01893754\n",
            "Loss (epoch:  142): 0.01892860\n",
            "Loss (epoch:  143): 0.01891980\n",
            "Loss (epoch:  144): 0.01891114\n",
            "Loss (epoch:  145): 0.01890260\n",
            "Loss (epoch:  146): 0.01889419\n",
            "Loss (epoch:  147): 0.01888589\n",
            "Loss (epoch:  148): 0.01887771\n",
            "Loss (epoch:  149): 0.01886963\n",
            "Loss (epoch:  150): 0.01886166\n",
            "Loss (epoch:  151): 0.01885378\n",
            "Loss (epoch:  152): 0.01884600\n",
            "Loss (epoch:  153): 0.01883829\n",
            "Loss (epoch:  154): 0.01883068\n",
            "Loss (epoch:  155): 0.01882314\n",
            "Loss (epoch:  156): 0.01881568\n",
            "Loss (epoch:  157): 0.01880829\n",
            "Loss (epoch:  158): 0.01880097\n",
            "Loss (epoch:  159): 0.01879370\n",
            "Loss (epoch:  160): 0.01878650\n",
            "Loss (epoch:  161): 0.01877935\n",
            "Loss (epoch:  162): 0.01877226\n",
            "Loss (epoch:  163): 0.01876521\n",
            "Loss (epoch:  164): 0.01875821\n",
            "Loss (epoch:  165): 0.01875126\n",
            "Loss (epoch:  166): 0.01874434\n",
            "Loss (epoch:  167): 0.01873746\n",
            "Loss (epoch:  168): 0.01873062\n",
            "Loss (epoch:  169): 0.01872380\n",
            "Loss (epoch:  170): 0.01871702\n",
            "Loss (epoch:  171): 0.01871026\n",
            "Loss (epoch:  172): 0.01870353\n",
            "Loss (epoch:  173): 0.01869682\n",
            "Loss (epoch:  174): 0.01869012\n",
            "Loss (epoch:  175): 0.01868345\n",
            "Loss (epoch:  176): 0.01867679\n",
            "Loss (epoch:  177): 0.01867014\n",
            "Loss (epoch:  178): 0.01866351\n",
            "Loss (epoch:  179): 0.01865689\n",
            "Loss (epoch:  180): 0.01865027\n",
            "Loss (epoch:  181): 0.01864366\n",
            "Loss (epoch:  182): 0.01863706\n",
            "Loss (epoch:  183): 0.01863046\n",
            "Loss (epoch:  184): 0.01862385\n",
            "Loss (epoch:  185): 0.01861725\n",
            "Loss (epoch:  186): 0.01861065\n",
            "Loss (epoch:  187): 0.01860405\n",
            "Loss (epoch:  188): 0.01859744\n",
            "Loss (epoch:  189): 0.01859083\n",
            "Loss (epoch:  190): 0.01858422\n",
            "Loss (epoch:  191): 0.01857759\n",
            "Loss (epoch:  192): 0.01857096\n",
            "Loss (epoch:  193): 0.01856431\n",
            "Loss (epoch:  194): 0.01855766\n",
            "Loss (epoch:  195): 0.01855100\n",
            "Loss (epoch:  196): 0.01854432\n",
            "Loss (epoch:  197): 0.01853763\n",
            "Loss (epoch:  198): 0.01853093\n",
            "Loss (epoch:  199): 0.01852420\n",
            "Loss (epoch:  200): 0.01851747\n",
            "Loss (epoch:  201): 0.01851072\n",
            "Loss (epoch:  202): 0.01850395\n",
            "Loss (epoch:  203): 0.01849717\n",
            "Loss (epoch:  204): 0.01849036\n",
            "Loss (epoch:  205): 0.01848355\n",
            "Loss (epoch:  206): 0.01847670\n",
            "Loss (epoch:  207): 0.01846984\n",
            "Loss (epoch:  208): 0.01846295\n",
            "Loss (epoch:  209): 0.01845605\n",
            "Loss (epoch:  210): 0.01844912\n",
            "Loss (epoch:  211): 0.01844218\n",
            "Loss (epoch:  212): 0.01843520\n",
            "Loss (epoch:  213): 0.01842821\n",
            "Loss (epoch:  214): 0.01842119\n",
            "Loss (epoch:  215): 0.01841415\n",
            "Loss (epoch:  216): 0.01840708\n",
            "Loss (epoch:  217): 0.01839999\n",
            "Loss (epoch:  218): 0.01839287\n",
            "Loss (epoch:  219): 0.01838573\n",
            "Loss (epoch:  220): 0.01837856\n",
            "Loss (epoch:  221): 0.01837136\n",
            "Loss (epoch:  222): 0.01836414\n",
            "Loss (epoch:  223): 0.01835689\n",
            "Loss (epoch:  224): 0.01834962\n",
            "Loss (epoch:  225): 0.01834231\n",
            "Loss (epoch:  226): 0.01833498\n",
            "Loss (epoch:  227): 0.01832762\n",
            "Loss (epoch:  228): 0.01832022\n",
            "Loss (epoch:  229): 0.01831281\n",
            "Loss (epoch:  230): 0.01830536\n",
            "Loss (epoch:  231): 0.01829788\n",
            "Loss (epoch:  232): 0.01829038\n",
            "Loss (epoch:  233): 0.01828284\n",
            "Loss (epoch:  234): 0.01827527\n",
            "Loss (epoch:  235): 0.01826767\n",
            "Loss (epoch:  236): 0.01826005\n",
            "Loss (epoch:  237): 0.01825239\n",
            "Loss (epoch:  238): 0.01824470\n",
            "Loss (epoch:  239): 0.01823698\n",
            "Loss (epoch:  240): 0.01822923\n",
            "Loss (epoch:  241): 0.01822144\n",
            "Loss (epoch:  242): 0.01821363\n",
            "Loss (epoch:  243): 0.01820578\n",
            "Loss (epoch:  244): 0.01819790\n",
            "Loss (epoch:  245): 0.01818999\n",
            "Loss (epoch:  246): 0.01818205\n",
            "Loss (epoch:  247): 0.01817406\n",
            "Loss (epoch:  248): 0.01816605\n",
            "Loss (epoch:  249): 0.01815801\n",
            "Loss (epoch:  250): 0.01814993\n",
            "Loss (epoch:  251): 0.01814182\n",
            "Loss (epoch:  252): 0.01813368\n",
            "Loss (epoch:  253): 0.01812551\n",
            "Loss (epoch:  254): 0.01811729\n",
            "Loss (epoch:  255): 0.01810905\n",
            "Loss (epoch:  256): 0.01810076\n",
            "Loss (epoch:  257): 0.01809245\n",
            "Loss (epoch:  258): 0.01808410\n",
            "Loss (epoch:  259): 0.01807572\n",
            "Loss (epoch:  260): 0.01806730\n",
            "Loss (epoch:  261): 0.01805884\n",
            "Loss (epoch:  262): 0.01805035\n",
            "Loss (epoch:  263): 0.01804183\n",
            "Loss (epoch:  264): 0.01803327\n",
            "Loss (epoch:  265): 0.01802467\n",
            "Loss (epoch:  266): 0.01801604\n",
            "Loss (epoch:  267): 0.01800738\n",
            "Loss (epoch:  268): 0.01799867\n",
            "Loss (epoch:  269): 0.01798993\n",
            "Loss (epoch:  270): 0.01798115\n",
            "Loss (epoch:  271): 0.01797234\n",
            "Loss (epoch:  272): 0.01796348\n",
            "Loss (epoch:  273): 0.01795460\n",
            "Loss (epoch:  274): 0.01794567\n",
            "Loss (epoch:  275): 0.01793671\n",
            "Loss (epoch:  276): 0.01792770\n",
            "Loss (epoch:  277): 0.01791867\n",
            "Loss (epoch:  278): 0.01790959\n",
            "Loss (epoch:  279): 0.01790048\n",
            "Loss (epoch:  280): 0.01789132\n",
            "Loss (epoch:  281): 0.01788213\n",
            "Loss (epoch:  282): 0.01787290\n",
            "Loss (epoch:  283): 0.01786363\n",
            "Loss (epoch:  284): 0.01785433\n",
            "Loss (epoch:  285): 0.01784499\n",
            "Loss (epoch:  286): 0.01783560\n",
            "Loss (epoch:  287): 0.01782618\n",
            "Loss (epoch:  288): 0.01781671\n",
            "Loss (epoch:  289): 0.01780720\n",
            "Loss (epoch:  290): 0.01779766\n",
            "Loss (epoch:  291): 0.01778808\n",
            "Loss (epoch:  292): 0.01777846\n",
            "Loss (epoch:  293): 0.01776879\n",
            "Loss (epoch:  294): 0.01775909\n",
            "Loss (epoch:  295): 0.01774934\n",
            "Loss (epoch:  296): 0.01773956\n",
            "Loss (epoch:  297): 0.01772973\n",
            "Loss (epoch:  298): 0.01771987\n",
            "Loss (epoch:  299): 0.01770995\n",
            "Loss (epoch:  300): 0.01770001\n",
            "Loss (epoch:  301): 0.01769001\n",
            "Loss (epoch:  302): 0.01767998\n",
            "Loss (epoch:  303): 0.01766991\n",
            "Loss (epoch:  304): 0.01765979\n",
            "Loss (epoch:  305): 0.01764963\n",
            "Loss (epoch:  306): 0.01763942\n",
            "Loss (epoch:  307): 0.01762918\n",
            "Loss (epoch:  308): 0.01761889\n",
            "Loss (epoch:  309): 0.01760857\n",
            "Loss (epoch:  310): 0.01759819\n",
            "Loss (epoch:  311): 0.01758778\n",
            "Loss (epoch:  312): 0.01757732\n",
            "Loss (epoch:  313): 0.01756681\n",
            "Loss (epoch:  314): 0.01755626\n",
            "Loss (epoch:  315): 0.01754567\n",
            "Loss (epoch:  316): 0.01753503\n",
            "Loss (epoch:  317): 0.01752436\n",
            "Loss (epoch:  318): 0.01751363\n",
            "Loss (epoch:  319): 0.01750286\n",
            "Loss (epoch:  320): 0.01749205\n",
            "Loss (epoch:  321): 0.01748119\n",
            "Loss (epoch:  322): 0.01747029\n",
            "Loss (epoch:  323): 0.01745934\n",
            "Loss (epoch:  324): 0.01744835\n",
            "Loss (epoch:  325): 0.01743731\n",
            "Loss (epoch:  326): 0.01742622\n",
            "Loss (epoch:  327): 0.01741508\n",
            "Loss (epoch:  328): 0.01740391\n",
            "Loss (epoch:  329): 0.01739269\n",
            "Loss (epoch:  330): 0.01738142\n",
            "Loss (epoch:  331): 0.01737010\n",
            "Loss (epoch:  332): 0.01735874\n",
            "Loss (epoch:  333): 0.01734733\n",
            "Loss (epoch:  334): 0.01733587\n",
            "Loss (epoch:  335): 0.01732436\n",
            "Loss (epoch:  336): 0.01731281\n",
            "Loss (epoch:  337): 0.01730121\n",
            "Loss (epoch:  338): 0.01728957\n",
            "Loss (epoch:  339): 0.01727787\n",
            "Loss (epoch:  340): 0.01726612\n",
            "Loss (epoch:  341): 0.01725434\n",
            "Loss (epoch:  342): 0.01724249\n",
            "Loss (epoch:  343): 0.01723061\n",
            "Loss (epoch:  344): 0.01721867\n",
            "Loss (epoch:  345): 0.01720668\n",
            "Loss (epoch:  346): 0.01719465\n",
            "Loss (epoch:  347): 0.01718256\n",
            "Loss (epoch:  348): 0.01717043\n",
            "Loss (epoch:  349): 0.01715825\n",
            "Loss (epoch:  350): 0.01714601\n",
            "Loss (epoch:  351): 0.01713373\n",
            "Loss (epoch:  352): 0.01712140\n",
            "Loss (epoch:  353): 0.01710902\n",
            "Loss (epoch:  354): 0.01709659\n",
            "Loss (epoch:  355): 0.01708410\n",
            "Loss (epoch:  356): 0.01707156\n",
            "Loss (epoch:  357): 0.01705899\n",
            "Loss (epoch:  358): 0.01704634\n",
            "Loss (epoch:  359): 0.01703366\n",
            "Loss (epoch:  360): 0.01702092\n",
            "Loss (epoch:  361): 0.01700813\n",
            "Loss (epoch:  362): 0.01699529\n",
            "Loss (epoch:  363): 0.01698240\n",
            "Loss (epoch:  364): 0.01696945\n",
            "Loss (epoch:  365): 0.01695646\n",
            "Loss (epoch:  366): 0.01694341\n",
            "Loss (epoch:  367): 0.01693031\n",
            "Loss (epoch:  368): 0.01691715\n",
            "Loss (epoch:  369): 0.01690395\n",
            "Loss (epoch:  370): 0.01689069\n",
            "Loss (epoch:  371): 0.01687738\n",
            "Loss (epoch:  372): 0.01686402\n",
            "Loss (epoch:  373): 0.01685060\n",
            "Loss (epoch:  374): 0.01683713\n",
            "Loss (epoch:  375): 0.01682361\n",
            "Loss (epoch:  376): 0.01681003\n",
            "Loss (epoch:  377): 0.01679640\n",
            "Loss (epoch:  378): 0.01678272\n",
            "Loss (epoch:  379): 0.01676899\n",
            "Loss (epoch:  380): 0.01675519\n",
            "Loss (epoch:  381): 0.01674135\n",
            "Loss (epoch:  382): 0.01672745\n",
            "Loss (epoch:  383): 0.01671350\n",
            "Loss (epoch:  384): 0.01669949\n",
            "Loss (epoch:  385): 0.01668542\n",
            "Loss (epoch:  386): 0.01667131\n",
            "Loss (epoch:  387): 0.01665714\n",
            "Loss (epoch:  388): 0.01664291\n",
            "Loss (epoch:  389): 0.01662863\n",
            "Loss (epoch:  390): 0.01661430\n",
            "Loss (epoch:  391): 0.01659991\n",
            "Loss (epoch:  392): 0.01658546\n",
            "Loss (epoch:  393): 0.01657096\n",
            "Loss (epoch:  394): 0.01655641\n",
            "Loss (epoch:  395): 0.01654179\n",
            "Loss (epoch:  396): 0.01652712\n",
            "Loss (epoch:  397): 0.01651240\n",
            "Loss (epoch:  398): 0.01649762\n",
            "Loss (epoch:  399): 0.01648279\n",
            "Loss (epoch:  400): 0.01646790\n",
            "Loss (epoch:  401): 0.01645295\n",
            "Loss (epoch:  402): 0.01643795\n",
            "Loss (epoch:  403): 0.01642289\n",
            "Loss (epoch:  404): 0.01640778\n",
            "Loss (epoch:  405): 0.01639261\n",
            "Loss (epoch:  406): 0.01637738\n",
            "Loss (epoch:  407): 0.01636210\n",
            "Loss (epoch:  408): 0.01634676\n",
            "Loss (epoch:  409): 0.01633137\n",
            "Loss (epoch:  410): 0.01631592\n",
            "Loss (epoch:  411): 0.01630041\n",
            "Loss (epoch:  412): 0.01628484\n",
            "Loss (epoch:  413): 0.01626923\n",
            "Loss (epoch:  414): 0.01625355\n",
            "Loss (epoch:  415): 0.01623782\n",
            "Loss (epoch:  416): 0.01622202\n",
            "Loss (epoch:  417): 0.01620618\n",
            "Loss (epoch:  418): 0.01619028\n",
            "Loss (epoch:  419): 0.01617432\n",
            "Loss (epoch:  420): 0.01615830\n",
            "Loss (epoch:  421): 0.01614223\n",
            "Loss (epoch:  422): 0.01612610\n",
            "Loss (epoch:  423): 0.01610992\n",
            "Loss (epoch:  424): 0.01609368\n",
            "Loss (epoch:  425): 0.01607738\n",
            "Loss (epoch:  426): 0.01606102\n",
            "Loss (epoch:  427): 0.01604462\n",
            "Loss (epoch:  428): 0.01602815\n",
            "Loss (epoch:  429): 0.01601162\n",
            "Loss (epoch:  430): 0.01599504\n",
            "Loss (epoch:  431): 0.01597841\n",
            "Loss (epoch:  432): 0.01596172\n",
            "Loss (epoch:  433): 0.01594497\n",
            "Loss (epoch:  434): 0.01592816\n",
            "Loss (epoch:  435): 0.01591130\n",
            "Loss (epoch:  436): 0.01589438\n",
            "Loss (epoch:  437): 0.01587741\n",
            "Loss (epoch:  438): 0.01586038\n",
            "Loss (epoch:  439): 0.01584330\n",
            "Loss (epoch:  440): 0.01582616\n",
            "Loss (epoch:  441): 0.01580896\n",
            "Loss (epoch:  442): 0.01579171\n",
            "Loss (epoch:  443): 0.01577440\n",
            "Loss (epoch:  444): 0.01575705\n",
            "Loss (epoch:  445): 0.01573963\n",
            "Loss (epoch:  446): 0.01572215\n",
            "Loss (epoch:  447): 0.01570463\n",
            "Loss (epoch:  448): 0.01568705\n",
            "Loss (epoch:  449): 0.01566941\n",
            "Loss (epoch:  450): 0.01565172\n",
            "Loss (epoch:  451): 0.01563398\n",
            "Loss (epoch:  452): 0.01561618\n",
            "Loss (epoch:  453): 0.01559833\n",
            "Loss (epoch:  454): 0.01558042\n",
            "Loss (epoch:  455): 0.01556246\n",
            "Loss (epoch:  456): 0.01554444\n",
            "Loss (epoch:  457): 0.01552638\n",
            "Loss (epoch:  458): 0.01550826\n",
            "Loss (epoch:  459): 0.01549008\n",
            "Loss (epoch:  460): 0.01547186\n",
            "Loss (epoch:  461): 0.01545358\n",
            "Loss (epoch:  462): 0.01543524\n",
            "Loss (epoch:  463): 0.01541686\n",
            "Loss (epoch:  464): 0.01539843\n",
            "Loss (epoch:  465): 0.01537994\n",
            "Loss (epoch:  466): 0.01536140\n",
            "Loss (epoch:  467): 0.01534281\n",
            "Loss (epoch:  468): 0.01532417\n",
            "Loss (epoch:  469): 0.01530548\n",
            "Loss (epoch:  470): 0.01528673\n",
            "Loss (epoch:  471): 0.01526794\n",
            "Loss (epoch:  472): 0.01524910\n",
            "Loss (epoch:  473): 0.01523021\n",
            "Loss (epoch:  474): 0.01521126\n",
            "Loss (epoch:  475): 0.01519227\n",
            "Loss (epoch:  476): 0.01517324\n",
            "Loss (epoch:  477): 0.01515415\n",
            "Loss (epoch:  478): 0.01513501\n",
            "Loss (epoch:  479): 0.01511583\n",
            "Loss (epoch:  480): 0.01509660\n",
            "Loss (epoch:  481): 0.01507732\n",
            "Loss (epoch:  482): 0.01505799\n",
            "Loss (epoch:  483): 0.01503862\n",
            "Loss (epoch:  484): 0.01501920\n",
            "Loss (epoch:  485): 0.01499973\n",
            "Loss (epoch:  486): 0.01498022\n",
            "Loss (epoch:  487): 0.01496067\n",
            "Loss (epoch:  488): 0.01494107\n",
            "Loss (epoch:  489): 0.01492142\n",
            "Loss (epoch:  490): 0.01490173\n",
            "Loss (epoch:  491): 0.01488200\n",
            "Loss (epoch:  492): 0.01486223\n",
            "Loss (epoch:  493): 0.01484240\n",
            "Loss (epoch:  494): 0.01482254\n",
            "Loss (epoch:  495): 0.01480264\n",
            "Loss (epoch:  496): 0.01478269\n",
            "Loss (epoch:  497): 0.01476271\n",
            "Loss (epoch:  498): 0.01474268\n",
            "Loss (epoch:  499): 0.01472261\n",
            "Loss (epoch:  500): 0.01470251\n",
            "Loss (epoch:  501): 0.01468236\n",
            "Loss (epoch:  502): 0.01466218\n",
            "Loss (epoch:  503): 0.01464195\n",
            "Loss (epoch:  504): 0.01462169\n",
            "Loss (epoch:  505): 0.01460139\n",
            "Loss (epoch:  506): 0.01458105\n",
            "Loss (epoch:  507): 0.01456068\n",
            "Loss (epoch:  508): 0.01454027\n",
            "Loss (epoch:  509): 0.01451982\n",
            "Loss (epoch:  510): 0.01449934\n",
            "Loss (epoch:  511): 0.01447882\n",
            "Loss (epoch:  512): 0.01445827\n",
            "Loss (epoch:  513): 0.01443769\n",
            "Loss (epoch:  514): 0.01441707\n",
            "Loss (epoch:  515): 0.01439642\n",
            "Loss (epoch:  516): 0.01437574\n",
            "Loss (epoch:  517): 0.01435503\n",
            "Loss (epoch:  518): 0.01433428\n",
            "Loss (epoch:  519): 0.01431351\n",
            "Loss (epoch:  520): 0.01429270\n",
            "Loss (epoch:  521): 0.01427187\n",
            "Loss (epoch:  522): 0.01425101\n",
            "Loss (epoch:  523): 0.01423012\n",
            "Loss (epoch:  524): 0.01420920\n",
            "Loss (epoch:  525): 0.01418826\n",
            "Loss (epoch:  526): 0.01416729\n",
            "Loss (epoch:  527): 0.01414629\n",
            "Loss (epoch:  528): 0.01412526\n",
            "Loss (epoch:  529): 0.01410422\n",
            "Loss (epoch:  530): 0.01408314\n",
            "Loss (epoch:  531): 0.01406205\n",
            "Loss (epoch:  532): 0.01404093\n",
            "Loss (epoch:  533): 0.01401979\n",
            "Loss (epoch:  534): 0.01399862\n",
            "Loss (epoch:  535): 0.01397744\n",
            "Loss (epoch:  536): 0.01395624\n",
            "Loss (epoch:  537): 0.01393501\n",
            "Loss (epoch:  538): 0.01391377\n",
            "Loss (epoch:  539): 0.01389251\n",
            "Loss (epoch:  540): 0.01387123\n",
            "Loss (epoch:  541): 0.01384993\n",
            "Loss (epoch:  542): 0.01382862\n",
            "Loss (epoch:  543): 0.01380729\n",
            "Loss (epoch:  544): 0.01378595\n",
            "Loss (epoch:  545): 0.01376458\n",
            "Loss (epoch:  546): 0.01374321\n",
            "Loss (epoch:  547): 0.01372182\n",
            "Loss (epoch:  548): 0.01370042\n",
            "Loss (epoch:  549): 0.01367901\n",
            "Loss (epoch:  550): 0.01365758\n",
            "Loss (epoch:  551): 0.01363615\n",
            "Loss (epoch:  552): 0.01361470\n",
            "Loss (epoch:  553): 0.01359325\n",
            "Loss (epoch:  554): 0.01357178\n",
            "Loss (epoch:  555): 0.01355031\n",
            "Loss (epoch:  556): 0.01352883\n",
            "Loss (epoch:  557): 0.01350734\n",
            "Loss (epoch:  558): 0.01348585\n",
            "Loss (epoch:  559): 0.01346435\n",
            "Loss (epoch:  560): 0.01344284\n",
            "Loss (epoch:  561): 0.01342134\n",
            "Loss (epoch:  562): 0.01339983\n",
            "Loss (epoch:  563): 0.01337831\n",
            "Loss (epoch:  564): 0.01335679\n",
            "Loss (epoch:  565): 0.01333527\n",
            "Loss (epoch:  566): 0.01331375\n",
            "Loss (epoch:  567): 0.01329223\n",
            "Loss (epoch:  568): 0.01327071\n",
            "Loss (epoch:  569): 0.01324919\n",
            "Loss (epoch:  570): 0.01322767\n",
            "Loss (epoch:  571): 0.01320616\n",
            "Loss (epoch:  572): 0.01318465\n",
            "Loss (epoch:  573): 0.01316314\n",
            "Loss (epoch:  574): 0.01314163\n",
            "Loss (epoch:  575): 0.01312013\n",
            "Loss (epoch:  576): 0.01309864\n",
            "Loss (epoch:  577): 0.01307715\n",
            "Loss (epoch:  578): 0.01305567\n",
            "Loss (epoch:  579): 0.01303420\n",
            "Loss (epoch:  580): 0.01301273\n",
            "Loss (epoch:  581): 0.01299127\n",
            "Loss (epoch:  582): 0.01296982\n",
            "Loss (epoch:  583): 0.01294839\n",
            "Loss (epoch:  584): 0.01292696\n",
            "Loss (epoch:  585): 0.01290554\n",
            "Loss (epoch:  586): 0.01288414\n",
            "Loss (epoch:  587): 0.01286275\n",
            "Loss (epoch:  588): 0.01284137\n",
            "Loss (epoch:  589): 0.01282000\n",
            "Loss (epoch:  590): 0.01279865\n",
            "Loss (epoch:  591): 0.01277732\n",
            "Loss (epoch:  592): 0.01275600\n",
            "Loss (epoch:  593): 0.01273469\n",
            "Loss (epoch:  594): 0.01271340\n",
            "Loss (epoch:  595): 0.01269213\n",
            "Loss (epoch:  596): 0.01267088\n",
            "Loss (epoch:  597): 0.01264965\n",
            "Loss (epoch:  598): 0.01262843\n",
            "Loss (epoch:  599): 0.01260724\n",
            "Loss (epoch:  600): 0.01258606\n",
            "Loss (epoch:  601): 0.01256490\n",
            "Loss (epoch:  602): 0.01254377\n",
            "Loss (epoch:  603): 0.01252266\n",
            "Loss (epoch:  604): 0.01250157\n",
            "Loss (epoch:  605): 0.01248050\n",
            "Loss (epoch:  606): 0.01245946\n",
            "Loss (epoch:  607): 0.01243844\n",
            "Loss (epoch:  608): 0.01241745\n",
            "Loss (epoch:  609): 0.01239647\n",
            "Loss (epoch:  610): 0.01237554\n",
            "Loss (epoch:  611): 0.01235462\n",
            "Loss (epoch:  612): 0.01233372\n",
            "Loss (epoch:  613): 0.01231286\n",
            "Loss (epoch:  614): 0.01229202\n",
            "Loss (epoch:  615): 0.01227121\n",
            "Loss (epoch:  616): 0.01225043\n",
            "Loss (epoch:  617): 0.01222967\n",
            "Loss (epoch:  618): 0.01220895\n",
            "Loss (epoch:  619): 0.01218826\n",
            "Loss (epoch:  620): 0.01216760\n",
            "Loss (epoch:  621): 0.01214696\n",
            "Loss (epoch:  622): 0.01212636\n",
            "Loss (epoch:  623): 0.01210580\n",
            "Loss (epoch:  624): 0.01208526\n",
            "Loss (epoch:  625): 0.01206475\n",
            "Loss (epoch:  626): 0.01204428\n",
            "Loss (epoch:  627): 0.01202384\n",
            "Loss (epoch:  628): 0.01200343\n",
            "Loss (epoch:  629): 0.01198306\n",
            "Loss (epoch:  630): 0.01196272\n",
            "Loss (epoch:  631): 0.01194242\n",
            "Loss (epoch:  632): 0.01192216\n",
            "Loss (epoch:  633): 0.01190192\n",
            "Loss (epoch:  634): 0.01188173\n",
            "Loss (epoch:  635): 0.01186157\n",
            "Loss (epoch:  636): 0.01184144\n",
            "Loss (epoch:  637): 0.01182136\n",
            "Loss (epoch:  638): 0.01180131\n",
            "Loss (epoch:  639): 0.01178130\n",
            "Loss (epoch:  640): 0.01176132\n",
            "Loss (epoch:  641): 0.01174139\n",
            "Loss (epoch:  642): 0.01172149\n",
            "Loss (epoch:  643): 0.01170164\n",
            "Loss (epoch:  644): 0.01168182\n",
            "Loss (epoch:  645): 0.01166203\n",
            "Loss (epoch:  646): 0.01164230\n",
            "Loss (epoch:  647): 0.01162260\n",
            "Loss (epoch:  648): 0.01160294\n",
            "Loss (epoch:  649): 0.01158332\n",
            "Loss (epoch:  650): 0.01156374\n",
            "Loss (epoch:  651): 0.01154421\n",
            "Loss (epoch:  652): 0.01152471\n",
            "Loss (epoch:  653): 0.01150526\n",
            "Loss (epoch:  654): 0.01148585\n",
            "Loss (epoch:  655): 0.01146647\n",
            "Loss (epoch:  656): 0.01144715\n",
            "Loss (epoch:  657): 0.01142786\n",
            "Loss (epoch:  658): 0.01140862\n",
            "Loss (epoch:  659): 0.01138942\n",
            "Loss (epoch:  660): 0.01137027\n",
            "Loss (epoch:  661): 0.01135115\n",
            "Loss (epoch:  662): 0.01133208\n",
            "Loss (epoch:  663): 0.01131306\n",
            "Loss (epoch:  664): 0.01129408\n",
            "Loss (epoch:  665): 0.01127514\n",
            "Loss (epoch:  666): 0.01125625\n",
            "Loss (epoch:  667): 0.01123740\n",
            "Loss (epoch:  668): 0.01121860\n",
            "Loss (epoch:  669): 0.01119984\n",
            "Loss (epoch:  670): 0.01118112\n",
            "Loss (epoch:  671): 0.01116245\n",
            "Loss (epoch:  672): 0.01114383\n",
            "Loss (epoch:  673): 0.01112525\n",
            "Loss (epoch:  674): 0.01110672\n",
            "Loss (epoch:  675): 0.01108823\n",
            "Loss (epoch:  676): 0.01106979\n",
            "Loss (epoch:  677): 0.01105139\n",
            "Loss (epoch:  678): 0.01103305\n",
            "Loss (epoch:  679): 0.01101474\n",
            "Loss (epoch:  680): 0.01099649\n",
            "Loss (epoch:  681): 0.01097827\n",
            "Loss (epoch:  682): 0.01096011\n",
            "Loss (epoch:  683): 0.01094199\n",
            "Loss (epoch:  684): 0.01092393\n",
            "Loss (epoch:  685): 0.01090590\n",
            "Loss (epoch:  686): 0.01088793\n",
            "Loss (epoch:  687): 0.01087000\n",
            "Loss (epoch:  688): 0.01085212\n",
            "Loss (epoch:  689): 0.01083428\n",
            "Loss (epoch:  690): 0.01081649\n",
            "Loss (epoch:  691): 0.01079876\n",
            "Loss (epoch:  692): 0.01078106\n",
            "Loss (epoch:  693): 0.01076342\n",
            "Loss (epoch:  694): 0.01074582\n",
            "Loss (epoch:  695): 0.01072827\n",
            "Loss (epoch:  696): 0.01071077\n",
            "Loss (epoch:  697): 0.01069332\n",
            "Loss (epoch:  698): 0.01067591\n",
            "Loss (epoch:  699): 0.01065856\n",
            "Loss (epoch:  700): 0.01064125\n",
            "Loss (epoch:  701): 0.01062399\n",
            "Loss (epoch:  702): 0.01060677\n",
            "Loss (epoch:  703): 0.01058961\n",
            "Loss (epoch:  704): 0.01057249\n",
            "Loss (epoch:  705): 0.01055542\n",
            "Loss (epoch:  706): 0.01053841\n",
            "Loss (epoch:  707): 0.01052144\n",
            "Loss (epoch:  708): 0.01050451\n",
            "Loss (epoch:  709): 0.01048764\n",
            "Loss (epoch:  710): 0.01047082\n",
            "Loss (epoch:  711): 0.01045404\n",
            "Loss (epoch:  712): 0.01043731\n",
            "Loss (epoch:  713): 0.01042064\n",
            "Loss (epoch:  714): 0.01040401\n",
            "Loss (epoch:  715): 0.01038742\n",
            "Loss (epoch:  716): 0.01037089\n",
            "Loss (epoch:  717): 0.01035441\n",
            "Loss (epoch:  718): 0.01033797\n",
            "Loss (epoch:  719): 0.01032159\n",
            "Loss (epoch:  720): 0.01030525\n",
            "Loss (epoch:  721): 0.01028897\n",
            "Loss (epoch:  722): 0.01027273\n",
            "Loss (epoch:  723): 0.01025654\n",
            "Loss (epoch:  724): 0.01024040\n",
            "Loss (epoch:  725): 0.01022431\n",
            "Loss (epoch:  726): 0.01020827\n",
            "Loss (epoch:  727): 0.01019228\n",
            "Loss (epoch:  728): 0.01017633\n",
            "Loss (epoch:  729): 0.01016044\n",
            "Loss (epoch:  730): 0.01014459\n",
            "Loss (epoch:  731): 0.01012880\n",
            "Loss (epoch:  732): 0.01011305\n",
            "Loss (epoch:  733): 0.01009736\n",
            "Loss (epoch:  734): 0.01008171\n",
            "Loss (epoch:  735): 0.01006611\n",
            "Loss (epoch:  736): 0.01005056\n",
            "Loss (epoch:  737): 0.01003507\n",
            "Loss (epoch:  738): 0.01001962\n",
            "Loss (epoch:  739): 0.01000422\n",
            "Loss (epoch:  740): 0.00998887\n",
            "Loss (epoch:  741): 0.00997357\n",
            "Loss (epoch:  742): 0.00995832\n",
            "Loss (epoch:  743): 0.00994311\n",
            "Loss (epoch:  744): 0.00992796\n",
            "Loss (epoch:  745): 0.00991286\n",
            "Loss (epoch:  746): 0.00989781\n",
            "Loss (epoch:  747): 0.00988280\n",
            "Loss (epoch:  748): 0.00986785\n",
            "Loss (epoch:  749): 0.00985294\n",
            "Loss (epoch:  750): 0.00983809\n",
            "Loss (epoch:  751): 0.00982329\n",
            "Loss (epoch:  752): 0.00980853\n",
            "Loss (epoch:  753): 0.00979383\n",
            "Loss (epoch:  754): 0.00977917\n",
            "Loss (epoch:  755): 0.00976456\n",
            "Loss (epoch:  756): 0.00975001\n",
            "Loss (epoch:  757): 0.00973550\n",
            "Loss (epoch:  758): 0.00972104\n",
            "Loss (epoch:  759): 0.00970663\n",
            "Loss (epoch:  760): 0.00969227\n",
            "Loss (epoch:  761): 0.00967796\n",
            "Loss (epoch:  762): 0.00966371\n",
            "Loss (epoch:  763): 0.00964950\n",
            "Loss (epoch:  764): 0.00963534\n",
            "Loss (epoch:  765): 0.00962123\n",
            "Loss (epoch:  766): 0.00960717\n",
            "Loss (epoch:  767): 0.00959316\n",
            "Loss (epoch:  768): 0.00957919\n",
            "Loss (epoch:  769): 0.00956528\n",
            "Loss (epoch:  770): 0.00955142\n",
            "Loss (epoch:  771): 0.00953761\n",
            "Loss (epoch:  772): 0.00952385\n",
            "Loss (epoch:  773): 0.00951013\n",
            "Loss (epoch:  774): 0.00949647\n",
            "Loss (epoch:  775): 0.00948285\n",
            "Loss (epoch:  776): 0.00946929\n",
            "Loss (epoch:  777): 0.00945577\n",
            "Loss (epoch:  778): 0.00944231\n",
            "Loss (epoch:  779): 0.00942889\n",
            "Loss (epoch:  780): 0.00941552\n",
            "Loss (epoch:  781): 0.00940221\n",
            "Loss (epoch:  782): 0.00938894\n",
            "Loss (epoch:  783): 0.00937572\n",
            "Loss (epoch:  784): 0.00936255\n",
            "Loss (epoch:  785): 0.00934943\n",
            "Loss (epoch:  786): 0.00933636\n",
            "Loss (epoch:  787): 0.00932333\n",
            "Loss (epoch:  788): 0.00931036\n",
            "Loss (epoch:  789): 0.00929744\n",
            "Loss (epoch:  790): 0.00928456\n",
            "Loss (epoch:  791): 0.00927174\n",
            "Loss (epoch:  792): 0.00925896\n",
            "Loss (epoch:  793): 0.00924624\n",
            "Loss (epoch:  794): 0.00923356\n",
            "Loss (epoch:  795): 0.00922093\n",
            "Loss (epoch:  796): 0.00920835\n",
            "Loss (epoch:  797): 0.00919582\n",
            "Loss (epoch:  798): 0.00918333\n",
            "Loss (epoch:  799): 0.00917090\n",
            "Loss (epoch:  800): 0.00915852\n",
            "Loss (epoch:  801): 0.00914618\n",
            "Loss (epoch:  802): 0.00913389\n",
            "Loss (epoch:  803): 0.00912165\n",
            "Loss (epoch:  804): 0.00910946\n",
            "Loss (epoch:  805): 0.00909732\n",
            "Loss (epoch:  806): 0.00908522\n",
            "Loss (epoch:  807): 0.00907318\n",
            "Loss (epoch:  808): 0.00906118\n",
            "Loss (epoch:  809): 0.00904923\n",
            "Loss (epoch:  810): 0.00903733\n",
            "Loss (epoch:  811): 0.00902548\n",
            "Loss (epoch:  812): 0.00901367\n",
            "Loss (epoch:  813): 0.00900192\n",
            "Loss (epoch:  814): 0.00899021\n",
            "Loss (epoch:  815): 0.00897854\n",
            "Loss (epoch:  816): 0.00896693\n",
            "Loss (epoch:  817): 0.00895537\n",
            "Loss (epoch:  818): 0.00894385\n",
            "Loss (epoch:  819): 0.00893237\n",
            "Loss (epoch:  820): 0.00892095\n",
            "Loss (epoch:  821): 0.00890957\n",
            "Loss (epoch:  822): 0.00889825\n",
            "Loss (epoch:  823): 0.00888696\n",
            "Loss (epoch:  824): 0.00887573\n",
            "Loss (epoch:  825): 0.00886454\n",
            "Loss (epoch:  826): 0.00885340\n",
            "Loss (epoch:  827): 0.00884230\n",
            "Loss (epoch:  828): 0.00883126\n",
            "Loss (epoch:  829): 0.00882025\n",
            "Loss (epoch:  830): 0.00880930\n",
            "Loss (epoch:  831): 0.00879839\n",
            "Loss (epoch:  832): 0.00878753\n",
            "Loss (epoch:  833): 0.00877671\n",
            "Loss (epoch:  834): 0.00876594\n",
            "Loss (epoch:  835): 0.00875522\n",
            "Loss (epoch:  836): 0.00874454\n",
            "Loss (epoch:  837): 0.00873391\n",
            "Loss (epoch:  838): 0.00872332\n",
            "Loss (epoch:  839): 0.00871278\n",
            "Loss (epoch:  840): 0.00870228\n",
            "Loss (epoch:  841): 0.00869183\n",
            "Loss (epoch:  842): 0.00868142\n",
            "Loss (epoch:  843): 0.00867106\n",
            "Loss (epoch:  844): 0.00866075\n",
            "Loss (epoch:  845): 0.00865047\n",
            "Loss (epoch:  846): 0.00864025\n",
            "Loss (epoch:  847): 0.00863006\n",
            "Loss (epoch:  848): 0.00861993\n",
            "Loss (epoch:  849): 0.00860983\n",
            "Loss (epoch:  850): 0.00859978\n",
            "Loss (epoch:  851): 0.00858977\n",
            "Loss (epoch:  852): 0.00857981\n",
            "Loss (epoch:  853): 0.00856989\n",
            "Loss (epoch:  854): 0.00856002\n",
            "Loss (epoch:  855): 0.00855018\n",
            "Loss (epoch:  856): 0.00854040\n",
            "Loss (epoch:  857): 0.00853065\n",
            "Loss (epoch:  858): 0.00852095\n",
            "Loss (epoch:  859): 0.00851129\n",
            "Loss (epoch:  860): 0.00850167\n",
            "Loss (epoch:  861): 0.00849209\n",
            "Loss (epoch:  862): 0.00848256\n",
            "Loss (epoch:  863): 0.00847307\n",
            "Loss (epoch:  864): 0.00846362\n",
            "Loss (epoch:  865): 0.00845422\n",
            "Loss (epoch:  866): 0.00844485\n",
            "Loss (epoch:  867): 0.00843553\n",
            "Loss (epoch:  868): 0.00842624\n",
            "Loss (epoch:  869): 0.00841700\n",
            "Loss (epoch:  870): 0.00840780\n",
            "Loss (epoch:  871): 0.00839864\n",
            "Loss (epoch:  872): 0.00838952\n",
            "Loss (epoch:  873): 0.00838044\n",
            "Loss (epoch:  874): 0.00837141\n",
            "Loss (epoch:  875): 0.00836241\n",
            "Loss (epoch:  876): 0.00835345\n",
            "Loss (epoch:  877): 0.00834453\n",
            "Loss (epoch:  878): 0.00833565\n",
            "Loss (epoch:  879): 0.00832681\n",
            "Loss (epoch:  880): 0.00831801\n",
            "Loss (epoch:  881): 0.00830925\n",
            "Loss (epoch:  882): 0.00830053\n",
            "Loss (epoch:  883): 0.00829185\n",
            "Loss (epoch:  884): 0.00828320\n",
            "Loss (epoch:  885): 0.00827459\n",
            "Loss (epoch:  886): 0.00826602\n",
            "Loss (epoch:  887): 0.00825749\n",
            "Loss (epoch:  888): 0.00824900\n",
            "Loss (epoch:  889): 0.00824055\n",
            "Loss (epoch:  890): 0.00823213\n",
            "Loss (epoch:  891): 0.00822374\n",
            "Loss (epoch:  892): 0.00821540\n",
            "Loss (epoch:  893): 0.00820709\n",
            "Loss (epoch:  894): 0.00819882\n",
            "Loss (epoch:  895): 0.00819059\n",
            "Loss (epoch:  896): 0.00818239\n",
            "Loss (epoch:  897): 0.00817423\n",
            "Loss (epoch:  898): 0.00816610\n",
            "Loss (epoch:  899): 0.00815801\n",
            "Loss (epoch:  900): 0.00814995\n",
            "Loss (epoch:  901): 0.00814193\n",
            "Loss (epoch:  902): 0.00813395\n",
            "Loss (epoch:  903): 0.00812600\n",
            "Loss (epoch:  904): 0.00811808\n",
            "Loss (epoch:  905): 0.00811020\n",
            "Loss (epoch:  906): 0.00810235\n",
            "Loss (epoch:  907): 0.00809454\n",
            "Loss (epoch:  908): 0.00808676\n",
            "Loss (epoch:  909): 0.00807901\n",
            "Loss (epoch:  910): 0.00807130\n",
            "Loss (epoch:  911): 0.00806362\n",
            "Loss (epoch:  912): 0.00805597\n",
            "Loss (epoch:  913): 0.00804836\n",
            "Loss (epoch:  914): 0.00804077\n",
            "Loss (epoch:  915): 0.00803322\n",
            "Loss (epoch:  916): 0.00802571\n",
            "Loss (epoch:  917): 0.00801822\n",
            "Loss (epoch:  918): 0.00801076\n",
            "Loss (epoch:  919): 0.00800334\n",
            "Loss (epoch:  920): 0.00799595\n",
            "Loss (epoch:  921): 0.00798859\n",
            "Loss (epoch:  922): 0.00798126\n",
            "Loss (epoch:  923): 0.00797396\n",
            "Loss (epoch:  924): 0.00796669\n",
            "Loss (epoch:  925): 0.00795945\n",
            "Loss (epoch:  926): 0.00795224\n",
            "Loss (epoch:  927): 0.00794506\n",
            "Loss (epoch:  928): 0.00793791\n",
            "Loss (epoch:  929): 0.00793079\n",
            "Loss (epoch:  930): 0.00792370\n",
            "Loss (epoch:  931): 0.00791663\n",
            "Loss (epoch:  932): 0.00790960\n",
            "Loss (epoch:  933): 0.00790259\n",
            "Loss (epoch:  934): 0.00789561\n",
            "Loss (epoch:  935): 0.00788867\n",
            "Loss (epoch:  936): 0.00788174\n",
            "Loss (epoch:  937): 0.00787485\n",
            "Loss (epoch:  938): 0.00786798\n",
            "Loss (epoch:  939): 0.00786114\n",
            "Loss (epoch:  940): 0.00785433\n",
            "Loss (epoch:  941): 0.00784754\n",
            "Loss (epoch:  942): 0.00784078\n",
            "Loss (epoch:  943): 0.00783404\n",
            "Loss (epoch:  944): 0.00782733\n",
            "Loss (epoch:  945): 0.00782065\n",
            "Loss (epoch:  946): 0.00781399\n",
            "Loss (epoch:  947): 0.00780736\n",
            "Loss (epoch:  948): 0.00780075\n",
            "Loss (epoch:  949): 0.00779417\n",
            "Loss (epoch:  950): 0.00778761\n",
            "Loss (epoch:  951): 0.00778108\n",
            "Loss (epoch:  952): 0.00777457\n",
            "Loss (epoch:  953): 0.00776809\n",
            "Loss (epoch:  954): 0.00776163\n",
            "Loss (epoch:  955): 0.00775519\n",
            "Loss (epoch:  956): 0.00774877\n",
            "Loss (epoch:  957): 0.00774238\n",
            "Loss (epoch:  958): 0.00773601\n",
            "Loss (epoch:  959): 0.00772967\n",
            "Loss (epoch:  960): 0.00772335\n",
            "Loss (epoch:  961): 0.00771705\n",
            "Loss (epoch:  962): 0.00771077\n",
            "Loss (epoch:  963): 0.00770451\n",
            "Loss (epoch:  964): 0.00769828\n",
            "Loss (epoch:  965): 0.00769206\n",
            "Loss (epoch:  966): 0.00768587\n",
            "Loss (epoch:  967): 0.00767970\n",
            "Loss (epoch:  968): 0.00767355\n",
            "Loss (epoch:  969): 0.00766742\n",
            "Loss (epoch:  970): 0.00766131\n",
            "Loss (epoch:  971): 0.00765522\n",
            "Loss (epoch:  972): 0.00764915\n",
            "Loss (epoch:  973): 0.00764310\n",
            "Loss (epoch:  974): 0.00763707\n",
            "Loss (epoch:  975): 0.00763106\n",
            "Loss (epoch:  976): 0.00762507\n",
            "Loss (epoch:  977): 0.00761909\n",
            "Loss (epoch:  978): 0.00761314\n",
            "Loss (epoch:  979): 0.00760720\n",
            "Loss (epoch:  980): 0.00760129\n",
            "Loss (epoch:  981): 0.00759539\n",
            "Loss (epoch:  982): 0.00758951\n",
            "Loss (epoch:  983): 0.00758364\n",
            "Loss (epoch:  984): 0.00757780\n",
            "Loss (epoch:  985): 0.00757197\n",
            "Loss (epoch:  986): 0.00756616\n",
            "Loss (epoch:  987): 0.00756036\n",
            "Loss (epoch:  988): 0.00755458\n",
            "Loss (epoch:  989): 0.00754882\n",
            "Loss (epoch:  990): 0.00754307\n",
            "Loss (epoch:  991): 0.00753735\n",
            "Loss (epoch:  992): 0.00753163\n",
            "Loss (epoch:  993): 0.00752594\n",
            "Loss (epoch:  994): 0.00752026\n",
            "Loss (epoch:  995): 0.00751459\n",
            "Loss (epoch:  996): 0.00750893\n",
            "Loss (epoch:  997): 0.00750330\n",
            "Loss (epoch:  998): 0.00749768\n",
            "Loss (epoch:  999): 0.00749207\n",
            "Loss (epoch: 1000): 0.00748648\n",
            "Loss (epoch: 1001): 0.00748090\n",
            "Loss (epoch: 1002): 0.00747533\n",
            "Loss (epoch: 1003): 0.00746978\n",
            "Loss (epoch: 1004): 0.00746425\n",
            "Loss (epoch: 1005): 0.00745872\n",
            "Loss (epoch: 1006): 0.00745321\n",
            "Loss (epoch: 1007): 0.00744772\n",
            "Loss (epoch: 1008): 0.00744223\n",
            "Loss (epoch: 1009): 0.00743676\n",
            "Loss (epoch: 1010): 0.00743130\n",
            "Loss (epoch: 1011): 0.00742585\n",
            "Loss (epoch: 1012): 0.00742042\n",
            "Loss (epoch: 1013): 0.00741500\n",
            "Loss (epoch: 1014): 0.00740959\n",
            "Loss (epoch: 1015): 0.00740419\n",
            "Loss (epoch: 1016): 0.00739880\n",
            "Loss (epoch: 1017): 0.00739342\n",
            "Loss (epoch: 1018): 0.00738806\n",
            "Loss (epoch: 1019): 0.00738270\n",
            "Loss (epoch: 1020): 0.00737736\n",
            "Loss (epoch: 1021): 0.00737203\n",
            "Loss (epoch: 1022): 0.00736671\n",
            "Loss (epoch: 1023): 0.00736139\n",
            "Loss (epoch: 1024): 0.00735609\n",
            "Loss (epoch: 1025): 0.00735080\n",
            "Loss (epoch: 1026): 0.00734552\n",
            "Loss (epoch: 1027): 0.00734025\n",
            "Loss (epoch: 1028): 0.00733498\n",
            "Loss (epoch: 1029): 0.00732973\n",
            "Loss (epoch: 1030): 0.00732449\n",
            "Loss (epoch: 1031): 0.00731925\n",
            "Loss (epoch: 1032): 0.00731403\n",
            "Loss (epoch: 1033): 0.00730881\n",
            "Loss (epoch: 1034): 0.00730360\n",
            "Loss (epoch: 1035): 0.00729840\n",
            "Loss (epoch: 1036): 0.00729321\n",
            "Loss (epoch: 1037): 0.00728802\n",
            "Loss (epoch: 1038): 0.00728285\n",
            "Loss (epoch: 1039): 0.00727768\n",
            "Loss (epoch: 1040): 0.00727252\n",
            "Loss (epoch: 1041): 0.00726736\n",
            "Loss (epoch: 1042): 0.00726222\n",
            "Loss (epoch: 1043): 0.00725708\n",
            "Loss (epoch: 1044): 0.00725195\n",
            "Loss (epoch: 1045): 0.00724682\n",
            "Loss (epoch: 1046): 0.00724171\n",
            "Loss (epoch: 1047): 0.00723659\n",
            "Loss (epoch: 1048): 0.00723149\n",
            "Loss (epoch: 1049): 0.00722639\n",
            "Loss (epoch: 1050): 0.00722130\n",
            "Loss (epoch: 1051): 0.00721621\n",
            "Loss (epoch: 1052): 0.00721113\n",
            "Loss (epoch: 1053): 0.00720606\n",
            "Loss (epoch: 1054): 0.00720099\n",
            "Loss (epoch: 1055): 0.00719593\n",
            "Loss (epoch: 1056): 0.00719087\n",
            "Loss (epoch: 1057): 0.00718582\n",
            "Loss (epoch: 1058): 0.00718077\n",
            "Loss (epoch: 1059): 0.00717573\n",
            "Loss (epoch: 1060): 0.00717069\n",
            "Loss (epoch: 1061): 0.00716566\n",
            "Loss (epoch: 1062): 0.00716063\n",
            "Loss (epoch: 1063): 0.00715561\n",
            "Loss (epoch: 1064): 0.00715059\n",
            "Loss (epoch: 1065): 0.00714557\n",
            "Loss (epoch: 1066): 0.00714056\n",
            "Loss (epoch: 1067): 0.00713556\n",
            "Loss (epoch: 1068): 0.00713056\n",
            "Loss (epoch: 1069): 0.00712556\n",
            "Loss (epoch: 1070): 0.00712056\n",
            "Loss (epoch: 1071): 0.00711557\n",
            "Loss (epoch: 1072): 0.00711058\n",
            "Loss (epoch: 1073): 0.00710560\n",
            "Loss (epoch: 1074): 0.00710061\n",
            "Loss (epoch: 1075): 0.00709564\n",
            "Loss (epoch: 1076): 0.00709066\n",
            "Loss (epoch: 1077): 0.00708569\n",
            "Loss (epoch: 1078): 0.00708072\n",
            "Loss (epoch: 1079): 0.00707575\n",
            "Loss (epoch: 1080): 0.00707079\n",
            "Loss (epoch: 1081): 0.00706583\n",
            "Loss (epoch: 1082): 0.00706087\n",
            "Loss (epoch: 1083): 0.00705591\n",
            "Loss (epoch: 1084): 0.00705096\n",
            "Loss (epoch: 1085): 0.00704600\n",
            "Loss (epoch: 1086): 0.00704105\n",
            "Loss (epoch: 1087): 0.00703610\n",
            "Loss (epoch: 1088): 0.00703116\n",
            "Loss (epoch: 1089): 0.00702621\n",
            "Loss (epoch: 1090): 0.00702126\n",
            "Loss (epoch: 1091): 0.00701632\n",
            "Loss (epoch: 1092): 0.00701138\n",
            "Loss (epoch: 1093): 0.00700644\n",
            "Loss (epoch: 1094): 0.00700150\n",
            "Loss (epoch: 1095): 0.00699656\n",
            "Loss (epoch: 1096): 0.00699162\n",
            "Loss (epoch: 1097): 0.00698669\n",
            "Loss (epoch: 1098): 0.00698175\n",
            "Loss (epoch: 1099): 0.00697682\n",
            "Loss (epoch: 1100): 0.00697188\n",
            "Loss (epoch: 1101): 0.00696695\n",
            "Loss (epoch: 1102): 0.00696202\n",
            "Loss (epoch: 1103): 0.00695708\n",
            "Loss (epoch: 1104): 0.00695215\n",
            "Loss (epoch: 1105): 0.00694722\n",
            "Loss (epoch: 1106): 0.00694229\n",
            "Loss (epoch: 1107): 0.00693736\n",
            "Loss (epoch: 1108): 0.00693242\n",
            "Loss (epoch: 1109): 0.00692749\n",
            "Loss (epoch: 1110): 0.00692256\n",
            "Loss (epoch: 1111): 0.00691763\n",
            "Loss (epoch: 1112): 0.00691269\n",
            "Loss (epoch: 1113): 0.00690776\n",
            "Loss (epoch: 1114): 0.00690282\n",
            "Loss (epoch: 1115): 0.00689789\n",
            "Loss (epoch: 1116): 0.00689295\n",
            "Loss (epoch: 1117): 0.00688802\n",
            "Loss (epoch: 1118): 0.00688308\n",
            "Loss (epoch: 1119): 0.00687814\n",
            "Loss (epoch: 1120): 0.00687321\n",
            "Loss (epoch: 1121): 0.00686827\n",
            "Loss (epoch: 1122): 0.00686333\n",
            "Loss (epoch: 1123): 0.00685838\n",
            "Loss (epoch: 1124): 0.00685344\n",
            "Loss (epoch: 1125): 0.00684849\n",
            "Loss (epoch: 1126): 0.00684355\n",
            "Loss (epoch: 1127): 0.00683860\n",
            "Loss (epoch: 1128): 0.00683365\n",
            "Loss (epoch: 1129): 0.00682870\n",
            "Loss (epoch: 1130): 0.00682375\n",
            "Loss (epoch: 1131): 0.00681879\n",
            "Loss (epoch: 1132): 0.00681384\n",
            "Loss (epoch: 1133): 0.00680888\n",
            "Loss (epoch: 1134): 0.00680392\n",
            "Loss (epoch: 1135): 0.00679896\n",
            "Loss (epoch: 1136): 0.00679400\n",
            "Loss (epoch: 1137): 0.00678903\n",
            "Loss (epoch: 1138): 0.00678406\n",
            "Loss (epoch: 1139): 0.00677910\n",
            "Loss (epoch: 1140): 0.00677412\n",
            "Loss (epoch: 1141): 0.00676915\n",
            "Loss (epoch: 1142): 0.00676417\n",
            "Loss (epoch: 1143): 0.00675919\n",
            "Loss (epoch: 1144): 0.00675421\n",
            "Loss (epoch: 1145): 0.00674923\n",
            "Loss (epoch: 1146): 0.00674424\n",
            "Loss (epoch: 1147): 0.00673925\n",
            "Loss (epoch: 1148): 0.00673426\n",
            "Loss (epoch: 1149): 0.00672926\n",
            "Loss (epoch: 1150): 0.00672426\n",
            "Loss (epoch: 1151): 0.00671927\n",
            "Loss (epoch: 1152): 0.00671426\n",
            "Loss (epoch: 1153): 0.00670926\n",
            "Loss (epoch: 1154): 0.00670425\n",
            "Loss (epoch: 1155): 0.00669923\n",
            "Loss (epoch: 1156): 0.00669422\n",
            "Loss (epoch: 1157): 0.00668920\n",
            "Loss (epoch: 1158): 0.00668418\n",
            "Loss (epoch: 1159): 0.00667915\n",
            "Loss (epoch: 1160): 0.00667413\n",
            "Loss (epoch: 1161): 0.00666909\n",
            "Loss (epoch: 1162): 0.00666406\n",
            "Loss (epoch: 1163): 0.00665902\n",
            "Loss (epoch: 1164): 0.00665398\n",
            "Loss (epoch: 1165): 0.00664894\n",
            "Loss (epoch: 1166): 0.00664388\n",
            "Loss (epoch: 1167): 0.00663884\n",
            "Loss (epoch: 1168): 0.00663378\n",
            "Loss (epoch: 1169): 0.00662872\n",
            "Loss (epoch: 1170): 0.00662366\n",
            "Loss (epoch: 1171): 0.00661859\n",
            "Loss (epoch: 1172): 0.00661352\n",
            "Loss (epoch: 1173): 0.00660844\n",
            "Loss (epoch: 1174): 0.00660337\n",
            "Loss (epoch: 1175): 0.00659829\n",
            "Loss (epoch: 1176): 0.00659320\n",
            "Loss (epoch: 1177): 0.00658811\n",
            "Loss (epoch: 1178): 0.00658301\n",
            "Loss (epoch: 1179): 0.00657792\n",
            "Loss (epoch: 1180): 0.00657282\n",
            "Loss (epoch: 1181): 0.00656771\n",
            "Loss (epoch: 1182): 0.00656260\n",
            "Loss (epoch: 1183): 0.00655749\n",
            "Loss (epoch: 1184): 0.00655237\n",
            "Loss (epoch: 1185): 0.00654725\n",
            "Loss (epoch: 1186): 0.00654212\n",
            "Loss (epoch: 1187): 0.00653699\n",
            "Loss (epoch: 1188): 0.00653185\n",
            "Loss (epoch: 1189): 0.00652671\n",
            "Loss (epoch: 1190): 0.00652156\n",
            "Loss (epoch: 1191): 0.00651642\n",
            "Loss (epoch: 1192): 0.00651127\n",
            "Loss (epoch: 1193): 0.00650611\n",
            "Loss (epoch: 1194): 0.00650094\n",
            "Loss (epoch: 1195): 0.00649578\n",
            "Loss (epoch: 1196): 0.00649061\n",
            "Loss (epoch: 1197): 0.00648543\n",
            "Loss (epoch: 1198): 0.00648025\n",
            "Loss (epoch: 1199): 0.00647507\n",
            "Loss (epoch: 1200): 0.00646988\n",
            "Loss (epoch: 1201): 0.00646468\n",
            "Loss (epoch: 1202): 0.00645949\n",
            "Loss (epoch: 1203): 0.00645428\n",
            "Loss (epoch: 1204): 0.00644907\n",
            "Loss (epoch: 1205): 0.00644386\n",
            "Loss (epoch: 1206): 0.00643864\n",
            "Loss (epoch: 1207): 0.00643342\n",
            "Loss (epoch: 1208): 0.00642819\n",
            "Loss (epoch: 1209): 0.00642296\n",
            "Loss (epoch: 1210): 0.00641772\n",
            "Loss (epoch: 1211): 0.00641248\n",
            "Loss (epoch: 1212): 0.00640723\n",
            "Loss (epoch: 1213): 0.00640198\n",
            "Loss (epoch: 1214): 0.00639672\n",
            "Loss (epoch: 1215): 0.00639146\n",
            "Loss (epoch: 1216): 0.00638619\n",
            "Loss (epoch: 1217): 0.00638092\n",
            "Loss (epoch: 1218): 0.00637564\n",
            "Loss (epoch: 1219): 0.00637036\n",
            "Loss (epoch: 1220): 0.00636507\n",
            "Loss (epoch: 1221): 0.00635978\n",
            "Loss (epoch: 1222): 0.00635448\n",
            "Loss (epoch: 1223): 0.00634918\n",
            "Loss (epoch: 1224): 0.00634387\n",
            "Loss (epoch: 1225): 0.00633856\n",
            "Loss (epoch: 1226): 0.00633324\n",
            "Loss (epoch: 1227): 0.00632792\n",
            "Loss (epoch: 1228): 0.00632259\n",
            "Loss (epoch: 1229): 0.00631725\n",
            "Loss (epoch: 1230): 0.00631191\n",
            "Loss (epoch: 1231): 0.00630657\n",
            "Loss (epoch: 1232): 0.00630122\n",
            "Loss (epoch: 1233): 0.00629586\n",
            "Loss (epoch: 1234): 0.00629050\n",
            "Loss (epoch: 1235): 0.00628514\n",
            "Loss (epoch: 1236): 0.00627976\n",
            "Loss (epoch: 1237): 0.00627438\n",
            "Loss (epoch: 1238): 0.00626900\n",
            "Loss (epoch: 1239): 0.00626361\n",
            "Loss (epoch: 1240): 0.00625822\n",
            "Loss (epoch: 1241): 0.00625282\n",
            "Loss (epoch: 1242): 0.00624742\n",
            "Loss (epoch: 1243): 0.00624201\n",
            "Loss (epoch: 1244): 0.00623659\n",
            "Loss (epoch: 1245): 0.00623117\n",
            "Loss (epoch: 1246): 0.00622575\n",
            "Loss (epoch: 1247): 0.00622031\n",
            "Loss (epoch: 1248): 0.00621488\n",
            "Loss (epoch: 1249): 0.00620943\n",
            "Loss (epoch: 1250): 0.00620398\n",
            "Loss (epoch: 1251): 0.00619853\n",
            "Loss (epoch: 1252): 0.00619307\n",
            "Loss (epoch: 1253): 0.00618760\n",
            "Loss (epoch: 1254): 0.00618213\n",
            "Loss (epoch: 1255): 0.00617666\n",
            "Loss (epoch: 1256): 0.00617117\n",
            "Loss (epoch: 1257): 0.00616569\n",
            "Loss (epoch: 1258): 0.00616019\n",
            "Loss (epoch: 1259): 0.00615469\n",
            "Loss (epoch: 1260): 0.00614919\n",
            "Loss (epoch: 1261): 0.00614368\n",
            "Loss (epoch: 1262): 0.00613816\n",
            "Loss (epoch: 1263): 0.00613264\n",
            "Loss (epoch: 1264): 0.00612711\n",
            "Loss (epoch: 1265): 0.00612158\n",
            "Loss (epoch: 1266): 0.00611604\n",
            "Loss (epoch: 1267): 0.00611050\n",
            "Loss (epoch: 1268): 0.00610494\n",
            "Loss (epoch: 1269): 0.00609939\n",
            "Loss (epoch: 1270): 0.00609383\n",
            "Loss (epoch: 1271): 0.00608826\n",
            "Loss (epoch: 1272): 0.00608268\n",
            "Loss (epoch: 1273): 0.00607710\n",
            "Loss (epoch: 1274): 0.00607152\n",
            "Loss (epoch: 1275): 0.00606593\n",
            "Loss (epoch: 1276): 0.00606033\n",
            "Loss (epoch: 1277): 0.00605473\n",
            "Loss (epoch: 1278): 0.00604912\n",
            "Loss (epoch: 1279): 0.00604351\n",
            "Loss (epoch: 1280): 0.00603789\n",
            "Loss (epoch: 1281): 0.00603226\n",
            "Loss (epoch: 1282): 0.00602663\n",
            "Loss (epoch: 1283): 0.00602099\n",
            "Loss (epoch: 1284): 0.00601535\n",
            "Loss (epoch: 1285): 0.00600970\n",
            "Loss (epoch: 1286): 0.00600404\n",
            "Loss (epoch: 1287): 0.00599838\n",
            "Loss (epoch: 1288): 0.00599272\n",
            "Loss (epoch: 1289): 0.00598704\n",
            "Loss (epoch: 1290): 0.00598136\n",
            "Loss (epoch: 1291): 0.00597568\n",
            "Loss (epoch: 1292): 0.00596999\n",
            "Loss (epoch: 1293): 0.00596429\n",
            "Loss (epoch: 1294): 0.00595859\n",
            "Loss (epoch: 1295): 0.00595289\n",
            "Loss (epoch: 1296): 0.00594717\n",
            "Loss (epoch: 1297): 0.00594145\n",
            "Loss (epoch: 1298): 0.00593573\n",
            "Loss (epoch: 1299): 0.00593000\n",
            "Loss (epoch: 1300): 0.00592426\n",
            "Loss (epoch: 1301): 0.00591851\n",
            "Loss (epoch: 1302): 0.00591276\n",
            "Loss (epoch: 1303): 0.00590701\n",
            "Loss (epoch: 1304): 0.00590125\n",
            "Loss (epoch: 1305): 0.00589548\n",
            "Loss (epoch: 1306): 0.00588971\n",
            "Loss (epoch: 1307): 0.00588393\n",
            "Loss (epoch: 1308): 0.00587815\n",
            "Loss (epoch: 1309): 0.00587236\n",
            "Loss (epoch: 1310): 0.00586656\n",
            "Loss (epoch: 1311): 0.00586076\n",
            "Loss (epoch: 1312): 0.00585495\n",
            "Loss (epoch: 1313): 0.00584914\n",
            "Loss (epoch: 1314): 0.00584332\n",
            "Loss (epoch: 1315): 0.00583749\n",
            "Loss (epoch: 1316): 0.00583166\n",
            "Loss (epoch: 1317): 0.00582582\n",
            "Loss (epoch: 1318): 0.00581998\n",
            "Loss (epoch: 1319): 0.00581413\n",
            "Loss (epoch: 1320): 0.00580828\n",
            "Loss (epoch: 1321): 0.00580241\n",
            "Loss (epoch: 1322): 0.00579655\n",
            "Loss (epoch: 1323): 0.00579067\n",
            "Loss (epoch: 1324): 0.00578480\n",
            "Loss (epoch: 1325): 0.00577891\n",
            "Loss (epoch: 1326): 0.00577302\n",
            "Loss (epoch: 1327): 0.00576712\n",
            "Loss (epoch: 1328): 0.00576122\n",
            "Loss (epoch: 1329): 0.00575532\n",
            "Loss (epoch: 1330): 0.00574940\n",
            "Loss (epoch: 1331): 0.00574348\n",
            "Loss (epoch: 1332): 0.00573756\n",
            "Loss (epoch: 1333): 0.00573163\n",
            "Loss (epoch: 1334): 0.00572569\n",
            "Loss (epoch: 1335): 0.00571975\n",
            "Loss (epoch: 1336): 0.00571380\n",
            "Loss (epoch: 1337): 0.00570784\n",
            "Loss (epoch: 1338): 0.00570188\n",
            "Loss (epoch: 1339): 0.00569592\n",
            "Loss (epoch: 1340): 0.00568994\n",
            "Loss (epoch: 1341): 0.00568396\n",
            "Loss (epoch: 1342): 0.00567798\n",
            "Loss (epoch: 1343): 0.00567199\n",
            "Loss (epoch: 1344): 0.00566600\n",
            "Loss (epoch: 1345): 0.00566000\n",
            "Loss (epoch: 1346): 0.00565399\n",
            "Loss (epoch: 1347): 0.00564798\n",
            "Loss (epoch: 1348): 0.00564196\n",
            "Loss (epoch: 1349): 0.00563593\n",
            "Loss (epoch: 1350): 0.00562990\n",
            "Loss (epoch: 1351): 0.00562387\n",
            "Loss (epoch: 1352): 0.00561783\n",
            "Loss (epoch: 1353): 0.00561178\n",
            "Loss (epoch: 1354): 0.00560573\n",
            "Loss (epoch: 1355): 0.00559967\n",
            "Loss (epoch: 1356): 0.00559361\n",
            "Loss (epoch: 1357): 0.00558753\n",
            "Loss (epoch: 1358): 0.00558146\n",
            "Loss (epoch: 1359): 0.00557538\n",
            "Loss (epoch: 1360): 0.00556929\n",
            "Loss (epoch: 1361): 0.00556320\n",
            "Loss (epoch: 1362): 0.00555710\n",
            "Loss (epoch: 1363): 0.00555100\n",
            "Loss (epoch: 1364): 0.00554489\n",
            "Loss (epoch: 1365): 0.00553877\n",
            "Loss (epoch: 1366): 0.00553265\n",
            "Loss (epoch: 1367): 0.00552653\n",
            "Loss (epoch: 1368): 0.00552039\n",
            "Loss (epoch: 1369): 0.00551426\n",
            "Loss (epoch: 1370): 0.00550811\n",
            "Loss (epoch: 1371): 0.00550197\n",
            "Loss (epoch: 1372): 0.00549581\n",
            "Loss (epoch: 1373): 0.00548965\n",
            "Loss (epoch: 1374): 0.00548349\n",
            "Loss (epoch: 1375): 0.00547732\n",
            "Loss (epoch: 1376): 0.00547114\n",
            "Loss (epoch: 1377): 0.00546496\n",
            "Loss (epoch: 1378): 0.00545877\n",
            "Loss (epoch: 1379): 0.00545258\n",
            "Loss (epoch: 1380): 0.00544638\n",
            "Loss (epoch: 1381): 0.00544018\n",
            "Loss (epoch: 1382): 0.00543397\n",
            "Loss (epoch: 1383): 0.00542775\n",
            "Loss (epoch: 1384): 0.00542154\n",
            "Loss (epoch: 1385): 0.00541531\n",
            "Loss (epoch: 1386): 0.00540908\n",
            "Loss (epoch: 1387): 0.00540284\n",
            "Loss (epoch: 1388): 0.00539660\n",
            "Loss (epoch: 1389): 0.00539036\n",
            "Loss (epoch: 1390): 0.00538411\n",
            "Loss (epoch: 1391): 0.00537785\n",
            "Loss (epoch: 1392): 0.00537159\n",
            "Loss (epoch: 1393): 0.00536532\n",
            "Loss (epoch: 1394): 0.00535905\n",
            "Loss (epoch: 1395): 0.00535277\n",
            "Loss (epoch: 1396): 0.00534648\n",
            "Loss (epoch: 1397): 0.00534020\n",
            "Loss (epoch: 1398): 0.00533390\n",
            "Loss (epoch: 1399): 0.00532760\n",
            "Loss (epoch: 1400): 0.00532130\n",
            "Loss (epoch: 1401): 0.00531499\n",
            "Loss (epoch: 1402): 0.00530868\n",
            "Loss (epoch: 1403): 0.00530236\n",
            "Loss (epoch: 1404): 0.00529603\n",
            "Loss (epoch: 1405): 0.00528971\n",
            "Loss (epoch: 1406): 0.00528337\n",
            "Loss (epoch: 1407): 0.00527703\n",
            "Loss (epoch: 1408): 0.00527069\n",
            "Loss (epoch: 1409): 0.00526434\n",
            "Loss (epoch: 1410): 0.00525799\n",
            "Loss (epoch: 1411): 0.00525163\n",
            "Loss (epoch: 1412): 0.00524526\n",
            "Loss (epoch: 1413): 0.00523889\n",
            "Loss (epoch: 1414): 0.00523252\n",
            "Loss (epoch: 1415): 0.00522614\n",
            "Loss (epoch: 1416): 0.00521976\n",
            "Loss (epoch: 1417): 0.00521337\n",
            "Loss (epoch: 1418): 0.00520697\n",
            "Loss (epoch: 1419): 0.00520058\n",
            "Loss (epoch: 1420): 0.00519418\n",
            "Loss (epoch: 1421): 0.00518777\n",
            "Loss (epoch: 1422): 0.00518136\n",
            "Loss (epoch: 1423): 0.00517494\n",
            "Loss (epoch: 1424): 0.00516852\n",
            "Loss (epoch: 1425): 0.00516209\n",
            "Loss (epoch: 1426): 0.00515566\n",
            "Loss (epoch: 1427): 0.00514923\n",
            "Loss (epoch: 1428): 0.00514279\n",
            "Loss (epoch: 1429): 0.00513634\n",
            "Loss (epoch: 1430): 0.00512989\n",
            "Loss (epoch: 1431): 0.00512344\n",
            "Loss (epoch: 1432): 0.00511698\n",
            "Loss (epoch: 1433): 0.00511052\n",
            "Loss (epoch: 1434): 0.00510405\n",
            "Loss (epoch: 1435): 0.00509758\n",
            "Loss (epoch: 1436): 0.00509111\n",
            "Loss (epoch: 1437): 0.00508463\n",
            "Loss (epoch: 1438): 0.00507814\n",
            "Loss (epoch: 1439): 0.00507165\n",
            "Loss (epoch: 1440): 0.00506516\n",
            "Loss (epoch: 1441): 0.00505866\n",
            "Loss (epoch: 1442): 0.00505216\n",
            "Loss (epoch: 1443): 0.00504566\n",
            "Loss (epoch: 1444): 0.00503915\n",
            "Loss (epoch: 1445): 0.00503263\n",
            "Loss (epoch: 1446): 0.00502612\n",
            "Loss (epoch: 1447): 0.00501960\n",
            "Loss (epoch: 1448): 0.00501307\n",
            "Loss (epoch: 1449): 0.00500654\n",
            "Loss (epoch: 1450): 0.00500001\n",
            "Loss (epoch: 1451): 0.00499347\n",
            "Loss (epoch: 1452): 0.00498693\n",
            "Loss (epoch: 1453): 0.00498038\n",
            "Loss (epoch: 1454): 0.00497383\n",
            "Loss (epoch: 1455): 0.00496728\n",
            "Loss (epoch: 1456): 0.00496072\n",
            "Loss (epoch: 1457): 0.00495416\n",
            "Loss (epoch: 1458): 0.00494760\n",
            "Loss (epoch: 1459): 0.00494103\n",
            "Loss (epoch: 1460): 0.00493446\n",
            "Loss (epoch: 1461): 0.00492788\n",
            "Loss (epoch: 1462): 0.00492130\n",
            "Loss (epoch: 1463): 0.00491472\n",
            "Loss (epoch: 1464): 0.00490813\n",
            "Loss (epoch: 1465): 0.00490154\n",
            "Loss (epoch: 1466): 0.00489495\n",
            "Loss (epoch: 1467): 0.00488836\n",
            "Loss (epoch: 1468): 0.00488175\n",
            "Loss (epoch: 1469): 0.00487515\n",
            "Loss (epoch: 1470): 0.00486854\n",
            "Loss (epoch: 1471): 0.00486194\n",
            "Loss (epoch: 1472): 0.00485532\n",
            "Loss (epoch: 1473): 0.00484870\n",
            "Loss (epoch: 1474): 0.00484209\n",
            "Loss (epoch: 1475): 0.00483546\n",
            "Loss (epoch: 1476): 0.00482884\n",
            "Loss (epoch: 1477): 0.00482221\n",
            "Loss (epoch: 1478): 0.00481558\n",
            "Loss (epoch: 1479): 0.00480894\n",
            "Loss (epoch: 1480): 0.00480230\n",
            "Loss (epoch: 1481): 0.00479566\n",
            "Loss (epoch: 1482): 0.00478902\n",
            "Loss (epoch: 1483): 0.00478237\n",
            "Loss (epoch: 1484): 0.00477572\n",
            "Loss (epoch: 1485): 0.00476907\n",
            "Loss (epoch: 1486): 0.00476242\n",
            "Loss (epoch: 1487): 0.00475576\n",
            "Loss (epoch: 1488): 0.00474910\n",
            "Loss (epoch: 1489): 0.00474244\n",
            "Loss (epoch: 1490): 0.00473577\n",
            "Loss (epoch: 1491): 0.00472910\n",
            "Loss (epoch: 1492): 0.00472243\n",
            "Loss (epoch: 1493): 0.00471576\n",
            "Loss (epoch: 1494): 0.00470908\n",
            "Loss (epoch: 1495): 0.00470240\n",
            "Loss (epoch: 1496): 0.00469573\n",
            "Loss (epoch: 1497): 0.00468904\n",
            "Loss (epoch: 1498): 0.00468236\n",
            "Loss (epoch: 1499): 0.00467567\n",
            "Loss (epoch: 1500): 0.00466898\n",
            "Loss (epoch: 1501): 0.00466229\n",
            "Loss (epoch: 1502): 0.00465560\n",
            "Loss (epoch: 1503): 0.00464890\n",
            "Loss (epoch: 1504): 0.00464221\n",
            "Loss (epoch: 1505): 0.00463551\n",
            "Loss (epoch: 1506): 0.00462880\n",
            "Loss (epoch: 1507): 0.00462210\n",
            "Loss (epoch: 1508): 0.00461540\n",
            "Loss (epoch: 1509): 0.00460869\n",
            "Loss (epoch: 1510): 0.00460198\n",
            "Loss (epoch: 1511): 0.00459527\n",
            "Loss (epoch: 1512): 0.00458856\n",
            "Loss (epoch: 1513): 0.00458185\n",
            "Loss (epoch: 1514): 0.00457514\n",
            "Loss (epoch: 1515): 0.00456842\n",
            "Loss (epoch: 1516): 0.00456170\n",
            "Loss (epoch: 1517): 0.00455499\n",
            "Loss (epoch: 1518): 0.00454827\n",
            "Loss (epoch: 1519): 0.00454154\n",
            "Loss (epoch: 1520): 0.00453482\n",
            "Loss (epoch: 1521): 0.00452810\n",
            "Loss (epoch: 1522): 0.00452137\n",
            "Loss (epoch: 1523): 0.00451465\n",
            "Loss (epoch: 1524): 0.00450792\n",
            "Loss (epoch: 1525): 0.00450119\n",
            "Loss (epoch: 1526): 0.00449447\n",
            "Loss (epoch: 1527): 0.00448773\n",
            "Loss (epoch: 1528): 0.00448101\n",
            "Loss (epoch: 1529): 0.00447427\n",
            "Loss (epoch: 1530): 0.00446754\n",
            "Loss (epoch: 1531): 0.00446081\n",
            "Loss (epoch: 1532): 0.00445408\n",
            "Loss (epoch: 1533): 0.00444734\n",
            "Loss (epoch: 1534): 0.00444061\n",
            "Loss (epoch: 1535): 0.00443387\n",
            "Loss (epoch: 1536): 0.00442714\n",
            "Loss (epoch: 1537): 0.00442040\n",
            "Loss (epoch: 1538): 0.00441367\n",
            "Loss (epoch: 1539): 0.00440693\n",
            "Loss (epoch: 1540): 0.00440020\n",
            "Loss (epoch: 1541): 0.00439346\n",
            "Loss (epoch: 1542): 0.00438673\n",
            "Loss (epoch: 1543): 0.00437999\n",
            "Loss (epoch: 1544): 0.00437325\n",
            "Loss (epoch: 1545): 0.00436652\n",
            "Loss (epoch: 1546): 0.00435978\n",
            "Loss (epoch: 1547): 0.00435304\n",
            "Loss (epoch: 1548): 0.00434631\n",
            "Loss (epoch: 1549): 0.00433957\n",
            "Loss (epoch: 1550): 0.00433284\n",
            "Loss (epoch: 1551): 0.00432611\n",
            "Loss (epoch: 1552): 0.00431937\n",
            "Loss (epoch: 1553): 0.00431264\n",
            "Loss (epoch: 1554): 0.00430590\n",
            "Loss (epoch: 1555): 0.00429917\n",
            "Loss (epoch: 1556): 0.00429244\n",
            "Loss (epoch: 1557): 0.00428571\n",
            "Loss (epoch: 1558): 0.00427898\n",
            "Loss (epoch: 1559): 0.00427225\n",
            "Loss (epoch: 1560): 0.00426553\n",
            "Loss (epoch: 1561): 0.00425880\n",
            "Loss (epoch: 1562): 0.00425208\n",
            "Loss (epoch: 1563): 0.00424535\n",
            "Loss (epoch: 1564): 0.00423863\n",
            "Loss (epoch: 1565): 0.00423191\n",
            "Loss (epoch: 1566): 0.00422518\n",
            "Loss (epoch: 1567): 0.00421847\n",
            "Loss (epoch: 1568): 0.00421175\n",
            "Loss (epoch: 1569): 0.00420503\n",
            "Loss (epoch: 1570): 0.00419832\n",
            "Loss (epoch: 1571): 0.00419161\n",
            "Loss (epoch: 1572): 0.00418489\n",
            "Loss (epoch: 1573): 0.00417818\n",
            "Loss (epoch: 1574): 0.00417148\n",
            "Loss (epoch: 1575): 0.00416477\n",
            "Loss (epoch: 1576): 0.00415807\n",
            "Loss (epoch: 1577): 0.00415137\n",
            "Loss (epoch: 1578): 0.00414467\n",
            "Loss (epoch: 1579): 0.00413797\n",
            "Loss (epoch: 1580): 0.00413127\n",
            "Loss (epoch: 1581): 0.00412458\n",
            "Loss (epoch: 1582): 0.00411789\n",
            "Loss (epoch: 1583): 0.00411120\n",
            "Loss (epoch: 1584): 0.00410452\n",
            "Loss (epoch: 1585): 0.00409783\n",
            "Loss (epoch: 1586): 0.00409115\n",
            "Loss (epoch: 1587): 0.00408448\n",
            "Loss (epoch: 1588): 0.00407780\n",
            "Loss (epoch: 1589): 0.00407113\n",
            "Loss (epoch: 1590): 0.00406446\n",
            "Loss (epoch: 1591): 0.00405779\n",
            "Loss (epoch: 1592): 0.00405113\n",
            "Loss (epoch: 1593): 0.00404447\n",
            "Loss (epoch: 1594): 0.00403781\n",
            "Loss (epoch: 1595): 0.00403116\n",
            "Loss (epoch: 1596): 0.00402451\n",
            "Loss (epoch: 1597): 0.00401786\n",
            "Loss (epoch: 1598): 0.00401121\n",
            "Loss (epoch: 1599): 0.00400457\n",
            "Loss (epoch: 1600): 0.00399794\n",
            "Loss (epoch: 1601): 0.00399130\n",
            "Loss (epoch: 1602): 0.00398467\n",
            "Loss (epoch: 1603): 0.00397805\n",
            "Loss (epoch: 1604): 0.00397143\n",
            "Loss (epoch: 1605): 0.00396481\n",
            "Loss (epoch: 1606): 0.00395819\n",
            "Loss (epoch: 1607): 0.00395158\n",
            "Loss (epoch: 1608): 0.00394498\n",
            "Loss (epoch: 1609): 0.00393838\n",
            "Loss (epoch: 1610): 0.00393178\n",
            "Loss (epoch: 1611): 0.00392518\n",
            "Loss (epoch: 1612): 0.00391859\n",
            "Loss (epoch: 1613): 0.00391201\n",
            "Loss (epoch: 1614): 0.00390543\n",
            "Loss (epoch: 1615): 0.00389885\n",
            "Loss (epoch: 1616): 0.00389228\n",
            "Loss (epoch: 1617): 0.00388571\n",
            "Loss (epoch: 1618): 0.00387915\n",
            "Loss (epoch: 1619): 0.00387260\n",
            "Loss (epoch: 1620): 0.00386604\n",
            "Loss (epoch: 1621): 0.00385950\n",
            "Loss (epoch: 1622): 0.00385296\n",
            "Loss (epoch: 1623): 0.00384642\n",
            "Loss (epoch: 1624): 0.00383989\n",
            "Loss (epoch: 1625): 0.00383336\n",
            "Loss (epoch: 1626): 0.00382684\n",
            "Loss (epoch: 1627): 0.00382032\n",
            "Loss (epoch: 1628): 0.00381381\n",
            "Loss (epoch: 1629): 0.00380731\n",
            "Loss (epoch: 1630): 0.00380081\n",
            "Loss (epoch: 1631): 0.00379432\n",
            "Loss (epoch: 1632): 0.00378783\n",
            "Loss (epoch: 1633): 0.00378135\n",
            "Loss (epoch: 1634): 0.00377487\n",
            "Loss (epoch: 1635): 0.00376840\n",
            "Loss (epoch: 1636): 0.00376194\n",
            "Loss (epoch: 1637): 0.00375548\n",
            "Loss (epoch: 1638): 0.00374903\n",
            "Loss (epoch: 1639): 0.00374258\n",
            "Loss (epoch: 1640): 0.00373615\n",
            "Loss (epoch: 1641): 0.00372971\n",
            "Loss (epoch: 1642): 0.00372329\n",
            "Loss (epoch: 1643): 0.00371687\n",
            "Loss (epoch: 1644): 0.00371046\n",
            "Loss (epoch: 1645): 0.00370405\n",
            "Loss (epoch: 1646): 0.00369765\n",
            "Loss (epoch: 1647): 0.00369126\n",
            "Loss (epoch: 1648): 0.00368487\n",
            "Loss (epoch: 1649): 0.00367849\n",
            "Loss (epoch: 1650): 0.00367212\n",
            "Loss (epoch: 1651): 0.00366576\n",
            "Loss (epoch: 1652): 0.00365940\n",
            "Loss (epoch: 1653): 0.00365305\n",
            "Loss (epoch: 1654): 0.00364671\n",
            "Loss (epoch: 1655): 0.00364037\n",
            "Loss (epoch: 1656): 0.00363404\n",
            "Loss (epoch: 1657): 0.00362772\n",
            "Loss (epoch: 1658): 0.00362141\n",
            "Loss (epoch: 1659): 0.00361510\n",
            "Loss (epoch: 1660): 0.00360880\n",
            "Loss (epoch: 1661): 0.00360252\n",
            "Loss (epoch: 1662): 0.00359623\n",
            "Loss (epoch: 1663): 0.00358996\n",
            "Loss (epoch: 1664): 0.00358369\n",
            "Loss (epoch: 1665): 0.00357743\n",
            "Loss (epoch: 1666): 0.00357118\n",
            "Loss (epoch: 1667): 0.00356494\n",
            "Loss (epoch: 1668): 0.00355871\n",
            "Loss (epoch: 1669): 0.00355248\n",
            "Loss (epoch: 1670): 0.00354627\n",
            "Loss (epoch: 1671): 0.00354006\n",
            "Loss (epoch: 1672): 0.00353386\n",
            "Loss (epoch: 1673): 0.00352767\n",
            "Loss (epoch: 1674): 0.00352148\n",
            "Loss (epoch: 1675): 0.00351531\n",
            "Loss (epoch: 1676): 0.00350915\n",
            "Loss (epoch: 1677): 0.00350299\n",
            "Loss (epoch: 1678): 0.00349684\n",
            "Loss (epoch: 1679): 0.00349070\n",
            "Loss (epoch: 1680): 0.00348458\n",
            "Loss (epoch: 1681): 0.00347846\n",
            "Loss (epoch: 1682): 0.00347235\n",
            "Loss (epoch: 1683): 0.00346624\n",
            "Loss (epoch: 1684): 0.00346015\n",
            "Loss (epoch: 1685): 0.00345407\n",
            "Loss (epoch: 1686): 0.00344800\n",
            "Loss (epoch: 1687): 0.00344193\n",
            "Loss (epoch: 1688): 0.00343588\n",
            "Loss (epoch: 1689): 0.00342983\n",
            "Loss (epoch: 1690): 0.00342380\n",
            "Loss (epoch: 1691): 0.00341777\n",
            "Loss (epoch: 1692): 0.00341176\n",
            "Loss (epoch: 1693): 0.00340575\n",
            "Loss (epoch: 1694): 0.00339976\n",
            "Loss (epoch: 1695): 0.00339377\n",
            "Loss (epoch: 1696): 0.00338780\n",
            "Loss (epoch: 1697): 0.00338183\n",
            "Loss (epoch: 1698): 0.00337588\n",
            "Loss (epoch: 1699): 0.00336994\n",
            "Loss (epoch: 1700): 0.00336400\n",
            "Loss (epoch: 1701): 0.00335808\n",
            "Loss (epoch: 1702): 0.00335216\n",
            "Loss (epoch: 1703): 0.00334626\n",
            "Loss (epoch: 1704): 0.00334037\n",
            "Loss (epoch: 1705): 0.00333449\n",
            "Loss (epoch: 1706): 0.00332862\n",
            "Loss (epoch: 1707): 0.00332276\n",
            "Loss (epoch: 1708): 0.00331691\n",
            "Loss (epoch: 1709): 0.00331107\n",
            "Loss (epoch: 1710): 0.00330525\n",
            "Loss (epoch: 1711): 0.00329943\n",
            "Loss (epoch: 1712): 0.00329362\n",
            "Loss (epoch: 1713): 0.00328783\n",
            "Loss (epoch: 1714): 0.00328205\n",
            "Loss (epoch: 1715): 0.00327628\n",
            "Loss (epoch: 1716): 0.00327052\n",
            "Loss (epoch: 1717): 0.00326477\n",
            "Loss (epoch: 1718): 0.00325903\n",
            "Loss (epoch: 1719): 0.00325330\n",
            "Loss (epoch: 1720): 0.00324759\n",
            "Loss (epoch: 1721): 0.00324189\n",
            "Loss (epoch: 1722): 0.00323619\n",
            "Loss (epoch: 1723): 0.00323051\n",
            "Loss (epoch: 1724): 0.00322485\n",
            "Loss (epoch: 1725): 0.00321919\n",
            "Loss (epoch: 1726): 0.00321355\n",
            "Loss (epoch: 1727): 0.00320791\n",
            "Loss (epoch: 1728): 0.00320229\n",
            "Loss (epoch: 1729): 0.00319668\n",
            "Loss (epoch: 1730): 0.00319109\n",
            "Loss (epoch: 1731): 0.00318550\n",
            "Loss (epoch: 1732): 0.00317993\n",
            "Loss (epoch: 1733): 0.00317437\n",
            "Loss (epoch: 1734): 0.00316882\n",
            "Loss (epoch: 1735): 0.00316329\n",
            "Loss (epoch: 1736): 0.00315776\n",
            "Loss (epoch: 1737): 0.00315225\n",
            "Loss (epoch: 1738): 0.00314676\n",
            "Loss (epoch: 1739): 0.00314127\n",
            "Loss (epoch: 1740): 0.00313579\n",
            "Loss (epoch: 1741): 0.00313033\n",
            "Loss (epoch: 1742): 0.00312489\n",
            "Loss (epoch: 1743): 0.00311945\n",
            "Loss (epoch: 1744): 0.00311403\n",
            "Loss (epoch: 1745): 0.00310862\n",
            "Loss (epoch: 1746): 0.00310322\n",
            "Loss (epoch: 1747): 0.00309783\n",
            "Loss (epoch: 1748): 0.00309246\n",
            "Loss (epoch: 1749): 0.00308710\n",
            "Loss (epoch: 1750): 0.00308176\n",
            "Loss (epoch: 1751): 0.00307643\n",
            "Loss (epoch: 1752): 0.00307111\n",
            "Loss (epoch: 1753): 0.00306580\n",
            "Loss (epoch: 1754): 0.00306051\n",
            "Loss (epoch: 1755): 0.00305523\n",
            "Loss (epoch: 1756): 0.00304996\n",
            "Loss (epoch: 1757): 0.00304470\n",
            "Loss (epoch: 1758): 0.00303946\n",
            "Loss (epoch: 1759): 0.00303424\n",
            "Loss (epoch: 1760): 0.00302902\n",
            "Loss (epoch: 1761): 0.00302382\n",
            "Loss (epoch: 1762): 0.00301863\n",
            "Loss (epoch: 1763): 0.00301346\n",
            "Loss (epoch: 1764): 0.00300830\n",
            "Loss (epoch: 1765): 0.00300315\n",
            "Loss (epoch: 1766): 0.00299802\n",
            "Loss (epoch: 1767): 0.00299290\n",
            "Loss (epoch: 1768): 0.00298779\n",
            "Loss (epoch: 1769): 0.00298270\n",
            "Loss (epoch: 1770): 0.00297762\n",
            "Loss (epoch: 1771): 0.00297256\n",
            "Loss (epoch: 1772): 0.00296751\n",
            "Loss (epoch: 1773): 0.00296247\n",
            "Loss (epoch: 1774): 0.00295745\n",
            "Loss (epoch: 1775): 0.00295244\n",
            "Loss (epoch: 1776): 0.00294744\n",
            "Loss (epoch: 1777): 0.00294246\n",
            "Loss (epoch: 1778): 0.00293749\n",
            "Loss (epoch: 1779): 0.00293254\n",
            "Loss (epoch: 1780): 0.00292760\n",
            "Loss (epoch: 1781): 0.00292267\n",
            "Loss (epoch: 1782): 0.00291776\n",
            "Loss (epoch: 1783): 0.00291286\n",
            "Loss (epoch: 1784): 0.00290798\n",
            "Loss (epoch: 1785): 0.00290311\n",
            "Loss (epoch: 1786): 0.00289825\n",
            "Loss (epoch: 1787): 0.00289341\n",
            "Loss (epoch: 1788): 0.00288858\n",
            "Loss (epoch: 1789): 0.00288377\n",
            "Loss (epoch: 1790): 0.00287897\n",
            "Loss (epoch: 1791): 0.00287418\n",
            "Loss (epoch: 1792): 0.00286941\n",
            "Loss (epoch: 1793): 0.00286466\n",
            "Loss (epoch: 1794): 0.00285991\n",
            "Loss (epoch: 1795): 0.00285519\n",
            "Loss (epoch: 1796): 0.00285047\n",
            "Loss (epoch: 1797): 0.00284577\n",
            "Loss (epoch: 1798): 0.00284109\n",
            "Loss (epoch: 1799): 0.00283642\n",
            "Loss (epoch: 1800): 0.00283176\n",
            "Loss (epoch: 1801): 0.00282712\n",
            "Loss (epoch: 1802): 0.00282249\n",
            "Loss (epoch: 1803): 0.00281788\n",
            "Loss (epoch: 1804): 0.00281328\n",
            "Loss (epoch: 1805): 0.00280869\n",
            "Loss (epoch: 1806): 0.00280412\n",
            "Loss (epoch: 1807): 0.00279957\n",
            "Loss (epoch: 1808): 0.00279503\n",
            "Loss (epoch: 1809): 0.00279050\n",
            "Loss (epoch: 1810): 0.00278599\n",
            "Loss (epoch: 1811): 0.00278149\n",
            "Loss (epoch: 1812): 0.00277701\n",
            "Loss (epoch: 1813): 0.00277254\n",
            "Loss (epoch: 1814): 0.00276809\n",
            "Loss (epoch: 1815): 0.00276365\n",
            "Loss (epoch: 1816): 0.00275922\n",
            "Loss (epoch: 1817): 0.00275481\n",
            "Loss (epoch: 1818): 0.00275042\n",
            "Loss (epoch: 1819): 0.00274604\n",
            "Loss (epoch: 1820): 0.00274167\n",
            "Loss (epoch: 1821): 0.00273732\n",
            "Loss (epoch: 1822): 0.00273298\n",
            "Loss (epoch: 1823): 0.00272866\n",
            "Loss (epoch: 1824): 0.00272435\n",
            "Loss (epoch: 1825): 0.00272005\n",
            "Loss (epoch: 1826): 0.00271578\n",
            "Loss (epoch: 1827): 0.00271151\n",
            "Loss (epoch: 1828): 0.00270726\n",
            "Loss (epoch: 1829): 0.00270302\n",
            "Loss (epoch: 1830): 0.00269880\n",
            "Loss (epoch: 1831): 0.00269460\n",
            "Loss (epoch: 1832): 0.00269040\n",
            "Loss (epoch: 1833): 0.00268623\n",
            "Loss (epoch: 1834): 0.00268206\n",
            "Loss (epoch: 1835): 0.00267791\n",
            "Loss (epoch: 1836): 0.00267378\n",
            "Loss (epoch: 1837): 0.00266966\n",
            "Loss (epoch: 1838): 0.00266556\n",
            "Loss (epoch: 1839): 0.00266147\n",
            "Loss (epoch: 1840): 0.00265739\n",
            "Loss (epoch: 1841): 0.00265333\n",
            "Loss (epoch: 1842): 0.00264928\n",
            "Loss (epoch: 1843): 0.00264525\n",
            "Loss (epoch: 1844): 0.00264123\n",
            "Loss (epoch: 1845): 0.00263723\n",
            "Loss (epoch: 1846): 0.00263324\n",
            "Loss (epoch: 1847): 0.00262927\n",
            "Loss (epoch: 1848): 0.00262531\n",
            "Loss (epoch: 1849): 0.00262136\n",
            "Loss (epoch: 1850): 0.00261743\n",
            "Loss (epoch: 1851): 0.00261351\n",
            "Loss (epoch: 1852): 0.00260961\n",
            "Loss (epoch: 1853): 0.00260572\n",
            "Loss (epoch: 1854): 0.00260185\n",
            "Loss (epoch: 1855): 0.00259799\n",
            "Loss (epoch: 1856): 0.00259414\n",
            "Loss (epoch: 1857): 0.00259031\n",
            "Loss (epoch: 1858): 0.00258650\n",
            "Loss (epoch: 1859): 0.00258269\n",
            "Loss (epoch: 1860): 0.00257891\n",
            "Loss (epoch: 1861): 0.00257513\n",
            "Loss (epoch: 1862): 0.00257137\n",
            "Loss (epoch: 1863): 0.00256763\n",
            "Loss (epoch: 1864): 0.00256390\n",
            "Loss (epoch: 1865): 0.00256018\n",
            "Loss (epoch: 1866): 0.00255648\n",
            "Loss (epoch: 1867): 0.00255279\n",
            "Loss (epoch: 1868): 0.00254911\n",
            "Loss (epoch: 1869): 0.00254545\n",
            "Loss (epoch: 1870): 0.00254181\n",
            "Loss (epoch: 1871): 0.00253817\n",
            "Loss (epoch: 1872): 0.00253456\n",
            "Loss (epoch: 1873): 0.00253095\n",
            "Loss (epoch: 1874): 0.00252736\n",
            "Loss (epoch: 1875): 0.00252379\n",
            "Loss (epoch: 1876): 0.00252022\n",
            "Loss (epoch: 1877): 0.00251667\n",
            "Loss (epoch: 1878): 0.00251314\n",
            "Loss (epoch: 1879): 0.00250962\n",
            "Loss (epoch: 1880): 0.00250611\n",
            "Loss (epoch: 1881): 0.00250262\n",
            "Loss (epoch: 1882): 0.00249914\n",
            "Loss (epoch: 1883): 0.00249567\n",
            "Loss (epoch: 1884): 0.00249222\n",
            "Loss (epoch: 1885): 0.00248878\n",
            "Loss (epoch: 1886): 0.00248536\n",
            "Loss (epoch: 1887): 0.00248195\n",
            "Loss (epoch: 1888): 0.00247855\n",
            "Loss (epoch: 1889): 0.00247517\n",
            "Loss (epoch: 1890): 0.00247180\n",
            "Loss (epoch: 1891): 0.00246844\n",
            "Loss (epoch: 1892): 0.00246510\n",
            "Loss (epoch: 1893): 0.00246177\n",
            "Loss (epoch: 1894): 0.00245845\n",
            "Loss (epoch: 1895): 0.00245515\n",
            "Loss (epoch: 1896): 0.00245186\n",
            "Loss (epoch: 1897): 0.00244858\n",
            "Loss (epoch: 1898): 0.00244532\n",
            "Loss (epoch: 1899): 0.00244207\n",
            "Loss (epoch: 1900): 0.00243883\n",
            "Loss (epoch: 1901): 0.00243561\n",
            "Loss (epoch: 1902): 0.00243240\n",
            "Loss (epoch: 1903): 0.00242920\n",
            "Loss (epoch: 1904): 0.00242602\n",
            "Loss (epoch: 1905): 0.00242285\n",
            "Loss (epoch: 1906): 0.00241969\n",
            "Loss (epoch: 1907): 0.00241655\n",
            "Loss (epoch: 1908): 0.00241342\n",
            "Loss (epoch: 1909): 0.00241030\n",
            "Loss (epoch: 1910): 0.00240719\n",
            "Loss (epoch: 1911): 0.00240410\n",
            "Loss (epoch: 1912): 0.00240102\n",
            "Loss (epoch: 1913): 0.00239795\n",
            "Loss (epoch: 1914): 0.00239490\n",
            "Loss (epoch: 1915): 0.00239186\n",
            "Loss (epoch: 1916): 0.00238883\n",
            "Loss (epoch: 1917): 0.00238581\n",
            "Loss (epoch: 1918): 0.00238281\n",
            "Loss (epoch: 1919): 0.00237982\n",
            "Loss (epoch: 1920): 0.00237684\n",
            "Loss (epoch: 1921): 0.00237387\n",
            "Loss (epoch: 1922): 0.00237092\n",
            "Loss (epoch: 1923): 0.00236798\n",
            "Loss (epoch: 1924): 0.00236505\n",
            "Loss (epoch: 1925): 0.00236213\n",
            "Loss (epoch: 1926): 0.00235923\n",
            "Loss (epoch: 1927): 0.00235634\n",
            "Loss (epoch: 1928): 0.00235346\n",
            "Loss (epoch: 1929): 0.00235059\n",
            "Loss (epoch: 1930): 0.00234774\n",
            "Loss (epoch: 1931): 0.00234489\n",
            "Loss (epoch: 1932): 0.00234206\n",
            "Loss (epoch: 1933): 0.00233924\n",
            "Loss (epoch: 1934): 0.00233644\n",
            "Loss (epoch: 1935): 0.00233364\n",
            "Loss (epoch: 1936): 0.00233086\n",
            "Loss (epoch: 1937): 0.00232809\n",
            "Loss (epoch: 1938): 0.00232533\n",
            "Loss (epoch: 1939): 0.00232258\n",
            "Loss (epoch: 1940): 0.00231985\n",
            "Loss (epoch: 1941): 0.00231712\n",
            "Loss (epoch: 1942): 0.00231441\n",
            "Loss (epoch: 1943): 0.00231171\n",
            "Loss (epoch: 1944): 0.00230902\n",
            "Loss (epoch: 1945): 0.00230634\n",
            "Loss (epoch: 1946): 0.00230368\n",
            "Loss (epoch: 1947): 0.00230102\n",
            "Loss (epoch: 1948): 0.00229838\n",
            "Loss (epoch: 1949): 0.00229575\n",
            "Loss (epoch: 1950): 0.00229313\n",
            "Loss (epoch: 1951): 0.00229052\n",
            "Loss (epoch: 1952): 0.00228792\n",
            "Loss (epoch: 1953): 0.00228533\n",
            "Loss (epoch: 1954): 0.00228276\n",
            "Loss (epoch: 1955): 0.00228019\n",
            "Loss (epoch: 1956): 0.00227764\n",
            "Loss (epoch: 1957): 0.00227510\n",
            "Loss (epoch: 1958): 0.00227257\n",
            "Loss (epoch: 1959): 0.00227005\n",
            "Loss (epoch: 1960): 0.00226754\n",
            "Loss (epoch: 1961): 0.00226504\n",
            "Loss (epoch: 1962): 0.00226255\n",
            "Loss (epoch: 1963): 0.00226007\n",
            "Loss (epoch: 1964): 0.00225761\n",
            "Loss (epoch: 1965): 0.00225515\n",
            "Loss (epoch: 1966): 0.00225271\n",
            "Loss (epoch: 1967): 0.00225027\n",
            "Loss (epoch: 1968): 0.00224785\n",
            "Loss (epoch: 1969): 0.00224544\n",
            "Loss (epoch: 1970): 0.00224303\n",
            "Loss (epoch: 1971): 0.00224064\n",
            "Loss (epoch: 1972): 0.00223826\n",
            "Loss (epoch: 1973): 0.00223589\n",
            "Loss (epoch: 1974): 0.00223353\n",
            "Loss (epoch: 1975): 0.00223118\n",
            "Loss (epoch: 1976): 0.00222883\n",
            "Loss (epoch: 1977): 0.00222650\n",
            "Loss (epoch: 1978): 0.00222418\n",
            "Loss (epoch: 1979): 0.00222187\n",
            "Loss (epoch: 1980): 0.00221957\n",
            "Loss (epoch: 1981): 0.00221728\n",
            "Loss (epoch: 1982): 0.00221500\n",
            "Loss (epoch: 1983): 0.00221273\n",
            "Loss (epoch: 1984): 0.00221047\n",
            "Loss (epoch: 1985): 0.00220822\n",
            "Loss (epoch: 1986): 0.00220598\n",
            "Loss (epoch: 1987): 0.00220375\n",
            "Loss (epoch: 1988): 0.00220153\n",
            "Loss (epoch: 1989): 0.00219931\n",
            "Loss (epoch: 1990): 0.00219711\n",
            "Loss (epoch: 1991): 0.00219492\n",
            "Loss (epoch: 1992): 0.00219274\n",
            "Loss (epoch: 1993): 0.00219056\n",
            "Loss (epoch: 1994): 0.00218840\n",
            "Loss (epoch: 1995): 0.00218624\n",
            "Loss (epoch: 1996): 0.00218410\n",
            "Loss (epoch: 1997): 0.00218196\n",
            "Loss (epoch: 1998): 0.00217983\n",
            "Loss (epoch: 1999): 0.00217771\n",
            "Loss (epoch: 2000): 0.00217561\n",
            "Loss (epoch: 2001): 0.00217351\n",
            "Loss (epoch: 2002): 0.00217142\n",
            "Loss (epoch: 2003): 0.00216933\n",
            "Loss (epoch: 2004): 0.00216726\n",
            "Loss (epoch: 2005): 0.00216520\n",
            "Loss (epoch: 2006): 0.00216314\n",
            "Loss (epoch: 2007): 0.00216110\n",
            "Loss (epoch: 2008): 0.00215906\n",
            "Loss (epoch: 2009): 0.00215703\n",
            "Loss (epoch: 2010): 0.00215501\n",
            "Loss (epoch: 2011): 0.00215300\n",
            "Loss (epoch: 2012): 0.00215100\n",
            "Loss (epoch: 2013): 0.00214901\n",
            "Loss (epoch: 2014): 0.00214702\n",
            "Loss (epoch: 2015): 0.00214504\n",
            "Loss (epoch: 2016): 0.00214308\n",
            "Loss (epoch: 2017): 0.00214112\n",
            "Loss (epoch: 2018): 0.00213916\n",
            "Loss (epoch: 2019): 0.00213722\n",
            "Loss (epoch: 2020): 0.00213529\n",
            "Loss (epoch: 2021): 0.00213336\n",
            "Loss (epoch: 2022): 0.00213144\n",
            "Loss (epoch: 2023): 0.00212953\n",
            "Loss (epoch: 2024): 0.00212763\n",
            "Loss (epoch: 2025): 0.00212574\n",
            "Loss (epoch: 2026): 0.00212385\n",
            "Loss (epoch: 2027): 0.00212197\n",
            "Loss (epoch: 2028): 0.00212010\n",
            "Loss (epoch: 2029): 0.00211824\n",
            "Loss (epoch: 2030): 0.00211639\n",
            "Loss (epoch: 2031): 0.00211454\n",
            "Loss (epoch: 2032): 0.00211270\n",
            "Loss (epoch: 2033): 0.00211087\n",
            "Loss (epoch: 2034): 0.00210905\n",
            "Loss (epoch: 2035): 0.00210724\n",
            "Loss (epoch: 2036): 0.00210543\n",
            "Loss (epoch: 2037): 0.00210363\n",
            "Loss (epoch: 2038): 0.00210184\n",
            "Loss (epoch: 2039): 0.00210005\n",
            "Loss (epoch: 2040): 0.00209827\n",
            "Loss (epoch: 2041): 0.00209651\n",
            "Loss (epoch: 2042): 0.00209474\n",
            "Loss (epoch: 2043): 0.00209299\n",
            "Loss (epoch: 2044): 0.00209124\n",
            "Loss (epoch: 2045): 0.00208950\n",
            "Loss (epoch: 2046): 0.00208777\n",
            "Loss (epoch: 2047): 0.00208604\n",
            "Loss (epoch: 2048): 0.00208432\n",
            "Loss (epoch: 2049): 0.00208261\n",
            "Loss (epoch: 2050): 0.00208091\n",
            "Loss (epoch: 2051): 0.00207921\n",
            "Loss (epoch: 2052): 0.00207752\n",
            "Loss (epoch: 2053): 0.00207584\n",
            "Loss (epoch: 2054): 0.00207416\n",
            "Loss (epoch: 2055): 0.00207249\n",
            "Loss (epoch: 2056): 0.00207082\n",
            "Loss (epoch: 2057): 0.00206917\n",
            "Loss (epoch: 2058): 0.00206752\n",
            "Loss (epoch: 2059): 0.00206588\n",
            "Loss (epoch: 2060): 0.00206424\n",
            "Loss (epoch: 2061): 0.00206261\n",
            "Loss (epoch: 2062): 0.00206099\n",
            "Loss (epoch: 2063): 0.00205937\n",
            "Loss (epoch: 2064): 0.00205777\n",
            "Loss (epoch: 2065): 0.00205616\n",
            "Loss (epoch: 2066): 0.00205457\n",
            "Loss (epoch: 2067): 0.00205298\n",
            "Loss (epoch: 2068): 0.00205139\n",
            "Loss (epoch: 2069): 0.00204982\n",
            "Loss (epoch: 2070): 0.00204825\n",
            "Loss (epoch: 2071): 0.00204668\n",
            "Loss (epoch: 2072): 0.00204512\n",
            "Loss (epoch: 2073): 0.00204357\n",
            "Loss (epoch: 2074): 0.00204203\n",
            "Loss (epoch: 2075): 0.00204049\n",
            "Loss (epoch: 2076): 0.00203896\n",
            "Loss (epoch: 2077): 0.00203743\n",
            "Loss (epoch: 2078): 0.00203591\n",
            "Loss (epoch: 2079): 0.00203439\n",
            "Loss (epoch: 2080): 0.00203288\n",
            "Loss (epoch: 2081): 0.00203138\n",
            "Loss (epoch: 2082): 0.00202988\n",
            "Loss (epoch: 2083): 0.00202839\n",
            "Loss (epoch: 2084): 0.00202691\n",
            "Loss (epoch: 2085): 0.00202543\n",
            "Loss (epoch: 2086): 0.00202395\n",
            "Loss (epoch: 2087): 0.00202248\n",
            "Loss (epoch: 2088): 0.00202102\n",
            "Loss (epoch: 2089): 0.00201956\n",
            "Loss (epoch: 2090): 0.00201811\n",
            "Loss (epoch: 2091): 0.00201667\n",
            "Loss (epoch: 2092): 0.00201523\n",
            "Loss (epoch: 2093): 0.00201379\n",
            "Loss (epoch: 2094): 0.00201236\n",
            "Loss (epoch: 2095): 0.00201094\n",
            "Loss (epoch: 2096): 0.00200952\n",
            "Loss (epoch: 2097): 0.00200811\n",
            "Loss (epoch: 2098): 0.00200670\n",
            "Loss (epoch: 2099): 0.00200530\n",
            "Loss (epoch: 2100): 0.00200391\n",
            "Loss (epoch: 2101): 0.00200252\n",
            "Loss (epoch: 2102): 0.00200113\n",
            "Loss (epoch: 2103): 0.00199975\n",
            "Loss (epoch: 2104): 0.00199837\n",
            "Loss (epoch: 2105): 0.00199700\n",
            "Loss (epoch: 2106): 0.00199564\n",
            "Loss (epoch: 2107): 0.00199428\n",
            "Loss (epoch: 2108): 0.00199292\n",
            "Loss (epoch: 2109): 0.00199158\n",
            "Loss (epoch: 2110): 0.00199023\n",
            "Loss (epoch: 2111): 0.00198889\n",
            "Loss (epoch: 2112): 0.00198756\n",
            "Loss (epoch: 2113): 0.00198623\n",
            "Loss (epoch: 2114): 0.00198490\n",
            "Loss (epoch: 2115): 0.00198358\n",
            "Loss (epoch: 2116): 0.00198227\n",
            "Loss (epoch: 2117): 0.00198095\n",
            "Loss (epoch: 2118): 0.00197965\n",
            "Loss (epoch: 2119): 0.00197835\n",
            "Loss (epoch: 2120): 0.00197705\n",
            "Loss (epoch: 2121): 0.00197576\n",
            "Loss (epoch: 2122): 0.00197447\n",
            "Loss (epoch: 2123): 0.00197319\n",
            "Loss (epoch: 2124): 0.00197191\n",
            "Loss (epoch: 2125): 0.00197064\n",
            "Loss (epoch: 2126): 0.00196937\n",
            "Loss (epoch: 2127): 0.00196811\n",
            "Loss (epoch: 2128): 0.00196685\n",
            "Loss (epoch: 2129): 0.00196560\n",
            "Loss (epoch: 2130): 0.00196434\n",
            "Loss (epoch: 2131): 0.00196310\n",
            "Loss (epoch: 2132): 0.00196186\n",
            "Loss (epoch: 2133): 0.00196062\n",
            "Loss (epoch: 2134): 0.00195939\n",
            "Loss (epoch: 2135): 0.00195816\n",
            "Loss (epoch: 2136): 0.00195693\n",
            "Loss (epoch: 2137): 0.00195571\n",
            "Loss (epoch: 2138): 0.00195450\n",
            "Loss (epoch: 2139): 0.00195329\n",
            "Loss (epoch: 2140): 0.00195208\n",
            "Loss (epoch: 2141): 0.00195087\n",
            "Loss (epoch: 2142): 0.00194968\n",
            "Loss (epoch: 2143): 0.00194848\n",
            "Loss (epoch: 2144): 0.00194729\n",
            "Loss (epoch: 2145): 0.00194610\n",
            "Loss (epoch: 2146): 0.00194492\n",
            "Loss (epoch: 2147): 0.00194374\n",
            "Loss (epoch: 2148): 0.00194257\n",
            "Loss (epoch: 2149): 0.00194139\n",
            "Loss (epoch: 2150): 0.00194023\n",
            "Loss (epoch: 2151): 0.00193906\n",
            "Loss (epoch: 2152): 0.00193790\n",
            "Loss (epoch: 2153): 0.00193675\n",
            "Loss (epoch: 2154): 0.00193560\n",
            "Loss (epoch: 2155): 0.00193445\n",
            "Loss (epoch: 2156): 0.00193331\n",
            "Loss (epoch: 2157): 0.00193216\n",
            "Loss (epoch: 2158): 0.00193103\n",
            "Loss (epoch: 2159): 0.00192990\n",
            "Loss (epoch: 2160): 0.00192877\n",
            "Loss (epoch: 2161): 0.00192764\n",
            "Loss (epoch: 2162): 0.00192652\n",
            "Loss (epoch: 2163): 0.00192540\n",
            "Loss (epoch: 2164): 0.00192429\n",
            "Loss (epoch: 2165): 0.00192317\n",
            "Loss (epoch: 2166): 0.00192207\n",
            "Loss (epoch: 2167): 0.00192096\n",
            "Loss (epoch: 2168): 0.00191986\n",
            "Loss (epoch: 2169): 0.00191876\n",
            "Loss (epoch: 2170): 0.00191767\n",
            "Loss (epoch: 2171): 0.00191658\n",
            "Loss (epoch: 2172): 0.00191549\n",
            "Loss (epoch: 2173): 0.00191441\n",
            "Loss (epoch: 2174): 0.00191333\n",
            "Loss (epoch: 2175): 0.00191225\n",
            "Loss (epoch: 2176): 0.00191118\n",
            "Loss (epoch: 2177): 0.00191011\n",
            "Loss (epoch: 2178): 0.00190904\n",
            "Loss (epoch: 2179): 0.00190798\n",
            "Loss (epoch: 2180): 0.00190692\n",
            "Loss (epoch: 2181): 0.00190586\n",
            "Loss (epoch: 2182): 0.00190481\n",
            "Loss (epoch: 2183): 0.00190376\n",
            "Loss (epoch: 2184): 0.00190271\n",
            "Loss (epoch: 2185): 0.00190167\n",
            "Loss (epoch: 2186): 0.00190062\n",
            "Loss (epoch: 2187): 0.00189959\n",
            "Loss (epoch: 2188): 0.00189855\n",
            "Loss (epoch: 2189): 0.00189752\n",
            "Loss (epoch: 2190): 0.00189649\n",
            "Loss (epoch: 2191): 0.00189547\n",
            "Loss (epoch: 2192): 0.00189444\n",
            "Loss (epoch: 2193): 0.00189342\n",
            "Loss (epoch: 2194): 0.00189240\n",
            "Loss (epoch: 2195): 0.00189139\n",
            "Loss (epoch: 2196): 0.00189038\n",
            "Loss (epoch: 2197): 0.00188937\n",
            "Loss (epoch: 2198): 0.00188837\n",
            "Loss (epoch: 2199): 0.00188736\n",
            "Loss (epoch: 2200): 0.00188636\n",
            "Loss (epoch: 2201): 0.00188537\n",
            "Loss (epoch: 2202): 0.00188437\n",
            "Loss (epoch: 2203): 0.00188338\n",
            "Loss (epoch: 2204): 0.00188240\n",
            "Loss (epoch: 2205): 0.00188141\n",
            "Loss (epoch: 2206): 0.00188043\n",
            "Loss (epoch: 2207): 0.00187945\n",
            "Loss (epoch: 2208): 0.00187847\n",
            "Loss (epoch: 2209): 0.00187749\n",
            "Loss (epoch: 2210): 0.00187652\n",
            "Loss (epoch: 2211): 0.00187555\n",
            "Loss (epoch: 2212): 0.00187459\n",
            "Loss (epoch: 2213): 0.00187362\n",
            "Loss (epoch: 2214): 0.00187266\n",
            "Loss (epoch: 2215): 0.00187170\n",
            "Loss (epoch: 2216): 0.00187075\n",
            "Loss (epoch: 2217): 0.00186979\n",
            "Loss (epoch: 2218): 0.00186884\n",
            "Loss (epoch: 2219): 0.00186789\n",
            "Loss (epoch: 2220): 0.00186695\n",
            "Loss (epoch: 2221): 0.00186600\n",
            "Loss (epoch: 2222): 0.00186506\n",
            "Loss (epoch: 2223): 0.00186412\n",
            "Loss (epoch: 2224): 0.00186319\n",
            "Loss (epoch: 2225): 0.00186225\n",
            "Loss (epoch: 2226): 0.00186132\n",
            "Loss (epoch: 2227): 0.00186039\n",
            "Loss (epoch: 2228): 0.00185947\n",
            "Loss (epoch: 2229): 0.00185854\n",
            "Loss (epoch: 2230): 0.00185762\n",
            "Loss (epoch: 2231): 0.00185670\n",
            "Loss (epoch: 2232): 0.00185578\n",
            "Loss (epoch: 2233): 0.00185487\n",
            "Loss (epoch: 2234): 0.00185396\n",
            "Loss (epoch: 2235): 0.00185305\n",
            "Loss (epoch: 2236): 0.00185214\n",
            "Loss (epoch: 2237): 0.00185123\n",
            "Loss (epoch: 2238): 0.00185033\n",
            "Loss (epoch: 2239): 0.00184943\n",
            "Loss (epoch: 2240): 0.00184853\n",
            "Loss (epoch: 2241): 0.00184763\n",
            "Loss (epoch: 2242): 0.00184674\n",
            "Loss (epoch: 2243): 0.00184585\n",
            "Loss (epoch: 2244): 0.00184495\n",
            "Loss (epoch: 2245): 0.00184407\n",
            "Loss (epoch: 2246): 0.00184318\n",
            "Loss (epoch: 2247): 0.00184230\n",
            "Loss (epoch: 2248): 0.00184141\n",
            "Loss (epoch: 2249): 0.00184054\n",
            "Loss (epoch: 2250): 0.00183966\n",
            "Loss (epoch: 2251): 0.00183878\n",
            "Loss (epoch: 2252): 0.00183791\n",
            "Loss (epoch: 2253): 0.00183704\n",
            "Loss (epoch: 2254): 0.00183617\n",
            "Loss (epoch: 2255): 0.00183530\n",
            "Loss (epoch: 2256): 0.00183443\n",
            "Loss (epoch: 2257): 0.00183357\n",
            "Loss (epoch: 2258): 0.00183271\n",
            "Loss (epoch: 2259): 0.00183185\n",
            "Loss (epoch: 2260): 0.00183099\n",
            "Loss (epoch: 2261): 0.00183014\n",
            "Loss (epoch: 2262): 0.00182928\n",
            "Loss (epoch: 2263): 0.00182843\n",
            "Loss (epoch: 2264): 0.00182758\n",
            "Loss (epoch: 2265): 0.00182673\n",
            "Loss (epoch: 2266): 0.00182589\n",
            "Loss (epoch: 2267): 0.00182504\n",
            "Loss (epoch: 2268): 0.00182420\n",
            "Loss (epoch: 2269): 0.00182336\n",
            "Loss (epoch: 2270): 0.00182252\n",
            "Loss (epoch: 2271): 0.00182168\n",
            "Loss (epoch: 2272): 0.00182085\n",
            "Loss (epoch: 2273): 0.00182001\n",
            "Loss (epoch: 2274): 0.00181918\n",
            "Loss (epoch: 2275): 0.00181835\n",
            "Loss (epoch: 2276): 0.00181752\n",
            "Loss (epoch: 2277): 0.00181670\n",
            "Loss (epoch: 2278): 0.00181587\n",
            "Loss (epoch: 2279): 0.00181505\n",
            "Loss (epoch: 2280): 0.00181423\n",
            "Loss (epoch: 2281): 0.00181341\n",
            "Loss (epoch: 2282): 0.00181259\n",
            "Loss (epoch: 2283): 0.00181178\n",
            "Loss (epoch: 2284): 0.00181096\n",
            "Loss (epoch: 2285): 0.00181015\n",
            "Loss (epoch: 2286): 0.00180934\n",
            "Loss (epoch: 2287): 0.00180853\n",
            "Loss (epoch: 2288): 0.00180772\n",
            "Loss (epoch: 2289): 0.00180691\n",
            "Loss (epoch: 2290): 0.00180611\n",
            "Loss (epoch: 2291): 0.00180531\n",
            "Loss (epoch: 2292): 0.00180450\n",
            "Loss (epoch: 2293): 0.00180371\n",
            "Loss (epoch: 2294): 0.00180291\n",
            "Loss (epoch: 2295): 0.00180211\n",
            "Loss (epoch: 2296): 0.00180132\n",
            "Loss (epoch: 2297): 0.00180052\n",
            "Loss (epoch: 2298): 0.00179973\n",
            "Loss (epoch: 2299): 0.00179894\n",
            "Loss (epoch: 2300): 0.00179815\n",
            "Loss (epoch: 2301): 0.00179736\n",
            "Loss (epoch: 2302): 0.00179658\n",
            "Loss (epoch: 2303): 0.00179579\n",
            "Loss (epoch: 2304): 0.00179501\n",
            "Loss (epoch: 2305): 0.00179423\n",
            "Loss (epoch: 2306): 0.00179345\n",
            "Loss (epoch: 2307): 0.00179267\n",
            "Loss (epoch: 2308): 0.00179189\n",
            "Loss (epoch: 2309): 0.00179112\n",
            "Loss (epoch: 2310): 0.00179034\n",
            "Loss (epoch: 2311): 0.00178957\n",
            "Loss (epoch: 2312): 0.00178880\n",
            "Loss (epoch: 2313): 0.00178803\n",
            "Loss (epoch: 2314): 0.00178726\n",
            "Loss (epoch: 2315): 0.00178649\n",
            "Loss (epoch: 2316): 0.00178573\n",
            "Loss (epoch: 2317): 0.00178496\n",
            "Loss (epoch: 2318): 0.00178420\n",
            "Loss (epoch: 2319): 0.00178344\n",
            "Loss (epoch: 2320): 0.00178268\n",
            "Loss (epoch: 2321): 0.00178192\n",
            "Loss (epoch: 2322): 0.00178116\n",
            "Loss (epoch: 2323): 0.00178040\n",
            "Loss (epoch: 2324): 0.00177965\n",
            "Loss (epoch: 2325): 0.00177890\n",
            "Loss (epoch: 2326): 0.00177814\n",
            "Loss (epoch: 2327): 0.00177739\n",
            "Loss (epoch: 2328): 0.00177664\n",
            "Loss (epoch: 2329): 0.00177589\n",
            "Loss (epoch: 2330): 0.00177515\n",
            "Loss (epoch: 2331): 0.00177440\n",
            "Loss (epoch: 2332): 0.00177366\n",
            "Loss (epoch: 2333): 0.00177291\n",
            "Loss (epoch: 2334): 0.00177217\n",
            "Loss (epoch: 2335): 0.00177143\n",
            "Loss (epoch: 2336): 0.00177069\n",
            "Loss (epoch: 2337): 0.00176995\n",
            "Loss (epoch: 2338): 0.00176921\n",
            "Loss (epoch: 2339): 0.00176848\n",
            "Loss (epoch: 2340): 0.00176774\n",
            "Loss (epoch: 2341): 0.00176701\n",
            "Loss (epoch: 2342): 0.00176627\n",
            "Loss (epoch: 2343): 0.00176554\n",
            "Loss (epoch: 2344): 0.00176481\n",
            "Loss (epoch: 2345): 0.00176408\n",
            "Loss (epoch: 2346): 0.00176336\n",
            "Loss (epoch: 2347): 0.00176263\n",
            "Loss (epoch: 2348): 0.00176190\n",
            "Loss (epoch: 2349): 0.00176118\n",
            "Loss (epoch: 2350): 0.00176046\n",
            "Loss (epoch: 2351): 0.00175973\n",
            "Loss (epoch: 2352): 0.00175901\n",
            "Loss (epoch: 2353): 0.00175829\n",
            "Loss (epoch: 2354): 0.00175757\n",
            "Loss (epoch: 2355): 0.00175686\n",
            "Loss (epoch: 2356): 0.00175614\n",
            "Loss (epoch: 2357): 0.00175542\n",
            "Loss (epoch: 2358): 0.00175471\n",
            "Loss (epoch: 2359): 0.00175400\n",
            "Loss (epoch: 2360): 0.00175328\n",
            "Loss (epoch: 2361): 0.00175257\n",
            "Loss (epoch: 2362): 0.00175186\n",
            "Loss (epoch: 2363): 0.00175115\n",
            "Loss (epoch: 2364): 0.00175044\n",
            "Loss (epoch: 2365): 0.00174974\n",
            "Loss (epoch: 2366): 0.00174903\n",
            "Loss (epoch: 2367): 0.00174833\n",
            "Loss (epoch: 2368): 0.00174762\n",
            "Loss (epoch: 2369): 0.00174692\n",
            "Loss (epoch: 2370): 0.00174622\n",
            "Loss (epoch: 2371): 0.00174552\n",
            "Loss (epoch: 2372): 0.00174482\n",
            "Loss (epoch: 2373): 0.00174412\n",
            "Loss (epoch: 2374): 0.00174342\n",
            "Loss (epoch: 2375): 0.00174272\n",
            "Loss (epoch: 2376): 0.00174203\n",
            "Loss (epoch: 2377): 0.00174133\n",
            "Loss (epoch: 2378): 0.00174064\n",
            "Loss (epoch: 2379): 0.00173995\n",
            "Loss (epoch: 2380): 0.00173925\n",
            "Loss (epoch: 2381): 0.00173856\n",
            "Loss (epoch: 2382): 0.00173787\n",
            "Loss (epoch: 2383): 0.00173718\n",
            "Loss (epoch: 2384): 0.00173650\n",
            "Loss (epoch: 2385): 0.00173581\n",
            "Loss (epoch: 2386): 0.00173512\n",
            "Loss (epoch: 2387): 0.00173444\n",
            "Loss (epoch: 2388): 0.00173375\n",
            "Loss (epoch: 2389): 0.00173307\n",
            "Loss (epoch: 2390): 0.00173239\n",
            "Loss (epoch: 2391): 0.00173170\n",
            "Loss (epoch: 2392): 0.00173102\n",
            "Loss (epoch: 2393): 0.00173034\n",
            "Loss (epoch: 2394): 0.00172966\n",
            "Loss (epoch: 2395): 0.00172899\n",
            "Loss (epoch: 2396): 0.00172831\n",
            "Loss (epoch: 2397): 0.00172763\n",
            "Loss (epoch: 2398): 0.00172696\n",
            "Loss (epoch: 2399): 0.00172628\n",
            "Loss (epoch: 2400): 0.00172561\n",
            "Loss (epoch: 2401): 0.00172494\n",
            "Loss (epoch: 2402): 0.00172426\n",
            "Loss (epoch: 2403): 0.00172359\n",
            "Loss (epoch: 2404): 0.00172292\n",
            "Loss (epoch: 2405): 0.00172225\n",
            "Loss (epoch: 2406): 0.00172158\n",
            "Loss (epoch: 2407): 0.00172092\n",
            "Loss (epoch: 2408): 0.00172025\n",
            "Loss (epoch: 2409): 0.00171958\n",
            "Loss (epoch: 2410): 0.00171892\n",
            "Loss (epoch: 2411): 0.00171825\n",
            "Loss (epoch: 2412): 0.00171759\n",
            "Loss (epoch: 2413): 0.00171693\n",
            "Loss (epoch: 2414): 0.00171627\n",
            "Loss (epoch: 2415): 0.00171561\n",
            "Loss (epoch: 2416): 0.00171494\n",
            "Loss (epoch: 2417): 0.00171429\n",
            "Loss (epoch: 2418): 0.00171363\n",
            "Loss (epoch: 2419): 0.00171297\n",
            "Loss (epoch: 2420): 0.00171231\n",
            "Loss (epoch: 2421): 0.00171165\n",
            "Loss (epoch: 2422): 0.00171100\n",
            "Loss (epoch: 2423): 0.00171034\n",
            "Loss (epoch: 2424): 0.00170969\n",
            "Loss (epoch: 2425): 0.00170904\n",
            "Loss (epoch: 2426): 0.00170838\n",
            "Loss (epoch: 2427): 0.00170773\n",
            "Loss (epoch: 2428): 0.00170708\n",
            "Loss (epoch: 2429): 0.00170643\n",
            "Loss (epoch: 2430): 0.00170578\n",
            "Loss (epoch: 2431): 0.00170513\n",
            "Loss (epoch: 2432): 0.00170449\n",
            "Loss (epoch: 2433): 0.00170384\n",
            "Loss (epoch: 2434): 0.00170319\n",
            "Loss (epoch: 2435): 0.00170255\n",
            "Loss (epoch: 2436): 0.00170190\n",
            "Loss (epoch: 2437): 0.00170126\n",
            "Loss (epoch: 2438): 0.00170061\n",
            "Loss (epoch: 2439): 0.00169997\n",
            "Loss (epoch: 2440): 0.00169933\n",
            "Loss (epoch: 2441): 0.00169869\n",
            "Loss (epoch: 2442): 0.00169805\n",
            "Loss (epoch: 2443): 0.00169741\n",
            "Loss (epoch: 2444): 0.00169677\n",
            "Loss (epoch: 2445): 0.00169613\n",
            "Loss (epoch: 2446): 0.00169549\n",
            "Loss (epoch: 2447): 0.00169485\n",
            "Loss (epoch: 2448): 0.00169422\n",
            "Loss (epoch: 2449): 0.00169358\n",
            "Loss (epoch: 2450): 0.00169295\n",
            "Loss (epoch: 2451): 0.00169231\n",
            "Loss (epoch: 2452): 0.00169168\n",
            "Loss (epoch: 2453): 0.00169105\n",
            "Loss (epoch: 2454): 0.00169041\n",
            "Loss (epoch: 2455): 0.00168978\n",
            "Loss (epoch: 2456): 0.00168915\n",
            "Loss (epoch: 2457): 0.00168852\n",
            "Loss (epoch: 2458): 0.00168789\n",
            "Loss (epoch: 2459): 0.00168726\n",
            "Loss (epoch: 2460): 0.00168663\n",
            "Loss (epoch: 2461): 0.00168601\n",
            "Loss (epoch: 2462): 0.00168538\n",
            "Loss (epoch: 2463): 0.00168475\n",
            "Loss (epoch: 2464): 0.00168413\n",
            "Loss (epoch: 2465): 0.00168350\n",
            "Loss (epoch: 2466): 0.00168288\n",
            "Loss (epoch: 2467): 0.00168225\n",
            "Loss (epoch: 2468): 0.00168163\n",
            "Loss (epoch: 2469): 0.00168101\n",
            "Loss (epoch: 2470): 0.00168039\n",
            "Loss (epoch: 2471): 0.00167977\n",
            "Loss (epoch: 2472): 0.00167914\n",
            "Loss (epoch: 2473): 0.00167852\n",
            "Loss (epoch: 2474): 0.00167791\n",
            "Loss (epoch: 2475): 0.00167729\n",
            "Loss (epoch: 2476): 0.00167667\n",
            "Loss (epoch: 2477): 0.00167605\n",
            "Loss (epoch: 2478): 0.00167543\n",
            "Loss (epoch: 2479): 0.00167482\n",
            "Loss (epoch: 2480): 0.00167420\n",
            "Loss (epoch: 2481): 0.00167359\n",
            "Loss (epoch: 2482): 0.00167297\n",
            "Loss (epoch: 2483): 0.00167236\n",
            "Loss (epoch: 2484): 0.00167175\n",
            "Loss (epoch: 2485): 0.00167113\n",
            "Loss (epoch: 2486): 0.00167052\n",
            "Loss (epoch: 2487): 0.00166991\n",
            "Loss (epoch: 2488): 0.00166930\n",
            "Loss (epoch: 2489): 0.00166869\n",
            "Loss (epoch: 2490): 0.00166808\n",
            "Loss (epoch: 2491): 0.00166747\n",
            "Loss (epoch: 2492): 0.00166686\n",
            "Loss (epoch: 2493): 0.00166625\n",
            "Loss (epoch: 2494): 0.00166565\n",
            "Loss (epoch: 2495): 0.00166504\n",
            "Loss (epoch: 2496): 0.00166443\n",
            "Loss (epoch: 2497): 0.00166383\n",
            "Loss (epoch: 2498): 0.00166322\n",
            "Loss (epoch: 2499): 0.00166262\n",
            "Loss (epoch: 2500): 0.00166201\n",
            "Loss (epoch: 2501): 0.00166141\n",
            "Loss (epoch: 2502): 0.00166081\n",
            "Loss (epoch: 2503): 0.00166021\n",
            "Loss (epoch: 2504): 0.00165960\n",
            "Loss (epoch: 2505): 0.00165900\n",
            "Loss (epoch: 2506): 0.00165840\n",
            "Loss (epoch: 2507): 0.00165780\n",
            "Loss (epoch: 2508): 0.00165720\n",
            "Loss (epoch: 2509): 0.00165660\n",
            "Loss (epoch: 2510): 0.00165601\n",
            "Loss (epoch: 2511): 0.00165541\n",
            "Loss (epoch: 2512): 0.00165481\n",
            "Loss (epoch: 2513): 0.00165421\n",
            "Loss (epoch: 2514): 0.00165362\n",
            "Loss (epoch: 2515): 0.00165302\n",
            "Loss (epoch: 2516): 0.00165243\n",
            "Loss (epoch: 2517): 0.00165183\n",
            "Loss (epoch: 2518): 0.00165124\n",
            "Loss (epoch: 2519): 0.00165064\n",
            "Loss (epoch: 2520): 0.00165005\n",
            "Loss (epoch: 2521): 0.00164946\n",
            "Loss (epoch: 2522): 0.00164887\n",
            "Loss (epoch: 2523): 0.00164827\n",
            "Loss (epoch: 2524): 0.00164768\n",
            "Loss (epoch: 2525): 0.00164709\n",
            "Loss (epoch: 2526): 0.00164650\n",
            "Loss (epoch: 2527): 0.00164591\n",
            "Loss (epoch: 2528): 0.00164532\n",
            "Loss (epoch: 2529): 0.00164474\n",
            "Loss (epoch: 2530): 0.00164415\n",
            "Loss (epoch: 2531): 0.00164356\n",
            "Loss (epoch: 2532): 0.00164297\n",
            "Loss (epoch: 2533): 0.00164239\n",
            "Loss (epoch: 2534): 0.00164180\n",
            "Loss (epoch: 2535): 0.00164122\n",
            "Loss (epoch: 2536): 0.00164063\n",
            "Loss (epoch: 2537): 0.00164005\n",
            "Loss (epoch: 2538): 0.00163946\n",
            "Loss (epoch: 2539): 0.00163888\n",
            "Loss (epoch: 2540): 0.00163830\n",
            "Loss (epoch: 2541): 0.00163771\n",
            "Loss (epoch: 2542): 0.00163713\n",
            "Loss (epoch: 2543): 0.00163655\n",
            "Loss (epoch: 2544): 0.00163597\n",
            "Loss (epoch: 2545): 0.00163539\n",
            "Loss (epoch: 2546): 0.00163481\n",
            "Loss (epoch: 2547): 0.00163423\n",
            "Loss (epoch: 2548): 0.00163365\n",
            "Loss (epoch: 2549): 0.00163307\n",
            "Loss (epoch: 2550): 0.00163249\n",
            "Loss (epoch: 2551): 0.00163192\n",
            "Loss (epoch: 2552): 0.00163134\n",
            "Loss (epoch: 2553): 0.00163076\n",
            "Loss (epoch: 2554): 0.00163018\n",
            "Loss (epoch: 2555): 0.00162961\n",
            "Loss (epoch: 2556): 0.00162903\n",
            "Loss (epoch: 2557): 0.00162846\n",
            "Loss (epoch: 2558): 0.00162788\n",
            "Loss (epoch: 2559): 0.00162731\n",
            "Loss (epoch: 2560): 0.00162674\n",
            "Loss (epoch: 2561): 0.00162616\n",
            "Loss (epoch: 2562): 0.00162559\n",
            "Loss (epoch: 2563): 0.00162502\n",
            "Loss (epoch: 2564): 0.00162445\n",
            "Loss (epoch: 2565): 0.00162388\n",
            "Loss (epoch: 2566): 0.00162330\n",
            "Loss (epoch: 2567): 0.00162273\n",
            "Loss (epoch: 2568): 0.00162216\n",
            "Loss (epoch: 2569): 0.00162159\n",
            "Loss (epoch: 2570): 0.00162102\n",
            "Loss (epoch: 2571): 0.00162046\n",
            "Loss (epoch: 2572): 0.00161989\n",
            "Loss (epoch: 2573): 0.00161932\n",
            "Loss (epoch: 2574): 0.00161875\n",
            "Loss (epoch: 2575): 0.00161819\n",
            "Loss (epoch: 2576): 0.00161762\n",
            "Loss (epoch: 2577): 0.00161705\n",
            "Loss (epoch: 2578): 0.00161649\n",
            "Loss (epoch: 2579): 0.00161592\n",
            "Loss (epoch: 2580): 0.00161536\n",
            "Loss (epoch: 2581): 0.00161479\n",
            "Loss (epoch: 2582): 0.00161423\n",
            "Loss (epoch: 2583): 0.00161366\n",
            "Loss (epoch: 2584): 0.00161310\n",
            "Loss (epoch: 2585): 0.00161254\n",
            "Loss (epoch: 2586): 0.00161198\n",
            "Loss (epoch: 2587): 0.00161141\n",
            "Loss (epoch: 2588): 0.00161085\n",
            "Loss (epoch: 2589): 0.00161029\n",
            "Loss (epoch: 2590): 0.00160973\n",
            "Loss (epoch: 2591): 0.00160917\n",
            "Loss (epoch: 2592): 0.00160861\n",
            "Loss (epoch: 2593): 0.00160805\n",
            "Loss (epoch: 2594): 0.00160749\n",
            "Loss (epoch: 2595): 0.00160694\n",
            "Loss (epoch: 2596): 0.00160638\n",
            "Loss (epoch: 2597): 0.00160582\n",
            "Loss (epoch: 2598): 0.00160526\n",
            "Loss (epoch: 2599): 0.00160470\n",
            "Loss (epoch: 2600): 0.00160415\n",
            "Loss (epoch: 2601): 0.00160359\n",
            "Loss (epoch: 2602): 0.00160304\n",
            "Loss (epoch: 2603): 0.00160248\n",
            "Loss (epoch: 2604): 0.00160193\n",
            "Loss (epoch: 2605): 0.00160137\n",
            "Loss (epoch: 2606): 0.00160082\n",
            "Loss (epoch: 2607): 0.00160026\n",
            "Loss (epoch: 2608): 0.00159971\n",
            "Loss (epoch: 2609): 0.00159916\n",
            "Loss (epoch: 2610): 0.00159860\n",
            "Loss (epoch: 2611): 0.00159805\n",
            "Loss (epoch: 2612): 0.00159750\n",
            "Loss (epoch: 2613): 0.00159695\n",
            "Loss (epoch: 2614): 0.00159640\n",
            "Loss (epoch: 2615): 0.00159585\n",
            "Loss (epoch: 2616): 0.00159530\n",
            "Loss (epoch: 2617): 0.00159475\n",
            "Loss (epoch: 2618): 0.00159420\n",
            "Loss (epoch: 2619): 0.00159365\n",
            "Loss (epoch: 2620): 0.00159310\n",
            "Loss (epoch: 2621): 0.00159255\n",
            "Loss (epoch: 2622): 0.00159200\n",
            "Loss (epoch: 2623): 0.00159145\n",
            "Loss (epoch: 2624): 0.00159091\n",
            "Loss (epoch: 2625): 0.00159036\n",
            "Loss (epoch: 2626): 0.00158981\n",
            "Loss (epoch: 2627): 0.00158927\n",
            "Loss (epoch: 2628): 0.00158872\n",
            "Loss (epoch: 2629): 0.00158817\n",
            "Loss (epoch: 2630): 0.00158763\n",
            "Loss (epoch: 2631): 0.00158709\n",
            "Loss (epoch: 2632): 0.00158654\n",
            "Loss (epoch: 2633): 0.00158600\n",
            "Loss (epoch: 2634): 0.00158545\n",
            "Loss (epoch: 2635): 0.00158491\n",
            "Loss (epoch: 2636): 0.00158437\n",
            "Loss (epoch: 2637): 0.00158382\n",
            "Loss (epoch: 2638): 0.00158328\n",
            "Loss (epoch: 2639): 0.00158274\n",
            "Loss (epoch: 2640): 0.00158220\n",
            "Loss (epoch: 2641): 0.00158166\n",
            "Loss (epoch: 2642): 0.00158112\n",
            "Loss (epoch: 2643): 0.00158058\n",
            "Loss (epoch: 2644): 0.00158003\n",
            "Loss (epoch: 2645): 0.00157950\n",
            "Loss (epoch: 2646): 0.00157896\n",
            "Loss (epoch: 2647): 0.00157842\n",
            "Loss (epoch: 2648): 0.00157788\n",
            "Loss (epoch: 2649): 0.00157734\n",
            "Loss (epoch: 2650): 0.00157680\n",
            "Loss (epoch: 2651): 0.00157626\n",
            "Loss (epoch: 2652): 0.00157573\n",
            "Loss (epoch: 2653): 0.00157519\n",
            "Loss (epoch: 2654): 0.00157465\n",
            "Loss (epoch: 2655): 0.00157412\n",
            "Loss (epoch: 2656): 0.00157358\n",
            "Loss (epoch: 2657): 0.00157304\n",
            "Loss (epoch: 2658): 0.00157251\n",
            "Loss (epoch: 2659): 0.00157197\n",
            "Loss (epoch: 2660): 0.00157144\n",
            "Loss (epoch: 2661): 0.00157090\n",
            "Loss (epoch: 2662): 0.00157037\n",
            "Loss (epoch: 2663): 0.00156984\n",
            "Loss (epoch: 2664): 0.00156930\n",
            "Loss (epoch: 2665): 0.00156877\n",
            "Loss (epoch: 2666): 0.00156824\n",
            "Loss (epoch: 2667): 0.00156770\n",
            "Loss (epoch: 2668): 0.00156717\n",
            "Loss (epoch: 2669): 0.00156664\n",
            "Loss (epoch: 2670): 0.00156611\n",
            "Loss (epoch: 2671): 0.00156558\n",
            "Loss (epoch: 2672): 0.00156505\n",
            "Loss (epoch: 2673): 0.00156452\n",
            "Loss (epoch: 2674): 0.00156398\n",
            "Loss (epoch: 2675): 0.00156345\n",
            "Loss (epoch: 2676): 0.00156293\n",
            "Loss (epoch: 2677): 0.00156240\n",
            "Loss (epoch: 2678): 0.00156187\n",
            "Loss (epoch: 2679): 0.00156134\n",
            "Loss (epoch: 2680): 0.00156081\n",
            "Loss (epoch: 2681): 0.00156028\n",
            "Loss (epoch: 2682): 0.00155975\n",
            "Loss (epoch: 2683): 0.00155923\n",
            "Loss (epoch: 2684): 0.00155870\n",
            "Loss (epoch: 2685): 0.00155817\n",
            "Loss (epoch: 2686): 0.00155765\n",
            "Loss (epoch: 2687): 0.00155712\n",
            "Loss (epoch: 2688): 0.00155659\n",
            "Loss (epoch: 2689): 0.00155607\n",
            "Loss (epoch: 2690): 0.00155554\n",
            "Loss (epoch: 2691): 0.00155502\n",
            "Loss (epoch: 2692): 0.00155449\n",
            "Loss (epoch: 2693): 0.00155397\n",
            "Loss (epoch: 2694): 0.00155345\n",
            "Loss (epoch: 2695): 0.00155292\n",
            "Loss (epoch: 2696): 0.00155240\n",
            "Loss (epoch: 2697): 0.00155187\n",
            "Loss (epoch: 2698): 0.00155135\n",
            "Loss (epoch: 2699): 0.00155083\n",
            "Loss (epoch: 2700): 0.00155031\n",
            "Loss (epoch: 2701): 0.00154978\n",
            "Loss (epoch: 2702): 0.00154926\n",
            "Loss (epoch: 2703): 0.00154874\n",
            "Loss (epoch: 2704): 0.00154822\n",
            "Loss (epoch: 2705): 0.00154770\n",
            "Loss (epoch: 2706): 0.00154718\n",
            "Loss (epoch: 2707): 0.00154666\n",
            "Loss (epoch: 2708): 0.00154614\n",
            "Loss (epoch: 2709): 0.00154562\n",
            "Loss (epoch: 2710): 0.00154510\n",
            "Loss (epoch: 2711): 0.00154458\n",
            "Loss (epoch: 2712): 0.00154406\n",
            "Loss (epoch: 2713): 0.00154354\n",
            "Loss (epoch: 2714): 0.00154302\n",
            "Loss (epoch: 2715): 0.00154250\n",
            "Loss (epoch: 2716): 0.00154199\n",
            "Loss (epoch: 2717): 0.00154147\n",
            "Loss (epoch: 2718): 0.00154095\n",
            "Loss (epoch: 2719): 0.00154043\n",
            "Loss (epoch: 2720): 0.00153992\n",
            "Loss (epoch: 2721): 0.00153940\n",
            "Loss (epoch: 2722): 0.00153889\n",
            "Loss (epoch: 2723): 0.00153837\n",
            "Loss (epoch: 2724): 0.00153785\n",
            "Loss (epoch: 2725): 0.00153734\n",
            "Loss (epoch: 2726): 0.00153682\n",
            "Loss (epoch: 2727): 0.00153631\n",
            "Loss (epoch: 2728): 0.00153579\n",
            "Loss (epoch: 2729): 0.00153528\n",
            "Loss (epoch: 2730): 0.00153476\n",
            "Loss (epoch: 2731): 0.00153425\n",
            "Loss (epoch: 2732): 0.00153374\n",
            "Loss (epoch: 2733): 0.00153322\n",
            "Loss (epoch: 2734): 0.00153271\n",
            "Loss (epoch: 2735): 0.00153220\n",
            "Loss (epoch: 2736): 0.00153169\n",
            "Loss (epoch: 2737): 0.00153117\n",
            "Loss (epoch: 2738): 0.00153066\n",
            "Loss (epoch: 2739): 0.00153015\n",
            "Loss (epoch: 2740): 0.00152964\n",
            "Loss (epoch: 2741): 0.00152913\n",
            "Loss (epoch: 2742): 0.00152862\n",
            "Loss (epoch: 2743): 0.00152810\n",
            "Loss (epoch: 2744): 0.00152759\n",
            "Loss (epoch: 2745): 0.00152708\n",
            "Loss (epoch: 2746): 0.00152657\n",
            "Loss (epoch: 2747): 0.00152606\n",
            "Loss (epoch: 2748): 0.00152555\n",
            "Loss (epoch: 2749): 0.00152505\n",
            "Loss (epoch: 2750): 0.00152454\n",
            "Loss (epoch: 2751): 0.00152403\n",
            "Loss (epoch: 2752): 0.00152352\n",
            "Loss (epoch: 2753): 0.00152301\n",
            "Loss (epoch: 2754): 0.00152250\n",
            "Loss (epoch: 2755): 0.00152200\n",
            "Loss (epoch: 2756): 0.00152149\n",
            "Loss (epoch: 2757): 0.00152098\n",
            "Loss (epoch: 2758): 0.00152047\n",
            "Loss (epoch: 2759): 0.00151997\n",
            "Loss (epoch: 2760): 0.00151946\n",
            "Loss (epoch: 2761): 0.00151895\n",
            "Loss (epoch: 2762): 0.00151845\n",
            "Loss (epoch: 2763): 0.00151794\n",
            "Loss (epoch: 2764): 0.00151744\n",
            "Loss (epoch: 2765): 0.00151693\n",
            "Loss (epoch: 2766): 0.00151642\n",
            "Loss (epoch: 2767): 0.00151592\n",
            "Loss (epoch: 2768): 0.00151542\n",
            "Loss (epoch: 2769): 0.00151491\n",
            "Loss (epoch: 2770): 0.00151441\n",
            "Loss (epoch: 2771): 0.00151390\n",
            "Loss (epoch: 2772): 0.00151340\n",
            "Loss (epoch: 2773): 0.00151290\n",
            "Loss (epoch: 2774): 0.00151239\n",
            "Loss (epoch: 2775): 0.00151189\n",
            "Loss (epoch: 2776): 0.00151139\n",
            "Loss (epoch: 2777): 0.00151088\n",
            "Loss (epoch: 2778): 0.00151038\n",
            "Loss (epoch: 2779): 0.00150988\n",
            "Loss (epoch: 2780): 0.00150938\n",
            "Loss (epoch: 2781): 0.00150888\n",
            "Loss (epoch: 2782): 0.00150837\n",
            "Loss (epoch: 2783): 0.00150787\n",
            "Loss (epoch: 2784): 0.00150737\n",
            "Loss (epoch: 2785): 0.00150687\n",
            "Loss (epoch: 2786): 0.00150637\n",
            "Loss (epoch: 2787): 0.00150587\n",
            "Loss (epoch: 2788): 0.00150537\n",
            "Loss (epoch: 2789): 0.00150487\n",
            "Loss (epoch: 2790): 0.00150437\n",
            "Loss (epoch: 2791): 0.00150387\n",
            "Loss (epoch: 2792): 0.00150337\n",
            "Loss (epoch: 2793): 0.00150287\n",
            "Loss (epoch: 2794): 0.00150237\n",
            "Loss (epoch: 2795): 0.00150187\n",
            "Loss (epoch: 2796): 0.00150137\n",
            "Loss (epoch: 2797): 0.00150088\n",
            "Loss (epoch: 2798): 0.00150038\n",
            "Loss (epoch: 2799): 0.00149988\n",
            "Loss (epoch: 2800): 0.00149938\n",
            "Loss (epoch: 2801): 0.00149888\n",
            "Loss (epoch: 2802): 0.00149839\n",
            "Loss (epoch: 2803): 0.00149789\n",
            "Loss (epoch: 2804): 0.00149739\n",
            "Loss (epoch: 2805): 0.00149690\n",
            "Loss (epoch: 2806): 0.00149640\n",
            "Loss (epoch: 2807): 0.00149590\n",
            "Loss (epoch: 2808): 0.00149541\n",
            "Loss (epoch: 2809): 0.00149491\n",
            "Loss (epoch: 2810): 0.00149442\n",
            "Loss (epoch: 2811): 0.00149392\n",
            "Loss (epoch: 2812): 0.00149342\n",
            "Loss (epoch: 2813): 0.00149293\n",
            "Loss (epoch: 2814): 0.00149243\n",
            "Loss (epoch: 2815): 0.00149194\n",
            "Loss (epoch: 2816): 0.00149144\n",
            "Loss (epoch: 2817): 0.00149095\n",
            "Loss (epoch: 2818): 0.00149046\n",
            "Loss (epoch: 2819): 0.00148996\n",
            "Loss (epoch: 2820): 0.00148947\n",
            "Loss (epoch: 2821): 0.00148897\n",
            "Loss (epoch: 2822): 0.00148848\n",
            "Loss (epoch: 2823): 0.00148799\n",
            "Loss (epoch: 2824): 0.00148750\n",
            "Loss (epoch: 2825): 0.00148700\n",
            "Loss (epoch: 2826): 0.00148651\n",
            "Loss (epoch: 2827): 0.00148602\n",
            "Loss (epoch: 2828): 0.00148553\n",
            "Loss (epoch: 2829): 0.00148503\n",
            "Loss (epoch: 2830): 0.00148454\n",
            "Loss (epoch: 2831): 0.00148405\n",
            "Loss (epoch: 2832): 0.00148356\n",
            "Loss (epoch: 2833): 0.00148307\n",
            "Loss (epoch: 2834): 0.00148257\n",
            "Loss (epoch: 2835): 0.00148208\n",
            "Loss (epoch: 2836): 0.00148159\n",
            "Loss (epoch: 2837): 0.00148110\n",
            "Loss (epoch: 2838): 0.00148061\n",
            "Loss (epoch: 2839): 0.00148012\n",
            "Loss (epoch: 2840): 0.00147963\n",
            "Loss (epoch: 2841): 0.00147914\n",
            "Loss (epoch: 2842): 0.00147865\n",
            "Loss (epoch: 2843): 0.00147816\n",
            "Loss (epoch: 2844): 0.00147767\n",
            "Loss (epoch: 2845): 0.00147718\n",
            "Loss (epoch: 2846): 0.00147669\n",
            "Loss (epoch: 2847): 0.00147620\n",
            "Loss (epoch: 2848): 0.00147572\n",
            "Loss (epoch: 2849): 0.00147523\n",
            "Loss (epoch: 2850): 0.00147474\n",
            "Loss (epoch: 2851): 0.00147425\n",
            "Loss (epoch: 2852): 0.00147376\n",
            "Loss (epoch: 2853): 0.00147327\n",
            "Loss (epoch: 2854): 0.00147279\n",
            "Loss (epoch: 2855): 0.00147230\n",
            "Loss (epoch: 2856): 0.00147181\n",
            "Loss (epoch: 2857): 0.00147132\n",
            "Loss (epoch: 2858): 0.00147084\n",
            "Loss (epoch: 2859): 0.00147035\n",
            "Loss (epoch: 2860): 0.00146986\n",
            "Loss (epoch: 2861): 0.00146938\n",
            "Loss (epoch: 2862): 0.00146889\n",
            "Loss (epoch: 2863): 0.00146840\n",
            "Loss (epoch: 2864): 0.00146792\n",
            "Loss (epoch: 2865): 0.00146743\n",
            "Loss (epoch: 2866): 0.00146694\n",
            "Loss (epoch: 2867): 0.00146646\n",
            "Loss (epoch: 2868): 0.00146597\n",
            "Loss (epoch: 2869): 0.00146549\n",
            "Loss (epoch: 2870): 0.00146500\n",
            "Loss (epoch: 2871): 0.00146452\n",
            "Loss (epoch: 2872): 0.00146403\n",
            "Loss (epoch: 2873): 0.00146355\n",
            "Loss (epoch: 2874): 0.00146306\n",
            "Loss (epoch: 2875): 0.00146258\n",
            "Loss (epoch: 2876): 0.00146209\n",
            "Loss (epoch: 2877): 0.00146161\n",
            "Loss (epoch: 2878): 0.00146112\n",
            "Loss (epoch: 2879): 0.00146064\n",
            "Loss (epoch: 2880): 0.00146016\n",
            "Loss (epoch: 2881): 0.00145967\n",
            "Loss (epoch: 2882): 0.00145919\n",
            "Loss (epoch: 2883): 0.00145871\n",
            "Loss (epoch: 2884): 0.00145822\n",
            "Loss (epoch: 2885): 0.00145774\n",
            "Loss (epoch: 2886): 0.00145726\n",
            "Loss (epoch: 2887): 0.00145677\n",
            "Loss (epoch: 2888): 0.00145629\n",
            "Loss (epoch: 2889): 0.00145581\n",
            "Loss (epoch: 2890): 0.00145533\n",
            "Loss (epoch: 2891): 0.00145484\n",
            "Loss (epoch: 2892): 0.00145436\n",
            "Loss (epoch: 2893): 0.00145388\n",
            "Loss (epoch: 2894): 0.00145340\n",
            "Loss (epoch: 2895): 0.00145292\n",
            "Loss (epoch: 2896): 0.00145243\n",
            "Loss (epoch: 2897): 0.00145195\n",
            "Loss (epoch: 2898): 0.00145147\n",
            "Loss (epoch: 2899): 0.00145099\n",
            "Loss (epoch: 2900): 0.00145051\n",
            "Loss (epoch: 2901): 0.00145003\n",
            "Loss (epoch: 2902): 0.00144955\n",
            "Loss (epoch: 2903): 0.00144907\n",
            "Loss (epoch: 2904): 0.00144859\n",
            "Loss (epoch: 2905): 0.00144811\n",
            "Loss (epoch: 2906): 0.00144763\n",
            "Loss (epoch: 2907): 0.00144714\n",
            "Loss (epoch: 2908): 0.00144666\n",
            "Loss (epoch: 2909): 0.00144618\n",
            "Loss (epoch: 2910): 0.00144570\n",
            "Loss (epoch: 2911): 0.00144522\n",
            "Loss (epoch: 2912): 0.00144475\n",
            "Loss (epoch: 2913): 0.00144427\n",
            "Loss (epoch: 2914): 0.00144379\n",
            "Loss (epoch: 2915): 0.00144331\n",
            "Loss (epoch: 2916): 0.00144283\n",
            "Loss (epoch: 2917): 0.00144235\n",
            "Loss (epoch: 2918): 0.00144187\n",
            "Loss (epoch: 2919): 0.00144139\n",
            "Loss (epoch: 2920): 0.00144091\n",
            "Loss (epoch: 2921): 0.00144043\n",
            "Loss (epoch: 2922): 0.00143996\n",
            "Loss (epoch: 2923): 0.00143948\n",
            "Loss (epoch: 2924): 0.00143900\n",
            "Loss (epoch: 2925): 0.00143852\n",
            "Loss (epoch: 2926): 0.00143804\n",
            "Loss (epoch: 2927): 0.00143756\n",
            "Loss (epoch: 2928): 0.00143709\n",
            "Loss (epoch: 2929): 0.00143661\n",
            "Loss (epoch: 2930): 0.00143613\n",
            "Loss (epoch: 2931): 0.00143565\n",
            "Loss (epoch: 2932): 0.00143518\n",
            "Loss (epoch: 2933): 0.00143470\n",
            "Loss (epoch: 2934): 0.00143422\n",
            "Loss (epoch: 2935): 0.00143375\n",
            "Loss (epoch: 2936): 0.00143327\n",
            "Loss (epoch: 2937): 0.00143279\n",
            "Loss (epoch: 2938): 0.00143232\n",
            "Loss (epoch: 2939): 0.00143184\n",
            "Loss (epoch: 2940): 0.00143136\n",
            "Loss (epoch: 2941): 0.00143088\n",
            "Loss (epoch: 2942): 0.00143041\n",
            "Loss (epoch: 2943): 0.00142993\n",
            "Loss (epoch: 2944): 0.00142946\n",
            "Loss (epoch: 2945): 0.00142898\n",
            "Loss (epoch: 2946): 0.00142850\n",
            "Loss (epoch: 2947): 0.00142803\n",
            "Loss (epoch: 2948): 0.00142755\n",
            "Loss (epoch: 2949): 0.00142708\n",
            "Loss (epoch: 2950): 0.00142660\n",
            "Loss (epoch: 2951): 0.00142613\n",
            "Loss (epoch: 2952): 0.00142565\n",
            "Loss (epoch: 2953): 0.00142518\n",
            "Loss (epoch: 2954): 0.00142470\n",
            "Loss (epoch: 2955): 0.00142423\n",
            "Loss (epoch: 2956): 0.00142375\n",
            "Loss (epoch: 2957): 0.00142328\n",
            "Loss (epoch: 2958): 0.00142280\n",
            "Loss (epoch: 2959): 0.00142232\n",
            "Loss (epoch: 2960): 0.00142185\n",
            "Loss (epoch: 2961): 0.00142138\n",
            "Loss (epoch: 2962): 0.00142090\n",
            "Loss (epoch: 2963): 0.00142043\n",
            "Loss (epoch: 2964): 0.00141995\n",
            "Loss (epoch: 2965): 0.00141948\n",
            "Loss (epoch: 2966): 0.00141900\n",
            "Loss (epoch: 2967): 0.00141853\n",
            "Loss (epoch: 2968): 0.00141806\n",
            "Loss (epoch: 2969): 0.00141758\n",
            "Loss (epoch: 2970): 0.00141711\n",
            "Loss (epoch: 2971): 0.00141663\n",
            "Loss (epoch: 2972): 0.00141616\n",
            "Loss (epoch: 2973): 0.00141569\n",
            "Loss (epoch: 2974): 0.00141521\n",
            "Loss (epoch: 2975): 0.00141474\n",
            "Loss (epoch: 2976): 0.00141427\n",
            "Loss (epoch: 2977): 0.00141379\n",
            "Loss (epoch: 2978): 0.00141332\n",
            "Loss (epoch: 2979): 0.00141285\n",
            "Loss (epoch: 2980): 0.00141237\n",
            "Loss (epoch: 2981): 0.00141190\n",
            "Loss (epoch: 2982): 0.00141143\n",
            "Loss (epoch: 2983): 0.00141096\n",
            "Loss (epoch: 2984): 0.00141048\n",
            "Loss (epoch: 2985): 0.00141001\n",
            "Loss (epoch: 2986): 0.00140954\n",
            "Loss (epoch: 2987): 0.00140907\n",
            "Loss (epoch: 2988): 0.00140859\n",
            "Loss (epoch: 2989): 0.00140812\n",
            "Loss (epoch: 2990): 0.00140765\n",
            "Loss (epoch: 2991): 0.00140718\n",
            "Loss (epoch: 2992): 0.00140670\n",
            "Loss (epoch: 2993): 0.00140623\n",
            "Loss (epoch: 2994): 0.00140576\n",
            "Loss (epoch: 2995): 0.00140529\n",
            "Loss (epoch: 2996): 0.00140482\n",
            "Loss (epoch: 2997): 0.00140434\n",
            "Loss (epoch: 2998): 0.00140387\n",
            "Loss (epoch: 2999): 0.00140340\n",
            "Loss (epoch: 3000): 0.00140293\n",
            "Loss (epoch: 3001): 0.00140246\n",
            "Loss (epoch: 3002): 0.00140198\n",
            "Loss (epoch: 3003): 0.00140151\n",
            "Loss (epoch: 3004): 0.00140104\n",
            "Loss (epoch: 3005): 0.00140057\n",
            "Loss (epoch: 3006): 0.00140010\n",
            "Loss (epoch: 3007): 0.00139963\n",
            "Loss (epoch: 3008): 0.00139916\n",
            "Loss (epoch: 3009): 0.00139868\n",
            "Loss (epoch: 3010): 0.00139821\n",
            "Loss (epoch: 3011): 0.00139774\n",
            "Loss (epoch: 3012): 0.00139727\n",
            "Loss (epoch: 3013): 0.00139680\n",
            "Loss (epoch: 3014): 0.00139633\n",
            "Loss (epoch: 3015): 0.00139586\n",
            "Loss (epoch: 3016): 0.00139539\n",
            "Loss (epoch: 3017): 0.00139492\n",
            "Loss (epoch: 3018): 0.00139445\n",
            "Loss (epoch: 3019): 0.00139398\n",
            "Loss (epoch: 3020): 0.00139350\n",
            "Loss (epoch: 3021): 0.00139303\n",
            "Loss (epoch: 3022): 0.00139256\n",
            "Loss (epoch: 3023): 0.00139209\n",
            "Loss (epoch: 3024): 0.00139162\n",
            "Loss (epoch: 3025): 0.00139115\n",
            "Loss (epoch: 3026): 0.00139068\n",
            "Loss (epoch: 3027): 0.00139021\n",
            "Loss (epoch: 3028): 0.00138974\n",
            "Loss (epoch: 3029): 0.00138927\n",
            "Loss (epoch: 3030): 0.00138880\n",
            "Loss (epoch: 3031): 0.00138833\n",
            "Loss (epoch: 3032): 0.00138786\n",
            "Loss (epoch: 3033): 0.00138739\n",
            "Loss (epoch: 3034): 0.00138692\n",
            "Loss (epoch: 3035): 0.00138645\n",
            "Loss (epoch: 3036): 0.00138598\n",
            "Loss (epoch: 3037): 0.00138551\n",
            "Loss (epoch: 3038): 0.00138504\n",
            "Loss (epoch: 3039): 0.00138457\n",
            "Loss (epoch: 3040): 0.00138410\n",
            "Loss (epoch: 3041): 0.00138363\n",
            "Loss (epoch: 3042): 0.00138316\n",
            "Loss (epoch: 3043): 0.00138269\n",
            "Loss (epoch: 3044): 0.00138222\n",
            "Loss (epoch: 3045): 0.00138175\n",
            "Loss (epoch: 3046): 0.00138128\n",
            "Loss (epoch: 3047): 0.00138081\n",
            "Loss (epoch: 3048): 0.00138034\n",
            "Loss (epoch: 3049): 0.00137987\n",
            "Loss (epoch: 3050): 0.00137940\n",
            "Loss (epoch: 3051): 0.00137893\n",
            "Loss (epoch: 3052): 0.00137847\n",
            "Loss (epoch: 3053): 0.00137800\n",
            "Loss (epoch: 3054): 0.00137753\n",
            "Loss (epoch: 3055): 0.00137706\n",
            "Loss (epoch: 3056): 0.00137659\n",
            "Loss (epoch: 3057): 0.00137612\n",
            "Loss (epoch: 3058): 0.00137565\n",
            "Loss (epoch: 3059): 0.00137518\n",
            "Loss (epoch: 3060): 0.00137471\n",
            "Loss (epoch: 3061): 0.00137424\n",
            "Loss (epoch: 3062): 0.00137377\n",
            "Loss (epoch: 3063): 0.00137330\n",
            "Loss (epoch: 3064): 0.00137283\n",
            "Loss (epoch: 3065): 0.00137236\n",
            "Loss (epoch: 3066): 0.00137190\n",
            "Loss (epoch: 3067): 0.00137143\n",
            "Loss (epoch: 3068): 0.00137096\n",
            "Loss (epoch: 3069): 0.00137049\n",
            "Loss (epoch: 3070): 0.00137002\n",
            "Loss (epoch: 3071): 0.00136955\n",
            "Loss (epoch: 3072): 0.00136908\n",
            "Loss (epoch: 3073): 0.00136861\n",
            "Loss (epoch: 3074): 0.00136814\n",
            "Loss (epoch: 3075): 0.00136767\n",
            "Loss (epoch: 3076): 0.00136720\n",
            "Loss (epoch: 3077): 0.00136674\n",
            "Loss (epoch: 3078): 0.00136627\n",
            "Loss (epoch: 3079): 0.00136580\n",
            "Loss (epoch: 3080): 0.00136533\n",
            "Loss (epoch: 3081): 0.00136486\n",
            "Loss (epoch: 3082): 0.00136439\n",
            "Loss (epoch: 3083): 0.00136392\n",
            "Loss (epoch: 3084): 0.00136345\n",
            "Loss (epoch: 3085): 0.00136298\n",
            "Loss (epoch: 3086): 0.00136252\n",
            "Loss (epoch: 3087): 0.00136205\n",
            "Loss (epoch: 3088): 0.00136158\n",
            "Loss (epoch: 3089): 0.00136111\n",
            "Loss (epoch: 3090): 0.00136064\n",
            "Loss (epoch: 3091): 0.00136017\n",
            "Loss (epoch: 3092): 0.00135970\n",
            "Loss (epoch: 3093): 0.00135923\n",
            "Loss (epoch: 3094): 0.00135876\n",
            "Loss (epoch: 3095): 0.00135830\n",
            "Loss (epoch: 3096): 0.00135783\n",
            "Loss (epoch: 3097): 0.00135736\n",
            "Loss (epoch: 3098): 0.00135689\n",
            "Loss (epoch: 3099): 0.00135642\n",
            "Loss (epoch: 3100): 0.00135595\n",
            "Loss (epoch: 3101): 0.00135548\n",
            "Loss (epoch: 3102): 0.00135501\n",
            "Loss (epoch: 3103): 0.00135455\n",
            "Loss (epoch: 3104): 0.00135408\n",
            "Loss (epoch: 3105): 0.00135361\n",
            "Loss (epoch: 3106): 0.00135314\n",
            "Loss (epoch: 3107): 0.00135267\n",
            "Loss (epoch: 3108): 0.00135220\n",
            "Loss (epoch: 3109): 0.00135173\n",
            "Loss (epoch: 3110): 0.00135126\n",
            "Loss (epoch: 3111): 0.00135079\n",
            "Loss (epoch: 3112): 0.00135033\n",
            "Loss (epoch: 3113): 0.00134986\n",
            "Loss (epoch: 3114): 0.00134939\n",
            "Loss (epoch: 3115): 0.00134892\n",
            "Loss (epoch: 3116): 0.00134845\n",
            "Loss (epoch: 3117): 0.00134798\n",
            "Loss (epoch: 3118): 0.00134751\n",
            "Loss (epoch: 3119): 0.00134704\n",
            "Loss (epoch: 3120): 0.00134657\n",
            "Loss (epoch: 3121): 0.00134610\n",
            "Loss (epoch: 3122): 0.00134564\n",
            "Loss (epoch: 3123): 0.00134517\n",
            "Loss (epoch: 3124): 0.00134470\n",
            "Loss (epoch: 3125): 0.00134423\n",
            "Loss (epoch: 3126): 0.00134376\n",
            "Loss (epoch: 3127): 0.00134329\n",
            "Loss (epoch: 3128): 0.00134282\n",
            "Loss (epoch: 3129): 0.00134235\n",
            "Loss (epoch: 3130): 0.00134188\n",
            "Loss (epoch: 3131): 0.00134141\n",
            "Loss (epoch: 3132): 0.00134094\n",
            "Loss (epoch: 3133): 0.00134048\n",
            "Loss (epoch: 3134): 0.00134000\n",
            "Loss (epoch: 3135): 0.00133954\n",
            "Loss (epoch: 3136): 0.00133907\n",
            "Loss (epoch: 3137): 0.00133860\n",
            "Loss (epoch: 3138): 0.00133813\n",
            "Loss (epoch: 3139): 0.00133766\n",
            "Loss (epoch: 3140): 0.00133719\n",
            "Loss (epoch: 3141): 0.00133672\n",
            "Loss (epoch: 3142): 0.00133625\n",
            "Loss (epoch: 3143): 0.00133578\n",
            "Loss (epoch: 3144): 0.00133531\n",
            "Loss (epoch: 3145): 0.00133484\n",
            "Loss (epoch: 3146): 0.00133437\n",
            "Loss (epoch: 3147): 0.00133390\n",
            "Loss (epoch: 3148): 0.00133343\n",
            "Loss (epoch: 3149): 0.00133296\n",
            "Loss (epoch: 3150): 0.00133249\n",
            "Loss (epoch: 3151): 0.00133202\n",
            "Loss (epoch: 3152): 0.00133156\n",
            "Loss (epoch: 3153): 0.00133109\n",
            "Loss (epoch: 3154): 0.00133062\n",
            "Loss (epoch: 3155): 0.00133015\n",
            "Loss (epoch: 3156): 0.00132968\n",
            "Loss (epoch: 3157): 0.00132921\n",
            "Loss (epoch: 3158): 0.00132874\n",
            "Loss (epoch: 3159): 0.00132827\n",
            "Loss (epoch: 3160): 0.00132780\n",
            "Loss (epoch: 3161): 0.00132733\n",
            "Loss (epoch: 3162): 0.00132686\n",
            "Loss (epoch: 3163): 0.00132639\n",
            "Loss (epoch: 3164): 0.00132592\n",
            "Loss (epoch: 3165): 0.00132545\n",
            "Loss (epoch: 3166): 0.00132498\n",
            "Loss (epoch: 3167): 0.00132451\n",
            "Loss (epoch: 3168): 0.00132404\n",
            "Loss (epoch: 3169): 0.00132357\n",
            "Loss (epoch: 3170): 0.00132310\n",
            "Loss (epoch: 3171): 0.00132262\n",
            "Loss (epoch: 3172): 0.00132215\n",
            "Loss (epoch: 3173): 0.00132168\n",
            "Loss (epoch: 3174): 0.00132121\n",
            "Loss (epoch: 3175): 0.00132074\n",
            "Loss (epoch: 3176): 0.00132027\n",
            "Loss (epoch: 3177): 0.00131980\n",
            "Loss (epoch: 3178): 0.00131933\n",
            "Loss (epoch: 3179): 0.00131886\n",
            "Loss (epoch: 3180): 0.00131839\n",
            "Loss (epoch: 3181): 0.00131792\n",
            "Loss (epoch: 3182): 0.00131745\n",
            "Loss (epoch: 3183): 0.00131698\n",
            "Loss (epoch: 3184): 0.00131651\n",
            "Loss (epoch: 3185): 0.00131603\n",
            "Loss (epoch: 3186): 0.00131556\n",
            "Loss (epoch: 3187): 0.00131509\n",
            "Loss (epoch: 3188): 0.00131462\n",
            "Loss (epoch: 3189): 0.00131415\n",
            "Loss (epoch: 3190): 0.00131368\n",
            "Loss (epoch: 3191): 0.00131321\n",
            "Loss (epoch: 3192): 0.00131274\n",
            "Loss (epoch: 3193): 0.00131227\n",
            "Loss (epoch: 3194): 0.00131179\n",
            "Loss (epoch: 3195): 0.00131132\n",
            "Loss (epoch: 3196): 0.00131085\n",
            "Loss (epoch: 3197): 0.00131038\n",
            "Loss (epoch: 3198): 0.00130991\n",
            "Loss (epoch: 3199): 0.00130943\n",
            "Loss (epoch: 3200): 0.00130896\n",
            "Loss (epoch: 3201): 0.00130849\n",
            "Loss (epoch: 3202): 0.00130802\n",
            "Loss (epoch: 3203): 0.00130755\n",
            "Loss (epoch: 3204): 0.00130708\n",
            "Loss (epoch: 3205): 0.00130660\n",
            "Loss (epoch: 3206): 0.00130613\n",
            "Loss (epoch: 3207): 0.00130566\n",
            "Loss (epoch: 3208): 0.00130519\n",
            "Loss (epoch: 3209): 0.00130471\n",
            "Loss (epoch: 3210): 0.00130424\n",
            "Loss (epoch: 3211): 0.00130377\n",
            "Loss (epoch: 3212): 0.00130330\n",
            "Loss (epoch: 3213): 0.00130283\n",
            "Loss (epoch: 3214): 0.00130235\n",
            "Loss (epoch: 3215): 0.00130188\n",
            "Loss (epoch: 3216): 0.00130141\n",
            "Loss (epoch: 3217): 0.00130093\n",
            "Loss (epoch: 3218): 0.00130046\n",
            "Loss (epoch: 3219): 0.00129999\n",
            "Loss (epoch: 3220): 0.00129952\n",
            "Loss (epoch: 3221): 0.00129904\n",
            "Loss (epoch: 3222): 0.00129857\n",
            "Loss (epoch: 3223): 0.00129810\n",
            "Loss (epoch: 3224): 0.00129762\n",
            "Loss (epoch: 3225): 0.00129715\n",
            "Loss (epoch: 3226): 0.00129668\n",
            "Loss (epoch: 3227): 0.00129620\n",
            "Loss (epoch: 3228): 0.00129573\n",
            "Loss (epoch: 3229): 0.00129526\n",
            "Loss (epoch: 3230): 0.00129478\n",
            "Loss (epoch: 3231): 0.00129431\n",
            "Loss (epoch: 3232): 0.00129384\n",
            "Loss (epoch: 3233): 0.00129336\n",
            "Loss (epoch: 3234): 0.00129289\n",
            "Loss (epoch: 3235): 0.00129241\n",
            "Loss (epoch: 3236): 0.00129194\n",
            "Loss (epoch: 3237): 0.00129146\n",
            "Loss (epoch: 3238): 0.00129099\n",
            "Loss (epoch: 3239): 0.00129052\n",
            "Loss (epoch: 3240): 0.00129004\n",
            "Loss (epoch: 3241): 0.00128957\n",
            "Loss (epoch: 3242): 0.00128909\n",
            "Loss (epoch: 3243): 0.00128862\n",
            "Loss (epoch: 3244): 0.00128814\n",
            "Loss (epoch: 3245): 0.00128767\n",
            "Loss (epoch: 3246): 0.00128719\n",
            "Loss (epoch: 3247): 0.00128672\n",
            "Loss (epoch: 3248): 0.00128624\n",
            "Loss (epoch: 3249): 0.00128577\n",
            "Loss (epoch: 3250): 0.00128529\n",
            "Loss (epoch: 3251): 0.00128482\n",
            "Loss (epoch: 3252): 0.00128434\n",
            "Loss (epoch: 3253): 0.00128387\n",
            "Loss (epoch: 3254): 0.00128339\n",
            "Loss (epoch: 3255): 0.00128292\n",
            "Loss (epoch: 3256): 0.00128244\n",
            "Loss (epoch: 3257): 0.00128197\n",
            "Loss (epoch: 3258): 0.00128149\n",
            "Loss (epoch: 3259): 0.00128101\n",
            "Loss (epoch: 3260): 0.00128054\n",
            "Loss (epoch: 3261): 0.00128006\n",
            "Loss (epoch: 3262): 0.00127959\n",
            "Loss (epoch: 3263): 0.00127911\n",
            "Loss (epoch: 3264): 0.00127863\n",
            "Loss (epoch: 3265): 0.00127816\n",
            "Loss (epoch: 3266): 0.00127768\n",
            "Loss (epoch: 3267): 0.00127720\n",
            "Loss (epoch: 3268): 0.00127673\n",
            "Loss (epoch: 3269): 0.00127625\n",
            "Loss (epoch: 3270): 0.00127578\n",
            "Loss (epoch: 3271): 0.00127530\n",
            "Loss (epoch: 3272): 0.00127482\n",
            "Loss (epoch: 3273): 0.00127434\n",
            "Loss (epoch: 3274): 0.00127387\n",
            "Loss (epoch: 3275): 0.00127339\n",
            "Loss (epoch: 3276): 0.00127291\n",
            "Loss (epoch: 3277): 0.00127243\n",
            "Loss (epoch: 3278): 0.00127196\n",
            "Loss (epoch: 3279): 0.00127148\n",
            "Loss (epoch: 3280): 0.00127100\n",
            "Loss (epoch: 3281): 0.00127052\n",
            "Loss (epoch: 3282): 0.00127005\n",
            "Loss (epoch: 3283): 0.00126957\n",
            "Loss (epoch: 3284): 0.00126909\n",
            "Loss (epoch: 3285): 0.00126861\n",
            "Loss (epoch: 3286): 0.00126813\n",
            "Loss (epoch: 3287): 0.00126766\n",
            "Loss (epoch: 3288): 0.00126718\n",
            "Loss (epoch: 3289): 0.00126670\n",
            "Loss (epoch: 3290): 0.00126622\n",
            "Loss (epoch: 3291): 0.00126574\n",
            "Loss (epoch: 3292): 0.00126526\n",
            "Loss (epoch: 3293): 0.00126479\n",
            "Loss (epoch: 3294): 0.00126431\n",
            "Loss (epoch: 3295): 0.00126383\n",
            "Loss (epoch: 3296): 0.00126335\n",
            "Loss (epoch: 3297): 0.00126287\n",
            "Loss (epoch: 3298): 0.00126239\n",
            "Loss (epoch: 3299): 0.00126191\n",
            "Loss (epoch: 3300): 0.00126143\n",
            "Loss (epoch: 3301): 0.00126095\n",
            "Loss (epoch: 3302): 0.00126047\n",
            "Loss (epoch: 3303): 0.00125999\n",
            "Loss (epoch: 3304): 0.00125951\n",
            "Loss (epoch: 3305): 0.00125903\n",
            "Loss (epoch: 3306): 0.00125855\n",
            "Loss (epoch: 3307): 0.00125807\n",
            "Loss (epoch: 3308): 0.00125759\n",
            "Loss (epoch: 3309): 0.00125711\n",
            "Loss (epoch: 3310): 0.00125663\n",
            "Loss (epoch: 3311): 0.00125615\n",
            "Loss (epoch: 3312): 0.00125567\n",
            "Loss (epoch: 3313): 0.00125519\n",
            "Loss (epoch: 3314): 0.00125471\n",
            "Loss (epoch: 3315): 0.00125423\n",
            "Loss (epoch: 3316): 0.00125375\n",
            "Loss (epoch: 3317): 0.00125326\n",
            "Loss (epoch: 3318): 0.00125278\n",
            "Loss (epoch: 3319): 0.00125230\n",
            "Loss (epoch: 3320): 0.00125182\n",
            "Loss (epoch: 3321): 0.00125134\n",
            "Loss (epoch: 3322): 0.00125086\n",
            "Loss (epoch: 3323): 0.00125037\n",
            "Loss (epoch: 3324): 0.00124989\n",
            "Loss (epoch: 3325): 0.00124941\n",
            "Loss (epoch: 3326): 0.00124893\n",
            "Loss (epoch: 3327): 0.00124845\n",
            "Loss (epoch: 3328): 0.00124796\n",
            "Loss (epoch: 3329): 0.00124748\n",
            "Loss (epoch: 3330): 0.00124700\n",
            "Loss (epoch: 3331): 0.00124652\n",
            "Loss (epoch: 3332): 0.00124603\n",
            "Loss (epoch: 3333): 0.00124555\n",
            "Loss (epoch: 3334): 0.00124507\n",
            "Loss (epoch: 3335): 0.00124458\n",
            "Loss (epoch: 3336): 0.00124410\n",
            "Loss (epoch: 3337): 0.00124362\n",
            "Loss (epoch: 3338): 0.00124313\n",
            "Loss (epoch: 3339): 0.00124265\n",
            "Loss (epoch: 3340): 0.00124217\n",
            "Loss (epoch: 3341): 0.00124168\n",
            "Loss (epoch: 3342): 0.00124120\n",
            "Loss (epoch: 3343): 0.00124071\n",
            "Loss (epoch: 3344): 0.00124023\n",
            "Loss (epoch: 3345): 0.00123975\n",
            "Loss (epoch: 3346): 0.00123926\n",
            "Loss (epoch: 3347): 0.00123878\n",
            "Loss (epoch: 3348): 0.00123829\n",
            "Loss (epoch: 3349): 0.00123781\n",
            "Loss (epoch: 3350): 0.00123732\n",
            "Loss (epoch: 3351): 0.00123684\n",
            "Loss (epoch: 3352): 0.00123635\n",
            "Loss (epoch: 3353): 0.00123587\n",
            "Loss (epoch: 3354): 0.00123538\n",
            "Loss (epoch: 3355): 0.00123490\n",
            "Loss (epoch: 3356): 0.00123441\n",
            "Loss (epoch: 3357): 0.00123393\n",
            "Loss (epoch: 3358): 0.00123344\n",
            "Loss (epoch: 3359): 0.00123296\n",
            "Loss (epoch: 3360): 0.00123247\n",
            "Loss (epoch: 3361): 0.00123198\n",
            "Loss (epoch: 3362): 0.00123150\n",
            "Loss (epoch: 3363): 0.00123101\n",
            "Loss (epoch: 3364): 0.00123053\n",
            "Loss (epoch: 3365): 0.00123004\n",
            "Loss (epoch: 3366): 0.00122955\n",
            "Loss (epoch: 3367): 0.00122907\n",
            "Loss (epoch: 3368): 0.00122858\n",
            "Loss (epoch: 3369): 0.00122809\n",
            "Loss (epoch: 3370): 0.00122761\n",
            "Loss (epoch: 3371): 0.00122712\n",
            "Loss (epoch: 3372): 0.00122663\n",
            "Loss (epoch: 3373): 0.00122614\n",
            "Loss (epoch: 3374): 0.00122565\n",
            "Loss (epoch: 3375): 0.00122517\n",
            "Loss (epoch: 3376): 0.00122468\n",
            "Loss (epoch: 3377): 0.00122419\n",
            "Loss (epoch: 3378): 0.00122370\n",
            "Loss (epoch: 3379): 0.00122322\n",
            "Loss (epoch: 3380): 0.00122273\n",
            "Loss (epoch: 3381): 0.00122224\n",
            "Loss (epoch: 3382): 0.00122175\n",
            "Loss (epoch: 3383): 0.00122126\n",
            "Loss (epoch: 3384): 0.00122077\n",
            "Loss (epoch: 3385): 0.00122028\n",
            "Loss (epoch: 3386): 0.00121979\n",
            "Loss (epoch: 3387): 0.00121931\n",
            "Loss (epoch: 3388): 0.00121882\n",
            "Loss (epoch: 3389): 0.00121833\n",
            "Loss (epoch: 3390): 0.00121784\n",
            "Loss (epoch: 3391): 0.00121735\n",
            "Loss (epoch: 3392): 0.00121686\n",
            "Loss (epoch: 3393): 0.00121637\n",
            "Loss (epoch: 3394): 0.00121588\n",
            "Loss (epoch: 3395): 0.00121539\n",
            "Loss (epoch: 3396): 0.00121490\n",
            "Loss (epoch: 3397): 0.00121441\n",
            "Loss (epoch: 3398): 0.00121392\n",
            "Loss (epoch: 3399): 0.00121343\n",
            "Loss (epoch: 3400): 0.00121294\n",
            "Loss (epoch: 3401): 0.00121245\n",
            "Loss (epoch: 3402): 0.00121196\n",
            "Loss (epoch: 3403): 0.00121147\n",
            "Loss (epoch: 3404): 0.00121097\n",
            "Loss (epoch: 3405): 0.00121048\n",
            "Loss (epoch: 3406): 0.00120999\n",
            "Loss (epoch: 3407): 0.00120950\n",
            "Loss (epoch: 3408): 0.00120901\n",
            "Loss (epoch: 3409): 0.00120852\n",
            "Loss (epoch: 3410): 0.00120803\n",
            "Loss (epoch: 3411): 0.00120753\n",
            "Loss (epoch: 3412): 0.00120704\n",
            "Loss (epoch: 3413): 0.00120655\n",
            "Loss (epoch: 3414): 0.00120606\n",
            "Loss (epoch: 3415): 0.00120556\n",
            "Loss (epoch: 3416): 0.00120507\n",
            "Loss (epoch: 3417): 0.00120458\n",
            "Loss (epoch: 3418): 0.00120409\n",
            "Loss (epoch: 3419): 0.00120359\n",
            "Loss (epoch: 3420): 0.00120310\n",
            "Loss (epoch: 3421): 0.00120261\n",
            "Loss (epoch: 3422): 0.00120211\n",
            "Loss (epoch: 3423): 0.00120162\n",
            "Loss (epoch: 3424): 0.00120113\n",
            "Loss (epoch: 3425): 0.00120063\n",
            "Loss (epoch: 3426): 0.00120014\n",
            "Loss (epoch: 3427): 0.00119964\n",
            "Loss (epoch: 3428): 0.00119915\n",
            "Loss (epoch: 3429): 0.00119866\n",
            "Loss (epoch: 3430): 0.00119816\n",
            "Loss (epoch: 3431): 0.00119767\n",
            "Loss (epoch: 3432): 0.00119717\n",
            "Loss (epoch: 3433): 0.00119668\n",
            "Loss (epoch: 3434): 0.00119618\n",
            "Loss (epoch: 3435): 0.00119569\n",
            "Loss (epoch: 3436): 0.00119519\n",
            "Loss (epoch: 3437): 0.00119470\n",
            "Loss (epoch: 3438): 0.00119420\n",
            "Loss (epoch: 3439): 0.00119371\n",
            "Loss (epoch: 3440): 0.00119321\n",
            "Loss (epoch: 3441): 0.00119272\n",
            "Loss (epoch: 3442): 0.00119222\n",
            "Loss (epoch: 3443): 0.00119172\n",
            "Loss (epoch: 3444): 0.00119123\n",
            "Loss (epoch: 3445): 0.00119073\n",
            "Loss (epoch: 3446): 0.00119024\n",
            "Loss (epoch: 3447): 0.00118974\n",
            "Loss (epoch: 3448): 0.00118924\n",
            "Loss (epoch: 3449): 0.00118875\n",
            "Loss (epoch: 3450): 0.00118825\n",
            "Loss (epoch: 3451): 0.00118775\n",
            "Loss (epoch: 3452): 0.00118726\n",
            "Loss (epoch: 3453): 0.00118676\n",
            "Loss (epoch: 3454): 0.00118626\n",
            "Loss (epoch: 3455): 0.00118576\n",
            "Loss (epoch: 3456): 0.00118527\n",
            "Loss (epoch: 3457): 0.00118477\n",
            "Loss (epoch: 3458): 0.00118427\n",
            "Loss (epoch: 3459): 0.00118377\n",
            "Loss (epoch: 3460): 0.00118327\n",
            "Loss (epoch: 3461): 0.00118278\n",
            "Loss (epoch: 3462): 0.00118228\n",
            "Loss (epoch: 3463): 0.00118178\n",
            "Loss (epoch: 3464): 0.00118128\n",
            "Loss (epoch: 3465): 0.00118078\n",
            "Loss (epoch: 3466): 0.00118028\n",
            "Loss (epoch: 3467): 0.00117979\n",
            "Loss (epoch: 3468): 0.00117929\n",
            "Loss (epoch: 3469): 0.00117879\n",
            "Loss (epoch: 3470): 0.00117829\n",
            "Loss (epoch: 3471): 0.00117779\n",
            "Loss (epoch: 3472): 0.00117729\n",
            "Loss (epoch: 3473): 0.00117679\n",
            "Loss (epoch: 3474): 0.00117629\n",
            "Loss (epoch: 3475): 0.00117579\n",
            "Loss (epoch: 3476): 0.00117529\n",
            "Loss (epoch: 3477): 0.00117479\n",
            "Loss (epoch: 3478): 0.00117429\n",
            "Loss (epoch: 3479): 0.00117379\n",
            "Loss (epoch: 3480): 0.00117329\n",
            "Loss (epoch: 3481): 0.00117279\n",
            "Loss (epoch: 3482): 0.00117229\n",
            "Loss (epoch: 3483): 0.00117179\n",
            "Loss (epoch: 3484): 0.00117128\n",
            "Loss (epoch: 3485): 0.00117078\n",
            "Loss (epoch: 3486): 0.00117028\n",
            "Loss (epoch: 3487): 0.00116978\n",
            "Loss (epoch: 3488): 0.00116928\n",
            "Loss (epoch: 3489): 0.00116878\n",
            "Loss (epoch: 3490): 0.00116828\n",
            "Loss (epoch: 3491): 0.00116777\n",
            "Loss (epoch: 3492): 0.00116727\n",
            "Loss (epoch: 3493): 0.00116677\n",
            "Loss (epoch: 3494): 0.00116627\n",
            "Loss (epoch: 3495): 0.00116577\n",
            "Loss (epoch: 3496): 0.00116526\n",
            "Loss (epoch: 3497): 0.00116476\n",
            "Loss (epoch: 3498): 0.00116426\n",
            "Loss (epoch: 3499): 0.00116375\n",
            "Loss (epoch: 3500): 0.00116325\n",
            "Loss (epoch: 3501): 0.00116275\n",
            "Loss (epoch: 3502): 0.00116225\n",
            "Loss (epoch: 3503): 0.00116174\n",
            "Loss (epoch: 3504): 0.00116124\n",
            "Loss (epoch: 3505): 0.00116074\n",
            "Loss (epoch: 3506): 0.00116023\n",
            "Loss (epoch: 3507): 0.00115973\n",
            "Loss (epoch: 3508): 0.00115922\n",
            "Loss (epoch: 3509): 0.00115872\n",
            "Loss (epoch: 3510): 0.00115822\n",
            "Loss (epoch: 3511): 0.00115771\n",
            "Loss (epoch: 3512): 0.00115721\n",
            "Loss (epoch: 3513): 0.00115670\n",
            "Loss (epoch: 3514): 0.00115620\n",
            "Loss (epoch: 3515): 0.00115569\n",
            "Loss (epoch: 3516): 0.00115519\n",
            "Loss (epoch: 3517): 0.00115468\n",
            "Loss (epoch: 3518): 0.00115418\n",
            "Loss (epoch: 3519): 0.00115367\n",
            "Loss (epoch: 3520): 0.00115317\n",
            "Loss (epoch: 3521): 0.00115266\n",
            "Loss (epoch: 3522): 0.00115216\n",
            "Loss (epoch: 3523): 0.00115165\n",
            "Loss (epoch: 3524): 0.00115115\n",
            "Loss (epoch: 3525): 0.00115064\n",
            "Loss (epoch: 3526): 0.00115013\n",
            "Loss (epoch: 3527): 0.00114963\n",
            "Loss (epoch: 3528): 0.00114912\n",
            "Loss (epoch: 3529): 0.00114861\n",
            "Loss (epoch: 3530): 0.00114811\n",
            "Loss (epoch: 3531): 0.00114760\n",
            "Loss (epoch: 3532): 0.00114709\n",
            "Loss (epoch: 3533): 0.00114659\n",
            "Loss (epoch: 3534): 0.00114608\n",
            "Loss (epoch: 3535): 0.00114557\n",
            "Loss (epoch: 3536): 0.00114507\n",
            "Loss (epoch: 3537): 0.00114456\n",
            "Loss (epoch: 3538): 0.00114405\n",
            "Loss (epoch: 3539): 0.00114354\n",
            "Loss (epoch: 3540): 0.00114304\n",
            "Loss (epoch: 3541): 0.00114253\n",
            "Loss (epoch: 3542): 0.00114202\n",
            "Loss (epoch: 3543): 0.00114151\n",
            "Loss (epoch: 3544): 0.00114100\n",
            "Loss (epoch: 3545): 0.00114050\n",
            "Loss (epoch: 3546): 0.00113999\n",
            "Loss (epoch: 3547): 0.00113948\n",
            "Loss (epoch: 3548): 0.00113897\n",
            "Loss (epoch: 3549): 0.00113846\n",
            "Loss (epoch: 3550): 0.00113795\n",
            "Loss (epoch: 3551): 0.00113744\n",
            "Loss (epoch: 3552): 0.00113694\n",
            "Loss (epoch: 3553): 0.00113643\n",
            "Loss (epoch: 3554): 0.00113592\n",
            "Loss (epoch: 3555): 0.00113541\n",
            "Loss (epoch: 3556): 0.00113490\n",
            "Loss (epoch: 3557): 0.00113439\n",
            "Loss (epoch: 3558): 0.00113388\n",
            "Loss (epoch: 3559): 0.00113337\n",
            "Loss (epoch: 3560): 0.00113286\n",
            "Loss (epoch: 3561): 0.00113235\n",
            "Loss (epoch: 3562): 0.00113184\n",
            "Loss (epoch: 3563): 0.00113133\n",
            "Loss (epoch: 3564): 0.00113082\n",
            "Loss (epoch: 3565): 0.00113031\n",
            "Loss (epoch: 3566): 0.00112980\n",
            "Loss (epoch: 3567): 0.00112929\n",
            "Loss (epoch: 3568): 0.00112878\n",
            "Loss (epoch: 3569): 0.00112827\n",
            "Loss (epoch: 3570): 0.00112775\n",
            "Loss (epoch: 3571): 0.00112724\n",
            "Loss (epoch: 3572): 0.00112673\n",
            "Loss (epoch: 3573): 0.00112622\n",
            "Loss (epoch: 3574): 0.00112571\n",
            "Loss (epoch: 3575): 0.00112520\n",
            "Loss (epoch: 3576): 0.00112469\n",
            "Loss (epoch: 3577): 0.00112418\n",
            "Loss (epoch: 3578): 0.00112366\n",
            "Loss (epoch: 3579): 0.00112315\n",
            "Loss (epoch: 3580): 0.00112264\n",
            "Loss (epoch: 3581): 0.00112213\n",
            "Loss (epoch: 3582): 0.00112162\n",
            "Loss (epoch: 3583): 0.00112110\n",
            "Loss (epoch: 3584): 0.00112059\n",
            "Loss (epoch: 3585): 0.00112008\n",
            "Loss (epoch: 3586): 0.00111957\n",
            "Loss (epoch: 3587): 0.00111905\n",
            "Loss (epoch: 3588): 0.00111854\n",
            "Loss (epoch: 3589): 0.00111803\n",
            "Loss (epoch: 3590): 0.00111752\n",
            "Loss (epoch: 3591): 0.00111700\n",
            "Loss (epoch: 3592): 0.00111649\n",
            "Loss (epoch: 3593): 0.00111598\n",
            "Loss (epoch: 3594): 0.00111546\n",
            "Loss (epoch: 3595): 0.00111495\n",
            "Loss (epoch: 3596): 0.00111444\n",
            "Loss (epoch: 3597): 0.00111392\n",
            "Loss (epoch: 3598): 0.00111341\n",
            "Loss (epoch: 3599): 0.00111290\n",
            "Loss (epoch: 3600): 0.00111238\n",
            "Loss (epoch: 3601): 0.00111187\n",
            "Loss (epoch: 3602): 0.00111135\n",
            "Loss (epoch: 3603): 0.00111084\n",
            "Loss (epoch: 3604): 0.00111033\n",
            "Loss (epoch: 3605): 0.00110981\n",
            "Loss (epoch: 3606): 0.00110930\n",
            "Loss (epoch: 3607): 0.00110878\n",
            "Loss (epoch: 3608): 0.00110827\n",
            "Loss (epoch: 3609): 0.00110775\n",
            "Loss (epoch: 3610): 0.00110724\n",
            "Loss (epoch: 3611): 0.00110672\n",
            "Loss (epoch: 3612): 0.00110621\n",
            "Loss (epoch: 3613): 0.00110569\n",
            "Loss (epoch: 3614): 0.00110518\n",
            "Loss (epoch: 3615): 0.00110466\n",
            "Loss (epoch: 3616): 0.00110415\n",
            "Loss (epoch: 3617): 0.00110363\n",
            "Loss (epoch: 3618): 0.00110312\n",
            "Loss (epoch: 3619): 0.00110260\n",
            "Loss (epoch: 3620): 0.00110209\n",
            "Loss (epoch: 3621): 0.00110157\n",
            "Loss (epoch: 3622): 0.00110106\n",
            "Loss (epoch: 3623): 0.00110054\n",
            "Loss (epoch: 3624): 0.00110002\n",
            "Loss (epoch: 3625): 0.00109951\n",
            "Loss (epoch: 3626): 0.00109899\n",
            "Loss (epoch: 3627): 0.00109847\n",
            "Loss (epoch: 3628): 0.00109796\n",
            "Loss (epoch: 3629): 0.00109744\n",
            "Loss (epoch: 3630): 0.00109693\n",
            "Loss (epoch: 3631): 0.00109641\n",
            "Loss (epoch: 3632): 0.00109589\n",
            "Loss (epoch: 3633): 0.00109538\n",
            "Loss (epoch: 3634): 0.00109486\n",
            "Loss (epoch: 3635): 0.00109434\n",
            "Loss (epoch: 3636): 0.00109383\n",
            "Loss (epoch: 3637): 0.00109331\n",
            "Loss (epoch: 3638): 0.00109279\n",
            "Loss (epoch: 3639): 0.00109228\n",
            "Loss (epoch: 3640): 0.00109176\n",
            "Loss (epoch: 3641): 0.00109124\n",
            "Loss (epoch: 3642): 0.00109072\n",
            "Loss (epoch: 3643): 0.00109021\n",
            "Loss (epoch: 3644): 0.00108969\n",
            "Loss (epoch: 3645): 0.00108917\n",
            "Loss (epoch: 3646): 0.00108865\n",
            "Loss (epoch: 3647): 0.00108814\n",
            "Loss (epoch: 3648): 0.00108762\n",
            "Loss (epoch: 3649): 0.00108710\n",
            "Loss (epoch: 3650): 0.00108658\n",
            "Loss (epoch: 3651): 0.00108607\n",
            "Loss (epoch: 3652): 0.00108555\n",
            "Loss (epoch: 3653): 0.00108503\n",
            "Loss (epoch: 3654): 0.00108451\n",
            "Loss (epoch: 3655): 0.00108399\n",
            "Loss (epoch: 3656): 0.00108348\n",
            "Loss (epoch: 3657): 0.00108296\n",
            "Loss (epoch: 3658): 0.00108244\n",
            "Loss (epoch: 3659): 0.00108192\n",
            "Loss (epoch: 3660): 0.00108140\n",
            "Loss (epoch: 3661): 0.00108088\n",
            "Loss (epoch: 3662): 0.00108037\n",
            "Loss (epoch: 3663): 0.00107985\n",
            "Loss (epoch: 3664): 0.00107933\n",
            "Loss (epoch: 3665): 0.00107881\n",
            "Loss (epoch: 3666): 0.00107829\n",
            "Loss (epoch: 3667): 0.00107777\n",
            "Loss (epoch: 3668): 0.00107725\n",
            "Loss (epoch: 3669): 0.00107673\n",
            "Loss (epoch: 3670): 0.00107621\n",
            "Loss (epoch: 3671): 0.00107569\n",
            "Loss (epoch: 3672): 0.00107518\n",
            "Loss (epoch: 3673): 0.00107466\n",
            "Loss (epoch: 3674): 0.00107414\n",
            "Loss (epoch: 3675): 0.00107362\n",
            "Loss (epoch: 3676): 0.00107310\n",
            "Loss (epoch: 3677): 0.00107258\n",
            "Loss (epoch: 3678): 0.00107206\n",
            "Loss (epoch: 3679): 0.00107154\n",
            "Loss (epoch: 3680): 0.00107102\n",
            "Loss (epoch: 3681): 0.00107050\n",
            "Loss (epoch: 3682): 0.00106998\n",
            "Loss (epoch: 3683): 0.00106946\n",
            "Loss (epoch: 3684): 0.00106894\n",
            "Loss (epoch: 3685): 0.00106842\n",
            "Loss (epoch: 3686): 0.00106790\n",
            "Loss (epoch: 3687): 0.00106738\n",
            "Loss (epoch: 3688): 0.00106686\n",
            "Loss (epoch: 3689): 0.00106634\n",
            "Loss (epoch: 3690): 0.00106582\n",
            "Loss (epoch: 3691): 0.00106530\n",
            "Loss (epoch: 3692): 0.00106478\n",
            "Loss (epoch: 3693): 0.00106426\n",
            "Loss (epoch: 3694): 0.00106374\n",
            "Loss (epoch: 3695): 0.00106322\n",
            "Loss (epoch: 3696): 0.00106270\n",
            "Loss (epoch: 3697): 0.00106218\n",
            "Loss (epoch: 3698): 0.00106166\n",
            "Loss (epoch: 3699): 0.00106114\n",
            "Loss (epoch: 3700): 0.00106062\n",
            "Loss (epoch: 3701): 0.00106010\n",
            "Loss (epoch: 3702): 0.00105958\n",
            "Loss (epoch: 3703): 0.00105906\n",
            "Loss (epoch: 3704): 0.00105854\n",
            "Loss (epoch: 3705): 0.00105802\n",
            "Loss (epoch: 3706): 0.00105750\n",
            "Loss (epoch: 3707): 0.00105698\n",
            "Loss (epoch: 3708): 0.00105646\n",
            "Loss (epoch: 3709): 0.00105593\n",
            "Loss (epoch: 3710): 0.00105541\n",
            "Loss (epoch: 3711): 0.00105489\n",
            "Loss (epoch: 3712): 0.00105437\n",
            "Loss (epoch: 3713): 0.00105385\n",
            "Loss (epoch: 3714): 0.00105333\n",
            "Loss (epoch: 3715): 0.00105281\n",
            "Loss (epoch: 3716): 0.00105229\n",
            "Loss (epoch: 3717): 0.00105177\n",
            "Loss (epoch: 3718): 0.00105125\n",
            "Loss (epoch: 3719): 0.00105073\n",
            "Loss (epoch: 3720): 0.00105021\n",
            "Loss (epoch: 3721): 0.00104968\n",
            "Loss (epoch: 3722): 0.00104916\n",
            "Loss (epoch: 3723): 0.00104864\n",
            "Loss (epoch: 3724): 0.00104812\n",
            "Loss (epoch: 3725): 0.00104760\n",
            "Loss (epoch: 3726): 0.00104708\n",
            "Loss (epoch: 3727): 0.00104656\n",
            "Loss (epoch: 3728): 0.00104604\n",
            "Loss (epoch: 3729): 0.00104552\n",
            "Loss (epoch: 3730): 0.00104499\n",
            "Loss (epoch: 3731): 0.00104447\n",
            "Loss (epoch: 3732): 0.00104395\n",
            "Loss (epoch: 3733): 0.00104343\n",
            "Loss (epoch: 3734): 0.00104291\n",
            "Loss (epoch: 3735): 0.00104239\n",
            "Loss (epoch: 3736): 0.00104187\n",
            "Loss (epoch: 3737): 0.00104134\n",
            "Loss (epoch: 3738): 0.00104082\n",
            "Loss (epoch: 3739): 0.00104030\n",
            "Loss (epoch: 3740): 0.00103978\n",
            "Loss (epoch: 3741): 0.00103926\n",
            "Loss (epoch: 3742): 0.00103874\n",
            "Loss (epoch: 3743): 0.00103822\n",
            "Loss (epoch: 3744): 0.00103770\n",
            "Loss (epoch: 3745): 0.00103717\n",
            "Loss (epoch: 3746): 0.00103665\n",
            "Loss (epoch: 3747): 0.00103613\n",
            "Loss (epoch: 3748): 0.00103561\n",
            "Loss (epoch: 3749): 0.00103509\n",
            "Loss (epoch: 3750): 0.00103457\n",
            "Loss (epoch: 3751): 0.00103405\n",
            "Loss (epoch: 3752): 0.00103352\n",
            "Loss (epoch: 3753): 0.00103300\n",
            "Loss (epoch: 3754): 0.00103248\n",
            "Loss (epoch: 3755): 0.00103196\n",
            "Loss (epoch: 3756): 0.00103144\n",
            "Loss (epoch: 3757): 0.00103092\n",
            "Loss (epoch: 3758): 0.00103039\n",
            "Loss (epoch: 3759): 0.00102987\n",
            "Loss (epoch: 3760): 0.00102935\n",
            "Loss (epoch: 3761): 0.00102883\n",
            "Loss (epoch: 3762): 0.00102831\n",
            "Loss (epoch: 3763): 0.00102779\n",
            "Loss (epoch: 3764): 0.00102727\n",
            "Loss (epoch: 3765): 0.00102675\n",
            "Loss (epoch: 3766): 0.00102622\n",
            "Loss (epoch: 3767): 0.00102570\n",
            "Loss (epoch: 3768): 0.00102518\n",
            "Loss (epoch: 3769): 0.00102466\n",
            "Loss (epoch: 3770): 0.00102414\n",
            "Loss (epoch: 3771): 0.00102362\n",
            "Loss (epoch: 3772): 0.00102310\n",
            "Loss (epoch: 3773): 0.00102257\n",
            "Loss (epoch: 3774): 0.00102205\n",
            "Loss (epoch: 3775): 0.00102153\n",
            "Loss (epoch: 3776): 0.00102101\n",
            "Loss (epoch: 3777): 0.00102049\n",
            "Loss (epoch: 3778): 0.00101997\n",
            "Loss (epoch: 3779): 0.00101945\n",
            "Loss (epoch: 3780): 0.00101892\n",
            "Loss (epoch: 3781): 0.00101840\n",
            "Loss (epoch: 3782): 0.00101788\n",
            "Loss (epoch: 3783): 0.00101736\n",
            "Loss (epoch: 3784): 0.00101684\n",
            "Loss (epoch: 3785): 0.00101632\n",
            "Loss (epoch: 3786): 0.00101580\n",
            "Loss (epoch: 3787): 0.00101528\n",
            "Loss (epoch: 3788): 0.00101475\n",
            "Loss (epoch: 3789): 0.00101423\n",
            "Loss (epoch: 3790): 0.00101371\n",
            "Loss (epoch: 3791): 0.00101319\n",
            "Loss (epoch: 3792): 0.00101267\n",
            "Loss (epoch: 3793): 0.00101215\n",
            "Loss (epoch: 3794): 0.00101163\n",
            "Loss (epoch: 3795): 0.00101111\n",
            "Loss (epoch: 3796): 0.00101059\n",
            "Loss (epoch: 3797): 0.00101007\n",
            "Loss (epoch: 3798): 0.00100954\n",
            "Loss (epoch: 3799): 0.00100902\n",
            "Loss (epoch: 3800): 0.00100850\n",
            "Loss (epoch: 3801): 0.00100798\n",
            "Loss (epoch: 3802): 0.00100746\n",
            "Loss (epoch: 3803): 0.00100694\n",
            "Loss (epoch: 3804): 0.00100642\n",
            "Loss (epoch: 3805): 0.00100590\n",
            "Loss (epoch: 3806): 0.00100538\n",
            "Loss (epoch: 3807): 0.00100486\n",
            "Loss (epoch: 3808): 0.00100434\n",
            "Loss (epoch: 3809): 0.00100382\n",
            "Loss (epoch: 3810): 0.00100330\n",
            "Loss (epoch: 3811): 0.00100278\n",
            "Loss (epoch: 3812): 0.00100226\n",
            "Loss (epoch: 3813): 0.00100174\n",
            "Loss (epoch: 3814): 0.00100122\n",
            "Loss (epoch: 3815): 0.00100070\n",
            "Loss (epoch: 3816): 0.00100018\n",
            "Loss (epoch: 3817): 0.00099965\n",
            "Loss (epoch: 3818): 0.00099913\n",
            "Loss (epoch: 3819): 0.00099862\n",
            "Loss (epoch: 3820): 0.00099809\n",
            "Loss (epoch: 3821): 0.00099757\n",
            "Loss (epoch: 3822): 0.00099705\n",
            "Loss (epoch: 3823): 0.00099653\n",
            "Loss (epoch: 3824): 0.00099601\n",
            "Loss (epoch: 3825): 0.00099549\n",
            "Loss (epoch: 3826): 0.00099498\n",
            "Loss (epoch: 3827): 0.00099445\n",
            "Loss (epoch: 3828): 0.00099394\n",
            "Loss (epoch: 3829): 0.00099342\n",
            "Loss (epoch: 3830): 0.00099290\n",
            "Loss (epoch: 3831): 0.00099238\n",
            "Loss (epoch: 3832): 0.00099186\n",
            "Loss (epoch: 3833): 0.00099134\n",
            "Loss (epoch: 3834): 0.00099082\n",
            "Loss (epoch: 3835): 0.00099030\n",
            "Loss (epoch: 3836): 0.00098978\n",
            "Loss (epoch: 3837): 0.00098926\n",
            "Loss (epoch: 3838): 0.00098874\n",
            "Loss (epoch: 3839): 0.00098822\n",
            "Loss (epoch: 3840): 0.00098770\n",
            "Loss (epoch: 3841): 0.00098718\n",
            "Loss (epoch: 3842): 0.00098666\n",
            "Loss (epoch: 3843): 0.00098615\n",
            "Loss (epoch: 3844): 0.00098563\n",
            "Loss (epoch: 3845): 0.00098511\n",
            "Loss (epoch: 3846): 0.00098459\n",
            "Loss (epoch: 3847): 0.00098407\n",
            "Loss (epoch: 3848): 0.00098355\n",
            "Loss (epoch: 3849): 0.00098303\n",
            "Loss (epoch: 3850): 0.00098251\n",
            "Loss (epoch: 3851): 0.00098200\n",
            "Loss (epoch: 3852): 0.00098148\n",
            "Loss (epoch: 3853): 0.00098096\n",
            "Loss (epoch: 3854): 0.00098044\n",
            "Loss (epoch: 3855): 0.00097992\n",
            "Loss (epoch: 3856): 0.00097940\n",
            "Loss (epoch: 3857): 0.00097889\n",
            "Loss (epoch: 3858): 0.00097837\n",
            "Loss (epoch: 3859): 0.00097785\n",
            "Loss (epoch: 3860): 0.00097733\n",
            "Loss (epoch: 3861): 0.00097682\n",
            "Loss (epoch: 3862): 0.00097630\n",
            "Loss (epoch: 3863): 0.00097578\n",
            "Loss (epoch: 3864): 0.00097526\n",
            "Loss (epoch: 3865): 0.00097474\n",
            "Loss (epoch: 3866): 0.00097423\n",
            "Loss (epoch: 3867): 0.00097371\n",
            "Loss (epoch: 3868): 0.00097319\n",
            "Loss (epoch: 3869): 0.00097268\n",
            "Loss (epoch: 3870): 0.00097216\n",
            "Loss (epoch: 3871): 0.00097164\n",
            "Loss (epoch: 3872): 0.00097112\n",
            "Loss (epoch: 3873): 0.00097061\n",
            "Loss (epoch: 3874): 0.00097009\n",
            "Loss (epoch: 3875): 0.00096957\n",
            "Loss (epoch: 3876): 0.00096906\n",
            "Loss (epoch: 3877): 0.00096854\n",
            "Loss (epoch: 3878): 0.00096802\n",
            "Loss (epoch: 3879): 0.00096751\n",
            "Loss (epoch: 3880): 0.00096699\n",
            "Loss (epoch: 3881): 0.00096647\n",
            "Loss (epoch: 3882): 0.00096596\n",
            "Loss (epoch: 3883): 0.00096544\n",
            "Loss (epoch: 3884): 0.00096493\n",
            "Loss (epoch: 3885): 0.00096441\n",
            "Loss (epoch: 3886): 0.00096389\n",
            "Loss (epoch: 3887): 0.00096338\n",
            "Loss (epoch: 3888): 0.00096286\n",
            "Loss (epoch: 3889): 0.00096235\n",
            "Loss (epoch: 3890): 0.00096183\n",
            "Loss (epoch: 3891): 0.00096132\n",
            "Loss (epoch: 3892): 0.00096080\n",
            "Loss (epoch: 3893): 0.00096028\n",
            "Loss (epoch: 3894): 0.00095977\n",
            "Loss (epoch: 3895): 0.00095926\n",
            "Loss (epoch: 3896): 0.00095874\n",
            "Loss (epoch: 3897): 0.00095822\n",
            "Loss (epoch: 3898): 0.00095771\n",
            "Loss (epoch: 3899): 0.00095719\n",
            "Loss (epoch: 3900): 0.00095668\n",
            "Loss (epoch: 3901): 0.00095616\n",
            "Loss (epoch: 3902): 0.00095565\n",
            "Loss (epoch: 3903): 0.00095514\n",
            "Loss (epoch: 3904): 0.00095462\n",
            "Loss (epoch: 3905): 0.00095411\n",
            "Loss (epoch: 3906): 0.00095359\n",
            "Loss (epoch: 3907): 0.00095308\n",
            "Loss (epoch: 3908): 0.00095257\n",
            "Loss (epoch: 3909): 0.00095205\n",
            "Loss (epoch: 3910): 0.00095154\n",
            "Loss (epoch: 3911): 0.00095102\n",
            "Loss (epoch: 3912): 0.00095051\n",
            "Loss (epoch: 3913): 0.00095000\n",
            "Loss (epoch: 3914): 0.00094948\n",
            "Loss (epoch: 3915): 0.00094897\n",
            "Loss (epoch: 3916): 0.00094846\n",
            "Loss (epoch: 3917): 0.00094794\n",
            "Loss (epoch: 3918): 0.00094743\n",
            "Loss (epoch: 3919): 0.00094692\n",
            "Loss (epoch: 3920): 0.00094641\n",
            "Loss (epoch: 3921): 0.00094589\n",
            "Loss (epoch: 3922): 0.00094538\n",
            "Loss (epoch: 3923): 0.00094487\n",
            "Loss (epoch: 3924): 0.00094436\n",
            "Loss (epoch: 3925): 0.00094384\n",
            "Loss (epoch: 3926): 0.00094333\n",
            "Loss (epoch: 3927): 0.00094282\n",
            "Loss (epoch: 3928): 0.00094231\n",
            "Loss (epoch: 3929): 0.00094180\n",
            "Loss (epoch: 3930): 0.00094128\n",
            "Loss (epoch: 3931): 0.00094077\n",
            "Loss (epoch: 3932): 0.00094026\n",
            "Loss (epoch: 3933): 0.00093975\n",
            "Loss (epoch: 3934): 0.00093924\n",
            "Loss (epoch: 3935): 0.00093873\n",
            "Loss (epoch: 3936): 0.00093822\n",
            "Loss (epoch: 3937): 0.00093771\n",
            "Loss (epoch: 3938): 0.00093719\n",
            "Loss (epoch: 3939): 0.00093668\n",
            "Loss (epoch: 3940): 0.00093617\n",
            "Loss (epoch: 3941): 0.00093566\n",
            "Loss (epoch: 3942): 0.00093515\n",
            "Loss (epoch: 3943): 0.00093464\n",
            "Loss (epoch: 3944): 0.00093413\n",
            "Loss (epoch: 3945): 0.00093362\n",
            "Loss (epoch: 3946): 0.00093311\n",
            "Loss (epoch: 3947): 0.00093260\n",
            "Loss (epoch: 3948): 0.00093209\n",
            "Loss (epoch: 3949): 0.00093158\n",
            "Loss (epoch: 3950): 0.00093107\n",
            "Loss (epoch: 3951): 0.00093056\n",
            "Loss (epoch: 3952): 0.00093005\n",
            "Loss (epoch: 3953): 0.00092955\n",
            "Loss (epoch: 3954): 0.00092904\n",
            "Loss (epoch: 3955): 0.00092853\n",
            "Loss (epoch: 3956): 0.00092802\n",
            "Loss (epoch: 3957): 0.00092751\n",
            "Loss (epoch: 3958): 0.00092700\n",
            "Loss (epoch: 3959): 0.00092649\n",
            "Loss (epoch: 3960): 0.00092598\n",
            "Loss (epoch: 3961): 0.00092548\n",
            "Loss (epoch: 3962): 0.00092497\n",
            "Loss (epoch: 3963): 0.00092446\n",
            "Loss (epoch: 3964): 0.00092395\n",
            "Loss (epoch: 3965): 0.00092345\n",
            "Loss (epoch: 3966): 0.00092294\n",
            "Loss (epoch: 3967): 0.00092243\n",
            "Loss (epoch: 3968): 0.00092192\n",
            "Loss (epoch: 3969): 0.00092142\n",
            "Loss (epoch: 3970): 0.00092091\n",
            "Loss (epoch: 3971): 0.00092040\n",
            "Loss (epoch: 3972): 0.00091990\n",
            "Loss (epoch: 3973): 0.00091939\n",
            "Loss (epoch: 3974): 0.00091888\n",
            "Loss (epoch: 3975): 0.00091837\n",
            "Loss (epoch: 3976): 0.00091787\n",
            "Loss (epoch: 3977): 0.00091736\n",
            "Loss (epoch: 3978): 0.00091686\n",
            "Loss (epoch: 3979): 0.00091635\n",
            "Loss (epoch: 3980): 0.00091584\n",
            "Loss (epoch: 3981): 0.00091534\n",
            "Loss (epoch: 3982): 0.00091483\n",
            "Loss (epoch: 3983): 0.00091433\n",
            "Loss (epoch: 3984): 0.00091382\n",
            "Loss (epoch: 3985): 0.00091332\n",
            "Loss (epoch: 3986): 0.00091281\n",
            "Loss (epoch: 3987): 0.00091231\n",
            "Loss (epoch: 3988): 0.00091180\n",
            "Loss (epoch: 3989): 0.00091130\n",
            "Loss (epoch: 3990): 0.00091079\n",
            "Loss (epoch: 3991): 0.00091029\n",
            "Loss (epoch: 3992): 0.00090979\n",
            "Loss (epoch: 3993): 0.00090928\n",
            "Loss (epoch: 3994): 0.00090878\n",
            "Loss (epoch: 3995): 0.00090827\n",
            "Loss (epoch: 3996): 0.00090777\n",
            "Loss (epoch: 3997): 0.00090727\n",
            "Loss (epoch: 3998): 0.00090676\n",
            "Loss (epoch: 3999): 0.00090626\n",
            "Loss (epoch: 4000): 0.00090576\n",
            "Loss (epoch: 4001): 0.00090525\n",
            "Loss (epoch: 4002): 0.00090475\n",
            "Loss (epoch: 4003): 0.00090425\n",
            "Loss (epoch: 4004): 0.00090375\n",
            "Loss (epoch: 4005): 0.00090324\n",
            "Loss (epoch: 4006): 0.00090274\n",
            "Loss (epoch: 4007): 0.00090224\n",
            "Loss (epoch: 4008): 0.00090174\n",
            "Loss (epoch: 4009): 0.00090124\n",
            "Loss (epoch: 4010): 0.00090073\n",
            "Loss (epoch: 4011): 0.00090023\n",
            "Loss (epoch: 4012): 0.00089973\n",
            "Loss (epoch: 4013): 0.00089923\n",
            "Loss (epoch: 4014): 0.00089873\n",
            "Loss (epoch: 4015): 0.00089823\n",
            "Loss (epoch: 4016): 0.00089773\n",
            "Loss (epoch: 4017): 0.00089723\n",
            "Loss (epoch: 4018): 0.00089673\n",
            "Loss (epoch: 4019): 0.00089623\n",
            "Loss (epoch: 4020): 0.00089573\n",
            "Loss (epoch: 4021): 0.00089523\n",
            "Loss (epoch: 4022): 0.00089473\n",
            "Loss (epoch: 4023): 0.00089423\n",
            "Loss (epoch: 4024): 0.00089373\n",
            "Loss (epoch: 4025): 0.00089323\n",
            "Loss (epoch: 4026): 0.00089273\n",
            "Loss (epoch: 4027): 0.00089223\n",
            "Loss (epoch: 4028): 0.00089173\n",
            "Loss (epoch: 4029): 0.00089123\n",
            "Loss (epoch: 4030): 0.00089073\n",
            "Loss (epoch: 4031): 0.00089023\n",
            "Loss (epoch: 4032): 0.00088973\n",
            "Loss (epoch: 4033): 0.00088924\n",
            "Loss (epoch: 4034): 0.00088874\n",
            "Loss (epoch: 4035): 0.00088824\n",
            "Loss (epoch: 4036): 0.00088774\n",
            "Loss (epoch: 4037): 0.00088724\n",
            "Loss (epoch: 4038): 0.00088675\n",
            "Loss (epoch: 4039): 0.00088625\n",
            "Loss (epoch: 4040): 0.00088575\n",
            "Loss (epoch: 4041): 0.00088526\n",
            "Loss (epoch: 4042): 0.00088476\n",
            "Loss (epoch: 4043): 0.00088426\n",
            "Loss (epoch: 4044): 0.00088377\n",
            "Loss (epoch: 4045): 0.00088327\n",
            "Loss (epoch: 4046): 0.00088277\n",
            "Loss (epoch: 4047): 0.00088228\n",
            "Loss (epoch: 4048): 0.00088178\n",
            "Loss (epoch: 4049): 0.00088128\n",
            "Loss (epoch: 4050): 0.00088079\n",
            "Loss (epoch: 4051): 0.00088029\n",
            "Loss (epoch: 4052): 0.00087980\n",
            "Loss (epoch: 4053): 0.00087930\n",
            "Loss (epoch: 4054): 0.00087881\n",
            "Loss (epoch: 4055): 0.00087831\n",
            "Loss (epoch: 4056): 0.00087782\n",
            "Loss (epoch: 4057): 0.00087732\n",
            "Loss (epoch: 4058): 0.00087683\n",
            "Loss (epoch: 4059): 0.00087634\n",
            "Loss (epoch: 4060): 0.00087584\n",
            "Loss (epoch: 4061): 0.00087535\n",
            "Loss (epoch: 4062): 0.00087485\n",
            "Loss (epoch: 4063): 0.00087436\n",
            "Loss (epoch: 4064): 0.00087387\n",
            "Loss (epoch: 4065): 0.00087337\n",
            "Loss (epoch: 4066): 0.00087288\n",
            "Loss (epoch: 4067): 0.00087239\n",
            "Loss (epoch: 4068): 0.00087189\n",
            "Loss (epoch: 4069): 0.00087140\n",
            "Loss (epoch: 4070): 0.00087091\n",
            "Loss (epoch: 4071): 0.00087042\n",
            "Loss (epoch: 4072): 0.00086992\n",
            "Loss (epoch: 4073): 0.00086943\n",
            "Loss (epoch: 4074): 0.00086894\n",
            "Loss (epoch: 4075): 0.00086845\n",
            "Loss (epoch: 4076): 0.00086796\n",
            "Loss (epoch: 4077): 0.00086747\n",
            "Loss (epoch: 4078): 0.00086698\n",
            "Loss (epoch: 4079): 0.00086648\n",
            "Loss (epoch: 4080): 0.00086599\n",
            "Loss (epoch: 4081): 0.00086550\n",
            "Loss (epoch: 4082): 0.00086501\n",
            "Loss (epoch: 4083): 0.00086452\n",
            "Loss (epoch: 4084): 0.00086403\n",
            "Loss (epoch: 4085): 0.00086354\n",
            "Loss (epoch: 4086): 0.00086305\n",
            "Loss (epoch: 4087): 0.00086256\n",
            "Loss (epoch: 4088): 0.00086207\n",
            "Loss (epoch: 4089): 0.00086158\n",
            "Loss (epoch: 4090): 0.00086110\n",
            "Loss (epoch: 4091): 0.00086061\n",
            "Loss (epoch: 4092): 0.00086012\n",
            "Loss (epoch: 4093): 0.00085963\n",
            "Loss (epoch: 4094): 0.00085914\n",
            "Loss (epoch: 4095): 0.00085865\n",
            "Loss (epoch: 4096): 0.00085816\n",
            "Loss (epoch: 4097): 0.00085768\n",
            "Loss (epoch: 4098): 0.00085719\n",
            "Loss (epoch: 4099): 0.00085670\n",
            "Loss (epoch: 4100): 0.00085621\n",
            "Loss (epoch: 4101): 0.00085573\n",
            "Loss (epoch: 4102): 0.00085524\n",
            "Loss (epoch: 4103): 0.00085475\n",
            "Loss (epoch: 4104): 0.00085427\n",
            "Loss (epoch: 4105): 0.00085378\n",
            "Loss (epoch: 4106): 0.00085329\n",
            "Loss (epoch: 4107): 0.00085281\n",
            "Loss (epoch: 4108): 0.00085232\n",
            "Loss (epoch: 4109): 0.00085184\n",
            "Loss (epoch: 4110): 0.00085135\n",
            "Loss (epoch: 4111): 0.00085086\n",
            "Loss (epoch: 4112): 0.00085038\n",
            "Loss (epoch: 4113): 0.00084990\n",
            "Loss (epoch: 4114): 0.00084941\n",
            "Loss (epoch: 4115): 0.00084893\n",
            "Loss (epoch: 4116): 0.00084844\n",
            "Loss (epoch: 4117): 0.00084796\n",
            "Loss (epoch: 4118): 0.00084747\n",
            "Loss (epoch: 4119): 0.00084699\n",
            "Loss (epoch: 4120): 0.00084650\n",
            "Loss (epoch: 4121): 0.00084602\n",
            "Loss (epoch: 4122): 0.00084554\n",
            "Loss (epoch: 4123): 0.00084505\n",
            "Loss (epoch: 4124): 0.00084457\n",
            "Loss (epoch: 4125): 0.00084409\n",
            "Loss (epoch: 4126): 0.00084361\n",
            "Loss (epoch: 4127): 0.00084312\n",
            "Loss (epoch: 4128): 0.00084264\n",
            "Loss (epoch: 4129): 0.00084216\n",
            "Loss (epoch: 4130): 0.00084168\n",
            "Loss (epoch: 4131): 0.00084120\n",
            "Loss (epoch: 4132): 0.00084071\n",
            "Loss (epoch: 4133): 0.00084023\n",
            "Loss (epoch: 4134): 0.00083975\n",
            "Loss (epoch: 4135): 0.00083927\n",
            "Loss (epoch: 4136): 0.00083879\n",
            "Loss (epoch: 4137): 0.00083831\n",
            "Loss (epoch: 4138): 0.00083783\n",
            "Loss (epoch: 4139): 0.00083735\n",
            "Loss (epoch: 4140): 0.00083687\n",
            "Loss (epoch: 4141): 0.00083639\n",
            "Loss (epoch: 4142): 0.00083591\n",
            "Loss (epoch: 4143): 0.00083543\n",
            "Loss (epoch: 4144): 0.00083495\n",
            "Loss (epoch: 4145): 0.00083447\n",
            "Loss (epoch: 4146): 0.00083399\n",
            "Loss (epoch: 4147): 0.00083351\n",
            "Loss (epoch: 4148): 0.00083303\n",
            "Loss (epoch: 4149): 0.00083256\n",
            "Loss (epoch: 4150): 0.00083208\n",
            "Loss (epoch: 4151): 0.00083160\n",
            "Loss (epoch: 4152): 0.00083112\n",
            "Loss (epoch: 4153): 0.00083064\n",
            "Loss (epoch: 4154): 0.00083017\n",
            "Loss (epoch: 4155): 0.00082969\n",
            "Loss (epoch: 4156): 0.00082921\n",
            "Loss (epoch: 4157): 0.00082874\n",
            "Loss (epoch: 4158): 0.00082826\n",
            "Loss (epoch: 4159): 0.00082778\n",
            "Loss (epoch: 4160): 0.00082731\n",
            "Loss (epoch: 4161): 0.00082683\n",
            "Loss (epoch: 4162): 0.00082635\n",
            "Loss (epoch: 4163): 0.00082588\n",
            "Loss (epoch: 4164): 0.00082540\n",
            "Loss (epoch: 4165): 0.00082493\n",
            "Loss (epoch: 4166): 0.00082445\n",
            "Loss (epoch: 4167): 0.00082398\n",
            "Loss (epoch: 4168): 0.00082350\n",
            "Loss (epoch: 4169): 0.00082303\n",
            "Loss (epoch: 4170): 0.00082255\n",
            "Loss (epoch: 4171): 0.00082208\n",
            "Loss (epoch: 4172): 0.00082161\n",
            "Loss (epoch: 4173): 0.00082113\n",
            "Loss (epoch: 4174): 0.00082066\n",
            "Loss (epoch: 4175): 0.00082019\n",
            "Loss (epoch: 4176): 0.00081971\n",
            "Loss (epoch: 4177): 0.00081924\n",
            "Loss (epoch: 4178): 0.00081877\n",
            "Loss (epoch: 4179): 0.00081829\n",
            "Loss (epoch: 4180): 0.00081782\n",
            "Loss (epoch: 4181): 0.00081735\n",
            "Loss (epoch: 4182): 0.00081688\n",
            "Loss (epoch: 4183): 0.00081641\n",
            "Loss (epoch: 4184): 0.00081593\n",
            "Loss (epoch: 4185): 0.00081546\n",
            "Loss (epoch: 4186): 0.00081499\n",
            "Loss (epoch: 4187): 0.00081452\n",
            "Loss (epoch: 4188): 0.00081405\n",
            "Loss (epoch: 4189): 0.00081358\n",
            "Loss (epoch: 4190): 0.00081311\n",
            "Loss (epoch: 4191): 0.00081264\n",
            "Loss (epoch: 4192): 0.00081217\n",
            "Loss (epoch: 4193): 0.00081170\n",
            "Loss (epoch: 4194): 0.00081123\n",
            "Loss (epoch: 4195): 0.00081076\n",
            "Loss (epoch: 4196): 0.00081029\n",
            "Loss (epoch: 4197): 0.00080983\n",
            "Loss (epoch: 4198): 0.00080936\n",
            "Loss (epoch: 4199): 0.00080889\n",
            "Loss (epoch: 4200): 0.00080842\n",
            "Loss (epoch: 4201): 0.00080795\n",
            "Loss (epoch: 4202): 0.00080748\n",
            "Loss (epoch: 4203): 0.00080701\n",
            "Loss (epoch: 4204): 0.00080655\n",
            "Loss (epoch: 4205): 0.00080608\n",
            "Loss (epoch: 4206): 0.00080561\n",
            "Loss (epoch: 4207): 0.00080515\n",
            "Loss (epoch: 4208): 0.00080468\n",
            "Loss (epoch: 4209): 0.00080421\n",
            "Loss (epoch: 4210): 0.00080375\n",
            "Loss (epoch: 4211): 0.00080328\n",
            "Loss (epoch: 4212): 0.00080282\n",
            "Loss (epoch: 4213): 0.00080235\n",
            "Loss (epoch: 4214): 0.00080189\n",
            "Loss (epoch: 4215): 0.00080142\n",
            "Loss (epoch: 4216): 0.00080096\n",
            "Loss (epoch: 4217): 0.00080049\n",
            "Loss (epoch: 4218): 0.00080003\n",
            "Loss (epoch: 4219): 0.00079956\n",
            "Loss (epoch: 4220): 0.00079910\n",
            "Loss (epoch: 4221): 0.00079863\n",
            "Loss (epoch: 4222): 0.00079817\n",
            "Loss (epoch: 4223): 0.00079771\n",
            "Loss (epoch: 4224): 0.00079724\n",
            "Loss (epoch: 4225): 0.00079678\n",
            "Loss (epoch: 4226): 0.00079632\n",
            "Loss (epoch: 4227): 0.00079586\n",
            "Loss (epoch: 4228): 0.00079539\n",
            "Loss (epoch: 4229): 0.00079493\n",
            "Loss (epoch: 4230): 0.00079447\n",
            "Loss (epoch: 4231): 0.00079401\n",
            "Loss (epoch: 4232): 0.00079355\n",
            "Loss (epoch: 4233): 0.00079308\n",
            "Loss (epoch: 4234): 0.00079262\n",
            "Loss (epoch: 4235): 0.00079216\n",
            "Loss (epoch: 4236): 0.00079170\n",
            "Loss (epoch: 4237): 0.00079124\n",
            "Loss (epoch: 4238): 0.00079078\n",
            "Loss (epoch: 4239): 0.00079032\n",
            "Loss (epoch: 4240): 0.00078986\n",
            "Loss (epoch: 4241): 0.00078940\n",
            "Loss (epoch: 4242): 0.00078894\n",
            "Loss (epoch: 4243): 0.00078848\n",
            "Loss (epoch: 4244): 0.00078803\n",
            "Loss (epoch: 4245): 0.00078757\n",
            "Loss (epoch: 4246): 0.00078711\n",
            "Loss (epoch: 4247): 0.00078665\n",
            "Loss (epoch: 4248): 0.00078619\n",
            "Loss (epoch: 4249): 0.00078573\n",
            "Loss (epoch: 4250): 0.00078528\n",
            "Loss (epoch: 4251): 0.00078482\n",
            "Loss (epoch: 4252): 0.00078436\n",
            "Loss (epoch: 4253): 0.00078391\n",
            "Loss (epoch: 4254): 0.00078345\n",
            "Loss (epoch: 4255): 0.00078299\n",
            "Loss (epoch: 4256): 0.00078254\n",
            "Loss (epoch: 4257): 0.00078208\n",
            "Loss (epoch: 4258): 0.00078163\n",
            "Loss (epoch: 4259): 0.00078117\n",
            "Loss (epoch: 4260): 0.00078071\n",
            "Loss (epoch: 4261): 0.00078026\n",
            "Loss (epoch: 4262): 0.00077980\n",
            "Loss (epoch: 4263): 0.00077935\n",
            "Loss (epoch: 4264): 0.00077890\n",
            "Loss (epoch: 4265): 0.00077844\n",
            "Loss (epoch: 4266): 0.00077799\n",
            "Loss (epoch: 4267): 0.00077753\n",
            "Loss (epoch: 4268): 0.00077708\n",
            "Loss (epoch: 4269): 0.00077663\n",
            "Loss (epoch: 4270): 0.00077617\n",
            "Loss (epoch: 4271): 0.00077572\n",
            "Loss (epoch: 4272): 0.00077527\n",
            "Loss (epoch: 4273): 0.00077482\n",
            "Loss (epoch: 4274): 0.00077436\n",
            "Loss (epoch: 4275): 0.00077391\n",
            "Loss (epoch: 4276): 0.00077346\n",
            "Loss (epoch: 4277): 0.00077301\n",
            "Loss (epoch: 4278): 0.00077256\n",
            "Loss (epoch: 4279): 0.00077211\n",
            "Loss (epoch: 4280): 0.00077166\n",
            "Loss (epoch: 4281): 0.00077121\n",
            "Loss (epoch: 4282): 0.00077076\n",
            "Loss (epoch: 4283): 0.00077031\n",
            "Loss (epoch: 4284): 0.00076986\n",
            "Loss (epoch: 4285): 0.00076941\n",
            "Loss (epoch: 4286): 0.00076896\n",
            "Loss (epoch: 4287): 0.00076851\n",
            "Loss (epoch: 4288): 0.00076806\n",
            "Loss (epoch: 4289): 0.00076761\n",
            "Loss (epoch: 4290): 0.00076716\n",
            "Loss (epoch: 4291): 0.00076672\n",
            "Loss (epoch: 4292): 0.00076627\n",
            "Loss (epoch: 4293): 0.00076582\n",
            "Loss (epoch: 4294): 0.00076537\n",
            "Loss (epoch: 4295): 0.00076493\n",
            "Loss (epoch: 4296): 0.00076448\n",
            "Loss (epoch: 4297): 0.00076403\n",
            "Loss (epoch: 4298): 0.00076359\n",
            "Loss (epoch: 4299): 0.00076314\n",
            "Loss (epoch: 4300): 0.00076269\n",
            "Loss (epoch: 4301): 0.00076225\n",
            "Loss (epoch: 4302): 0.00076180\n",
            "Loss (epoch: 4303): 0.00076136\n",
            "Loss (epoch: 4304): 0.00076091\n",
            "Loss (epoch: 4305): 0.00076047\n",
            "Loss (epoch: 4306): 0.00076002\n",
            "Loss (epoch: 4307): 0.00075958\n",
            "Loss (epoch: 4308): 0.00075913\n",
            "Loss (epoch: 4309): 0.00075869\n",
            "Loss (epoch: 4310): 0.00075825\n",
            "Loss (epoch: 4311): 0.00075780\n",
            "Loss (epoch: 4312): 0.00075736\n",
            "Loss (epoch: 4313): 0.00075692\n",
            "Loss (epoch: 4314): 0.00075647\n",
            "Loss (epoch: 4315): 0.00075603\n",
            "Loss (epoch: 4316): 0.00075559\n",
            "Loss (epoch: 4317): 0.00075515\n",
            "Loss (epoch: 4318): 0.00075471\n",
            "Loss (epoch: 4319): 0.00075426\n",
            "Loss (epoch: 4320): 0.00075382\n",
            "Loss (epoch: 4321): 0.00075338\n",
            "Loss (epoch: 4322): 0.00075294\n",
            "Loss (epoch: 4323): 0.00075250\n",
            "Loss (epoch: 4324): 0.00075206\n",
            "Loss (epoch: 4325): 0.00075162\n",
            "Loss (epoch: 4326): 0.00075118\n",
            "Loss (epoch: 4327): 0.00075074\n",
            "Loss (epoch: 4328): 0.00075030\n",
            "Loss (epoch: 4329): 0.00074986\n",
            "Loss (epoch: 4330): 0.00074942\n",
            "Loss (epoch: 4331): 0.00074899\n",
            "Loss (epoch: 4332): 0.00074855\n",
            "Loss (epoch: 4333): 0.00074811\n",
            "Loss (epoch: 4334): 0.00074767\n",
            "Loss (epoch: 4335): 0.00074724\n",
            "Loss (epoch: 4336): 0.00074680\n",
            "Loss (epoch: 4337): 0.00074636\n",
            "Loss (epoch: 4338): 0.00074592\n",
            "Loss (epoch: 4339): 0.00074549\n",
            "Loss (epoch: 4340): 0.00074505\n",
            "Loss (epoch: 4341): 0.00074462\n",
            "Loss (epoch: 4342): 0.00074418\n",
            "Loss (epoch: 4343): 0.00074374\n",
            "Loss (epoch: 4344): 0.00074331\n",
            "Loss (epoch: 4345): 0.00074287\n",
            "Loss (epoch: 4346): 0.00074244\n",
            "Loss (epoch: 4347): 0.00074200\n",
            "Loss (epoch: 4348): 0.00074157\n",
            "Loss (epoch: 4349): 0.00074114\n",
            "Loss (epoch: 4350): 0.00074070\n",
            "Loss (epoch: 4351): 0.00074027\n",
            "Loss (epoch: 4352): 0.00073984\n",
            "Loss (epoch: 4353): 0.00073940\n",
            "Loss (epoch: 4354): 0.00073897\n",
            "Loss (epoch: 4355): 0.00073854\n",
            "Loss (epoch: 4356): 0.00073811\n",
            "Loss (epoch: 4357): 0.00073767\n",
            "Loss (epoch: 4358): 0.00073724\n",
            "Loss (epoch: 4359): 0.00073681\n",
            "Loss (epoch: 4360): 0.00073638\n",
            "Loss (epoch: 4361): 0.00073595\n",
            "Loss (epoch: 4362): 0.00073552\n",
            "Loss (epoch: 4363): 0.00073509\n",
            "Loss (epoch: 4364): 0.00073466\n",
            "Loss (epoch: 4365): 0.00073423\n",
            "Loss (epoch: 4366): 0.00073380\n",
            "Loss (epoch: 4367): 0.00073337\n",
            "Loss (epoch: 4368): 0.00073294\n",
            "Loss (epoch: 4369): 0.00073251\n",
            "Loss (epoch: 4370): 0.00073208\n",
            "Loss (epoch: 4371): 0.00073165\n",
            "Loss (epoch: 4372): 0.00073122\n",
            "Loss (epoch: 4373): 0.00073080\n",
            "Loss (epoch: 4374): 0.00073037\n",
            "Loss (epoch: 4375): 0.00072994\n",
            "Loss (epoch: 4376): 0.00072951\n",
            "Loss (epoch: 4377): 0.00072909\n",
            "Loss (epoch: 4378): 0.00072866\n",
            "Loss (epoch: 4379): 0.00072823\n",
            "Loss (epoch: 4380): 0.00072781\n",
            "Loss (epoch: 4381): 0.00072738\n",
            "Loss (epoch: 4382): 0.00072696\n",
            "Loss (epoch: 4383): 0.00072653\n",
            "Loss (epoch: 4384): 0.00072611\n",
            "Loss (epoch: 4385): 0.00072568\n",
            "Loss (epoch: 4386): 0.00072526\n",
            "Loss (epoch: 4387): 0.00072483\n",
            "Loss (epoch: 4388): 0.00072441\n",
            "Loss (epoch: 4389): 0.00072399\n",
            "Loss (epoch: 4390): 0.00072356\n",
            "Loss (epoch: 4391): 0.00072314\n",
            "Loss (epoch: 4392): 0.00072272\n",
            "Loss (epoch: 4393): 0.00072229\n",
            "Loss (epoch: 4394): 0.00072187\n",
            "Loss (epoch: 4395): 0.00072145\n",
            "Loss (epoch: 4396): 0.00072103\n",
            "Loss (epoch: 4397): 0.00072061\n",
            "Loss (epoch: 4398): 0.00072018\n",
            "Loss (epoch: 4399): 0.00071976\n",
            "Loss (epoch: 4400): 0.00071934\n",
            "Loss (epoch: 4401): 0.00071892\n",
            "Loss (epoch: 4402): 0.00071850\n",
            "Loss (epoch: 4403): 0.00071808\n",
            "Loss (epoch: 4404): 0.00071766\n",
            "Loss (epoch: 4405): 0.00071724\n",
            "Loss (epoch: 4406): 0.00071682\n",
            "Loss (epoch: 4407): 0.00071640\n",
            "Loss (epoch: 4408): 0.00071599\n",
            "Loss (epoch: 4409): 0.00071557\n",
            "Loss (epoch: 4410): 0.00071515\n",
            "Loss (epoch: 4411): 0.00071473\n",
            "Loss (epoch: 4412): 0.00071431\n",
            "Loss (epoch: 4413): 0.00071390\n",
            "Loss (epoch: 4414): 0.00071348\n",
            "Loss (epoch: 4415): 0.00071306\n",
            "Loss (epoch: 4416): 0.00071265\n",
            "Loss (epoch: 4417): 0.00071223\n",
            "Loss (epoch: 4418): 0.00071181\n",
            "Loss (epoch: 4419): 0.00071140\n",
            "Loss (epoch: 4420): 0.00071098\n",
            "Loss (epoch: 4421): 0.00071057\n",
            "Loss (epoch: 4422): 0.00071015\n",
            "Loss (epoch: 4423): 0.00070974\n",
            "Loss (epoch: 4424): 0.00070932\n",
            "Loss (epoch: 4425): 0.00070891\n",
            "Loss (epoch: 4426): 0.00070850\n",
            "Loss (epoch: 4427): 0.00070808\n",
            "Loss (epoch: 4428): 0.00070767\n",
            "Loss (epoch: 4429): 0.00070726\n",
            "Loss (epoch: 4430): 0.00070684\n",
            "Loss (epoch: 4431): 0.00070643\n",
            "Loss (epoch: 4432): 0.00070602\n",
            "Loss (epoch: 4433): 0.00070561\n",
            "Loss (epoch: 4434): 0.00070519\n",
            "Loss (epoch: 4435): 0.00070478\n",
            "Loss (epoch: 4436): 0.00070437\n",
            "Loss (epoch: 4437): 0.00070396\n",
            "Loss (epoch: 4438): 0.00070355\n",
            "Loss (epoch: 4439): 0.00070314\n",
            "Loss (epoch: 4440): 0.00070273\n",
            "Loss (epoch: 4441): 0.00070232\n",
            "Loss (epoch: 4442): 0.00070191\n",
            "Loss (epoch: 4443): 0.00070150\n",
            "Loss (epoch: 4444): 0.00070109\n",
            "Loss (epoch: 4445): 0.00070068\n",
            "Loss (epoch: 4446): 0.00070028\n",
            "Loss (epoch: 4447): 0.00069987\n",
            "Loss (epoch: 4448): 0.00069946\n",
            "Loss (epoch: 4449): 0.00069905\n",
            "Loss (epoch: 4450): 0.00069864\n",
            "Loss (epoch: 4451): 0.00069824\n",
            "Loss (epoch: 4452): 0.00069783\n",
            "Loss (epoch: 4453): 0.00069742\n",
            "Loss (epoch: 4454): 0.00069702\n",
            "Loss (epoch: 4455): 0.00069661\n",
            "Loss (epoch: 4456): 0.00069621\n",
            "Loss (epoch: 4457): 0.00069580\n",
            "Loss (epoch: 4458): 0.00069540\n",
            "Loss (epoch: 4459): 0.00069499\n",
            "Loss (epoch: 4460): 0.00069459\n",
            "Loss (epoch: 4461): 0.00069418\n",
            "Loss (epoch: 4462): 0.00069378\n",
            "Loss (epoch: 4463): 0.00069338\n",
            "Loss (epoch: 4464): 0.00069297\n",
            "Loss (epoch: 4465): 0.00069257\n",
            "Loss (epoch: 4466): 0.00069217\n",
            "Loss (epoch: 4467): 0.00069176\n",
            "Loss (epoch: 4468): 0.00069136\n",
            "Loss (epoch: 4469): 0.00069096\n",
            "Loss (epoch: 4470): 0.00069056\n",
            "Loss (epoch: 4471): 0.00069016\n",
            "Loss (epoch: 4472): 0.00068975\n",
            "Loss (epoch: 4473): 0.00068935\n",
            "Loss (epoch: 4474): 0.00068895\n",
            "Loss (epoch: 4475): 0.00068855\n",
            "Loss (epoch: 4476): 0.00068815\n",
            "Loss (epoch: 4477): 0.00068775\n",
            "Loss (epoch: 4478): 0.00068735\n",
            "Loss (epoch: 4479): 0.00068695\n",
            "Loss (epoch: 4480): 0.00068656\n",
            "Loss (epoch: 4481): 0.00068616\n",
            "Loss (epoch: 4482): 0.00068576\n",
            "Loss (epoch: 4483): 0.00068536\n",
            "Loss (epoch: 4484): 0.00068496\n",
            "Loss (epoch: 4485): 0.00068456\n",
            "Loss (epoch: 4486): 0.00068417\n",
            "Loss (epoch: 4487): 0.00068377\n",
            "Loss (epoch: 4488): 0.00068337\n",
            "Loss (epoch: 4489): 0.00068298\n",
            "Loss (epoch: 4490): 0.00068258\n",
            "Loss (epoch: 4491): 0.00068219\n",
            "Loss (epoch: 4492): 0.00068179\n",
            "Loss (epoch: 4493): 0.00068139\n",
            "Loss (epoch: 4494): 0.00068100\n",
            "Loss (epoch: 4495): 0.00068061\n",
            "Loss (epoch: 4496): 0.00068021\n",
            "Loss (epoch: 4497): 0.00067982\n",
            "Loss (epoch: 4498): 0.00067942\n",
            "Loss (epoch: 4499): 0.00067903\n",
            "Loss (epoch: 4500): 0.00067864\n",
            "Loss (epoch: 4501): 0.00067824\n",
            "Loss (epoch: 4502): 0.00067785\n",
            "Loss (epoch: 4503): 0.00067746\n",
            "Loss (epoch: 4504): 0.00067706\n",
            "Loss (epoch: 4505): 0.00067667\n",
            "Loss (epoch: 4506): 0.00067628\n",
            "Loss (epoch: 4507): 0.00067589\n",
            "Loss (epoch: 4508): 0.00067550\n",
            "Loss (epoch: 4509): 0.00067511\n",
            "Loss (epoch: 4510): 0.00067472\n",
            "Loss (epoch: 4511): 0.00067433\n",
            "Loss (epoch: 4512): 0.00067394\n",
            "Loss (epoch: 4513): 0.00067355\n",
            "Loss (epoch: 4514): 0.00067316\n",
            "Loss (epoch: 4515): 0.00067277\n",
            "Loss (epoch: 4516): 0.00067238\n",
            "Loss (epoch: 4517): 0.00067199\n",
            "Loss (epoch: 4518): 0.00067160\n",
            "Loss (epoch: 4519): 0.00067122\n",
            "Loss (epoch: 4520): 0.00067083\n",
            "Loss (epoch: 4521): 0.00067044\n",
            "Loss (epoch: 4522): 0.00067005\n",
            "Loss (epoch: 4523): 0.00066967\n",
            "Loss (epoch: 4524): 0.00066928\n",
            "Loss (epoch: 4525): 0.00066890\n",
            "Loss (epoch: 4526): 0.00066851\n",
            "Loss (epoch: 4527): 0.00066812\n",
            "Loss (epoch: 4528): 0.00066774\n",
            "Loss (epoch: 4529): 0.00066735\n",
            "Loss (epoch: 4530): 0.00066697\n",
            "Loss (epoch: 4531): 0.00066658\n",
            "Loss (epoch: 4532): 0.00066620\n",
            "Loss (epoch: 4533): 0.00066582\n",
            "Loss (epoch: 4534): 0.00066543\n",
            "Loss (epoch: 4535): 0.00066505\n",
            "Loss (epoch: 4536): 0.00066467\n",
            "Loss (epoch: 4537): 0.00066428\n",
            "Loss (epoch: 4538): 0.00066390\n",
            "Loss (epoch: 4539): 0.00066352\n",
            "Loss (epoch: 4540): 0.00066314\n",
            "Loss (epoch: 4541): 0.00066276\n",
            "Loss (epoch: 4542): 0.00066237\n",
            "Loss (epoch: 4543): 0.00066199\n",
            "Loss (epoch: 4544): 0.00066161\n",
            "Loss (epoch: 4545): 0.00066123\n",
            "Loss (epoch: 4546): 0.00066085\n",
            "Loss (epoch: 4547): 0.00066047\n",
            "Loss (epoch: 4548): 0.00066009\n",
            "Loss (epoch: 4549): 0.00065971\n",
            "Loss (epoch: 4550): 0.00065933\n",
            "Loss (epoch: 4551): 0.00065896\n",
            "Loss (epoch: 4552): 0.00065858\n",
            "Loss (epoch: 4553): 0.00065820\n",
            "Loss (epoch: 4554): 0.00065782\n",
            "Loss (epoch: 4555): 0.00065744\n",
            "Loss (epoch: 4556): 0.00065707\n",
            "Loss (epoch: 4557): 0.00065669\n",
            "Loss (epoch: 4558): 0.00065631\n",
            "Loss (epoch: 4559): 0.00065594\n",
            "Loss (epoch: 4560): 0.00065556\n",
            "Loss (epoch: 4561): 0.00065518\n",
            "Loss (epoch: 4562): 0.00065481\n",
            "Loss (epoch: 4563): 0.00065443\n",
            "Loss (epoch: 4564): 0.00065406\n",
            "Loss (epoch: 4565): 0.00065368\n",
            "Loss (epoch: 4566): 0.00065331\n",
            "Loss (epoch: 4567): 0.00065294\n",
            "Loss (epoch: 4568): 0.00065256\n",
            "Loss (epoch: 4569): 0.00065219\n",
            "Loss (epoch: 4570): 0.00065182\n",
            "Loss (epoch: 4571): 0.00065144\n",
            "Loss (epoch: 4572): 0.00065107\n",
            "Loss (epoch: 4573): 0.00065070\n",
            "Loss (epoch: 4574): 0.00065032\n",
            "Loss (epoch: 4575): 0.00064995\n",
            "Loss (epoch: 4576): 0.00064958\n",
            "Loss (epoch: 4577): 0.00064921\n",
            "Loss (epoch: 4578): 0.00064884\n",
            "Loss (epoch: 4579): 0.00064847\n",
            "Loss (epoch: 4580): 0.00064810\n",
            "Loss (epoch: 4581): 0.00064773\n",
            "Loss (epoch: 4582): 0.00064736\n",
            "Loss (epoch: 4583): 0.00064699\n",
            "Loss (epoch: 4584): 0.00064662\n",
            "Loss (epoch: 4585): 0.00064625\n",
            "Loss (epoch: 4586): 0.00064588\n",
            "Loss (epoch: 4587): 0.00064551\n",
            "Loss (epoch: 4588): 0.00064515\n",
            "Loss (epoch: 4589): 0.00064478\n",
            "Loss (epoch: 4590): 0.00064441\n",
            "Loss (epoch: 4591): 0.00064404\n",
            "Loss (epoch: 4592): 0.00064368\n",
            "Loss (epoch: 4593): 0.00064331\n",
            "Loss (epoch: 4594): 0.00064294\n",
            "Loss (epoch: 4595): 0.00064258\n",
            "Loss (epoch: 4596): 0.00064221\n",
            "Loss (epoch: 4597): 0.00064185\n",
            "Loss (epoch: 4598): 0.00064148\n",
            "Loss (epoch: 4599): 0.00064112\n",
            "Loss (epoch: 4600): 0.00064075\n",
            "Loss (epoch: 4601): 0.00064039\n",
            "Loss (epoch: 4602): 0.00064003\n",
            "Loss (epoch: 4603): 0.00063966\n",
            "Loss (epoch: 4604): 0.00063930\n",
            "Loss (epoch: 4605): 0.00063893\n",
            "Loss (epoch: 4606): 0.00063857\n",
            "Loss (epoch: 4607): 0.00063821\n",
            "Loss (epoch: 4608): 0.00063785\n",
            "Loss (epoch: 4609): 0.00063748\n",
            "Loss (epoch: 4610): 0.00063712\n",
            "Loss (epoch: 4611): 0.00063676\n",
            "Loss (epoch: 4612): 0.00063640\n",
            "Loss (epoch: 4613): 0.00063604\n",
            "Loss (epoch: 4614): 0.00063568\n",
            "Loss (epoch: 4615): 0.00063532\n",
            "Loss (epoch: 4616): 0.00063496\n",
            "Loss (epoch: 4617): 0.00063460\n",
            "Loss (epoch: 4618): 0.00063424\n",
            "Loss (epoch: 4619): 0.00063388\n",
            "Loss (epoch: 4620): 0.00063352\n",
            "Loss (epoch: 4621): 0.00063316\n",
            "Loss (epoch: 4622): 0.00063281\n",
            "Loss (epoch: 4623): 0.00063245\n",
            "Loss (epoch: 4624): 0.00063209\n",
            "Loss (epoch: 4625): 0.00063173\n",
            "Loss (epoch: 4626): 0.00063138\n",
            "Loss (epoch: 4627): 0.00063102\n",
            "Loss (epoch: 4628): 0.00063066\n",
            "Loss (epoch: 4629): 0.00063031\n",
            "Loss (epoch: 4630): 0.00062995\n",
            "Loss (epoch: 4631): 0.00062960\n",
            "Loss (epoch: 4632): 0.00062924\n",
            "Loss (epoch: 4633): 0.00062888\n",
            "Loss (epoch: 4634): 0.00062853\n",
            "Loss (epoch: 4635): 0.00062818\n",
            "Loss (epoch: 4636): 0.00062782\n",
            "Loss (epoch: 4637): 0.00062747\n",
            "Loss (epoch: 4638): 0.00062711\n",
            "Loss (epoch: 4639): 0.00062676\n",
            "Loss (epoch: 4640): 0.00062641\n",
            "Loss (epoch: 4641): 0.00062605\n",
            "Loss (epoch: 4642): 0.00062570\n",
            "Loss (epoch: 4643): 0.00062535\n",
            "Loss (epoch: 4644): 0.00062500\n",
            "Loss (epoch: 4645): 0.00062465\n",
            "Loss (epoch: 4646): 0.00062430\n",
            "Loss (epoch: 4647): 0.00062395\n",
            "Loss (epoch: 4648): 0.00062359\n",
            "Loss (epoch: 4649): 0.00062324\n",
            "Loss (epoch: 4650): 0.00062289\n",
            "Loss (epoch: 4651): 0.00062254\n",
            "Loss (epoch: 4652): 0.00062219\n",
            "Loss (epoch: 4653): 0.00062184\n",
            "Loss (epoch: 4654): 0.00062150\n",
            "Loss (epoch: 4655): 0.00062115\n",
            "Loss (epoch: 4656): 0.00062080\n",
            "Loss (epoch: 4657): 0.00062045\n",
            "Loss (epoch: 4658): 0.00062010\n",
            "Loss (epoch: 4659): 0.00061976\n",
            "Loss (epoch: 4660): 0.00061941\n",
            "Loss (epoch: 4661): 0.00061906\n",
            "Loss (epoch: 4662): 0.00061871\n",
            "Loss (epoch: 4663): 0.00061837\n",
            "Loss (epoch: 4664): 0.00061802\n",
            "Loss (epoch: 4665): 0.00061768\n",
            "Loss (epoch: 4666): 0.00061733\n",
            "Loss (epoch: 4667): 0.00061699\n",
            "Loss (epoch: 4668): 0.00061664\n",
            "Loss (epoch: 4669): 0.00061630\n",
            "Loss (epoch: 4670): 0.00061595\n",
            "Loss (epoch: 4671): 0.00061561\n",
            "Loss (epoch: 4672): 0.00061526\n",
            "Loss (epoch: 4673): 0.00061492\n",
            "Loss (epoch: 4674): 0.00061458\n",
            "Loss (epoch: 4675): 0.00061423\n",
            "Loss (epoch: 4676): 0.00061389\n",
            "Loss (epoch: 4677): 0.00061355\n",
            "Loss (epoch: 4678): 0.00061320\n",
            "Loss (epoch: 4679): 0.00061286\n",
            "Loss (epoch: 4680): 0.00061252\n",
            "Loss (epoch: 4681): 0.00061218\n",
            "Loss (epoch: 4682): 0.00061184\n",
            "Loss (epoch: 4683): 0.00061150\n",
            "Loss (epoch: 4684): 0.00061116\n",
            "Loss (epoch: 4685): 0.00061082\n",
            "Loss (epoch: 4686): 0.00061048\n",
            "Loss (epoch: 4687): 0.00061014\n",
            "Loss (epoch: 4688): 0.00060980\n",
            "Loss (epoch: 4689): 0.00060946\n",
            "Loss (epoch: 4690): 0.00060912\n",
            "Loss (epoch: 4691): 0.00060878\n",
            "Loss (epoch: 4692): 0.00060845\n",
            "Loss (epoch: 4693): 0.00060811\n",
            "Loss (epoch: 4694): 0.00060777\n",
            "Loss (epoch: 4695): 0.00060743\n",
            "Loss (epoch: 4696): 0.00060710\n",
            "Loss (epoch: 4697): 0.00060676\n",
            "Loss (epoch: 4698): 0.00060642\n",
            "Loss (epoch: 4699): 0.00060608\n",
            "Loss (epoch: 4700): 0.00060575\n",
            "Loss (epoch: 4701): 0.00060541\n",
            "Loss (epoch: 4702): 0.00060508\n",
            "Loss (epoch: 4703): 0.00060474\n",
            "Loss (epoch: 4704): 0.00060441\n",
            "Loss (epoch: 4705): 0.00060407\n",
            "Loss (epoch: 4706): 0.00060374\n",
            "Loss (epoch: 4707): 0.00060340\n",
            "Loss (epoch: 4708): 0.00060307\n",
            "Loss (epoch: 4709): 0.00060274\n",
            "Loss (epoch: 4710): 0.00060241\n",
            "Loss (epoch: 4711): 0.00060207\n",
            "Loss (epoch: 4712): 0.00060174\n",
            "Loss (epoch: 4713): 0.00060141\n",
            "Loss (epoch: 4714): 0.00060107\n",
            "Loss (epoch: 4715): 0.00060074\n",
            "Loss (epoch: 4716): 0.00060041\n",
            "Loss (epoch: 4717): 0.00060008\n",
            "Loss (epoch: 4718): 0.00059975\n",
            "Loss (epoch: 4719): 0.00059942\n",
            "Loss (epoch: 4720): 0.00059909\n",
            "Loss (epoch: 4721): 0.00059876\n",
            "Loss (epoch: 4722): 0.00059843\n",
            "Loss (epoch: 4723): 0.00059810\n",
            "Loss (epoch: 4724): 0.00059777\n",
            "Loss (epoch: 4725): 0.00059744\n",
            "Loss (epoch: 4726): 0.00059711\n",
            "Loss (epoch: 4727): 0.00059678\n",
            "Loss (epoch: 4728): 0.00059645\n",
            "Loss (epoch: 4729): 0.00059613\n",
            "Loss (epoch: 4730): 0.00059580\n",
            "Loss (epoch: 4731): 0.00059547\n",
            "Loss (epoch: 4732): 0.00059514\n",
            "Loss (epoch: 4733): 0.00059482\n",
            "Loss (epoch: 4734): 0.00059449\n",
            "Loss (epoch: 4735): 0.00059416\n",
            "Loss (epoch: 4736): 0.00059384\n",
            "Loss (epoch: 4737): 0.00059351\n",
            "Loss (epoch: 4738): 0.00059319\n",
            "Loss (epoch: 4739): 0.00059286\n",
            "Loss (epoch: 4740): 0.00059254\n",
            "Loss (epoch: 4741): 0.00059221\n",
            "Loss (epoch: 4742): 0.00059189\n",
            "Loss (epoch: 4743): 0.00059156\n",
            "Loss (epoch: 4744): 0.00059124\n",
            "Loss (epoch: 4745): 0.00059092\n",
            "Loss (epoch: 4746): 0.00059059\n",
            "Loss (epoch: 4747): 0.00059027\n",
            "Loss (epoch: 4748): 0.00058995\n",
            "Loss (epoch: 4749): 0.00058962\n",
            "Loss (epoch: 4750): 0.00058930\n",
            "Loss (epoch: 4751): 0.00058898\n",
            "Loss (epoch: 4752): 0.00058866\n",
            "Loss (epoch: 4753): 0.00058834\n",
            "Loss (epoch: 4754): 0.00058802\n",
            "Loss (epoch: 4755): 0.00058769\n",
            "Loss (epoch: 4756): 0.00058737\n",
            "Loss (epoch: 4757): 0.00058705\n",
            "Loss (epoch: 4758): 0.00058673\n",
            "Loss (epoch: 4759): 0.00058641\n",
            "Loss (epoch: 4760): 0.00058609\n",
            "Loss (epoch: 4761): 0.00058577\n",
            "Loss (epoch: 4762): 0.00058546\n",
            "Loss (epoch: 4763): 0.00058514\n",
            "Loss (epoch: 4764): 0.00058482\n",
            "Loss (epoch: 4765): 0.00058450\n",
            "Loss (epoch: 4766): 0.00058418\n",
            "Loss (epoch: 4767): 0.00058386\n",
            "Loss (epoch: 4768): 0.00058355\n",
            "Loss (epoch: 4769): 0.00058323\n",
            "Loss (epoch: 4770): 0.00058291\n",
            "Loss (epoch: 4771): 0.00058260\n",
            "Loss (epoch: 4772): 0.00058228\n",
            "Loss (epoch: 4773): 0.00058196\n",
            "Loss (epoch: 4774): 0.00058165\n",
            "Loss (epoch: 4775): 0.00058133\n",
            "Loss (epoch: 4776): 0.00058102\n",
            "Loss (epoch: 4777): 0.00058070\n",
            "Loss (epoch: 4778): 0.00058039\n",
            "Loss (epoch: 4779): 0.00058007\n",
            "Loss (epoch: 4780): 0.00057976\n",
            "Loss (epoch: 4781): 0.00057944\n",
            "Loss (epoch: 4782): 0.00057913\n",
            "Loss (epoch: 4783): 0.00057882\n",
            "Loss (epoch: 4784): 0.00057850\n",
            "Loss (epoch: 4785): 0.00057819\n",
            "Loss (epoch: 4786): 0.00057788\n",
            "Loss (epoch: 4787): 0.00057756\n",
            "Loss (epoch: 4788): 0.00057725\n",
            "Loss (epoch: 4789): 0.00057694\n",
            "Loss (epoch: 4790): 0.00057663\n",
            "Loss (epoch: 4791): 0.00057632\n",
            "Loss (epoch: 4792): 0.00057600\n",
            "Loss (epoch: 4793): 0.00057569\n",
            "Loss (epoch: 4794): 0.00057538\n",
            "Loss (epoch: 4795): 0.00057507\n",
            "Loss (epoch: 4796): 0.00057476\n",
            "Loss (epoch: 4797): 0.00057445\n",
            "Loss (epoch: 4798): 0.00057414\n",
            "Loss (epoch: 4799): 0.00057383\n",
            "Loss (epoch: 4800): 0.00057352\n",
            "Loss (epoch: 4801): 0.00057322\n",
            "Loss (epoch: 4802): 0.00057291\n",
            "Loss (epoch: 4803): 0.00057260\n",
            "Loss (epoch: 4804): 0.00057229\n",
            "Loss (epoch: 4805): 0.00057198\n",
            "Loss (epoch: 4806): 0.00057167\n",
            "Loss (epoch: 4807): 0.00057137\n",
            "Loss (epoch: 4808): 0.00057106\n",
            "Loss (epoch: 4809): 0.00057075\n",
            "Loss (epoch: 4810): 0.00057045\n",
            "Loss (epoch: 4811): 0.00057014\n",
            "Loss (epoch: 4812): 0.00056983\n",
            "Loss (epoch: 4813): 0.00056953\n",
            "Loss (epoch: 4814): 0.00056922\n",
            "Loss (epoch: 4815): 0.00056892\n",
            "Loss (epoch: 4816): 0.00056861\n",
            "Loss (epoch: 4817): 0.00056831\n",
            "Loss (epoch: 4818): 0.00056800\n",
            "Loss (epoch: 4819): 0.00056770\n",
            "Loss (epoch: 4820): 0.00056739\n",
            "Loss (epoch: 4821): 0.00056709\n",
            "Loss (epoch: 4822): 0.00056679\n",
            "Loss (epoch: 4823): 0.00056648\n",
            "Loss (epoch: 4824): 0.00056618\n",
            "Loss (epoch: 4825): 0.00056588\n",
            "Loss (epoch: 4826): 0.00056557\n",
            "Loss (epoch: 4827): 0.00056527\n",
            "Loss (epoch: 4828): 0.00056497\n",
            "Loss (epoch: 4829): 0.00056467\n",
            "Loss (epoch: 4830): 0.00056436\n",
            "Loss (epoch: 4831): 0.00056406\n",
            "Loss (epoch: 4832): 0.00056376\n",
            "Loss (epoch: 4833): 0.00056346\n",
            "Loss (epoch: 4834): 0.00056316\n",
            "Loss (epoch: 4835): 0.00056286\n",
            "Loss (epoch: 4836): 0.00056256\n",
            "Loss (epoch: 4837): 0.00056226\n",
            "Loss (epoch: 4838): 0.00056196\n",
            "Loss (epoch: 4839): 0.00056166\n",
            "Loss (epoch: 4840): 0.00056136\n",
            "Loss (epoch: 4841): 0.00056106\n",
            "Loss (epoch: 4842): 0.00056076\n",
            "Loss (epoch: 4843): 0.00056046\n",
            "Loss (epoch: 4844): 0.00056017\n",
            "Loss (epoch: 4845): 0.00055987\n",
            "Loss (epoch: 4846): 0.00055957\n",
            "Loss (epoch: 4847): 0.00055927\n",
            "Loss (epoch: 4848): 0.00055898\n",
            "Loss (epoch: 4849): 0.00055868\n",
            "Loss (epoch: 4850): 0.00055838\n",
            "Loss (epoch: 4851): 0.00055808\n",
            "Loss (epoch: 4852): 0.00055779\n",
            "Loss (epoch: 4853): 0.00055749\n",
            "Loss (epoch: 4854): 0.00055720\n",
            "Loss (epoch: 4855): 0.00055690\n",
            "Loss (epoch: 4856): 0.00055660\n",
            "Loss (epoch: 4857): 0.00055631\n",
            "Loss (epoch: 4858): 0.00055601\n",
            "Loss (epoch: 4859): 0.00055572\n",
            "Loss (epoch: 4860): 0.00055542\n",
            "Loss (epoch: 4861): 0.00055513\n",
            "Loss (epoch: 4862): 0.00055484\n",
            "Loss (epoch: 4863): 0.00055454\n",
            "Loss (epoch: 4864): 0.00055425\n",
            "Loss (epoch: 4865): 0.00055396\n",
            "Loss (epoch: 4866): 0.00055366\n",
            "Loss (epoch: 4867): 0.00055337\n",
            "Loss (epoch: 4868): 0.00055308\n",
            "Loss (epoch: 4869): 0.00055278\n",
            "Loss (epoch: 4870): 0.00055249\n",
            "Loss (epoch: 4871): 0.00055220\n",
            "Loss (epoch: 4872): 0.00055191\n",
            "Loss (epoch: 4873): 0.00055162\n",
            "Loss (epoch: 4874): 0.00055133\n",
            "Loss (epoch: 4875): 0.00055103\n",
            "Loss (epoch: 4876): 0.00055074\n",
            "Loss (epoch: 4877): 0.00055045\n",
            "Loss (epoch: 4878): 0.00055016\n",
            "Loss (epoch: 4879): 0.00054987\n",
            "Loss (epoch: 4880): 0.00054958\n",
            "Loss (epoch: 4881): 0.00054929\n",
            "Loss (epoch: 4882): 0.00054900\n",
            "Loss (epoch: 4883): 0.00054871\n",
            "Loss (epoch: 4884): 0.00054842\n",
            "Loss (epoch: 4885): 0.00054813\n",
            "Loss (epoch: 4886): 0.00054785\n",
            "Loss (epoch: 4887): 0.00054756\n",
            "Loss (epoch: 4888): 0.00054727\n",
            "Loss (epoch: 4889): 0.00054698\n",
            "Loss (epoch: 4890): 0.00054669\n",
            "Loss (epoch: 4891): 0.00054641\n",
            "Loss (epoch: 4892): 0.00054612\n",
            "Loss (epoch: 4893): 0.00054583\n",
            "Loss (epoch: 4894): 0.00054554\n",
            "Loss (epoch: 4895): 0.00054526\n",
            "Loss (epoch: 4896): 0.00054497\n",
            "Loss (epoch: 4897): 0.00054469\n",
            "Loss (epoch: 4898): 0.00054440\n",
            "Loss (epoch: 4899): 0.00054411\n",
            "Loss (epoch: 4900): 0.00054383\n",
            "Loss (epoch: 4901): 0.00054354\n",
            "Loss (epoch: 4902): 0.00054326\n",
            "Loss (epoch: 4903): 0.00054297\n",
            "Loss (epoch: 4904): 0.00054269\n",
            "Loss (epoch: 4905): 0.00054240\n",
            "Loss (epoch: 4906): 0.00054212\n",
            "Loss (epoch: 4907): 0.00054184\n",
            "Loss (epoch: 4908): 0.00054155\n",
            "Loss (epoch: 4909): 0.00054127\n",
            "Loss (epoch: 4910): 0.00054098\n",
            "Loss (epoch: 4911): 0.00054070\n",
            "Loss (epoch: 4912): 0.00054042\n",
            "Loss (epoch: 4913): 0.00054013\n",
            "Loss (epoch: 4914): 0.00053985\n",
            "Loss (epoch: 4915): 0.00053957\n",
            "Loss (epoch: 4916): 0.00053929\n",
            "Loss (epoch: 4917): 0.00053900\n",
            "Loss (epoch: 4918): 0.00053872\n",
            "Loss (epoch: 4919): 0.00053844\n",
            "Loss (epoch: 4920): 0.00053816\n",
            "Loss (epoch: 4921): 0.00053788\n",
            "Loss (epoch: 4922): 0.00053760\n",
            "Loss (epoch: 4923): 0.00053732\n",
            "Loss (epoch: 4924): 0.00053704\n",
            "Loss (epoch: 4925): 0.00053675\n",
            "Loss (epoch: 4926): 0.00053647\n",
            "Loss (epoch: 4927): 0.00053620\n",
            "Loss (epoch: 4928): 0.00053592\n",
            "Loss (epoch: 4929): 0.00053564\n",
            "Loss (epoch: 4930): 0.00053536\n",
            "Loss (epoch: 4931): 0.00053508\n",
            "Loss (epoch: 4932): 0.00053480\n",
            "Loss (epoch: 4933): 0.00053452\n",
            "Loss (epoch: 4934): 0.00053424\n",
            "Loss (epoch: 4935): 0.00053396\n",
            "Loss (epoch: 4936): 0.00053368\n",
            "Loss (epoch: 4937): 0.00053341\n",
            "Loss (epoch: 4938): 0.00053313\n",
            "Loss (epoch: 4939): 0.00053285\n",
            "Loss (epoch: 4940): 0.00053257\n",
            "Loss (epoch: 4941): 0.00053230\n",
            "Loss (epoch: 4942): 0.00053202\n",
            "Loss (epoch: 4943): 0.00053174\n",
            "Loss (epoch: 4944): 0.00053147\n",
            "Loss (epoch: 4945): 0.00053119\n",
            "Loss (epoch: 4946): 0.00053091\n",
            "Loss (epoch: 4947): 0.00053064\n",
            "Loss (epoch: 4948): 0.00053036\n",
            "Loss (epoch: 4949): 0.00053009\n",
            "Loss (epoch: 4950): 0.00052981\n",
            "Loss (epoch: 4951): 0.00052954\n",
            "Loss (epoch: 4952): 0.00052926\n",
            "Loss (epoch: 4953): 0.00052899\n",
            "Loss (epoch: 4954): 0.00052871\n",
            "Loss (epoch: 4955): 0.00052844\n",
            "Loss (epoch: 4956): 0.00052816\n",
            "Loss (epoch: 4957): 0.00052789\n",
            "Loss (epoch: 4958): 0.00052761\n",
            "Loss (epoch: 4959): 0.00052734\n",
            "Loss (epoch: 4960): 0.00052707\n",
            "Loss (epoch: 4961): 0.00052679\n",
            "Loss (epoch: 4962): 0.00052652\n",
            "Loss (epoch: 4963): 0.00052625\n",
            "Loss (epoch: 4964): 0.00052597\n",
            "Loss (epoch: 4965): 0.00052570\n",
            "Loss (epoch: 4966): 0.00052543\n",
            "Loss (epoch: 4967): 0.00052516\n",
            "Loss (epoch: 4968): 0.00052489\n",
            "Loss (epoch: 4969): 0.00052461\n",
            "Loss (epoch: 4970): 0.00052434\n",
            "Loss (epoch: 4971): 0.00052407\n",
            "Loss (epoch: 4972): 0.00052380\n",
            "Loss (epoch: 4973): 0.00052353\n",
            "Loss (epoch: 4974): 0.00052326\n",
            "Loss (epoch: 4975): 0.00052299\n",
            "Loss (epoch: 4976): 0.00052272\n",
            "Loss (epoch: 4977): 0.00052244\n",
            "Loss (epoch: 4978): 0.00052217\n",
            "Loss (epoch: 4979): 0.00052190\n",
            "Loss (epoch: 4980): 0.00052163\n",
            "Loss (epoch: 4981): 0.00052136\n",
            "Loss (epoch: 4982): 0.00052110\n",
            "Loss (epoch: 4983): 0.00052083\n",
            "Loss (epoch: 4984): 0.00052056\n",
            "Loss (epoch: 4985): 0.00052029\n",
            "Loss (epoch: 4986): 0.00052002\n",
            "Loss (epoch: 4987): 0.00051975\n",
            "Loss (epoch: 4988): 0.00051948\n",
            "Loss (epoch: 4989): 0.00051921\n",
            "Loss (epoch: 4990): 0.00051894\n",
            "Loss (epoch: 4991): 0.00051868\n",
            "Loss (epoch: 4992): 0.00051841\n",
            "Loss (epoch: 4993): 0.00051814\n",
            "Loss (epoch: 4994): 0.00051787\n",
            "Loss (epoch: 4995): 0.00051761\n",
            "Loss (epoch: 4996): 0.00051734\n",
            "Loss (epoch: 4997): 0.00051707\n",
            "Loss (epoch: 4998): 0.00051681\n",
            "Loss (epoch: 4999): 0.00051654\n",
            "Loss (epoch: 5000): 0.00051627\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "from re import L\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import random\n",
        "\n",
        "from torch import optim\n",
        "from torch.utils import data\n",
        "from torch.optim import lr_scheduler\n",
        "from torchvision import datasets, models, transforms\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import statistics\n",
        "import datetime\n",
        "import os\n",
        "import csv\n",
        "import math\n",
        "import time\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_tensor = torch.tensor(V, dtype=torch.float32)\n",
        "Y_tensor = torch.tensor(I, dtype=torch.float32)\n",
        "x_train, x_test, y_train, y_test = train_test_split(X_tensor, Y_tensor, test_size=0.1, random_state=41)\n",
        "dataset = TensorDataset(x_train, y_train)\n",
        "dataloader = DataLoader(dataset, batch_size = 16)\n",
        "testdataloader = DataLoader(TensorDataset(x_test, y_test))\n",
        "\n",
        "n1 = 16\n",
        "n2 = 8\n",
        "\n",
        "# Define the neural network class\n",
        "class MLP(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MLP, self).__init__()\n",
        "        self.fc1 = torch.nn.Linear(2, n1)\n",
        "        self.fc2 = torch.nn.Linear(n1, n2)\n",
        "        self.fc3 = torch.nn.Linear(n2, 2)\n",
        "        self.dropout = torch.nn.Dropout(0.1)\n",
        "        self.tanh = torch.nn.Tanh()\n",
        "        self.bn1 = torch.nn.BatchNorm1d(n1)\n",
        "        self.bn2 = torch.nn.BatchNorm1d(n2)\n",
        "        self.bn3 = torch.nn.BatchNorm1d(1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        #x = self.bn1(x)\n",
        "        x = self.tanh(x)\n",
        "        #x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        #x = self.bn2(x)\n",
        "        x = self.tanh(x)\n",
        "        #x = self.dropout(x)\n",
        "        x = self.fc3(x)\n",
        "        #x = self.bn3(x)\n",
        "        return x\n",
        "\n",
        "# Create an instance of the MLP class\n",
        "model = MLP()\n",
        "\n",
        "def initialize_weights(m):\n",
        "    if isinstance(m, torch.nn.Linear):\n",
        "        torch.nn.init.xavier_uniform_(m.weight)\n",
        "        if m.bias is not None:\n",
        "            torch.nn.init.constant_(m.bias, 0)\n",
        "\n",
        "model.apply(initialize_weights)\n",
        "\n",
        "loss_function = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5)\n",
        "#torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "nb_epochs = 5000\n",
        "MLoss = []\n",
        "for epoch in range(0, nb_epochs):\n",
        "\n",
        "    current_loss = 0.0\n",
        "    losses = []\n",
        "    # Iterate over the dataloader for training data\n",
        "    for i, data in enumerate(dataloader, 0):\n",
        "        inputs, targets = data\n",
        "        inputs, targets = inputs.float(), targets.float()\n",
        "        targets = targets.reshape((targets.shape[0],1))\n",
        "        #zero the gradients\n",
        "        optimizer.zero_grad()\n",
        "        #perform forward pass\n",
        "        outputs = model(inputs)\n",
        "        L_weight = 3\n",
        "        #compute loss\n",
        "        batch_loss = []\n",
        "        for j in range(inputs.size(0)):\n",
        "            input_j = inputs[j].reshape((1, inputs.shape[1]))\n",
        "            if input_j[0,0]>0.3:\n",
        "                batch_loss.append(L_weight*loss_function(outputs[j], targets[j]))\n",
        "            else:\n",
        "                batch_loss.append(loss_function(outputs[j], targets[j]))\n",
        "        loss = torch.stack(batch_loss).mean()\n",
        "        losses.append(loss.item())\n",
        "        #perform backward pass\n",
        "        loss.backward()\n",
        "        #perform optimization\n",
        "        optimizer.step()\n",
        "        # Print statistics\n",
        "\n",
        "    mean_loss = sum(losses)/len(losses)\n",
        "    scheduler.step(mean_loss)\n",
        "\n",
        "    print('Loss (epoch: %4d): %.8f' %(epoch+1, mean_loss))\n",
        "    current_loss = 0.0\n",
        "    MLoss.append(mean_loss)\n",
        "\n",
        "# Process is complete.\n",
        "#print('Training process has finished.')\n",
        "\n",
        "torch.save(model, 'IWO_idvg.pt')\n",
        "torch.save(model.state_dict(), 'IWO_idvg_state_dict.pt')\n",
        "\n",
        "####### loss vs. epoch #######\n",
        "xloss = list(range(0, nb_epochs))\n",
        "plt.plot(xloss, np.log10(MLoss))\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## HV-IWO-EMODE\n",
        "with torch.no_grad():\n",
        "\n",
        "    output = []\n",
        "    # Iterate over the dataloader for training data\n",
        "    for i, data in enumerate(testdataloader, 0):\n",
        "        inputs, targets = data\n",
        "        inputs, targets = inputs.float(), targets.float()\n",
        "        targets = targets.reshape((targets.shape[0],1))\n",
        "\n",
        "        #zero the gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        #perform forward pass\n",
        "        outputs = model(inputs)\n",
        "        output.append(outputs)\n",
        "# Process is complete.\n",
        "#print('Training process has finished.')\n",
        "\n",
        "# Extract the weights and biases from the model\n",
        "weights_1 = model.fc1.weight.detach().numpy()\n",
        "bias_1 = model.fc1.bias.detach().numpy()\n",
        "weights_2 = model.fc2.weight.detach().numpy()\n",
        "bias_2 = model.fc2.bias.detach().numpy()\n",
        "weights_3 = model.fc3.weight.detach().numpy()\n",
        "bias_3 = model.fc3.bias.detach().numpy()\n",
        "\n",
        "def generate_variable_declarations(weights_shape, layer_prefix):\n",
        "    declarations = \"\"\n",
        "    num_neurons = weights_shape  # Number of neurons is determined by the first dimension of the weights matrix\n",
        "    layer_declarations = \", \".join([f\"{layer_prefix}_{i}\" for i in range(num_neurons)]) + \";\"\n",
        "    declarations += layer_declarations\n",
        "    return declarations\n",
        "\n",
        "# Use the function to generate declarations for each layer\n",
        "h1_declarations = generate_variable_declarations(weights_1.shape[0], \"hvth1\")\n",
        "h2_declarations = generate_variable_declarations(weights_2.shape[0], \"hvth2\")\n",
        "\n",
        "verilog_code = \"\"\"\n",
        "// VerilogA for ML_HV, ML_HV_emode, veriloga\n",
        "\n",
        "`include \"constants.vams\"\n",
        "`include \"disciplines.vams\"\n",
        "\n",
        "module ML_HV_emode_rev3(d, g, s);\n",
        "\n",
        "\n",
        "inout d, g, s;\n",
        "electrical d, g, s;\n",
        "\n",
        "//****** Parameters L and W ********\n",
        "parameter real W = 3;\n",
        "parameter real L = 0.6;\n",
        "parameter MinVg = -6.0 ;\n",
        "parameter normVg = 0.05555555555555555 ;\n",
        "parameter MinVd = 0.0 ;\n",
        "parameter normVd = 0.08333333333333333 ;\n",
        "parameter MinLg = 0 ;\n",
        "parameter normLg = 1.66666667 ;\n",
        "parameter MinI = -14.614393726401689 ;\n",
        "parameter normI = 0.0975145951626748;\n",
        "parameter vth =0;\n",
        "parameter real T_stress = 1; //set on cadence as variable\n",
        "parameter real Temp = 25; //set on cadence as variable\n",
        "\n",
        "parameter MinVg_cgs = -6.0 ;\n",
        "parameter normVg_cgs = 0.05555555555555555 ;\n",
        "parameter MinVd_cgs = -6.0 ;\n",
        "parameter normVd_cgs = 0.05555555555555555 ;\n",
        "parameter MinLg_cgs = 0.6 ;\n",
        "parameter normLg_cgs = 0 ;\n",
        "parameter MinO_cgs = -13.164309428507574 ;\n",
        "parameter normO_cgs =2.3582977043250914;\n",
        "\n",
        "parameter MinVg_cgd = -6.0 ;\n",
        "parameter normVg_cgd = 0.05555555555555555 ;\n",
        "parameter MinVd_cgd = -4.0 ;\n",
        "parameter normVd_cgd = 0.06349206349206349 ;\n",
        "parameter MinLg_cgd = 0.6 ;\n",
        "parameter normLg_cgd = 0 ;\n",
        "parameter MinO_cgd = -14.010995384301463 ;\n",
        "parameter normO_cgd =1.5234203322209112;\n",
        "\n",
        "parameter Mint_stress = {} ;\n",
        "parameter normt_stress = {} ;\n",
        "parameter Mintemperature = {} ;\n",
        "parameter normtemperature = {} ;\n",
        "parameter Mindelta_Vth = {} ;\n",
        "parameter normdelta_Vth = {} ;\n",
        "\n",
        "real {}\n",
        "real {}\n",
        "\n",
        "\n",
        "real Vg, Vd, Vs, Vgs, Vds, Lg, Id, Cgg, Cgs,Cgd, Vgd;\n",
        "real y, y_cgg, y_cgs, y_cgd, yvth;\n",
        "real Vgs_cgg, Vgs_cgs, Vgs_cgd;\n",
        "real Vds_cgg, Vds_cgs, Vds_cgd;\n",
        "real Lg_cgg, Lg_cgs, Lg_cgd;\n",
        "real Vgsraw, Vgdraw, dir;\n",
        "real Vdi, Vsi;\n",
        "real t_stress, v_ov, t_rec, clk_loops, temp, delta_Vth;\n",
        "\n",
        "\n",
        "real h1_0, h1_1, h1_2, h1_3, h1_4, h1_5, h1_6, h1_7, h1_8, h1_9, h1_10, h1_11, h1_12, h1_13, h1_14, h1_15, h1_16, h1_17, h1_18, h1_19, h1_20, h1_21, h1_22, h1_23, h1_24;\n",
        "real h2_0, h2_1, h2_2, h2_3, h2_4, h2_5, h2_6, h2_7, h2_8, h2_9, h2_10, h2_11;\n",
        "real hcgg1_0, hcgg1_1, hcgg1_2, hcgg1_3, hcgg1_4, hcgg1_5, hcgg1_6, hcgg1_7, hcgg1_8, hcgg1_9, hcgg1_10, hcgg1_11, hcgg1_12, hcgg1_13, hcgg1_14, hcgg1_15, hcgg1_16, hcgg1_17, hcgg1_18, hcgg1_19, hcgg1_20, hcgg1_21, hcgg1_22, hcgg1_23, hcgg1_24;\n",
        "real hcgg2_0, hcgg2_1, hcgg2_2, hcgg2_3, hcgg2_4, hcgg2_5, hcgg2_6, hcgg2_7, hcgg2_8, hcgg2_9, hcgg2_10, hcgg2_11;\n",
        "real hcgs1_0, hcgs1_1, hcgs1_2, hcgs1_3, hcgs1_4, hcgs1_5, hcgs1_6, hcgs1_7, hcgs1_8, hcgs1_9, hcgs1_10, hcgs1_11, hcgs1_12, hcgs1_13, hcgs1_14, hcgs1_15, hcgs1_16, hcgs1_17, hcgs1_18, hcgs1_19, hcgs1_20, hcgs1_21, hcgs1_22, hcgs1_23, hcgs1_24;\n",
        "real hcgs2_0, hcgs2_1, hcgs2_2, hcgs2_3, hcgs2_4, hcgs2_5, hcgs2_6, hcgs2_7, hcgs2_8, hcgs2_9, hcgs2_10, hcgs2_11;\n",
        "real hcgd1_0, hcgd1_1, hcgd1_2, hcgd1_3, hcgd1_4, hcgd1_5, hcgd1_6, hcgd1_7, hcgd1_8, hcgd1_9, hcgd1_10, hcgd1_11, hcgd1_12, hcgd1_13, hcgd1_14, hcgd1_15, hcgd1_16, hcgd1_17, hcgd1_18, hcgd1_19, hcgd1_20, hcgd1_21, hcgd1_22, hcgd1_23, hcgd1_24;\n",
        "real hcgd2_0, hcgd2_1, hcgd2_2, hcgd2_3, hcgd2_4, hcgd2_5, hcgd2_6, hcgd2_7, hcgd2_8, hcgd2_9, hcgd2_10, hcgd2_11;\n",
        "\n",
        "real Qgd, Qgs;\n",
        "\n",
        "\n",
        "analog begin\n",
        "\n",
        "temp = (Temp - Mintemperature)*normtemperature;\n",
        "t_stress = (T_stress - Mint_stress)*normt_stress;\n",
        "\n",
        "//******************** delta_Vth NN **********************************//\n",
        "\n",
        "\"\"\".format(Mint_stress, normt_stress, Mintemperature, normtemperature, Mindelta_Vth, 1/normdelta_Vth, h1_declarations, h2_declarations)\n",
        "# V_ov = (V_ov - MinV_ov)*normV_ov ;\n",
        "# t_stress = (T_stress - Mint_stress)*normt_stress ;\n",
        "\n",
        "# Create the Verilog-A code for the 1st hidden layer\n",
        "for i in range(n1):\n",
        "    inputs = [\"t_stress\", \"temp\"]\n",
        "    inputs = [\"*\".join([str(weights_1[i][j]), inp]) for j, inp in enumerate(inputs)]\n",
        "    inputs = \"+\".join(inputs)\n",
        "    inputs = \"+\".join([inputs, str(bias_1[i])])\n",
        "    verilog_code += \"hvth1_{} = tanh({});\\n\".format(i, inputs)\n",
        "verilog_code += \"\\n\"\n",
        "\n",
        "# Create the Verilog-A code for the 2nd hidden layer\n",
        "for i in range(n2):\n",
        "    inputs = [\"hvth1_{}\".format(j) for j in range(n1)]\n",
        "    inputs = [\"*\".join([str(weights_2[i][j]), inp]) for j, inp in enumerate(inputs)]\n",
        "    inputs = \"+\".join(inputs)\n",
        "    inputs = \"+\".join([inputs, str(bias_2[i])])\n",
        "    verilog_code += \"hvth2_{} = tanh({});\\n\".format(i, inputs)\n",
        "verilog_code += \"\\n\"\n",
        "\n",
        "# Create the Verilog-A code for the output layer\n",
        "inputs = [\"hvth2_{}\".format(i) for i in range(n2)]\n",
        "inputs = [\"*\".join([str(weights_3[0][i]), inp]) for i, inp in enumerate(inputs)]\n",
        "inputs = \"+\".join(inputs)\n",
        "inputs = \"+\".join([inputs, str(bias_3[0])])\n",
        "verilog_code += \"yvth = {};\\n\\n\".format(inputs)\n",
        "verilog_code += \"delta_Vth = (yvth - Mindelta_Vth) * normdelta_Vth;\\n\"\n",
        "verilog_code += \"\"\"$strobe(\"dvth=$g\",delta_Vth);\"\"\"\n",
        "verilog_code += \"\"\"\n",
        "\n",
        "\n",
        "\tVg = V(g);\n",
        "\tVs = V(s);\n",
        "\tVd = V(d);\n",
        "    Vgsraw = Vg-Vs ;\n",
        "    Vgdraw = Vg-Vd ;\n",
        "if (Vgsraw>=Vgdraw) begin\n",
        "\tVgs = ((Vg-Vs) - MinVg - vth - delta_Vth) * normVg ;\n",
        "\n",
        "\tVgs_cgs = ((Vg-Vs) - MinVg_cgs - vth - delta_Vth) * normVg_cgs ;\n",
        "\tVgs_cgd = ((Vg-Vs) - MinVg_cgd - vth - delta_Vth) * normVg_cgd ;\n",
        "\tdir = 1 ;\n",
        "end\n",
        "else begin\n",
        "\tVgs = ((Vg-Vd) - MinVg - vth) * normVg ;\n",
        "\n",
        "\tVgs_cgs = ((Vg-Vd) - MinVg_cgs - vth - delta_Vth) * normVg_cgs ;\n",
        "\tVgs_cgd = ((Vg-Vd) - MinVg_cgd - vth - delta_Vth) * normVg_cgd ;\n",
        "\tdir = -1 ;\n",
        "end\n",
        "\n",
        "\tVds = (abs(Vd-Vs) - MinVd) * normVd ;\n",
        "\n",
        "\tVds_cgs = (abs(Vd-Vs) - MinVd_cgs) * normVd_cgs ;\n",
        "\tVds_cgd = (abs(Vd-Vs) - MinVd_cgd) * normVd_cgd ;\n",
        "\n",
        "\tLg = (L -MinLg)*normLg ;\n",
        "\n",
        "\tLg_cgs = (L -MinLg_cgs)*normLg_cgs ;\n",
        "\tLg_cgd = (L -MinLg_cgd)*normLg_cgd ;\n",
        "\n",
        "hcgd1_0 = tanh(-0.073090345*Vgs_cgd+1.7493699*Vds_cgd+-0.25351274*Lg_cgd+-0.30203047);\n",
        "hcgd1_1 = tanh(-0.81192595*Vgs_cgd+0.7702285*Vds_cgd+0.32290834*Lg_cgd+0.12260828);\n",
        "hcgd1_2 = tanh(-1.4992723*Vgs_cgd+1.0054874*Vds_cgd+-0.040279582*Lg_cgd+0.23167521);\n",
        "hcgd1_3 = tanh(-0.27198792*Vgs_cgd+0.14240761*Vds_cgd+-0.45070043*Lg_cgd+-0.47009295);\n",
        "hcgd1_4 = tanh(0.5909458*Vgs_cgd+0.1271886*Vds_cgd+0.25604323*Lg_cgd+-0.3614964);\n",
        "hcgd1_5 = tanh(1.1455232*Vgs_cgd+-0.1336111*Vds_cgd+0.16404216*Lg_cgd+-0.3844855);\n",
        "hcgd1_6 = tanh(0.14468843*Vgs_cgd+2.6229973*Vds_cgd+-0.39613384*Lg_cgd+0.23941508);\n",
        "hcgd1_7 = tanh(-0.20886186*Vgs_cgd+3.2628205*Vds_cgd+0.132442*Lg_cgd+-0.49246147);\n",
        "hcgd1_8 = tanh(0.48230776*Vgs_cgd+0.20745495*Vds_cgd+0.17956811*Lg_cgd+0.13416706);\n",
        "hcgd1_9 = tanh(0.11828926*Vgs_cgd+-0.36482674*Vds_cgd+0.38881582*Lg_cgd+0.090164326);\n",
        "hcgd1_10 = tanh(0.013121906*Vgs_cgd+2.042608*Vds_cgd+0.15619394*Lg_cgd+-0.17228433);\n",
        "hcgd1_11 = tanh(0.85692364*Vgs_cgd+1.0132248*Vds_cgd+0.20936537*Lg_cgd+-1.0232097);\n",
        "hcgd1_12 = tanh(-0.07165884*Vgs_cgd+-0.3623631*Vds_cgd+0.14471789*Lg_cgd+0.55825126);\n",
        "hcgd1_13 = tanh(-0.45831093*Vgs_cgd+-0.27587467*Vds_cgd+0.087761946*Lg_cgd+0.24322145);\n",
        "hcgd1_14 = tanh(-0.12094344*Vgs_cgd+-1.3313415*Vds_cgd+0.3058515*Lg_cgd+0.38591075);\n",
        "hcgd1_15 = tanh(-0.48209256*Vgs_cgd+4.390121*Vds_cgd+-0.22924025*Lg_cgd+-0.22238722);\n",
        "hcgd1_16 = tanh(-0.13342527*Vgs_cgd+-0.17032403*Vds_cgd+-0.19245708*Lg_cgd+-0.34511408);\n",
        "hcgd1_17 = tanh(-0.019135991*Vgs_cgd+1.6390436*Vds_cgd+-0.39458174*Lg_cgd+-0.374234);\n",
        "hcgd1_18 = tanh(1.9993974*Vgs_cgd+-0.6025967*Vds_cgd+0.051945195*Lg_cgd+-0.2606078);\n",
        "hcgd1_19 = tanh(-1.319852*Vgs_cgd+-1.4124283*Vds_cgd+-0.16422051*Lg_cgd+0.60615516);\n",
        "hcgd1_20 = tanh(0.7590748*Vgs_cgd+0.35662842*Vds_cgd+-0.32554984*Lg_cgd+-0.45226586);\n",
        "hcgd1_21 = tanh(-0.0644961*Vgs_cgd+-1.4253865*Vds_cgd+0.31603792*Lg_cgd+0.34580982);\n",
        "hcgd1_22 = tanh(-0.024503596*Vgs_cgd+-1.2409326*Vds_cgd+0.266246*Lg_cgd+0.05068574);\n",
        "hcgd1_23 = tanh(0.40497908*Vgs_cgd+-0.5119794*Vds_cgd+0.27652997*Lg_cgd+-0.49364495);\n",
        "hcgd1_24 = tanh(-0.66816944*Vgs_cgd+0.23831755*Vds_cgd+0.3236202*Lg_cgd+0.45229998);\n",
        "hcgd2_0 = tanh(-0.8581911*hcgd1_0+-1.7806648*hcgd1_1+-2.3480477*hcgd1_2+-0.060092464*hcgd1_3+1.0001435*hcgd1_4+1.0480509*hcgd1_5+-0.3004585*hcgd1_6+-0.98489136*hcgd1_7+0.15593737*hcgd1_8+0.70743585*hcgd1_9+-0.24457291*hcgd1_10+0.9940044*hcgd1_11+0.23668307*hcgd1_12+-0.5144086*hcgd1_13+0.16627982*hcgd1_14+-1.4144597*hcgd1_15+-0.11155476*hcgd1_16+-0.3225649*hcgd1_17+1.4172044*hcgd1_18+0.61510265*hcgd1_19+0.8610263*hcgd1_20+-0.044497408*hcgd1_21+0.46571997*hcgd1_22+-0.013894911*hcgd1_23+-1.1907585*hcgd1_24+0.14715073);\n",
        "hcgd2_1 = tanh(0.74444413*hcgd1_0+0.60665065*hcgd1_1+0.396881*hcgd1_2+-0.0940794*hcgd1_3+-0.27899864*hcgd1_4+-0.087556325*hcgd1_5+0.40643474*hcgd1_6+1.366464*hcgd1_7+-0.17410953*hcgd1_8+-0.012924953*hcgd1_9+0.56350505*hcgd1_10+0.36612725*hcgd1_11+-0.26115453*hcgd1_12+-0.29767877*hcgd1_13+-0.4200406*hcgd1_14+1.1142844*hcgd1_15+0.19850636*hcgd1_16+0.8344988*hcgd1_17+0.11088754*hcgd1_18+-0.26612458*hcgd1_19+-0.27794132*hcgd1_20+-0.80967337*hcgd1_21+-0.11269937*hcgd1_22+-0.23162273*hcgd1_23+-0.098596975*hcgd1_24+0.09286805);\n",
        "hcgd2_2 = tanh(-0.7313956*hcgd1_0+-0.7514108*hcgd1_1+-0.5736202*hcgd1_2+-0.49893183*hcgd1_3+0.1765418*hcgd1_4+0.14255758*hcgd1_5+0.07957998*hcgd1_6+-0.66936827*hcgd1_7+0.2132876*hcgd1_8+0.13260117*hcgd1_9+-0.22507648*hcgd1_10+0.069224365*hcgd1_11+-0.031068536*hcgd1_12+0.0057172063*hcgd1_13+0.9005941*hcgd1_14+0.045649208*hcgd1_15+-0.4302063*hcgd1_16+-1.064374*hcgd1_17+0.100322045*hcgd1_18+0.1628234*hcgd1_19+-0.17792495*hcgd1_20+0.73057646*hcgd1_21+-0.04239934*hcgd1_22+0.07292476*hcgd1_23+-0.03253051*hcgd1_24+-0.069609255);\n",
        "hcgd2_3 = tanh(0.14440651*hcgd1_0+0.3216678*hcgd1_1+0.51213056*hcgd1_2+0.42667162*hcgd1_3+-0.610052*hcgd1_4+-0.54205084*hcgd1_5+0.060548224*hcgd1_6+0.49424285*hcgd1_7+-0.5282787*hcgd1_8+0.046284102*hcgd1_9+-0.0018920621*hcgd1_10+-0.4451091*hcgd1_11+0.04757394*hcgd1_12+0.5863736*hcgd1_13+0.15133414*hcgd1_14+0.3255952*hcgd1_15+0.27865914*hcgd1_16+0.42173597*hcgd1_17+-0.6422138*hcgd1_18+0.1657985*hcgd1_19+-1.5046545*hcgd1_20+0.5070087*hcgd1_21+0.18432228*hcgd1_22+0.14772636*hcgd1_23+-0.35788894*hcgd1_24+-0.31562403);\n",
        "hcgd2_4 = tanh(0.70184183*hcgd1_0+0.5248185*hcgd1_1+0.21824042*hcgd1_2+-0.1301908*hcgd1_3+0.29407513*hcgd1_4+0.17626354*hcgd1_5+1.0718364*hcgd1_6+1.0510215*hcgd1_7+0.696749*hcgd1_8+0.29669785*hcgd1_9+2.1009989*hcgd1_10+0.6151884*hcgd1_11+0.48851284*hcgd1_12+-0.56238925*hcgd1_13+-0.47341812*hcgd1_14+1.2261282*hcgd1_15+-0.35043222*hcgd1_16+0.12345494*hcgd1_17+0.106367536*hcgd1_18+-1.5799632*hcgd1_19+0.70326567*hcgd1_20+-1.4121299*hcgd1_21+-2.5126958*hcgd1_22+-0.34464827*hcgd1_23+0.2420689*hcgd1_24+0.3827352);\n",
        "hcgd2_5 = tanh(0.07186469*hcgd1_0+-0.3057672*hcgd1_1+0.18167095*hcgd1_2+0.24679431*hcgd1_3+0.2861313*hcgd1_4+0.20745918*hcgd1_5+0.28961903*hcgd1_6+0.4754423*hcgd1_7+0.10851595*hcgd1_8+-0.57396716*hcgd1_9+0.19753388*hcgd1_10+0.25443172*hcgd1_11+-0.058893647*hcgd1_12+0.4978357*hcgd1_13+0.40544015*hcgd1_14+0.19733265*hcgd1_15+0.024174226*hcgd1_16+0.09329063*hcgd1_17+0.12140121*hcgd1_18+0.6866702*hcgd1_19+-0.090300076*hcgd1_20+0.4295404*hcgd1_21+0.21769053*hcgd1_22+0.06967293*hcgd1_23+0.13865103*hcgd1_24+0.0039349254);\n",
        "hcgd2_6 = tanh(-0.13215303*hcgd1_0+-0.25777414*hcgd1_1+-0.24420205*hcgd1_2+0.15015651*hcgd1_3+0.7435888*hcgd1_4+0.5808469*hcgd1_5+0.23431543*hcgd1_6+-0.17212716*hcgd1_7+-0.08636741*hcgd1_8+-0.16209634*hcgd1_9+-0.24451429*hcgd1_10+0.64819676*hcgd1_11+-0.401811*hcgd1_12+-0.041673224*hcgd1_13+-0.1843273*hcgd1_14+-0.03913859*hcgd1_15+0.4648355*hcgd1_16+0.33676878*hcgd1_17+0.16188923*hcgd1_18+0.09970059*hcgd1_19+0.26570147*hcgd1_20+-0.34857365*hcgd1_21+-0.20715532*hcgd1_22+0.45225534*hcgd1_23+-0.67448044*hcgd1_24+-0.20020215);\n",
        "hcgd2_7 = tanh(0.5814423*hcgd1_0+0.30674064*hcgd1_1+0.41730982*hcgd1_2+0.1271082*hcgd1_3+0.46041217*hcgd1_4+-0.14026274*hcgd1_5+0.38002715*hcgd1_6+0.15430027*hcgd1_7+0.37405762*hcgd1_8+-0.27523136*hcgd1_9+0.5767433*hcgd1_10+0.75470895*hcgd1_11+0.39034903*hcgd1_12+-0.08490812*hcgd1_13+-0.46258885*hcgd1_14+0.52702457*hcgd1_15+-0.36667594*hcgd1_16+-0.008625877*hcgd1_17+0.09137123*hcgd1_18+-0.7821849*hcgd1_19+0.70662326*hcgd1_20+-0.20326899*hcgd1_21+-0.11657497*hcgd1_22+-0.22169054*hcgd1_23+0.23076789*hcgd1_24+0.1994013);\n",
        "hcgd2_8 = tanh(-7.4423213*hcgd1_0+-1.7918187*hcgd1_1+-2.273679*hcgd1_2+-0.35889605*hcgd1_3+-0.252701*hcgd1_4+0.9470301*hcgd1_5+-0.41792682*hcgd1_6+-7.6462445*hcgd1_7+0.018614734*hcgd1_8+1.2025093*hcgd1_9+-4.4235744*hcgd1_10+-1.8856794*hcgd1_11+0.31407484*hcgd1_12+0.08381061*hcgd1_13+2.0082252*hcgd1_14+-4.4953437*hcgd1_15+0.18078373*hcgd1_16+-2.2936556*hcgd1_17+2.0334883*hcgd1_18+-0.34214598*hcgd1_19+-0.29436678*hcgd1_20+4.8436127*hcgd1_21+1.8703012*hcgd1_22+0.32469234*hcgd1_23+-0.12540866*hcgd1_24+0.1331769);\n",
        "hcgd2_9 = tanh(-0.008969171*hcgd1_0+0.25442964*hcgd1_1+0.33559665*hcgd1_2+0.059060458*hcgd1_3+-0.6700384*hcgd1_4+-0.595369*hcgd1_5+0.432537*hcgd1_6+0.48487276*hcgd1_7+-0.014844808*hcgd1_8+-0.029969316*hcgd1_9+0.4608879*hcgd1_10+-0.67345583*hcgd1_11+0.10385167*hcgd1_12+0.47509405*hcgd1_13+0.057980414*hcgd1_14+0.57206994*hcgd1_15+-0.083018914*hcgd1_16+0.24574327*hcgd1_17+-0.76715064*hcgd1_18+0.2543998*hcgd1_19+-0.210734*hcgd1_20+-0.17915764*hcgd1_21+-0.17944436*hcgd1_22+-0.17101662*hcgd1_23+0.27301717*hcgd1_24+-0.020215586);\n",
        "hcgd2_10 = tanh(0.047154456*hcgd1_0+0.177622*hcgd1_1+-0.35302344*hcgd1_2+0.3784709*hcgd1_3+-0.21668567*hcgd1_4+-0.20013903*hcgd1_5+-0.4485513*hcgd1_6+-0.5680783*hcgd1_7+-0.25446296*hcgd1_8+0.0111656105*hcgd1_9+-0.4455899*hcgd1_10+-0.20770787*hcgd1_11+-0.27411363*hcgd1_12+0.6049824*hcgd1_13+0.13715369*hcgd1_14+-0.5386368*hcgd1_15+0.46778303*hcgd1_16+-0.21815602*hcgd1_17+0.47233215*hcgd1_18+0.5560222*hcgd1_19+-0.30228803*hcgd1_20+-0.057146665*hcgd1_21+0.446087*hcgd1_22+0.40810114*hcgd1_23+0.44648725*hcgd1_24+-0.06979678);\n",
        "hcgd2_11 = tanh(0.52387154*hcgd1_0+-0.48277956*hcgd1_1+-0.7682235*hcgd1_2+-0.190172*hcgd1_3+1.175332*hcgd1_4+1.1926033*hcgd1_5+-0.36767244*hcgd1_6+0.3228836*hcgd1_7+0.295783*hcgd1_8+-0.20389546*hcgd1_9+0.11585899*hcgd1_10+1.6549429*hcgd1_11+-0.3883799*hcgd1_12+-0.54141515*hcgd1_13+-0.30675817*hcgd1_14+-0.34997156*hcgd1_15+0.2086648*hcgd1_16+0.36170647*hcgd1_17+1.6473606*hcgd1_18+-0.4781146*hcgd1_19+1.5642655*hcgd1_20+-0.7633239*hcgd1_21+-0.47981572*hcgd1_22+0.41335562*hcgd1_23+-0.3726435*hcgd1_24+-0.0653709);\n",
        "y_cgd = -0.30169475*hcgd2_0+0.7017093*hcgd2_1+-0.18068637*hcgd2_2+-0.6699334*hcgd2_3+0.53903127*hcgd2_4+0.16665317*hcgd2_5+-0.3536876*hcgd2_6+-0.36382276*hcgd2_7+0.6040847*hcgd2_8+-0.43945295*hcgd2_9+0.5195485*hcgd2_10+-0.3194021*hcgd2_11+0.21705215;\n",
        "\n",
        "hcgs1_0 = tanh(-0.9447249*Vgs_cgs+3.848607*Vds_cgs+-0.019748846*Lg_cgs+-1.5617282);\n",
        "hcgs1_1 = tanh(-0.029012304*Vgs_cgs+-1.5533222*Vds_cgs+0.44389084*Lg_cgs+1.2160019);\n",
        "hcgs1_2 = tanh(0.8694287*Vgs_cgs+-0.03134334*Vds_cgs+-0.052811515*Lg_cgs+-0.4674938);\n",
        "hcgs1_3 = tanh(0.11125248*Vgs_cgs+-0.08053268*Vds_cgs+-0.14715484*Lg_cgs+0.2079814);\n",
        "hcgs1_4 = tanh(0.9503431*Vgs_cgs+1.2066041*Vds_cgs+0.07638908*Lg_cgs+-1.5212817);\n",
        "hcgs1_5 = tanh(1.2675861*Vgs_cgs+-0.20858626*Vds_cgs+-0.40374485*Lg_cgs+-0.6321944);\n",
        "hcgs1_6 = tanh(0.32068986*Vgs_cgs+-0.55139714*Vds_cgs+0.29643092*Lg_cgs+0.09015624);\n",
        "hcgs1_7 = tanh(-3.6914701*Vgs_cgs+0.6843926*Vds_cgs+0.14611645*Lg_cgs+1.1698359);\n",
        "hcgs1_8 = tanh(-0.51712066*Vgs_cgs+0.8325061*Vds_cgs+0.2879668*Lg_cgs+-0.07329451);\n",
        "hcgs1_9 = tanh(1.7683635*Vgs_cgs+-0.3458703*Vds_cgs+-0.16947505*Lg_cgs+-0.80866075);\n",
        "hcgs1_10 = tanh(0.010235257*Vgs_cgs+-0.009958806*Vds_cgs+-0.046799034*Lg_cgs+0.0067180935);\n",
        "hcgs1_11 = tanh(-0.56219107*Vgs_cgs+0.14688006*Vds_cgs+0.20843746*Lg_cgs+0.26059148);\n",
        "hcgs1_12 = tanh(-0.07906894*Vgs_cgs+-0.10666679*Vds_cgs+-0.27857378*Lg_cgs+0.30169338);\n",
        "hcgs1_13 = tanh(-1.6767093*Vgs_cgs+0.5625225*Vds_cgs+0.15607017*Lg_cgs+0.7490984);\n",
        "hcgs1_14 = tanh(0.004656209*Vgs_cgs+-0.0067486507*Vds_cgs+-0.4438495*Lg_cgs+-0.00023583561);\n",
        "hcgs1_15 = tanh(-0.87449557*Vgs_cgs+3.5775394*Vds_cgs+0.058419783*Lg_cgs+1.0822651);\n",
        "hcgs1_16 = tanh(2.652287*Vgs_cgs+-2.3323097*Vds_cgs+-0.3422341*Lg_cgs+0.3537005);\n",
        "hcgs1_17 = tanh(0.88335234*Vgs_cgs+-0.3338322*Vds_cgs+-0.11836058*Lg_cgs+-0.40532526);\n",
        "hcgs1_18 = tanh(-0.6324544*Vgs_cgs+5.362017*Vds_cgs+0.20589639*Lg_cgs+-1.5765152);\n",
        "hcgs1_19 = tanh(0.52068394*Vgs_cgs+-1.1275563*Vds_cgs+0.19503203*Lg_cgs+0.16029519);\n",
        "hcgs1_20 = tanh(-1.3330969*Vgs_cgs+0.12449422*Vds_cgs+-0.0033548062*Lg_cgs+0.709236);\n",
        "hcgs1_21 = tanh(0.19407558*Vgs_cgs+0.072130404*Vds_cgs+-0.43343088*Lg_cgs+0.7703464);\n",
        "hcgs1_22 = tanh(-1.1008723*Vgs_cgs+0.7357464*Vds_cgs+-0.35802382*Lg_cgs+0.35450834);\n",
        "hcgs1_23 = tanh(-1.0641274*Vgs_cgs+-0.023895433*Vds_cgs+0.312323*Lg_cgs+0.58924264);\n",
        "hcgs1_24 = tanh(1.1386739*Vgs_cgs+-0.5871172*Vds_cgs+-0.12789965*Lg_cgs+-0.42580304);\n",
        "hcgs2_0 = tanh(-0.8456805*hcgs1_0+0.81710136*hcgs1_1+-0.22456154*hcgs1_2+-0.14914224*hcgs1_3+-2.0348866*hcgs1_4+-0.747218*hcgs1_5+-0.2158404*hcgs1_6+2.553406*hcgs1_7+0.42889893*hcgs1_8+-2.4390838*hcgs1_9+0.01141642*hcgs1_10+0.29778206*hcgs1_11+0.2695455*hcgs1_12+1.7889941*hcgs1_13+0.009541916*hcgs1_14+0.8799212*hcgs1_15+-1.8207581*hcgs1_16+-0.5563907*hcgs1_17+-2.550983*hcgs1_18+-0.24775197*hcgs1_19+0.3296183*hcgs1_20+0.25748515*hcgs1_21+0.8575212*hcgs1_22+0.19629534*hcgs1_23+-3.185923*hcgs1_24+0.30510485);\n",
        "hcgs2_1 = tanh(0.5944262*hcgs1_0+0.047796533*hcgs1_1+-0.5502816*hcgs1_2+0.00890337*hcgs1_3+-0.68498886*hcgs1_4+-2.5434804*hcgs1_5+0.38445947*hcgs1_6+1.0074053*hcgs1_7+-0.22382292*hcgs1_8+-3.1384215*hcgs1_9+0.0029475465*hcgs1_10+0.30534053*hcgs1_11+0.4773358*hcgs1_12+2.1924813*hcgs1_13+0.014758705*hcgs1_14+0.32548565*hcgs1_15+-1.1178577*hcgs1_16+-1.3329682*hcgs1_17+-1.9556054*hcgs1_18+0.6174226*hcgs1_19+2.260632*hcgs1_20+0.18014935*hcgs1_21+1.1065633*hcgs1_22+0.87714285*hcgs1_23+-2.8133676*hcgs1_24+0.68831825);\n",
        "hcgs2_2 = tanh(-0.16991197*hcgs1_0+1.8664104*hcgs1_1+-0.3474432*hcgs1_2+-0.21696573*hcgs1_3+-2.5008323*hcgs1_4+-0.2903099*hcgs1_5+1.6678329*hcgs1_6+-1.6792856*hcgs1_7+-1.7562099*hcgs1_8+-0.25893897*hcgs1_9+-0.04291705*hcgs1_10+-0.00012147065*hcgs1_11+0.026284702*hcgs1_12+-0.73358375*hcgs1_13+0.15483585*hcgs1_14+-1.0945101*hcgs1_15+-0.56847095*hcgs1_16+0.18457307*hcgs1_17+1.4761636*hcgs1_18+1.4470772*hcgs1_19+0.47771305*hcgs1_20+-0.1977162*hcgs1_21+-1.5351593*hcgs1_22+0.29518163*hcgs1_23+0.60733813*hcgs1_24+-0.10970557);\n",
        "hcgs2_3 = tanh(0.23839453*hcgs1_0+-0.008309026*hcgs1_1+0.54371554*hcgs1_2+-0.10062454*hcgs1_3+0.27675664*hcgs1_4+1.0899066*hcgs1_5+-0.3777212*hcgs1_6+0.13016528*hcgs1_7+-0.012133315*hcgs1_8+1.1119828*hcgs1_9+-0.04014963*hcgs1_10+-0.4047089*hcgs1_11+-0.3015341*hcgs1_12+-1.6089342*hcgs1_13+-0.05156553*hcgs1_14+-0.92389685*hcgs1_15+0.120922476*hcgs1_16+1.1602905*hcgs1_17+-1.4947407*hcgs1_18+-0.18796265*hcgs1_19+-1.6901983*hcgs1_20+-0.035788976*hcgs1_21+-0.99925464*hcgs1_22+-0.67925924*hcgs1_23+1.6767167*hcgs1_24+-0.3653112);\n",
        "hcgs2_4 = tanh(2.0752943*hcgs1_0+-0.3572386*hcgs1_1+-0.3549206*hcgs1_2+-0.29379958*hcgs1_3+-1.2869327*hcgs1_4+-0.4479244*hcgs1_5+0.031573102*hcgs1_6+1.4112504*hcgs1_7+0.5260651*hcgs1_8+-2.0343685*hcgs1_9+0.0187355*hcgs1_10+0.04147746*hcgs1_11+0.28817755*hcgs1_12+1.6051354*hcgs1_13+-0.008745322*hcgs1_14+1.0508798*hcgs1_15+-0.7435123*hcgs1_16+0.019853538*hcgs1_17+-1.1326213*hcgs1_18+-0.49617997*hcgs1_19+0.5881124*hcgs1_20+0.3453093*hcgs1_21+0.697278*hcgs1_22+0.34144053*hcgs1_23+-0.39954275*hcgs1_24+0.24203716);\n",
        "hcgs2_5 = tanh(-0.8049178*hcgs1_0+0.9928581*hcgs1_1+0.3324538*hcgs1_2+-0.32453984*hcgs1_3+-1.2684071*hcgs1_4+0.5504112*hcgs1_5+-0.4549791*hcgs1_6+-1.4632839*hcgs1_7+0.37171695*hcgs1_8+0.72572297*hcgs1_9+-0.025831597*hcgs1_10+-0.21172419*hcgs1_11+-0.85458016*hcgs1_12+-0.8990451*hcgs1_13+-0.021322189*hcgs1_14+-1.4535208*hcgs1_15+-0.35931605*hcgs1_16+0.488443*hcgs1_17+0.0592027*hcgs1_18+-0.3993243*hcgs1_19+-0.7132404*hcgs1_20+-0.7187561*hcgs1_21+-0.021962876*hcgs1_22+-0.1357851*hcgs1_23+0.233105*hcgs1_24+-0.5199506);\n",
        "hcgs2_6 = tanh(0.5850198*hcgs1_0+0.19749781*hcgs1_1+1.0971322*hcgs1_2+1.451523*hcgs1_3+-0.18752462*hcgs1_4+0.5928157*hcgs1_5+0.26702794*hcgs1_6+-1.1799154*hcgs1_7+0.42238823*hcgs1_8+0.34430432*hcgs1_9+0.36709026*hcgs1_10+0.76817995*hcgs1_11+1.4431945*hcgs1_12+0.28747907*hcgs1_13+0.1555853*hcgs1_14+1.4119282*hcgs1_15+-0.23534247*hcgs1_16+-0.043365728*hcgs1_17+-0.45434403*hcgs1_18+-0.6113481*hcgs1_19+-0.071436815*hcgs1_20+1.336255*hcgs1_21+0.048494633*hcgs1_22+-0.5316062*hcgs1_23+0.1436653*hcgs1_24+0.9528472);\n",
        "hcgs2_7 = tanh(-0.37940532*hcgs1_0+-1.2876422*hcgs1_1+0.13731669*hcgs1_2+0.17509227*hcgs1_3+0.8986726*hcgs1_4+1.0229409*hcgs1_5+-0.69503534*hcgs1_6+0.20359293*hcgs1_7+0.80313087*hcgs1_8+1.0088204*hcgs1_9+-0.06411503*hcgs1_10+-0.23810008*hcgs1_11+-0.22483361*hcgs1_12+-0.7514442*hcgs1_13+-0.022351535*hcgs1_14+-0.029597659*hcgs1_15+-0.81831634*hcgs1_16+0.25210202*hcgs1_17+-1.454332*hcgs1_18+-0.59304476*hcgs1_19+-0.89324987*hcgs1_20+0.32196864*hcgs1_21+0.43833193*hcgs1_22+-0.8081428*hcgs1_23+0.38283762*hcgs1_24+-0.023465121);\n",
        "hcgs2_8 = tanh(0.06450101*hcgs1_0+-0.32645157*hcgs1_1+0.33345243*hcgs1_2+0.12592125*hcgs1_3+0.9819371*hcgs1_4+1.568468*hcgs1_5+0.21460815*hcgs1_6+-1.0372665*hcgs1_7+-0.29271138*hcgs1_8+2.8030427*hcgs1_9+0.028697932*hcgs1_10+-0.57627803*hcgs1_11+-0.063452154*hcgs1_12+-1.590045*hcgs1_13+0.014249704*hcgs1_14+0.045245085*hcgs1_15+0.4989924*hcgs1_16+0.8713646*hcgs1_17+1.1842577*hcgs1_18+-0.008994069*hcgs1_19+-1.1241162*hcgs1_20+0.00919466*hcgs1_21+-1.1247435*hcgs1_22+-0.8297448*hcgs1_23+2.3253732*hcgs1_24+0.1950006);\n",
        "hcgs2_9 = tanh(-0.45197833*hcgs1_0+0.045947105*hcgs1_1+-0.25987926*hcgs1_2+0.5171293*hcgs1_3+-0.1316219*hcgs1_4+0.067848496*hcgs1_5+0.8087117*hcgs1_6+-0.9547865*hcgs1_7+-0.5715785*hcgs1_8+0.32714853*hcgs1_9+0.036904156*hcgs1_10+0.13872579*hcgs1_11+0.12824427*hcgs1_12+0.11992727*hcgs1_13+0.07935975*hcgs1_14+-0.5361036*hcgs1_15+0.53884923*hcgs1_16+-0.039713964*hcgs1_17+0.7091841*hcgs1_18+0.48865792*hcgs1_19+0.18420732*hcgs1_20+0.07967269*hcgs1_21+-0.30234754*hcgs1_22+0.17830539*hcgs1_23+-0.11796092*hcgs1_24+-0.13467003);\n",
        "hcgs2_10 = tanh(-1.6391398*hcgs1_0+-0.34285977*hcgs1_1+-0.3121803*hcgs1_2+0.41148263*hcgs1_3+-0.3336083*hcgs1_4+-1.5920037*hcgs1_5+-0.18015313*hcgs1_6+1.850319*hcgs1_7+0.17927983*hcgs1_8+-2.8341568*hcgs1_9+-0.030394657*hcgs1_10+0.69309235*hcgs1_11+0.2665585*hcgs1_12+2.2477663*hcgs1_13+-0.009332433*hcgs1_14+-0.3326121*hcgs1_15+-2.0456257*hcgs1_16+-0.9294434*hcgs1_17+-1.1023426*hcgs1_18+0.111194305*hcgs1_19+1.141862*hcgs1_20+0.71554065*hcgs1_21+1.1074047*hcgs1_22+0.88629806*hcgs1_23+-2.3181875*hcgs1_24+0.6508097);\n",
        "hcgs2_11 = tanh(-0.24050619*hcgs1_0+-0.41003644*hcgs1_1+-1.2268633*hcgs1_2+-1.3124051*hcgs1_3+-0.1936789*hcgs1_4+-0.87841165*hcgs1_5+-0.020659437*hcgs1_6+-1.9815321*hcgs1_7+-0.087647766*hcgs1_8+-0.86529773*hcgs1_9+-0.1343042*hcgs1_10+-0.13795137*hcgs1_11+-1.2411678*hcgs1_12+-0.5560807*hcgs1_13+0.009826684*hcgs1_14+-1.6039407*hcgs1_15+-0.05220601*hcgs1_16+-0.25667432*hcgs1_17+-0.093644366*hcgs1_18+0.53593457*hcgs1_19+0.87013644*hcgs1_20+-1.2723198*hcgs1_21+0.009109739*hcgs1_22+1.0373011*hcgs1_23+-0.06875387*hcgs1_24+-0.96754915);\n",
        "y_cgs = -0.16241902*hcgs2_0+-0.12451764*hcgs2_1+-0.04949069*hcgs2_2+0.082015745*hcgs2_3+-0.12691256*hcgs2_4+-0.18902193*hcgs2_5+-0.45962262*hcgs2_6+0.07102054*hcgs2_7+-0.59871113*hcgs2_8+0.095522754*hcgs2_9+-0.4854874*hcgs2_10+-0.4512705*hcgs2_11+0.3220441;\n",
        "\n",
        "\n",
        "h1_0 = tanh(0.0067332466*Vgs+0.0014811404*Vds+-0.089835726*Lg+0.09066404);\n",
        "h1_1 = tanh(0.11106648*Vgs+0.40663162*Vds+0.0077667693*Lg+-0.10081332);\n",
        "h1_2 = tanh(1.5560061*Vgs+2.696172*Vds+-0.2839149*Lg+-0.3486124);\n",
        "h1_3 = tanh(-2.0344324*Vgs+-0.28681883*Vds+0.8085338*Lg+0.0026162667);\n",
        "h1_4 = tanh(-0.03341659*Vgs+-0.15408932*Vds+-0.015416802*Lg+0.04370213);\n",
        "h1_5 = tanh(1.4746779*Vgs+0.33599785*Vds+-0.2669822*Lg+-0.42193142);\n",
        "h1_6 = tanh(-0.16598122*Vgs+-2.3765175*Vds+0.11762294*Lg+0.019988127);\n",
        "h1_7 = tanh(4.3143744*Vgs+-0.5703343*Vds+-0.3085829*Lg+-0.50043374);\n",
        "h1_8 = tanh(-0.4936719*Vgs+36.42876*Vds+0.32744452*Lg+0.24776502);\n",
        "h1_9 = tanh(0.4869826*Vgs+-0.13276859*Vds+0.28959593*Lg+-0.3828168);\n",
        "h1_10 = tanh(0.000804813*Vgs+0.0004923456*Vds+-0.41243985*Lg+0.41356155);\n",
        "h1_11 = tanh(2.6073072*Vgs+-0.56419706*Vds+-0.111921*Lg+-0.37480572);\n",
        "h1_12 = tanh(0.0020444687*Vgs+0.0014842966*Vds+0.21013871*Lg+-0.20708004);\n",
        "h1_13 = tanh(-2.6335895*Vgs+0.28439838*Vds+0.321686*Lg+0.41039804);\n",
        "h1_14 = tanh(0.0034599572*Vgs+0.0084134*Vds+-0.034978658*Lg+0.037748303);\n",
        "h1_15 = tanh(0.0025394042*Vgs+0.00083049946*Vds+-0.13070253*Lg+0.13411763);\n",
        "h1_16 = tanh(-2.1379406*Vgs+-0.57477415*Vds+0.19879702*Lg+0.6043196);\n",
        "h1_17 = tanh(-0.9647693*Vgs+3.0132833*Vds+0.13054319*Lg+0.38879853);\n",
        "h1_18 = tanh(-1.6564686*Vgs+-0.06445829*Vds+0.39810786*Lg+0.21570432);\n",
        "h1_19 = tanh(-0.120745726*Vgs+-0.37977916*Vds+-0.20842548*Lg+0.33059505);\n",
        "h1_20 = tanh(-0.5346294*Vgs+-0.24575421*Vds+0.42274392*Lg+-0.095065445);\n",
        "h1_21 = tanh(-0.17100386*Vgs+-0.36405364*Vds+-0.32086778*Lg+-0.45500055);\n",
        "h1_22 = tanh(-0.3456514*Vgs+-0.43013823*Vds+-0.14385213*Lg+0.4731826);\n",
        "h1_23 = tanh(0.28758252*Vgs+-0.55982685*Vds+0.44342747*Lg+-0.34977767);\n",
        "h1_24 = tanh(0.08224837*Vgs+0.35958192*Vds+0.059084654*Lg+-0.1397865);\n",
        "\n",
        "h2_0 = tanh(-0.8091375*h1_0+-0.43552017*h1_1+-0.8155236*h1_2+0.24130166*h1_3+0.14224924*h1_4+-0.62978196*h1_5+0.6101532*h1_6+-0.43199107*h1_7+-2.3122218*h1_8+-0.23163876*h1_9+0.068008065*h1_10+-0.03888623*h1_11+-0.014134661*h1_12+-0.06591678*h1_13+0.022440521*h1_14+-0.27041718*h1_15+0.090823404*h1_16+-0.26772383*h1_17+0.47534344*h1_18+0.73750144*h1_19+0.50188285*h1_20+0.3184862*h1_21+0.3608627*h1_22+0.79056877*h1_23+-0.037502155*h1_24+-0.24878933);\n",
        "h2_1 = tanh(-0.03261362*h1_0+-0.06024928*h1_1+-0.10186445*h1_2+0.018033225*h1_3+0.51713187*h1_4+0.022300791*h1_5+0.015979188*h1_6+0.056999434*h1_7+0.035186633*h1_8+0.25107002*h1_9+-0.012781056*h1_10+-0.3380685*h1_11+0.05043952*h1_12+-0.24957459*h1_13+0.024472937*h1_14+-0.08896992*h1_15+0.053890992*h1_16+-0.0101861*h1_17+-0.031921674*h1_18+0.17910303*h1_19+-0.03255347*h1_20+-0.38824677*h1_21+-0.14763784*h1_22+-0.24868536*h1_23+-0.024086645*h1_24+-0.18488966);\n",
        "h2_2 = tanh(0.12152682*h1_0+0.009792632*h1_1+-0.29032546*h1_2+0.5965219*h1_3+-0.021902572*h1_4+-0.5883061*h1_5+0.17799598*h1_6+-0.92929643*h1_7+0.32916892*h1_8+-0.2568552*h1_9+-0.067421295*h1_10+-1.1449344*h1_11+0.007887112*h1_12+0.276745*h1_13+-0.033592574*h1_14+0.1040227*h1_15+0.23562248*h1_16+-0.18375605*h1_17+1.0453725*h1_18+0.33611628*h1_19+0.6393779*h1_20+0.06726488*h1_21+0.7879763*h1_22+-0.14922209*h1_23+-0.18613423*h1_24+0.28223407);\n",
        "h2_3 = tanh(-0.18859175*h1_0+-0.9337733*h1_1+-0.938285*h1_2+0.7002026*h1_3+0.6061784*h1_4+-0.52529997*h1_5+0.71481675*h1_6+-0.38263503*h1_7+-0.5072332*h1_8+0.1263292*h1_9+0.04664359*h1_10+-0.5296314*h1_11+-0.033964746*h1_12+0.6460025*h1_13+-0.0231704*h1_14+-0.38591123*h1_15+0.0872077*h1_16+-0.5042024*h1_17+0.47931066*h1_18+0.50751704*h1_19+-0.063635096*h1_20+0.018421784*h1_21+0.08422998*h1_22+0.30641475*h1_23+-0.524482*h1_24+-0.26979908);\n",
        "h2_4 = tanh(-0.034289524*h1_0+-0.11348127*h1_1+0.48059276*h1_2+-1.123194*h1_3+0.12442129*h1_4+0.49476302*h1_5+0.0863118*h1_6+0.47754285*h1_7+0.2202004*h1_8+0.41435316*h1_9+0.028558401*h1_10+0.36081842*h1_11+-0.008486773*h1_12+-0.9903806*h1_13+0.015499473*h1_14+-0.063613005*h1_15+-1.4330287*h1_16+-0.74456465*h1_17+-0.5569409*h1_18+-0.019323817*h1_19+-0.14176102*h1_20+0.49098673*h1_21+-0.057230044*h1_22+0.515492*h1_23+-0.13265039*h1_24+-0.18507467);\n",
        "h2_5 = tanh(-0.09933434*h1_0+-0.15035452*h1_1+-0.5133741*h1_2+-0.47027865*h1_3+0.21104498*h1_4+0.14945817*h1_5+0.10640489*h1_6+0.1787739*h1_7+0.060723394*h1_8+-0.5602644*h1_9+-0.0019350805*h1_10+0.5330786*h1_11+0.012645531*h1_12+-0.42403966*h1_13+0.018393166*h1_14+-0.00062784646*h1_15+-0.0052586636*h1_16+1.0866524*h1_17+0.078084014*h1_18+0.059300236*h1_19+0.32314536*h1_20+0.49137965*h1_21+0.09876712*h1_22+-0.583638*h1_23+-0.1759503*h1_24+-0.18313222);\n",
        "h2_6 = tanh(-0.1285073*h1_0+-1.032602*h1_1+-1.224141*h1_2+0.14631094*h1_3+0.65741867*h1_4+-0.22908972*h1_5+0.63241625*h1_6+-0.88521755*h1_7+0.09097795*h1_8+0.09149886*h1_9+0.027507082*h1_10+-0.63233715*h1_11+-0.02061426*h1_12+0.1104531*h1_13+-0.012734477*h1_14+-0.34623924*h1_15+-0.18294835*h1_16+-0.045275174*h1_17+0.35704163*h1_18+0.5909152*h1_19+0.073851526*h1_20+0.49004716*h1_21+0.19812724*h1_22+0.3784807*h1_23+-0.6437184*h1_24+-0.3594689);\n",
        "h2_7 = tanh(0.119550616*h1_0+-0.035170615*h1_1+-0.60996103*h1_2+0.40164798*h1_3+-0.007347775*h1_4+-0.30736676*h1_5+-0.13570982*h1_6+0.9896757*h1_7+-4.0502048*h1_8+0.053264104*h1_9+-0.012164457*h1_10+0.48190302*h1_11+0.014136277*h1_12+-0.5162279*h1_13+0.014227704*h1_14+0.05181294*h1_15+0.7717682*h1_16+-0.111988194*h1_17+0.062060844*h1_18+-0.044897318*h1_19+-0.15565042*h1_20+0.24919312*h1_21+-0.07993477*h1_22+-0.09538691*h1_23+-0.0052977665*h1_24+-0.10968423);\n",
        "h2_8 = tanh(0.079706475*h1_0+-0.5814533*h1_1+0.6996235*h1_2+-0.16522586*h1_3+0.630409*h1_4+0.29656532*h1_5+-0.5331364*h1_6+0.114606544*h1_7+-0.21106683*h1_8+0.0467022*h1_9+-0.06535354*h1_10+-0.24001631*h1_11+0.059368264*h1_12+0.15451497*h1_13+0.053622272*h1_14+0.12981157*h1_15+0.5445277*h1_16+0.24295534*h1_17+-0.26532695*h1_18+0.56733835*h1_19+0.12550347*h1_20+0.048389148*h1_21+0.5622483*h1_22+0.6891362*h1_23+-0.76375955*h1_24+-0.2676767);\n",
        "h2_9 = tanh(-0.03992191*h1_0+0.09928779*h1_1+1.0056654*h1_2+-2.3181243*h1_3+-0.0916434*h1_4+2.143644*h1_5+0.19818373*h1_6+2.8020356*h1_7+-0.47644797*h1_8+0.24747562*h1_9+0.02024143*h1_10+1.3736787*h1_11+0.006931587*h1_12+-3.888161*h1_13+0.018778825*h1_14+-0.0041158814*h1_15+-1.5533967*h1_16+-0.71390355*h1_17+-2.8108795*h1_18+-0.24579133*h1_19+-1.128639*h1_20+0.2231992*h1_21+-1.2298777*h1_22+0.043120787*h1_23+0.403965*h1_24+-0.17281148);\n",
        "h2_10 = tanh(0.35093632*h1_0+-0.05374117*h1_1+0.16745348*h1_2+-0.19781992*h1_3+-0.07950959*h1_4+0.29579905*h1_5+-0.5073861*h1_6+-0.14164843*h1_7+0.70651436*h1_8+0.29345548*h1_9+-0.054172043*h1_10+-0.28257352*h1_11+0.033599235*h1_12+0.017732412*h1_13+0.02154612*h1_14+0.1586723*h1_15+-0.23733242*h1_16+0.43727452*h1_17+-0.084653825*h1_18+-0.054423757*h1_19+-0.36601782*h1_20+-0.07235579*h1_21+-0.03668117*h1_22+-0.05778476*h1_23+-0.020711297*h1_24+0.24233255);\n",
        "h2_11 = tanh(0.009613743*h1_0+-0.26873523*h1_1+-0.0937895*h1_2+-0.17233954*h1_3+0.2600588*h1_4+-0.41362444*h1_5+-0.13627699*h1_6+-0.03051018*h1_7+0.028052136*h1_8+0.06536644*h1_9+-0.029295161*h1_10+-0.017168272*h1_11+0.016685432*h1_12+-0.15576635*h1_13+0.08295777*h1_14+-0.16339928*h1_15+-0.069266655*h1_16+-0.07842072*h1_17+0.22933975*h1_18+-0.07057512*h1_19+0.172754*h1_20+-0.21173981*h1_21+-0.8436922*h1_22+0.28658*h1_23+-0.018899215*h1_24+-0.11145156);\n",
        "y = 0.13345447*h2_0+0.0023934222*h2_1+0.1793076*h2_2+0.51211137*h2_3+0.2699503*h2_4+0.11368197*h2_5+-0.5013421*h2_6+-0.61606425*h2_7+-0.037382632*h2_8+0.18645377*h2_9+0.32913932*h2_10+-0.00013747951*h2_11+-0.1796401;\n",
        "\n",
        "Cgd = pow(10, (y_cgd/normO_cgd + MinO_cgd))*W/15; //traning width was 15um\n",
        "Cgs = pow(10, (y_cgs/normO_cgs + MinO_cgs))*W/15;\n",
        "Cgg = Cgd+Cgs;\n",
        "\n",
        "Id = pow(10, (y/normI + MinI))*W*3;\n",
        "I(g, d) <+ Cgd*ddt(Vg-Vd) ;\n",
        "I(g, s) <+ Cgs*ddt(Vg-Vs) ;\n",
        "\n",
        "if (Vd >= Vs) begin\n",
        "\tI(d, s) <+ dir*Id;\n",
        "end\n",
        "\n",
        "else begin\n",
        "\tI(d, s) <+ dir*Id;\n",
        "end\n",
        "\n",
        "end\n",
        "endmodule\n",
        "\n",
        "\"\"\"\n",
        "print(verilog_code)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xiik2dIbnGVG",
        "outputId": "f62e934d-c3ed-431d-9652-28d115d4626a"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "// VerilogA for ML_HV, ML_HV_emode, veriloga\n",
            "\n",
            "`include \"constants.vams\"\n",
            "`include \"disciplines.vams\"\n",
            "\n",
            "module ML_HV_emode_rev3(d, g, s);\n",
            "\n",
            "\n",
            "inout d, g, s;\n",
            "electrical d, g, s;\n",
            "\n",
            "//****** Parameters L and W ********\n",
            "parameter real W = 3;\n",
            "parameter real L = 0.6;\n",
            "parameter MinVg = -6.0 ;\n",
            "parameter normVg = 0.05555555555555555 ;\n",
            "parameter MinVd = 0.0 ;\n",
            "parameter normVd = 0.08333333333333333 ;\n",
            "parameter MinLg = 0 ;\n",
            "parameter normLg = 1.66666667 ;\n",
            "parameter MinI = -14.614393726401689 ;\n",
            "parameter normI = 0.0975145951626748;\n",
            "parameter vth =0;\n",
            "parameter real T_stress = 1; //set on cadence as variable\n",
            "parameter real Temp = 25; //set on cadence as variable\n",
            "\n",
            "parameter MinVg_cgs = -6.0 ;\n",
            "parameter normVg_cgs = 0.05555555555555555 ;\n",
            "parameter MinVd_cgs = -6.0 ;\n",
            "parameter normVd_cgs = 0.05555555555555555 ;\n",
            "parameter MinLg_cgs = 0.6 ;\n",
            "parameter normLg_cgs = 0 ;\n",
            "parameter MinO_cgs = -13.164309428507574 ;\n",
            "parameter normO_cgs =2.3582977043250914;\n",
            "\n",
            "parameter MinVg_cgd = -6.0 ;\n",
            "parameter normVg_cgd = 0.05555555555555555 ;\n",
            "parameter MinVd_cgd = -4.0 ;\n",
            "parameter normVd_cgd = 0.06349206349206349 ;\n",
            "parameter MinLg_cgd = 0.6 ;\n",
            "parameter normLg_cgd = 0 ;\n",
            "parameter MinO_cgd = -14.010995384301463 ;\n",
            "parameter normO_cgd =1.5234203322209112;\n",
            "\n",
            "parameter Mint_stress = 1.0 ;\n",
            "parameter normt_stress = 0.001001001001001001 ;\n",
            "parameter Mintemperature = 25 ;\n",
            "parameter normtemperature = 0.016666666666666666 ;\n",
            "parameter Mindelta_Vth = 0.00614 ;\n",
            "parameter normdelta_Vth = 0.021388891 ;\n",
            "\n",
            "real hvth1_0, hvth1_1, hvth1_2, hvth1_3, hvth1_4, hvth1_5, hvth1_6, hvth1_7, hvth1_8, hvth1_9, hvth1_10, hvth1_11, hvth1_12, hvth1_13, hvth1_14, hvth1_15;\n",
            "real hvth2_0, hvth2_1, hvth2_2, hvth2_3, hvth2_4, hvth2_5, hvth2_6, hvth2_7;\n",
            "\n",
            "\n",
            "real Vg, Vd, Vs, Vgs, Vds, Lg, Id, Cgg, Cgs,Cgd, Vgd;\n",
            "real y, y_cgg, y_cgs, y_cgd, yvth;\n",
            "real Vgs_cgg, Vgs_cgs, Vgs_cgd;\n",
            "real Vds_cgg, Vds_cgs, Vds_cgd;\n",
            "real Lg_cgg, Lg_cgs, Lg_cgd;\n",
            "real Vgsraw, Vgdraw, dir;\n",
            "real Vdi, Vsi;\n",
            "real t_stress, v_ov, t_rec, clk_loops, temp, delta_Vth;\n",
            "\n",
            "\n",
            "real h1_0, h1_1, h1_2, h1_3, h1_4, h1_5, h1_6, h1_7, h1_8, h1_9, h1_10, h1_11, h1_12, h1_13, h1_14, h1_15, h1_16, h1_17, h1_18, h1_19, h1_20, h1_21, h1_22, h1_23, h1_24;\n",
            "real h2_0, h2_1, h2_2, h2_3, h2_4, h2_5, h2_6, h2_7, h2_8, h2_9, h2_10, h2_11;\n",
            "real hcgg1_0, hcgg1_1, hcgg1_2, hcgg1_3, hcgg1_4, hcgg1_5, hcgg1_6, hcgg1_7, hcgg1_8, hcgg1_9, hcgg1_10, hcgg1_11, hcgg1_12, hcgg1_13, hcgg1_14, hcgg1_15, hcgg1_16, hcgg1_17, hcgg1_18, hcgg1_19, hcgg1_20, hcgg1_21, hcgg1_22, hcgg1_23, hcgg1_24;\n",
            "real hcgg2_0, hcgg2_1, hcgg2_2, hcgg2_3, hcgg2_4, hcgg2_5, hcgg2_6, hcgg2_7, hcgg2_8, hcgg2_9, hcgg2_10, hcgg2_11;\n",
            "real hcgs1_0, hcgs1_1, hcgs1_2, hcgs1_3, hcgs1_4, hcgs1_5, hcgs1_6, hcgs1_7, hcgs1_8, hcgs1_9, hcgs1_10, hcgs1_11, hcgs1_12, hcgs1_13, hcgs1_14, hcgs1_15, hcgs1_16, hcgs1_17, hcgs1_18, hcgs1_19, hcgs1_20, hcgs1_21, hcgs1_22, hcgs1_23, hcgs1_24;\n",
            "real hcgs2_0, hcgs2_1, hcgs2_2, hcgs2_3, hcgs2_4, hcgs2_5, hcgs2_6, hcgs2_7, hcgs2_8, hcgs2_9, hcgs2_10, hcgs2_11;\n",
            "real hcgd1_0, hcgd1_1, hcgd1_2, hcgd1_3, hcgd1_4, hcgd1_5, hcgd1_6, hcgd1_7, hcgd1_8, hcgd1_9, hcgd1_10, hcgd1_11, hcgd1_12, hcgd1_13, hcgd1_14, hcgd1_15, hcgd1_16, hcgd1_17, hcgd1_18, hcgd1_19, hcgd1_20, hcgd1_21, hcgd1_22, hcgd1_23, hcgd1_24;\n",
            "real hcgd2_0, hcgd2_1, hcgd2_2, hcgd2_3, hcgd2_4, hcgd2_5, hcgd2_6, hcgd2_7, hcgd2_8, hcgd2_9, hcgd2_10, hcgd2_11;\n",
            "\n",
            "real Qgd, Qgs;\n",
            "\n",
            "\n",
            "analog begin\n",
            "\n",
            "temp = (Temp - Mintemperature)*normtemperature;\n",
            "t_stress = (T_stress - Mint_stress)*normt_stress;\n",
            "\n",
            "//******************** delta_Vth NN **********************************//\n",
            "\n",
            "hvth1_0 = tanh(4.4002986*t_stress+0.13983922*temp+0.0095059415);\n",
            "hvth1_1 = tanh(-0.731448*t_stress+0.14707367*temp+-0.23541754);\n",
            "hvth1_2 = tanh(0.18999684*t_stress+0.120208435*temp+-0.09435243);\n",
            "hvth1_3 = tanh(3.3266392*t_stress+-0.1338074*temp+0.0023328918);\n",
            "hvth1_4 = tanh(-0.7420221*t_stress+-0.26855975*temp+-0.095105216);\n",
            "hvth1_5 = tanh(-0.40053654*t_stress+0.25632176*temp+0.06787888);\n",
            "hvth1_6 = tanh(0.5816374*t_stress+-0.23239218*temp+0.02243891);\n",
            "hvth1_7 = tanh(0.36332148*t_stress+-0.06948452*temp+-0.004198026);\n",
            "hvth1_8 = tanh(0.6795921*t_stress+-0.32956377*temp+0.18402661);\n",
            "hvth1_9 = tanh(0.45967075*t_stress+-0.30783534*temp+0.013147709);\n",
            "hvth1_10 = tanh(0.10367114*t_stress+-0.10897806*temp+0.012378313);\n",
            "hvth1_11 = tanh(0.5241847*t_stress+0.22551975*temp+-0.008120251);\n",
            "hvth1_12 = tanh(-1.3924043*t_stress+0.02966609*temp+-0.04992909);\n",
            "hvth1_13 = tanh(-0.1829997*t_stress+0.30073678*temp+0.015427229);\n",
            "hvth1_14 = tanh(-0.8810281*t_stress+-0.73506755*temp+-0.034594454);\n",
            "hvth1_15 = tanh(-2.0657573*t_stress+0.05542216*temp+-0.036831368);\n",
            "\n",
            "hvth2_0 = tanh(0.14449877*hvth1_0+0.3203474*hvth1_1+-0.32588643*hvth1_2+0.45089495*hvth1_3+0.47596216*hvth1_4+-0.35819858*hvth1_5+0.2326565*hvth1_6+-0.36326265*hvth1_7+-0.48811617*hvth1_8+-0.30419973*hvth1_9+0.33879533*hvth1_10+-0.03204754*hvth1_11+0.42893898*hvth1_12+-0.2426072*hvth1_13+0.020931425*hvth1_14+-0.009312627*hvth1_15+-0.07166866);\n",
            "hvth2_1 = tanh(0.13595577*hvth1_0+0.16891788*hvth1_1+-0.49537298*hvth1_2+0.08869149*hvth1_3+-0.18422693*hvth1_4+-0.11253915*hvth1_5+0.0444903*hvth1_6+-0.28849855*hvth1_7+0.2552169*hvth1_8+-0.5797979*hvth1_9+0.23429447*hvth1_10+-0.34624717*hvth1_11+0.4789288*hvth1_12+0.2960094*hvth1_13+-0.026964663*hvth1_14+-0.19546272*hvth1_15+0.051302083);\n",
            "hvth2_2 = tanh(0.4781589*hvth1_0+-0.39161503*hvth1_1+-0.26649016*hvth1_2+-0.61682725*hvth1_3+-0.23353283*hvth1_4+-0.10713724*hvth1_5+-0.2666957*hvth1_6+-0.15914343*hvth1_7+0.11189942*hvth1_8+-0.35434946*hvth1_9+0.4025544*hvth1_10+-0.26625735*hvth1_11+0.10293101*hvth1_12+-0.10723214*hvth1_13+0.25552425*hvth1_14+-0.109538235*hvth1_15+0.06242121);\n",
            "hvth2_3 = tanh(-0.25318062*hvth1_0+0.373996*hvth1_1+0.21388936*hvth1_2+-0.36720476*hvth1_3+-0.4728468*hvth1_4+0.5544033*hvth1_5+-0.059715677*hvth1_6+0.16308662*hvth1_7+-0.7545961*hvth1_8+-0.12749965*hvth1_9+-0.20710278*hvth1_10+0.34646377*hvth1_11+0.04571012*hvth1_12+0.12754895*hvth1_13+0.26191953*hvth1_14+0.025788136*hvth1_15+0.0020722284);\n",
            "hvth2_4 = tanh(-0.09721173*hvth1_0+-0.24874628*hvth1_1+0.38726363*hvth1_2+-0.050579343*hvth1_3+0.08149567*hvth1_4+0.3621912*hvth1_5+0.4242709*hvth1_6+0.32088947*hvth1_7+0.3525533*hvth1_8+-0.21818145*hvth1_9+-0.1545736*hvth1_10+0.206645*hvth1_11+-0.3135045*hvth1_12+-0.33971882*hvth1_13+0.10592691*hvth1_14+0.14754814*hvth1_15+0.11487955);\n",
            "hvth2_5 = tanh(-0.26740152*hvth1_0+-0.6982515*hvth1_1+-0.08474242*hvth1_2+0.019018041*hvth1_3+-0.3589899*hvth1_4+-0.0973528*hvth1_5+0.11607169*hvth1_6+0.19930331*hvth1_7+0.36271593*hvth1_8+-0.26375148*hvth1_9+-0.015975302*hvth1_10+0.29052237*hvth1_11+-0.47726715*hvth1_12+0.23121832*hvth1_13+0.27116385*hvth1_14+0.261655*hvth1_15+0.04005981);\n",
            "hvth2_6 = tanh(-0.05076342*hvth1_0+0.10022507*hvth1_1+0.0138145285*hvth1_2+-0.081644274*hvth1_3+-0.1592732*hvth1_4+0.11312352*hvth1_5+-0.34587634*hvth1_6+0.19398122*hvth1_7+-0.22699691*hvth1_8+-0.52661735*hvth1_9+-0.29047686*hvth1_10+-0.572356*hvth1_11+0.3649812*hvth1_12+0.4685821*hvth1_13+0.26052803*hvth1_14+0.0049602045*hvth1_15+0.0067401035);\n",
            "hvth2_7 = tanh(-2.302981*hvth1_0+0.88205665*hvth1_1+-0.014244103*hvth1_2+-2.7229753*hvth1_3+0.8704846*hvth1_4+0.20269625*hvth1_5+-0.5046953*hvth1_6+-0.23645547*hvth1_7+-0.9837086*hvth1_8+-0.3482821*hvth1_9+-0.20679511*hvth1_10+-0.60126776*hvth1_11+1.4770023*hvth1_12+0.060563616*hvth1_13+1.0323116*hvth1_14+2.2257605*hvth1_15+-0.050422482);\n",
            "\n",
            "yvth = 0.4027771*hvth2_0+-0.6521628*hvth2_1+-0.36696523*hvth2_2+0.23569995*hvth2_3+-0.7517589*hvth2_4+0.17004673*hvth2_5+-0.60027295*hvth2_6+-0.93044275*hvth2_7+-0.069171295;\n",
            "\n",
            "delta_Vth = (yvth - Mindelta_Vth) * normdelta_Vth;\n",
            "$strobe(\"dvth=$g\",delta_Vth);\n",
            "\n",
            "\n",
            "\tVg = V(g);\n",
            "\tVs = V(s);\n",
            "\tVd = V(d);\n",
            "    Vgsraw = Vg-Vs ;\n",
            "    Vgdraw = Vg-Vd ;\n",
            "if (Vgsraw>=Vgdraw) begin\n",
            "\tVgs = ((Vg-Vs) - MinVg - vth - delta_Vth) * normVg ;\n",
            "\n",
            "\tVgs_cgs = ((Vg-Vs) - MinVg_cgs - vth - delta_Vth) * normVg_cgs ;\n",
            "\tVgs_cgd = ((Vg-Vs) - MinVg_cgd - vth - delta_Vth) * normVg_cgd ;\n",
            "\tdir = 1 ;\n",
            "end\n",
            "else begin\n",
            "\tVgs = ((Vg-Vd) - MinVg - vth) * normVg ;\n",
            "\n",
            "\tVgs_cgs = ((Vg-Vd) - MinVg_cgs - vth - delta_Vth) * normVg_cgs ;\n",
            "\tVgs_cgd = ((Vg-Vd) - MinVg_cgd - vth - delta_Vth) * normVg_cgd ;\n",
            "\tdir = -1 ;\n",
            "end\n",
            "\n",
            "\tVds = (abs(Vd-Vs) - MinVd) * normVd ;\n",
            "\n",
            "\tVds_cgs = (abs(Vd-Vs) - MinVd_cgs) * normVd_cgs ;\n",
            "\tVds_cgd = (abs(Vd-Vs) - MinVd_cgd) * normVd_cgd ;\n",
            "\n",
            "\tLg = (L -MinLg)*normLg ;\n",
            "\n",
            "\tLg_cgs = (L -MinLg_cgs)*normLg_cgs ;\n",
            "\tLg_cgd = (L -MinLg_cgd)*normLg_cgd ;\n",
            "\n",
            "hcgd1_0 = tanh(-0.073090345*Vgs_cgd+1.7493699*Vds_cgd+-0.25351274*Lg_cgd+-0.30203047);\n",
            "hcgd1_1 = tanh(-0.81192595*Vgs_cgd+0.7702285*Vds_cgd+0.32290834*Lg_cgd+0.12260828);\n",
            "hcgd1_2 = tanh(-1.4992723*Vgs_cgd+1.0054874*Vds_cgd+-0.040279582*Lg_cgd+0.23167521);\n",
            "hcgd1_3 = tanh(-0.27198792*Vgs_cgd+0.14240761*Vds_cgd+-0.45070043*Lg_cgd+-0.47009295);\n",
            "hcgd1_4 = tanh(0.5909458*Vgs_cgd+0.1271886*Vds_cgd+0.25604323*Lg_cgd+-0.3614964);\n",
            "hcgd1_5 = tanh(1.1455232*Vgs_cgd+-0.1336111*Vds_cgd+0.16404216*Lg_cgd+-0.3844855);\n",
            "hcgd1_6 = tanh(0.14468843*Vgs_cgd+2.6229973*Vds_cgd+-0.39613384*Lg_cgd+0.23941508);\n",
            "hcgd1_7 = tanh(-0.20886186*Vgs_cgd+3.2628205*Vds_cgd+0.132442*Lg_cgd+-0.49246147);\n",
            "hcgd1_8 = tanh(0.48230776*Vgs_cgd+0.20745495*Vds_cgd+0.17956811*Lg_cgd+0.13416706);\n",
            "hcgd1_9 = tanh(0.11828926*Vgs_cgd+-0.36482674*Vds_cgd+0.38881582*Lg_cgd+0.090164326);\n",
            "hcgd1_10 = tanh(0.013121906*Vgs_cgd+2.042608*Vds_cgd+0.15619394*Lg_cgd+-0.17228433);\n",
            "hcgd1_11 = tanh(0.85692364*Vgs_cgd+1.0132248*Vds_cgd+0.20936537*Lg_cgd+-1.0232097);\n",
            "hcgd1_12 = tanh(-0.07165884*Vgs_cgd+-0.3623631*Vds_cgd+0.14471789*Lg_cgd+0.55825126);\n",
            "hcgd1_13 = tanh(-0.45831093*Vgs_cgd+-0.27587467*Vds_cgd+0.087761946*Lg_cgd+0.24322145);\n",
            "hcgd1_14 = tanh(-0.12094344*Vgs_cgd+-1.3313415*Vds_cgd+0.3058515*Lg_cgd+0.38591075);\n",
            "hcgd1_15 = tanh(-0.48209256*Vgs_cgd+4.390121*Vds_cgd+-0.22924025*Lg_cgd+-0.22238722);\n",
            "hcgd1_16 = tanh(-0.13342527*Vgs_cgd+-0.17032403*Vds_cgd+-0.19245708*Lg_cgd+-0.34511408);\n",
            "hcgd1_17 = tanh(-0.019135991*Vgs_cgd+1.6390436*Vds_cgd+-0.39458174*Lg_cgd+-0.374234);\n",
            "hcgd1_18 = tanh(1.9993974*Vgs_cgd+-0.6025967*Vds_cgd+0.051945195*Lg_cgd+-0.2606078);\n",
            "hcgd1_19 = tanh(-1.319852*Vgs_cgd+-1.4124283*Vds_cgd+-0.16422051*Lg_cgd+0.60615516);\n",
            "hcgd1_20 = tanh(0.7590748*Vgs_cgd+0.35662842*Vds_cgd+-0.32554984*Lg_cgd+-0.45226586);\n",
            "hcgd1_21 = tanh(-0.0644961*Vgs_cgd+-1.4253865*Vds_cgd+0.31603792*Lg_cgd+0.34580982);\n",
            "hcgd1_22 = tanh(-0.024503596*Vgs_cgd+-1.2409326*Vds_cgd+0.266246*Lg_cgd+0.05068574);\n",
            "hcgd1_23 = tanh(0.40497908*Vgs_cgd+-0.5119794*Vds_cgd+0.27652997*Lg_cgd+-0.49364495);\n",
            "hcgd1_24 = tanh(-0.66816944*Vgs_cgd+0.23831755*Vds_cgd+0.3236202*Lg_cgd+0.45229998);\n",
            "hcgd2_0 = tanh(-0.8581911*hcgd1_0+-1.7806648*hcgd1_1+-2.3480477*hcgd1_2+-0.060092464*hcgd1_3+1.0001435*hcgd1_4+1.0480509*hcgd1_5+-0.3004585*hcgd1_6+-0.98489136*hcgd1_7+0.15593737*hcgd1_8+0.70743585*hcgd1_9+-0.24457291*hcgd1_10+0.9940044*hcgd1_11+0.23668307*hcgd1_12+-0.5144086*hcgd1_13+0.16627982*hcgd1_14+-1.4144597*hcgd1_15+-0.11155476*hcgd1_16+-0.3225649*hcgd1_17+1.4172044*hcgd1_18+0.61510265*hcgd1_19+0.8610263*hcgd1_20+-0.044497408*hcgd1_21+0.46571997*hcgd1_22+-0.013894911*hcgd1_23+-1.1907585*hcgd1_24+0.14715073);\n",
            "hcgd2_1 = tanh(0.74444413*hcgd1_0+0.60665065*hcgd1_1+0.396881*hcgd1_2+-0.0940794*hcgd1_3+-0.27899864*hcgd1_4+-0.087556325*hcgd1_5+0.40643474*hcgd1_6+1.366464*hcgd1_7+-0.17410953*hcgd1_8+-0.012924953*hcgd1_9+0.56350505*hcgd1_10+0.36612725*hcgd1_11+-0.26115453*hcgd1_12+-0.29767877*hcgd1_13+-0.4200406*hcgd1_14+1.1142844*hcgd1_15+0.19850636*hcgd1_16+0.8344988*hcgd1_17+0.11088754*hcgd1_18+-0.26612458*hcgd1_19+-0.27794132*hcgd1_20+-0.80967337*hcgd1_21+-0.11269937*hcgd1_22+-0.23162273*hcgd1_23+-0.098596975*hcgd1_24+0.09286805);\n",
            "hcgd2_2 = tanh(-0.7313956*hcgd1_0+-0.7514108*hcgd1_1+-0.5736202*hcgd1_2+-0.49893183*hcgd1_3+0.1765418*hcgd1_4+0.14255758*hcgd1_5+0.07957998*hcgd1_6+-0.66936827*hcgd1_7+0.2132876*hcgd1_8+0.13260117*hcgd1_9+-0.22507648*hcgd1_10+0.069224365*hcgd1_11+-0.031068536*hcgd1_12+0.0057172063*hcgd1_13+0.9005941*hcgd1_14+0.045649208*hcgd1_15+-0.4302063*hcgd1_16+-1.064374*hcgd1_17+0.100322045*hcgd1_18+0.1628234*hcgd1_19+-0.17792495*hcgd1_20+0.73057646*hcgd1_21+-0.04239934*hcgd1_22+0.07292476*hcgd1_23+-0.03253051*hcgd1_24+-0.069609255);\n",
            "hcgd2_3 = tanh(0.14440651*hcgd1_0+0.3216678*hcgd1_1+0.51213056*hcgd1_2+0.42667162*hcgd1_3+-0.610052*hcgd1_4+-0.54205084*hcgd1_5+0.060548224*hcgd1_6+0.49424285*hcgd1_7+-0.5282787*hcgd1_8+0.046284102*hcgd1_9+-0.0018920621*hcgd1_10+-0.4451091*hcgd1_11+0.04757394*hcgd1_12+0.5863736*hcgd1_13+0.15133414*hcgd1_14+0.3255952*hcgd1_15+0.27865914*hcgd1_16+0.42173597*hcgd1_17+-0.6422138*hcgd1_18+0.1657985*hcgd1_19+-1.5046545*hcgd1_20+0.5070087*hcgd1_21+0.18432228*hcgd1_22+0.14772636*hcgd1_23+-0.35788894*hcgd1_24+-0.31562403);\n",
            "hcgd2_4 = tanh(0.70184183*hcgd1_0+0.5248185*hcgd1_1+0.21824042*hcgd1_2+-0.1301908*hcgd1_3+0.29407513*hcgd1_4+0.17626354*hcgd1_5+1.0718364*hcgd1_6+1.0510215*hcgd1_7+0.696749*hcgd1_8+0.29669785*hcgd1_9+2.1009989*hcgd1_10+0.6151884*hcgd1_11+0.48851284*hcgd1_12+-0.56238925*hcgd1_13+-0.47341812*hcgd1_14+1.2261282*hcgd1_15+-0.35043222*hcgd1_16+0.12345494*hcgd1_17+0.106367536*hcgd1_18+-1.5799632*hcgd1_19+0.70326567*hcgd1_20+-1.4121299*hcgd1_21+-2.5126958*hcgd1_22+-0.34464827*hcgd1_23+0.2420689*hcgd1_24+0.3827352);\n",
            "hcgd2_5 = tanh(0.07186469*hcgd1_0+-0.3057672*hcgd1_1+0.18167095*hcgd1_2+0.24679431*hcgd1_3+0.2861313*hcgd1_4+0.20745918*hcgd1_5+0.28961903*hcgd1_6+0.4754423*hcgd1_7+0.10851595*hcgd1_8+-0.57396716*hcgd1_9+0.19753388*hcgd1_10+0.25443172*hcgd1_11+-0.058893647*hcgd1_12+0.4978357*hcgd1_13+0.40544015*hcgd1_14+0.19733265*hcgd1_15+0.024174226*hcgd1_16+0.09329063*hcgd1_17+0.12140121*hcgd1_18+0.6866702*hcgd1_19+-0.090300076*hcgd1_20+0.4295404*hcgd1_21+0.21769053*hcgd1_22+0.06967293*hcgd1_23+0.13865103*hcgd1_24+0.0039349254);\n",
            "hcgd2_6 = tanh(-0.13215303*hcgd1_0+-0.25777414*hcgd1_1+-0.24420205*hcgd1_2+0.15015651*hcgd1_3+0.7435888*hcgd1_4+0.5808469*hcgd1_5+0.23431543*hcgd1_6+-0.17212716*hcgd1_7+-0.08636741*hcgd1_8+-0.16209634*hcgd1_9+-0.24451429*hcgd1_10+0.64819676*hcgd1_11+-0.401811*hcgd1_12+-0.041673224*hcgd1_13+-0.1843273*hcgd1_14+-0.03913859*hcgd1_15+0.4648355*hcgd1_16+0.33676878*hcgd1_17+0.16188923*hcgd1_18+0.09970059*hcgd1_19+0.26570147*hcgd1_20+-0.34857365*hcgd1_21+-0.20715532*hcgd1_22+0.45225534*hcgd1_23+-0.67448044*hcgd1_24+-0.20020215);\n",
            "hcgd2_7 = tanh(0.5814423*hcgd1_0+0.30674064*hcgd1_1+0.41730982*hcgd1_2+0.1271082*hcgd1_3+0.46041217*hcgd1_4+-0.14026274*hcgd1_5+0.38002715*hcgd1_6+0.15430027*hcgd1_7+0.37405762*hcgd1_8+-0.27523136*hcgd1_9+0.5767433*hcgd1_10+0.75470895*hcgd1_11+0.39034903*hcgd1_12+-0.08490812*hcgd1_13+-0.46258885*hcgd1_14+0.52702457*hcgd1_15+-0.36667594*hcgd1_16+-0.008625877*hcgd1_17+0.09137123*hcgd1_18+-0.7821849*hcgd1_19+0.70662326*hcgd1_20+-0.20326899*hcgd1_21+-0.11657497*hcgd1_22+-0.22169054*hcgd1_23+0.23076789*hcgd1_24+0.1994013);\n",
            "hcgd2_8 = tanh(-7.4423213*hcgd1_0+-1.7918187*hcgd1_1+-2.273679*hcgd1_2+-0.35889605*hcgd1_3+-0.252701*hcgd1_4+0.9470301*hcgd1_5+-0.41792682*hcgd1_6+-7.6462445*hcgd1_7+0.018614734*hcgd1_8+1.2025093*hcgd1_9+-4.4235744*hcgd1_10+-1.8856794*hcgd1_11+0.31407484*hcgd1_12+0.08381061*hcgd1_13+2.0082252*hcgd1_14+-4.4953437*hcgd1_15+0.18078373*hcgd1_16+-2.2936556*hcgd1_17+2.0334883*hcgd1_18+-0.34214598*hcgd1_19+-0.29436678*hcgd1_20+4.8436127*hcgd1_21+1.8703012*hcgd1_22+0.32469234*hcgd1_23+-0.12540866*hcgd1_24+0.1331769);\n",
            "hcgd2_9 = tanh(-0.008969171*hcgd1_0+0.25442964*hcgd1_1+0.33559665*hcgd1_2+0.059060458*hcgd1_3+-0.6700384*hcgd1_4+-0.595369*hcgd1_5+0.432537*hcgd1_6+0.48487276*hcgd1_7+-0.014844808*hcgd1_8+-0.029969316*hcgd1_9+0.4608879*hcgd1_10+-0.67345583*hcgd1_11+0.10385167*hcgd1_12+0.47509405*hcgd1_13+0.057980414*hcgd1_14+0.57206994*hcgd1_15+-0.083018914*hcgd1_16+0.24574327*hcgd1_17+-0.76715064*hcgd1_18+0.2543998*hcgd1_19+-0.210734*hcgd1_20+-0.17915764*hcgd1_21+-0.17944436*hcgd1_22+-0.17101662*hcgd1_23+0.27301717*hcgd1_24+-0.020215586);\n",
            "hcgd2_10 = tanh(0.047154456*hcgd1_0+0.177622*hcgd1_1+-0.35302344*hcgd1_2+0.3784709*hcgd1_3+-0.21668567*hcgd1_4+-0.20013903*hcgd1_5+-0.4485513*hcgd1_6+-0.5680783*hcgd1_7+-0.25446296*hcgd1_8+0.0111656105*hcgd1_9+-0.4455899*hcgd1_10+-0.20770787*hcgd1_11+-0.27411363*hcgd1_12+0.6049824*hcgd1_13+0.13715369*hcgd1_14+-0.5386368*hcgd1_15+0.46778303*hcgd1_16+-0.21815602*hcgd1_17+0.47233215*hcgd1_18+0.5560222*hcgd1_19+-0.30228803*hcgd1_20+-0.057146665*hcgd1_21+0.446087*hcgd1_22+0.40810114*hcgd1_23+0.44648725*hcgd1_24+-0.06979678);\n",
            "hcgd2_11 = tanh(0.52387154*hcgd1_0+-0.48277956*hcgd1_1+-0.7682235*hcgd1_2+-0.190172*hcgd1_3+1.175332*hcgd1_4+1.1926033*hcgd1_5+-0.36767244*hcgd1_6+0.3228836*hcgd1_7+0.295783*hcgd1_8+-0.20389546*hcgd1_9+0.11585899*hcgd1_10+1.6549429*hcgd1_11+-0.3883799*hcgd1_12+-0.54141515*hcgd1_13+-0.30675817*hcgd1_14+-0.34997156*hcgd1_15+0.2086648*hcgd1_16+0.36170647*hcgd1_17+1.6473606*hcgd1_18+-0.4781146*hcgd1_19+1.5642655*hcgd1_20+-0.7633239*hcgd1_21+-0.47981572*hcgd1_22+0.41335562*hcgd1_23+-0.3726435*hcgd1_24+-0.0653709);\n",
            "y_cgd = -0.30169475*hcgd2_0+0.7017093*hcgd2_1+-0.18068637*hcgd2_2+-0.6699334*hcgd2_3+0.53903127*hcgd2_4+0.16665317*hcgd2_5+-0.3536876*hcgd2_6+-0.36382276*hcgd2_7+0.6040847*hcgd2_8+-0.43945295*hcgd2_9+0.5195485*hcgd2_10+-0.3194021*hcgd2_11+0.21705215;\n",
            "\n",
            "hcgs1_0 = tanh(-0.9447249*Vgs_cgs+3.848607*Vds_cgs+-0.019748846*Lg_cgs+-1.5617282);\n",
            "hcgs1_1 = tanh(-0.029012304*Vgs_cgs+-1.5533222*Vds_cgs+0.44389084*Lg_cgs+1.2160019);\n",
            "hcgs1_2 = tanh(0.8694287*Vgs_cgs+-0.03134334*Vds_cgs+-0.052811515*Lg_cgs+-0.4674938);\n",
            "hcgs1_3 = tanh(0.11125248*Vgs_cgs+-0.08053268*Vds_cgs+-0.14715484*Lg_cgs+0.2079814);\n",
            "hcgs1_4 = tanh(0.9503431*Vgs_cgs+1.2066041*Vds_cgs+0.07638908*Lg_cgs+-1.5212817);\n",
            "hcgs1_5 = tanh(1.2675861*Vgs_cgs+-0.20858626*Vds_cgs+-0.40374485*Lg_cgs+-0.6321944);\n",
            "hcgs1_6 = tanh(0.32068986*Vgs_cgs+-0.55139714*Vds_cgs+0.29643092*Lg_cgs+0.09015624);\n",
            "hcgs1_7 = tanh(-3.6914701*Vgs_cgs+0.6843926*Vds_cgs+0.14611645*Lg_cgs+1.1698359);\n",
            "hcgs1_8 = tanh(-0.51712066*Vgs_cgs+0.8325061*Vds_cgs+0.2879668*Lg_cgs+-0.07329451);\n",
            "hcgs1_9 = tanh(1.7683635*Vgs_cgs+-0.3458703*Vds_cgs+-0.16947505*Lg_cgs+-0.80866075);\n",
            "hcgs1_10 = tanh(0.010235257*Vgs_cgs+-0.009958806*Vds_cgs+-0.046799034*Lg_cgs+0.0067180935);\n",
            "hcgs1_11 = tanh(-0.56219107*Vgs_cgs+0.14688006*Vds_cgs+0.20843746*Lg_cgs+0.26059148);\n",
            "hcgs1_12 = tanh(-0.07906894*Vgs_cgs+-0.10666679*Vds_cgs+-0.27857378*Lg_cgs+0.30169338);\n",
            "hcgs1_13 = tanh(-1.6767093*Vgs_cgs+0.5625225*Vds_cgs+0.15607017*Lg_cgs+0.7490984);\n",
            "hcgs1_14 = tanh(0.004656209*Vgs_cgs+-0.0067486507*Vds_cgs+-0.4438495*Lg_cgs+-0.00023583561);\n",
            "hcgs1_15 = tanh(-0.87449557*Vgs_cgs+3.5775394*Vds_cgs+0.058419783*Lg_cgs+1.0822651);\n",
            "hcgs1_16 = tanh(2.652287*Vgs_cgs+-2.3323097*Vds_cgs+-0.3422341*Lg_cgs+0.3537005);\n",
            "hcgs1_17 = tanh(0.88335234*Vgs_cgs+-0.3338322*Vds_cgs+-0.11836058*Lg_cgs+-0.40532526);\n",
            "hcgs1_18 = tanh(-0.6324544*Vgs_cgs+5.362017*Vds_cgs+0.20589639*Lg_cgs+-1.5765152);\n",
            "hcgs1_19 = tanh(0.52068394*Vgs_cgs+-1.1275563*Vds_cgs+0.19503203*Lg_cgs+0.16029519);\n",
            "hcgs1_20 = tanh(-1.3330969*Vgs_cgs+0.12449422*Vds_cgs+-0.0033548062*Lg_cgs+0.709236);\n",
            "hcgs1_21 = tanh(0.19407558*Vgs_cgs+0.072130404*Vds_cgs+-0.43343088*Lg_cgs+0.7703464);\n",
            "hcgs1_22 = tanh(-1.1008723*Vgs_cgs+0.7357464*Vds_cgs+-0.35802382*Lg_cgs+0.35450834);\n",
            "hcgs1_23 = tanh(-1.0641274*Vgs_cgs+-0.023895433*Vds_cgs+0.312323*Lg_cgs+0.58924264);\n",
            "hcgs1_24 = tanh(1.1386739*Vgs_cgs+-0.5871172*Vds_cgs+-0.12789965*Lg_cgs+-0.42580304);\n",
            "hcgs2_0 = tanh(-0.8456805*hcgs1_0+0.81710136*hcgs1_1+-0.22456154*hcgs1_2+-0.14914224*hcgs1_3+-2.0348866*hcgs1_4+-0.747218*hcgs1_5+-0.2158404*hcgs1_6+2.553406*hcgs1_7+0.42889893*hcgs1_8+-2.4390838*hcgs1_9+0.01141642*hcgs1_10+0.29778206*hcgs1_11+0.2695455*hcgs1_12+1.7889941*hcgs1_13+0.009541916*hcgs1_14+0.8799212*hcgs1_15+-1.8207581*hcgs1_16+-0.5563907*hcgs1_17+-2.550983*hcgs1_18+-0.24775197*hcgs1_19+0.3296183*hcgs1_20+0.25748515*hcgs1_21+0.8575212*hcgs1_22+0.19629534*hcgs1_23+-3.185923*hcgs1_24+0.30510485);\n",
            "hcgs2_1 = tanh(0.5944262*hcgs1_0+0.047796533*hcgs1_1+-0.5502816*hcgs1_2+0.00890337*hcgs1_3+-0.68498886*hcgs1_4+-2.5434804*hcgs1_5+0.38445947*hcgs1_6+1.0074053*hcgs1_7+-0.22382292*hcgs1_8+-3.1384215*hcgs1_9+0.0029475465*hcgs1_10+0.30534053*hcgs1_11+0.4773358*hcgs1_12+2.1924813*hcgs1_13+0.014758705*hcgs1_14+0.32548565*hcgs1_15+-1.1178577*hcgs1_16+-1.3329682*hcgs1_17+-1.9556054*hcgs1_18+0.6174226*hcgs1_19+2.260632*hcgs1_20+0.18014935*hcgs1_21+1.1065633*hcgs1_22+0.87714285*hcgs1_23+-2.8133676*hcgs1_24+0.68831825);\n",
            "hcgs2_2 = tanh(-0.16991197*hcgs1_0+1.8664104*hcgs1_1+-0.3474432*hcgs1_2+-0.21696573*hcgs1_3+-2.5008323*hcgs1_4+-0.2903099*hcgs1_5+1.6678329*hcgs1_6+-1.6792856*hcgs1_7+-1.7562099*hcgs1_8+-0.25893897*hcgs1_9+-0.04291705*hcgs1_10+-0.00012147065*hcgs1_11+0.026284702*hcgs1_12+-0.73358375*hcgs1_13+0.15483585*hcgs1_14+-1.0945101*hcgs1_15+-0.56847095*hcgs1_16+0.18457307*hcgs1_17+1.4761636*hcgs1_18+1.4470772*hcgs1_19+0.47771305*hcgs1_20+-0.1977162*hcgs1_21+-1.5351593*hcgs1_22+0.29518163*hcgs1_23+0.60733813*hcgs1_24+-0.10970557);\n",
            "hcgs2_3 = tanh(0.23839453*hcgs1_0+-0.008309026*hcgs1_1+0.54371554*hcgs1_2+-0.10062454*hcgs1_3+0.27675664*hcgs1_4+1.0899066*hcgs1_5+-0.3777212*hcgs1_6+0.13016528*hcgs1_7+-0.012133315*hcgs1_8+1.1119828*hcgs1_9+-0.04014963*hcgs1_10+-0.4047089*hcgs1_11+-0.3015341*hcgs1_12+-1.6089342*hcgs1_13+-0.05156553*hcgs1_14+-0.92389685*hcgs1_15+0.120922476*hcgs1_16+1.1602905*hcgs1_17+-1.4947407*hcgs1_18+-0.18796265*hcgs1_19+-1.6901983*hcgs1_20+-0.035788976*hcgs1_21+-0.99925464*hcgs1_22+-0.67925924*hcgs1_23+1.6767167*hcgs1_24+-0.3653112);\n",
            "hcgs2_4 = tanh(2.0752943*hcgs1_0+-0.3572386*hcgs1_1+-0.3549206*hcgs1_2+-0.29379958*hcgs1_3+-1.2869327*hcgs1_4+-0.4479244*hcgs1_5+0.031573102*hcgs1_6+1.4112504*hcgs1_7+0.5260651*hcgs1_8+-2.0343685*hcgs1_9+0.0187355*hcgs1_10+0.04147746*hcgs1_11+0.28817755*hcgs1_12+1.6051354*hcgs1_13+-0.008745322*hcgs1_14+1.0508798*hcgs1_15+-0.7435123*hcgs1_16+0.019853538*hcgs1_17+-1.1326213*hcgs1_18+-0.49617997*hcgs1_19+0.5881124*hcgs1_20+0.3453093*hcgs1_21+0.697278*hcgs1_22+0.34144053*hcgs1_23+-0.39954275*hcgs1_24+0.24203716);\n",
            "hcgs2_5 = tanh(-0.8049178*hcgs1_0+0.9928581*hcgs1_1+0.3324538*hcgs1_2+-0.32453984*hcgs1_3+-1.2684071*hcgs1_4+0.5504112*hcgs1_5+-0.4549791*hcgs1_6+-1.4632839*hcgs1_7+0.37171695*hcgs1_8+0.72572297*hcgs1_9+-0.025831597*hcgs1_10+-0.21172419*hcgs1_11+-0.85458016*hcgs1_12+-0.8990451*hcgs1_13+-0.021322189*hcgs1_14+-1.4535208*hcgs1_15+-0.35931605*hcgs1_16+0.488443*hcgs1_17+0.0592027*hcgs1_18+-0.3993243*hcgs1_19+-0.7132404*hcgs1_20+-0.7187561*hcgs1_21+-0.021962876*hcgs1_22+-0.1357851*hcgs1_23+0.233105*hcgs1_24+-0.5199506);\n",
            "hcgs2_6 = tanh(0.5850198*hcgs1_0+0.19749781*hcgs1_1+1.0971322*hcgs1_2+1.451523*hcgs1_3+-0.18752462*hcgs1_4+0.5928157*hcgs1_5+0.26702794*hcgs1_6+-1.1799154*hcgs1_7+0.42238823*hcgs1_8+0.34430432*hcgs1_9+0.36709026*hcgs1_10+0.76817995*hcgs1_11+1.4431945*hcgs1_12+0.28747907*hcgs1_13+0.1555853*hcgs1_14+1.4119282*hcgs1_15+-0.23534247*hcgs1_16+-0.043365728*hcgs1_17+-0.45434403*hcgs1_18+-0.6113481*hcgs1_19+-0.071436815*hcgs1_20+1.336255*hcgs1_21+0.048494633*hcgs1_22+-0.5316062*hcgs1_23+0.1436653*hcgs1_24+0.9528472);\n",
            "hcgs2_7 = tanh(-0.37940532*hcgs1_0+-1.2876422*hcgs1_1+0.13731669*hcgs1_2+0.17509227*hcgs1_3+0.8986726*hcgs1_4+1.0229409*hcgs1_5+-0.69503534*hcgs1_6+0.20359293*hcgs1_7+0.80313087*hcgs1_8+1.0088204*hcgs1_9+-0.06411503*hcgs1_10+-0.23810008*hcgs1_11+-0.22483361*hcgs1_12+-0.7514442*hcgs1_13+-0.022351535*hcgs1_14+-0.029597659*hcgs1_15+-0.81831634*hcgs1_16+0.25210202*hcgs1_17+-1.454332*hcgs1_18+-0.59304476*hcgs1_19+-0.89324987*hcgs1_20+0.32196864*hcgs1_21+0.43833193*hcgs1_22+-0.8081428*hcgs1_23+0.38283762*hcgs1_24+-0.023465121);\n",
            "hcgs2_8 = tanh(0.06450101*hcgs1_0+-0.32645157*hcgs1_1+0.33345243*hcgs1_2+0.12592125*hcgs1_3+0.9819371*hcgs1_4+1.568468*hcgs1_5+0.21460815*hcgs1_6+-1.0372665*hcgs1_7+-0.29271138*hcgs1_8+2.8030427*hcgs1_9+0.028697932*hcgs1_10+-0.57627803*hcgs1_11+-0.063452154*hcgs1_12+-1.590045*hcgs1_13+0.014249704*hcgs1_14+0.045245085*hcgs1_15+0.4989924*hcgs1_16+0.8713646*hcgs1_17+1.1842577*hcgs1_18+-0.008994069*hcgs1_19+-1.1241162*hcgs1_20+0.00919466*hcgs1_21+-1.1247435*hcgs1_22+-0.8297448*hcgs1_23+2.3253732*hcgs1_24+0.1950006);\n",
            "hcgs2_9 = tanh(-0.45197833*hcgs1_0+0.045947105*hcgs1_1+-0.25987926*hcgs1_2+0.5171293*hcgs1_3+-0.1316219*hcgs1_4+0.067848496*hcgs1_5+0.8087117*hcgs1_6+-0.9547865*hcgs1_7+-0.5715785*hcgs1_8+0.32714853*hcgs1_9+0.036904156*hcgs1_10+0.13872579*hcgs1_11+0.12824427*hcgs1_12+0.11992727*hcgs1_13+0.07935975*hcgs1_14+-0.5361036*hcgs1_15+0.53884923*hcgs1_16+-0.039713964*hcgs1_17+0.7091841*hcgs1_18+0.48865792*hcgs1_19+0.18420732*hcgs1_20+0.07967269*hcgs1_21+-0.30234754*hcgs1_22+0.17830539*hcgs1_23+-0.11796092*hcgs1_24+-0.13467003);\n",
            "hcgs2_10 = tanh(-1.6391398*hcgs1_0+-0.34285977*hcgs1_1+-0.3121803*hcgs1_2+0.41148263*hcgs1_3+-0.3336083*hcgs1_4+-1.5920037*hcgs1_5+-0.18015313*hcgs1_6+1.850319*hcgs1_7+0.17927983*hcgs1_8+-2.8341568*hcgs1_9+-0.030394657*hcgs1_10+0.69309235*hcgs1_11+0.2665585*hcgs1_12+2.2477663*hcgs1_13+-0.009332433*hcgs1_14+-0.3326121*hcgs1_15+-2.0456257*hcgs1_16+-0.9294434*hcgs1_17+-1.1023426*hcgs1_18+0.111194305*hcgs1_19+1.141862*hcgs1_20+0.71554065*hcgs1_21+1.1074047*hcgs1_22+0.88629806*hcgs1_23+-2.3181875*hcgs1_24+0.6508097);\n",
            "hcgs2_11 = tanh(-0.24050619*hcgs1_0+-0.41003644*hcgs1_1+-1.2268633*hcgs1_2+-1.3124051*hcgs1_3+-0.1936789*hcgs1_4+-0.87841165*hcgs1_5+-0.020659437*hcgs1_6+-1.9815321*hcgs1_7+-0.087647766*hcgs1_8+-0.86529773*hcgs1_9+-0.1343042*hcgs1_10+-0.13795137*hcgs1_11+-1.2411678*hcgs1_12+-0.5560807*hcgs1_13+0.009826684*hcgs1_14+-1.6039407*hcgs1_15+-0.05220601*hcgs1_16+-0.25667432*hcgs1_17+-0.093644366*hcgs1_18+0.53593457*hcgs1_19+0.87013644*hcgs1_20+-1.2723198*hcgs1_21+0.009109739*hcgs1_22+1.0373011*hcgs1_23+-0.06875387*hcgs1_24+-0.96754915);\n",
            "y_cgs = -0.16241902*hcgs2_0+-0.12451764*hcgs2_1+-0.04949069*hcgs2_2+0.082015745*hcgs2_3+-0.12691256*hcgs2_4+-0.18902193*hcgs2_5+-0.45962262*hcgs2_6+0.07102054*hcgs2_7+-0.59871113*hcgs2_8+0.095522754*hcgs2_9+-0.4854874*hcgs2_10+-0.4512705*hcgs2_11+0.3220441;\n",
            "\n",
            "\n",
            "h1_0 = tanh(0.0067332466*Vgs+0.0014811404*Vds+-0.089835726*Lg+0.09066404);\n",
            "h1_1 = tanh(0.11106648*Vgs+0.40663162*Vds+0.0077667693*Lg+-0.10081332);\n",
            "h1_2 = tanh(1.5560061*Vgs+2.696172*Vds+-0.2839149*Lg+-0.3486124);\n",
            "h1_3 = tanh(-2.0344324*Vgs+-0.28681883*Vds+0.8085338*Lg+0.0026162667);\n",
            "h1_4 = tanh(-0.03341659*Vgs+-0.15408932*Vds+-0.015416802*Lg+0.04370213);\n",
            "h1_5 = tanh(1.4746779*Vgs+0.33599785*Vds+-0.2669822*Lg+-0.42193142);\n",
            "h1_6 = tanh(-0.16598122*Vgs+-2.3765175*Vds+0.11762294*Lg+0.019988127);\n",
            "h1_7 = tanh(4.3143744*Vgs+-0.5703343*Vds+-0.3085829*Lg+-0.50043374);\n",
            "h1_8 = tanh(-0.4936719*Vgs+36.42876*Vds+0.32744452*Lg+0.24776502);\n",
            "h1_9 = tanh(0.4869826*Vgs+-0.13276859*Vds+0.28959593*Lg+-0.3828168);\n",
            "h1_10 = tanh(0.000804813*Vgs+0.0004923456*Vds+-0.41243985*Lg+0.41356155);\n",
            "h1_11 = tanh(2.6073072*Vgs+-0.56419706*Vds+-0.111921*Lg+-0.37480572);\n",
            "h1_12 = tanh(0.0020444687*Vgs+0.0014842966*Vds+0.21013871*Lg+-0.20708004);\n",
            "h1_13 = tanh(-2.6335895*Vgs+0.28439838*Vds+0.321686*Lg+0.41039804);\n",
            "h1_14 = tanh(0.0034599572*Vgs+0.0084134*Vds+-0.034978658*Lg+0.037748303);\n",
            "h1_15 = tanh(0.0025394042*Vgs+0.00083049946*Vds+-0.13070253*Lg+0.13411763);\n",
            "h1_16 = tanh(-2.1379406*Vgs+-0.57477415*Vds+0.19879702*Lg+0.6043196);\n",
            "h1_17 = tanh(-0.9647693*Vgs+3.0132833*Vds+0.13054319*Lg+0.38879853);\n",
            "h1_18 = tanh(-1.6564686*Vgs+-0.06445829*Vds+0.39810786*Lg+0.21570432);\n",
            "h1_19 = tanh(-0.120745726*Vgs+-0.37977916*Vds+-0.20842548*Lg+0.33059505);\n",
            "h1_20 = tanh(-0.5346294*Vgs+-0.24575421*Vds+0.42274392*Lg+-0.095065445);\n",
            "h1_21 = tanh(-0.17100386*Vgs+-0.36405364*Vds+-0.32086778*Lg+-0.45500055);\n",
            "h1_22 = tanh(-0.3456514*Vgs+-0.43013823*Vds+-0.14385213*Lg+0.4731826);\n",
            "h1_23 = tanh(0.28758252*Vgs+-0.55982685*Vds+0.44342747*Lg+-0.34977767);\n",
            "h1_24 = tanh(0.08224837*Vgs+0.35958192*Vds+0.059084654*Lg+-0.1397865);\n",
            "\n",
            "h2_0 = tanh(-0.8091375*h1_0+-0.43552017*h1_1+-0.8155236*h1_2+0.24130166*h1_3+0.14224924*h1_4+-0.62978196*h1_5+0.6101532*h1_6+-0.43199107*h1_7+-2.3122218*h1_8+-0.23163876*h1_9+0.068008065*h1_10+-0.03888623*h1_11+-0.014134661*h1_12+-0.06591678*h1_13+0.022440521*h1_14+-0.27041718*h1_15+0.090823404*h1_16+-0.26772383*h1_17+0.47534344*h1_18+0.73750144*h1_19+0.50188285*h1_20+0.3184862*h1_21+0.3608627*h1_22+0.79056877*h1_23+-0.037502155*h1_24+-0.24878933);\n",
            "h2_1 = tanh(-0.03261362*h1_0+-0.06024928*h1_1+-0.10186445*h1_2+0.018033225*h1_3+0.51713187*h1_4+0.022300791*h1_5+0.015979188*h1_6+0.056999434*h1_7+0.035186633*h1_8+0.25107002*h1_9+-0.012781056*h1_10+-0.3380685*h1_11+0.05043952*h1_12+-0.24957459*h1_13+0.024472937*h1_14+-0.08896992*h1_15+0.053890992*h1_16+-0.0101861*h1_17+-0.031921674*h1_18+0.17910303*h1_19+-0.03255347*h1_20+-0.38824677*h1_21+-0.14763784*h1_22+-0.24868536*h1_23+-0.024086645*h1_24+-0.18488966);\n",
            "h2_2 = tanh(0.12152682*h1_0+0.009792632*h1_1+-0.29032546*h1_2+0.5965219*h1_3+-0.021902572*h1_4+-0.5883061*h1_5+0.17799598*h1_6+-0.92929643*h1_7+0.32916892*h1_8+-0.2568552*h1_9+-0.067421295*h1_10+-1.1449344*h1_11+0.007887112*h1_12+0.276745*h1_13+-0.033592574*h1_14+0.1040227*h1_15+0.23562248*h1_16+-0.18375605*h1_17+1.0453725*h1_18+0.33611628*h1_19+0.6393779*h1_20+0.06726488*h1_21+0.7879763*h1_22+-0.14922209*h1_23+-0.18613423*h1_24+0.28223407);\n",
            "h2_3 = tanh(-0.18859175*h1_0+-0.9337733*h1_1+-0.938285*h1_2+0.7002026*h1_3+0.6061784*h1_4+-0.52529997*h1_5+0.71481675*h1_6+-0.38263503*h1_7+-0.5072332*h1_8+0.1263292*h1_9+0.04664359*h1_10+-0.5296314*h1_11+-0.033964746*h1_12+0.6460025*h1_13+-0.0231704*h1_14+-0.38591123*h1_15+0.0872077*h1_16+-0.5042024*h1_17+0.47931066*h1_18+0.50751704*h1_19+-0.063635096*h1_20+0.018421784*h1_21+0.08422998*h1_22+0.30641475*h1_23+-0.524482*h1_24+-0.26979908);\n",
            "h2_4 = tanh(-0.034289524*h1_0+-0.11348127*h1_1+0.48059276*h1_2+-1.123194*h1_3+0.12442129*h1_4+0.49476302*h1_5+0.0863118*h1_6+0.47754285*h1_7+0.2202004*h1_8+0.41435316*h1_9+0.028558401*h1_10+0.36081842*h1_11+-0.008486773*h1_12+-0.9903806*h1_13+0.015499473*h1_14+-0.063613005*h1_15+-1.4330287*h1_16+-0.74456465*h1_17+-0.5569409*h1_18+-0.019323817*h1_19+-0.14176102*h1_20+0.49098673*h1_21+-0.057230044*h1_22+0.515492*h1_23+-0.13265039*h1_24+-0.18507467);\n",
            "h2_5 = tanh(-0.09933434*h1_0+-0.15035452*h1_1+-0.5133741*h1_2+-0.47027865*h1_3+0.21104498*h1_4+0.14945817*h1_5+0.10640489*h1_6+0.1787739*h1_7+0.060723394*h1_8+-0.5602644*h1_9+-0.0019350805*h1_10+0.5330786*h1_11+0.012645531*h1_12+-0.42403966*h1_13+0.018393166*h1_14+-0.00062784646*h1_15+-0.0052586636*h1_16+1.0866524*h1_17+0.078084014*h1_18+0.059300236*h1_19+0.32314536*h1_20+0.49137965*h1_21+0.09876712*h1_22+-0.583638*h1_23+-0.1759503*h1_24+-0.18313222);\n",
            "h2_6 = tanh(-0.1285073*h1_0+-1.032602*h1_1+-1.224141*h1_2+0.14631094*h1_3+0.65741867*h1_4+-0.22908972*h1_5+0.63241625*h1_6+-0.88521755*h1_7+0.09097795*h1_8+0.09149886*h1_9+0.027507082*h1_10+-0.63233715*h1_11+-0.02061426*h1_12+0.1104531*h1_13+-0.012734477*h1_14+-0.34623924*h1_15+-0.18294835*h1_16+-0.045275174*h1_17+0.35704163*h1_18+0.5909152*h1_19+0.073851526*h1_20+0.49004716*h1_21+0.19812724*h1_22+0.3784807*h1_23+-0.6437184*h1_24+-0.3594689);\n",
            "h2_7 = tanh(0.119550616*h1_0+-0.035170615*h1_1+-0.60996103*h1_2+0.40164798*h1_3+-0.007347775*h1_4+-0.30736676*h1_5+-0.13570982*h1_6+0.9896757*h1_7+-4.0502048*h1_8+0.053264104*h1_9+-0.012164457*h1_10+0.48190302*h1_11+0.014136277*h1_12+-0.5162279*h1_13+0.014227704*h1_14+0.05181294*h1_15+0.7717682*h1_16+-0.111988194*h1_17+0.062060844*h1_18+-0.044897318*h1_19+-0.15565042*h1_20+0.24919312*h1_21+-0.07993477*h1_22+-0.09538691*h1_23+-0.0052977665*h1_24+-0.10968423);\n",
            "h2_8 = tanh(0.079706475*h1_0+-0.5814533*h1_1+0.6996235*h1_2+-0.16522586*h1_3+0.630409*h1_4+0.29656532*h1_5+-0.5331364*h1_6+0.114606544*h1_7+-0.21106683*h1_8+0.0467022*h1_9+-0.06535354*h1_10+-0.24001631*h1_11+0.059368264*h1_12+0.15451497*h1_13+0.053622272*h1_14+0.12981157*h1_15+0.5445277*h1_16+0.24295534*h1_17+-0.26532695*h1_18+0.56733835*h1_19+0.12550347*h1_20+0.048389148*h1_21+0.5622483*h1_22+0.6891362*h1_23+-0.76375955*h1_24+-0.2676767);\n",
            "h2_9 = tanh(-0.03992191*h1_0+0.09928779*h1_1+1.0056654*h1_2+-2.3181243*h1_3+-0.0916434*h1_4+2.143644*h1_5+0.19818373*h1_6+2.8020356*h1_7+-0.47644797*h1_8+0.24747562*h1_9+0.02024143*h1_10+1.3736787*h1_11+0.006931587*h1_12+-3.888161*h1_13+0.018778825*h1_14+-0.0041158814*h1_15+-1.5533967*h1_16+-0.71390355*h1_17+-2.8108795*h1_18+-0.24579133*h1_19+-1.128639*h1_20+0.2231992*h1_21+-1.2298777*h1_22+0.043120787*h1_23+0.403965*h1_24+-0.17281148);\n",
            "h2_10 = tanh(0.35093632*h1_0+-0.05374117*h1_1+0.16745348*h1_2+-0.19781992*h1_3+-0.07950959*h1_4+0.29579905*h1_5+-0.5073861*h1_6+-0.14164843*h1_7+0.70651436*h1_8+0.29345548*h1_9+-0.054172043*h1_10+-0.28257352*h1_11+0.033599235*h1_12+0.017732412*h1_13+0.02154612*h1_14+0.1586723*h1_15+-0.23733242*h1_16+0.43727452*h1_17+-0.084653825*h1_18+-0.054423757*h1_19+-0.36601782*h1_20+-0.07235579*h1_21+-0.03668117*h1_22+-0.05778476*h1_23+-0.020711297*h1_24+0.24233255);\n",
            "h2_11 = tanh(0.009613743*h1_0+-0.26873523*h1_1+-0.0937895*h1_2+-0.17233954*h1_3+0.2600588*h1_4+-0.41362444*h1_5+-0.13627699*h1_6+-0.03051018*h1_7+0.028052136*h1_8+0.06536644*h1_9+-0.029295161*h1_10+-0.017168272*h1_11+0.016685432*h1_12+-0.15576635*h1_13+0.08295777*h1_14+-0.16339928*h1_15+-0.069266655*h1_16+-0.07842072*h1_17+0.22933975*h1_18+-0.07057512*h1_19+0.172754*h1_20+-0.21173981*h1_21+-0.8436922*h1_22+0.28658*h1_23+-0.018899215*h1_24+-0.11145156);\n",
            "y = 0.13345447*h2_0+0.0023934222*h2_1+0.1793076*h2_2+0.51211137*h2_3+0.2699503*h2_4+0.11368197*h2_5+-0.5013421*h2_6+-0.61606425*h2_7+-0.037382632*h2_8+0.18645377*h2_9+0.32913932*h2_10+-0.00013747951*h2_11+-0.1796401;\n",
            "\n",
            "Cgd = pow(10, (y_cgd/normO_cgd + MinO_cgd))*W/15; //traning width was 15um\n",
            "Cgs = pow(10, (y_cgs/normO_cgs + MinO_cgs))*W/15;\n",
            "Cgg = Cgd+Cgs;\n",
            "\n",
            "Id = pow(10, (y/normI + MinI))*W*3;\n",
            "I(g, d) <+ Cgd*ddt(Vg-Vd) ;\n",
            "I(g, s) <+ Cgs*ddt(Vg-Vs) ;\n",
            "\n",
            "if (Vd >= Vs) begin\n",
            "\tI(d, s) <+ dir*Id;\n",
            "end\n",
            "\n",
            "else begin\n",
            "\tI(d, s) <+ dir*Id;\n",
            "end\n",
            "\n",
            "end\n",
            "endmodule\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## HV-IWO-DMODE\n",
        "\n",
        "with torch.no_grad():\n",
        "\n",
        "    output = []\n",
        "    # Iterate over the dataloader for training data\n",
        "    for i, data in enumerate(testdataloader, 0):\n",
        "        inputs, targets = data\n",
        "        inputs, targets = inputs.float(), targets.float()\n",
        "        targets = targets.reshape((targets.shape[0],1))\n",
        "\n",
        "        #zero the gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        #perform forward pass\n",
        "        outputs = model(inputs)\n",
        "        output.append(outputs)\n",
        "# Process is complete.\n",
        "#print('Training process has finished.')\n",
        "\n",
        "# Extract the weights and biases from the model\n",
        "weights_1 = model.fc1.weight.detach().numpy()\n",
        "bias_1 = model.fc1.bias.detach().numpy()\n",
        "weights_2 = model.fc2.weight.detach().numpy()\n",
        "bias_2 = model.fc2.bias.detach().numpy()\n",
        "weights_3 = model.fc3.weight.detach().numpy()\n",
        "bias_3 = model.fc3.bias.detach().numpy()\n",
        "\n",
        "def generate_variable_declarations(weights_shape, layer_prefix):\n",
        "    declarations = \"\"\n",
        "    num_neurons = weights_shape  # Number of neurons is determined by the first dimension of the weights matrix\n",
        "    layer_declarations = \", \".join([f\"{layer_prefix}_{i}\" for i in range(num_neurons)]) + \";\"\n",
        "    declarations += layer_declarations\n",
        "    return declarations\n",
        "\n",
        "# Use the function to generate declarations for each layer\n",
        "h1_declarations = generate_variable_declarations(weights_1.shape[0], \"hvth1\")\n",
        "h2_declarations = generate_variable_declarations(weights_2.shape[0], \"hvth2\")\n",
        "\n",
        "verilog_code = \"\"\"\n",
        "// VerilogA for ML_HV, ML_HV_emode, veriloga\n",
        "\n",
        "`include \"constants.vams\"\n",
        "`include \"disciplines.vams\"\n",
        "\n",
        "module ML_HV_emode_rev3(d, g, s);\n",
        "\n",
        "\n",
        "inout d, g, s;\n",
        "electrical d, g, s;\n",
        "\n",
        "//****** Parameters L and W ********\n",
        "parameter real W = 3;\n",
        "parameter real L = 0.6;\n",
        "parameter MinVg = -6.0 ;\n",
        "parameter normVg = 0.05555555555555555 ;\n",
        "parameter MinVd = 0.0 ;\n",
        "parameter normVd = 0.08333333333333333 ;\n",
        "parameter MinLg = 0 ;\n",
        "parameter normLg = 1.66666667 ;\n",
        "parameter MinI = -14.614393726401689 ;\n",
        "parameter normI =0.0975145951626748;\n",
        "parameter vth =0;\n",
        "parameter real T_stress = 1; //set on cadence as variable\n",
        "\n",
        "parameter MinVg_cgd = -6.0 ;\n",
        "parameter normVg_cgd = 0.05555555555555555 ;\n",
        "parameter MinVd_cgd = -6.0 ;\n",
        "parameter normVd_cgd = 0.05555555555555555 ;\n",
        "parameter MinLg_cgd = 0.6 ;\n",
        "parameter normLg_cgd = 0 ;\n",
        "parameter MinO_cgd = -13.926054483271312 ;\n",
        "parameter normO_cgd =1.4486107458852509 ;\n",
        "\n",
        "parameter MinVg_cgs = -6.0 ;\n",
        "parameter normVg_cgs = 0.05555555555555555 ;\n",
        "parameter MinVd_cgs = -6.0 ;\n",
        "parameter normVd_cgs = 0.05555555555555555 ;\n",
        "parameter MinLg_cgs = 0.6 ;\n",
        "parameter normLg_cgs = 0 ;\n",
        "parameter MinO_cgs = -13.08359343161434 ;\n",
        "parameter normO_cgs =2.886734269461609;\n",
        "\n",
        "\n",
        "parameter Mint_stress = {} ;\n",
        "parameter normt_stress = {} ;\n",
        "parameter Mindelta_Vth = {} ;\n",
        "parameter normdelta_Vth = {} ;\n",
        "\n",
        "real {}\n",
        "real {}\n",
        "\n",
        "\n",
        "real Vg, Vd, Vs, Vgs, Vds, Lg, Id, Cgg, Cgs,Cgd, Vgd;\n",
        "real y, y_cgg, y_cgs, y_cgd, yvth;\n",
        "real Vgs_cgg, Vgs_cgs, Vgs_cgd;\n",
        "real Vds_cgg, Vds_cgs, Vds_cgd;\n",
        "real Lg_cgg, Lg_cgs, Lg_cgd;\n",
        "real Vgsraw, Vgdraw, dir;\n",
        "real Vdi, Vsi;\n",
        "real t_stress, v_ov, t_rec, clk_loops, temp, delta_Vth;\n",
        "\n",
        "\n",
        "real h1_0, h1_1, h1_2, h1_3, h1_4, h1_5, h1_6, h1_7, h1_8, h1_9, h1_10, h1_11, h1_12, h1_13, h1_14, h1_15, h1_16, h1_17, h1_18, h1_19, h1_20, h1_21, h1_22, h1_23, h1_24;\n",
        "real h2_0, h2_1, h2_2, h2_3, h2_4, h2_5, h2_6, h2_7, h2_8, h2_9, h2_10, h2_11;\n",
        "real hcgg1_0, hcgg1_1, hcgg1_2, hcgg1_3, hcgg1_4, hcgg1_5, hcgg1_6, hcgg1_7, hcgg1_8, hcgg1_9, hcgg1_10, hcgg1_11, hcgg1_12, hcgg1_13, hcgg1_14, hcgg1_15, hcgg1_16, hcgg1_17, hcgg1_18, hcgg1_19, hcgg1_20, hcgg1_21, hcgg1_22, hcgg1_23, hcgg1_24;\n",
        "real hcgg2_0, hcgg2_1, hcgg2_2, hcgg2_3, hcgg2_4, hcgg2_5, hcgg2_6, hcgg2_7, hcgg2_8, hcgg2_9, hcgg2_10, hcgg2_11;\n",
        "real hcgs1_0, hcgs1_1, hcgs1_2, hcgs1_3, hcgs1_4, hcgs1_5, hcgs1_6, hcgs1_7, hcgs1_8, hcgs1_9, hcgs1_10, hcgs1_11, hcgs1_12, hcgs1_13, hcgs1_14, hcgs1_15, hcgs1_16, hcgs1_17, hcgs1_18, hcgs1_19, hcgs1_20, hcgs1_21, hcgs1_22, hcgs1_23, hcgs1_24;\n",
        "real hcgs2_0, hcgs2_1, hcgs2_2, hcgs2_3, hcgs2_4, hcgs2_5, hcgs2_6, hcgs2_7, hcgs2_8, hcgs2_9, hcgs2_10, hcgs2_11;\n",
        "real hcgd1_0, hcgd1_1, hcgd1_2, hcgd1_3, hcgd1_4, hcgd1_5, hcgd1_6, hcgd1_7, hcgd1_8, hcgd1_9, hcgd1_10, hcgd1_11, hcgd1_12, hcgd1_13, hcgd1_14, hcgd1_15, hcgd1_16, hcgd1_17, hcgd1_18, hcgd1_19, hcgd1_20, hcgd1_21, hcgd1_22, hcgd1_23, hcgd1_24;\n",
        "real hcgd2_0, hcgd2_1, hcgd2_2, hcgd2_3, hcgd2_4, hcgd2_5, hcgd2_6, hcgd2_7, hcgd2_8, hcgd2_9, hcgd2_10, hcgd2_11;\n",
        "\n",
        "real Qgd, Qgs;\n",
        "\n",
        "analog begin\n",
        "\n",
        "t_stress = (T_stress - Mint_stress)*normt_stress;\n",
        "\n",
        "//******************** delta_Vth NN **********************************//\n",
        "\n",
        "\"\"\".format(Mint_stress, normt_stress, Mindelta_Vth, normdelta_Vth, h1_declarations, h2_declarations)\n",
        "# V_ov = (V_ov - MinV_ov)*normV_ov ;\n",
        "# t_stress = (T_stress - Mint_stress)*normt_stress ;\n",
        "\n",
        "# Create the Verilog-A code for the 1st hidden layer\n",
        "for i in range(n1):\n",
        "    inputs = [\"t_stress\"]\n",
        "    inputs = [\"*\".join([str(weights_1[i][j]), inp]) for j, inp in enumerate(inputs)]\n",
        "    inputs = \"+\".join(inputs)\n",
        "    inputs = \"+\".join([inputs, str(bias_1[i])])\n",
        "    verilog_code += \"hvth1_{} = tanh({});\\n\".format(i, inputs)\n",
        "verilog_code += \"\\n\"\n",
        "\n",
        "# Create the Verilog-A code for the 2nd hidden layer\n",
        "for i in range(n2):\n",
        "    inputs = [\"hvth1_{}\".format(j) for j in range(n1)]\n",
        "    inputs = [\"*\".join([str(weights_2[i][j]), inp]) for j, inp in enumerate(inputs)]\n",
        "    inputs = \"+\".join(inputs)\n",
        "    inputs = \"+\".join([inputs, str(bias_2[i])])\n",
        "    verilog_code += \"hvth2_{} = tanh({});\\n\".format(i, inputs)\n",
        "verilog_code += \"\\n\"\n",
        "\n",
        "# Create the Verilog-A code for the output layer\n",
        "inputs = [\"hvth2_{}\".format(i) for i in range(n2)]\n",
        "inputs = [\"*\".join([str(weights_3[0][i]), inp]) for i, inp in enumerate(inputs)]\n",
        "inputs = \"+\".join(inputs)\n",
        "inputs = \"+\".join([inputs, str(bias_3[0])])\n",
        "verilog_code += \"yvth = {};\\n\\n\".format(inputs)\n",
        "verilog_code += \"delta_Vth = (yvth - Mindelta_Vth) * normdelta_Vth;\\n\"\n",
        "verilog_code += \"\"\"$strobe(\"dvth=$g\",delta_Vth);\"\"\"\n",
        "verilog_code += \"\"\"\n",
        "\tVg = V(g);\n",
        "\tVs = V(s);\n",
        "\tVd = V(d);\n",
        "    Vgsraw = Vg-Vs ;\n",
        "    Vgdraw = Vg-Vd ;\n",
        "if (Vgsraw>=Vgdraw) begin\n",
        "\tVgs = ((Vg-Vs) - MinVg - vth - delta_Vth) * normVg ;\n",
        "\n",
        "\tVgs_cgs = ((Vg-Vs) - MinVg_cgs - vth - delta_Vth) * normVg_cgs ;\n",
        "\tVgs_cgd = ((Vg-Vs) - MinVg_cgd - vth - delta_Vth) * normVg_cgd ;\n",
        "\tdir = 1 ;\n",
        "end\n",
        "else begin\n",
        "\tVgs = ((Vg-Vd) - MinVg - vth - delta_Vth) * normVg ;\n",
        "\tVgs_cgs = ((Vg-Vd) - MinVg_cgs - vth - delta_Vth) * normVg_cgs ;\n",
        "\tVgs_cgd = ((Vg-Vd) - MinVg_cgd - vth - delta_Vth) * normVg_cgd ;\n",
        "\tdir = -1 ;\n",
        "end\n",
        "\n",
        "\tVds = (abs(Vd-Vs) - MinVd) * normVd ;\n",
        "\n",
        "\tVds_cgs = (abs(Vd-Vs) - MinVd_cgs) * normVd_cgs ;\n",
        "\tVds_cgd = (abs(Vd-Vs) - MinVd_cgd) * normVd_cgd ;\n",
        "\n",
        "\tLg = (L -MinLg)*normLg ;\n",
        "\n",
        "\tLg_cgs = (L -MinLg_cgs)*normLg_cgs ;\n",
        "\tLg_cgd = (L -MinLg_cgd)*normLg_cgd ;\n",
        "\n",
        "hcgd1_0 = tanh(-1.6088891*Vgs_cgd+6.3197856*Vds_cgd+-0.34359035*Lg_cgd+-1.2290763);\n",
        "hcgd1_1 = tanh(12.800741*Vgs_cgd+-0.7317432*Vds_cgd+-0.13210897*Lg_cgd+-0.9875373);\n",
        "hcgd1_2 = tanh(-0.0054054493*Vgs_cgd+-0.0007543899*Vds_cgd+0.23912339*Lg_cgd+0.0017988982);\n",
        "hcgd1_3 = tanh(0.22807486*Vgs_cgd+-0.5850707*Vds_cgd+0.1802489*Lg_cgd+-0.041721515);\n",
        "hcgd1_4 = tanh(-1.9484413*Vgs_cgd+0.5350963*Vds_cgd+0.094985865*Lg_cgd+0.38261548);\n",
        "hcgd1_5 = tanh(0.07684944*Vgs_cgd+0.73550975*Vds_cgd+0.24519779*Lg_cgd+-0.33061126);\n",
        "hcgd1_6 = tanh(-0.89280343*Vgs_cgd+-0.7184284*Vds_cgd+-0.3183667*Lg_cgd+0.8391025);\n",
        "hcgd1_7 = tanh(-0.39399716*Vgs_cgd+-0.48556596*Vds_cgd+0.03430429*Lg_cgd+-1.3344768);\n",
        "hcgd1_8 = tanh(-0.3645768*Vgs_cgd+0.28501382*Vds_cgd+0.21822546*Lg_cgd+0.05729416);\n",
        "hcgd1_9 = tanh(0.16371249*Vgs_cgd+-7.4085484*Vds_cgd+0.16751836*Lg_cgd+0.94246316);\n",
        "hcgd1_10 = tanh(-0.00981918*Vgs_cgd+1.5533135*Vds_cgd+0.3354165*Lg_cgd+-0.987391);\n",
        "hcgd1_11 = tanh(-0.8166236*Vgs_cgd+-1.5672101*Vds_cgd+-0.37568504*Lg_cgd+1.5318174);\n",
        "hcgd1_12 = tanh(0.2171772*Vgs_cgd+0.27744094*Vds_cgd+-0.3758668*Lg_cgd+0.22765432);\n",
        "hcgd1_13 = tanh(5.4901214*Vgs_cgd+-2.3752038*Vds_cgd+-0.37384644*Lg_cgd+-1.8590863);\n",
        "hcgd1_14 = tanh(0.0019571404*Vgs_cgd+-0.004179957*Vds_cgd+-0.19047788*Lg_cgd+-0.0013728535);\n",
        "hcgd1_15 = tanh(0.3415198*Vgs_cgd+-0.7726733*Vds_cgd+-0.2894067*Lg_cgd+0.28510723);\n",
        "hcgd1_16 = tanh(0.60382843*Vgs_cgd+0.29274306*Vds_cgd+0.3782456*Lg_cgd+-0.42503095);\n",
        "hcgd1_17 = tanh(-1.4589765*Vgs_cgd+-28.384256*Vds_cgd+0.14227134*Lg_cgd+0.27606094);\n",
        "hcgd1_18 = tanh(-0.925936*Vgs_cgd+-0.8781763*Vds_cgd+-0.015472701*Lg_cgd+1.0576844);\n",
        "hcgd1_19 = tanh(-0.24114211*Vgs_cgd+-0.23577656*Vds_cgd+-0.15397707*Lg_cgd+0.21749781);\n",
        "hcgd1_20 = tanh(-0.10808995*Vgs_cgd+-0.47354648*Vds_cgd+-0.4505364*Lg_cgd+0.20475943);\n",
        "hcgd1_21 = tanh(0.0030588252*Vgs_cgd+0.0031867356*Vds_cgd+0.36055082*Lg_cgd+0.0012428315);\n",
        "hcgd1_22 = tanh(-0.5569403*Vgs_cgd+0.49564502*Vds_cgd+-0.3408415*Lg_cgd+0.110713266);\n",
        "hcgd1_23 = tanh(-8.529932*Vgs_cgd+3.9473457*Vds_cgd+0.18583581*Lg_cgd+1.5164791);\n",
        "hcgd1_24 = tanh(-0.46144176*Vgs_cgd+0.17818846*Vds_cgd+0.40154326*Lg_cgd+0.013512757);\n",
        "hcgd2_0 = tanh(-1.2514079*hcgd1_0+1.8445047*hcgd1_1+-0.060258187*hcgd1_2+0.54575056*hcgd1_3+0.27076387*hcgd1_4+-0.7876322*hcgd1_5+0.77317226*hcgd1_6+-0.3889938*hcgd1_7+-0.21885906*hcgd1_8+-1.1528927*hcgd1_9+0.14252257*hcgd1_10+0.740463*hcgd1_11+0.08602427*hcgd1_12+0.7715963*hcgd1_13+-0.101067945*hcgd1_14+0.44884506*hcgd1_15+-0.2603153*hcgd1_16+-0.39788675*hcgd1_17+0.9237374*hcgd1_18+0.39531583*hcgd1_19+0.49464858*hcgd1_20+-0.12488924*hcgd1_21+-0.7705938*hcgd1_22+-0.86613214*hcgd1_23+-0.38659856*hcgd1_24+0.66808015);\n",
        "hcgd2_1 = tanh(-0.11062574*hcgd1_0+0.083067186*hcgd1_1+-0.037825696*hcgd1_2+0.20563497*hcgd1_3+0.33369613*hcgd1_4+0.0886166*hcgd1_5+0.09043529*hcgd1_6+0.030820973*hcgd1_7+0.33438814*hcgd1_8+-0.039903246*hcgd1_9+0.069336206*hcgd1_10+0.20417899*hcgd1_11+-0.082516894*hcgd1_12+0.024985574*hcgd1_13+0.048390143*hcgd1_14+0.06470794*hcgd1_15+0.02195431*hcgd1_16+0.09177701*hcgd1_17+-0.10868985*hcgd1_18+-0.6351576*hcgd1_19+-0.6435663*hcgd1_20+0.016601902*hcgd1_21+0.11345903*hcgd1_22+-0.106875926*hcgd1_23+-0.22184639*hcgd1_24+0.06259622);\n",
        "hcgd2_2 = tanh(0.9100889*hcgd1_0+0.8600478*hcgd1_1+-0.0020481474*hcgd1_2+0.07100766*hcgd1_3+-0.47693938*hcgd1_4+0.63060385*hcgd1_5+-1.7883018*hcgd1_6+-0.10218202*hcgd1_7+0.3747722*hcgd1_8+1.0218011*hcgd1_9+0.93496245*hcgd1_10+-1.7450128*hcgd1_11+-0.113168575*hcgd1_12+-0.66238743*hcgd1_13+-0.030654324*hcgd1_14+-0.92108375*hcgd1_15+1.0528872*hcgd1_16+0.033458687*hcgd1_17+-1.3917034*hcgd1_18+-0.35307285*hcgd1_19+-0.48046726*hcgd1_20+-0.014765772*hcgd1_21+0.23623382*hcgd1_22+-2.6411834*hcgd1_23+0.027243387*hcgd1_24+0.03591177);\n",
        "hcgd2_3 = tanh(0.62506354*hcgd1_0+0.44420892*hcgd1_1+-0.022703474*hcgd1_2+0.77745813*hcgd1_3+0.24668725*hcgd1_4+-0.39535686*hcgd1_5+-0.36517298*hcgd1_6+0.6084278*hcgd1_7+-0.3068207*hcgd1_8+-0.630984*hcgd1_9+-0.76083857*hcgd1_10+0.33673477*hcgd1_11+-0.38794822*hcgd1_12+0.8889784*hcgd1_13+-0.032760695*hcgd1_14+0.31031358*hcgd1_15+0.44298455*hcgd1_16+-1.8597292*hcgd1_17+0.1456184*hcgd1_18+0.29207367*hcgd1_19+0.32642275*hcgd1_20+-0.04083263*hcgd1_21+-0.8064821*hcgd1_22+-0.8100279*hcgd1_23+-0.40088394*hcgd1_24+-0.46441332);\n",
        "hcgd2_4 = tanh(1.8506681*hcgd1_0+-0.9750525*hcgd1_1+0.003659014*hcgd1_2+-0.6878417*hcgd1_3+-0.5547116*hcgd1_4+0.1308772*hcgd1_5+0.45871595*hcgd1_6+0.15744588*hcgd1_7+0.26718134*hcgd1_8+0.9983965*hcgd1_9+0.5910452*hcgd1_10+0.37368506*hcgd1_11+0.029024748*hcgd1_12+-1.3510306*hcgd1_13+0.0152292065*hcgd1_14+-0.35220924*hcgd1_15+-0.6619339*hcgd1_16+1.342967*hcgd1_17+0.46826208*hcgd1_18+-0.29376435*hcgd1_19+-0.3991243*hcgd1_20+-0.015383459*hcgd1_21+0.85965115*hcgd1_22+0.60075265*hcgd1_23+0.18427542*hcgd1_24+-0.32940418);\n",
        "hcgd2_5 = tanh(-0.8664879*hcgd1_0+0.44505116*hcgd1_1+0.013057195*hcgd1_2+-0.0031339983*hcgd1_3+0.19921027*hcgd1_4+-0.22984676*hcgd1_5+0.3673192*hcgd1_6+0.56390464*hcgd1_7+-0.034784332*hcgd1_8+-0.4111533*hcgd1_9+-0.118535414*hcgd1_10+0.5688084*hcgd1_11+-0.19346042*hcgd1_12+0.26538432*hcgd1_13+0.0061995205*hcgd1_14+-0.10955226*hcgd1_15+0.015352882*hcgd1_16+-0.16454482*hcgd1_17+0.32237643*hcgd1_18+0.088658795*hcgd1_19+0.20537783*hcgd1_20+0.0050276928*hcgd1_21+-0.015647996*hcgd1_22+-0.2880599*hcgd1_23+0.2548945*hcgd1_24+-0.16771647);\n",
        "hcgd2_6 = tanh(1.1495414*hcgd1_0+-0.22627151*hcgd1_1+0.015214143*hcgd1_2+0.27357554*hcgd1_3+-0.36256093*hcgd1_4+-0.92208475*hcgd1_5+0.24147213*hcgd1_6+0.38718736*hcgd1_7+-0.41560674*hcgd1_8+0.94879586*hcgd1_9+-0.5408847*hcgd1_10+0.32851028*hcgd1_11+0.040295456*hcgd1_12+-0.033835553*hcgd1_13+0.011123846*hcgd1_14+0.52912486*hcgd1_15+0.03981201*hcgd1_16+-0.30909863*hcgd1_17+0.3929471*hcgd1_18+0.32624614*hcgd1_19+0.8135803*hcgd1_20+0.009368981*hcgd1_21+-0.51490057*hcgd1_22+-0.45729175*hcgd1_23+0.26873454*hcgd1_24+-0.01673438);\n",
        "hcgd2_7 = tanh(-3.966802*hcgd1_0+0.042795368*hcgd1_1+0.054317914*hcgd1_2+-0.91404206*hcgd1_3+1.1362458*hcgd1_4+-0.07761897*hcgd1_5+1.5529021*hcgd1_6+-0.48278892*hcgd1_7+-0.5192229*hcgd1_8+-0.9233961*hcgd1_9+-0.73843503*hcgd1_10+2.1569836*hcgd1_11+0.7887896*hcgd1_12+1.366921*hcgd1_13+0.07727752*hcgd1_14+0.38954172*hcgd1_15+-0.47272664*hcgd1_16+-0.4188385*hcgd1_17+1.5436481*hcgd1_18+0.1914369*hcgd1_19+-0.024450772*hcgd1_20+0.010537377*hcgd1_21+0.051735442*hcgd1_22+2.4417806*hcgd1_23+-0.50142026*hcgd1_24+0.95721453);\n",
        "hcgd2_8 = tanh(-0.11731646*hcgd1_0+0.08615463*hcgd1_1+0.013967979*hcgd1_2+-0.3801745*hcgd1_3+0.26075113*hcgd1_4+0.05496182*hcgd1_5+0.26613086*hcgd1_6+0.053695574*hcgd1_7+-0.28042397*hcgd1_8+-0.03794526*hcgd1_9+-0.0068770945*hcgd1_10+0.30859926*hcgd1_11+-0.0229647*hcgd1_12+0.025560087*hcgd1_13+0.017060574*hcgd1_14+-0.25769773*hcgd1_15+0.06637153*hcgd1_16+0.07093841*hcgd1_17+-0.5513341*hcgd1_18+-0.19939162*hcgd1_19+0.08702792*hcgd1_20+0.09092291*hcgd1_21+0.31653336*hcgd1_22+-0.09402478*hcgd1_23+-0.2813343*hcgd1_24+-0.00752968);\n",
        "hcgd2_9 = tanh(0.110342234*hcgd1_0+-0.0826157*hcgd1_1+-0.14750138*hcgd1_2+-0.24709061*hcgd1_3+-0.25432354*hcgd1_4+-0.09893609*hcgd1_5+-0.015331019*hcgd1_6+0.19696374*hcgd1_7+-0.019382935*hcgd1_8+0.08544908*hcgd1_9+-0.09507841*hcgd1_10+0.11304415*hcgd1_11+0.2705397*hcgd1_12+-0.04126459*hcgd1_13+-0.1613513*hcgd1_14+0.3283118*hcgd1_15+-0.4909294*hcgd1_16+-0.066513576*hcgd1_17+-0.17823151*hcgd1_18+-0.2690935*hcgd1_19+-0.1837275*hcgd1_20+-0.11801412*hcgd1_21+-0.010364476*hcgd1_22+0.088502504*hcgd1_23+-0.10050535*hcgd1_24+-0.06808074);\n",
        "hcgd2_10 = tanh(-4.403549*hcgd1_0+-2.471523*hcgd1_1+-0.00089143997*hcgd1_2+-0.70052093*hcgd1_3+1.5336959*hcgd1_4+0.034311388*hcgd1_5+0.47960278*hcgd1_6+0.48460782*hcgd1_7+0.14602353*hcgd1_8+-2.0514007*hcgd1_9+0.85300875*hcgd1_10+0.6684621*hcgd1_11+-0.06943985*hcgd1_12+-0.34655583*hcgd1_13+0.024638627*hcgd1_14+-0.07107706*hcgd1_15+-0.6265861*hcgd1_16+-1.1899346*hcgd1_17+0.758323*hcgd1_18+0.078832895*hcgd1_19+-0.3047711*hcgd1_20+0.00015650672*hcgd1_21+0.56323916*hcgd1_22+0.5847528*hcgd1_23+0.07073806*hcgd1_24+-0.15086874);\n",
        "hcgd2_11 = tanh(0.47815424*hcgd1_0+0.048465684*hcgd1_1+-0.002923328*hcgd1_2+-0.37546745*hcgd1_3+0.66585714*hcgd1_4+0.09604421*hcgd1_5+-1.2400401*hcgd1_6+-1.5856158*hcgd1_7+-0.16395986*hcgd1_8+-0.12747197*hcgd1_9+0.029356737*hcgd1_10+0.10224382*hcgd1_11+0.76213807*hcgd1_12+-1.8363391*hcgd1_13+-0.008269408*hcgd1_14+0.2678933*hcgd1_15+0.46798977*hcgd1_16+-1.9006412*hcgd1_17+-0.4502235*hcgd1_18+-0.09107671*hcgd1_19+-0.2680099*hcgd1_20+-0.010051073*hcgd1_21+0.050625626*hcgd1_22+1.8481784*hcgd1_23+-0.6646883*hcgd1_24+0.9775716);\n",
        "y_cgd = -0.19379665*hcgd2_0+0.0061848215*hcgd2_1+-0.32997322*hcgd2_2+0.33700317*hcgd2_3+0.19574322*hcgd2_4+0.55552804*hcgd2_5+-0.18057454*hcgd2_6+-0.18055588*hcgd2_7+0.006111857*hcgd2_8+-0.006008425*hcgd2_9+-0.27025312*hcgd2_10+0.58404464*hcgd2_11+-0.059968084;\n",
        "\n",
        "hcgs1_0 = tanh(0.003866898*Vgs_cgs+-0.0061709154*Vds_cgs+0.07367021*Lg_cgs+0.005350336);\n",
        "hcgs1_1 = tanh(0.0038147226*Vgs_cgs+-0.006049334*Vds_cgs+-0.17519544*Lg_cgs+0.005365436);\n",
        "hcgs1_2 = tanh(-0.14925069*Vgs_cgs+0.31913087*Vds_cgs+-0.3709501*Lg_cgs+0.94486016);\n",
        "hcgs1_3 = tanh(-5.7050853*Vgs_cgs+3.0125842*Vds_cgs+-0.3576562*Lg_cgs+0.6098174);\n",
        "hcgs1_4 = tanh(0.73557264*Vgs_cgs+0.17232454*Vds_cgs+-0.408028*Lg_cgs+-0.41273043);\n",
        "hcgs1_5 = tanh(0.47328284*Vgs_cgs+-0.25681034*Vds_cgs+0.16284285*Lg_cgs+-0.033205096);\n",
        "hcgs1_6 = tanh(1.8763449*Vgs_cgs+-0.9621107*Vds_cgs+0.24538922*Lg_cgs+0.024004985);\n",
        "hcgs1_7 = tanh(-1.5798868*Vgs_cgs+1.9016485*Vds_cgs+-0.26374397*Lg_cgs+-1.0214508);\n",
        "hcgs1_8 = tanh(1.8300656*Vgs_cgs+-0.45609108*Vds_cgs+-0.11383694*Lg_cgs+-0.69372463);\n",
        "hcgs1_9 = tanh(0.0038994814*Vgs_cgs+-0.0062413956*Vds_cgs+0.16813117*Lg_cgs+0.005379745);\n",
        "hcgs1_10 = tanh(1.6561933*Vgs_cgs+-0.09678654*Vds_cgs+0.4255832*Lg_cgs+-1.0611489);\n",
        "hcgs1_11 = tanh(-0.28797054*Vgs_cgs+0.13389988*Vds_cgs+0.43625653*Lg_cgs+0.033546243);\n",
        "hcgs1_12 = tanh(0.8129892*Vgs_cgs+-0.029293621*Vds_cgs+0.1894619*Lg_cgs+-0.4490194);\n",
        "hcgs1_13 = tanh(0.008404021*Vgs_cgs+-0.5041153*Vds_cgs+0.29847372*Lg_cgs+-0.1161287);\n",
        "hcgs1_14 = tanh(0.32958084*Vgs_cgs+-0.30009958*Vds_cgs+-0.22462274*Lg_cgs+0.0048046284);\n",
        "hcgs1_15 = tanh(2.3854203*Vgs_cgs+-3.2975893*Vds_cgs+0.042711172*Lg_cgs+0.53147864);\n",
        "hcgs1_16 = tanh(0.0039317203*Vgs_cgs+-0.00632388*Vds_cgs+0.28598666*Lg_cgs+0.0053823735);\n",
        "hcgs1_17 = tanh(0.0037897201*Vgs_cgs+-0.005991189*Vds_cgs+-0.37188497*Lg_cgs+0.0053383214);\n",
        "hcgs1_18 = tanh(1.8098192*Vgs_cgs+-2.3450627*Vds_cgs+-0.15223776*Lg_cgs+1.3056793);\n",
        "hcgs1_19 = tanh(-0.01902712*Vgs_cgs+-2.2522266*Vds_cgs+0.42196727*Lg_cgs+1.2842451);\n",
        "hcgs1_20 = tanh(-1.3350194*Vgs_cgs+0.4994975*Vds_cgs+-0.17475094*Lg_cgs+0.18653236);\n",
        "hcgs1_21 = tanh(-4.302233*Vgs_cgs+-3.9769611*Vds_cgs+0.07871075*Lg_cgs+3.110047);\n",
        "hcgs1_22 = tanh(0.023444308*Vgs_cgs+-0.58594865*Vds_cgs+-0.02238914*Lg_cgs+-0.052440178);\n",
        "hcgs1_23 = tanh(0.15485905*Vgs_cgs+1.2359923*Vds_cgs+-0.15997769*Lg_cgs+-0.2019675);\n",
        "hcgs1_24 = tanh(-0.3575714*Vgs_cgs+-2.5208488*Vds_cgs+-0.1896365*Lg_cgs+2.4808133);\n",
        "hcgs2_0 = tanh(0.023202578*hcgs1_0+0.022973271*hcgs1_1+-0.06634169*hcgs1_2+0.7479778*hcgs1_3+-0.29115394*hcgs1_4+0.5281517*hcgs1_5+1.4476869*hcgs1_6+-0.57092524*hcgs1_7+1.342318*hcgs1_8+0.023113564*hcgs1_9+0.5039525*hcgs1_10+-0.28507996*hcgs1_11+0.33107176*hcgs1_12+0.59429955*hcgs1_13+-0.063477024*hcgs1_14+1.4652793*hcgs1_15+0.0230467*hcgs1_16+0.02331026*hcgs1_17+0.067940354*hcgs1_18+1.9354934*hcgs1_19+-0.6783489*hcgs1_20+-0.3415898*hcgs1_21+0.0735886*hcgs1_22+-1.0008856*hcgs1_23+0.009945778*hcgs1_24+-0.2625346);\n",
        "hcgs2_1 = tanh(0.044127226*hcgs1_0+0.045110825*hcgs1_1+-1.5823554*hcgs1_2+-2.1134062*hcgs1_3+0.279113*hcgs1_4+1.0231125*hcgs1_5+-0.47619388*hcgs1_6+0.64811915*hcgs1_7+-3.5616899*hcgs1_8+0.043715417*hcgs1_9+-0.96364224*hcgs1_10+0.93940836*hcgs1_11+0.90396035*hcgs1_12+2.3027606*hcgs1_13+2.184711*hcgs1_14+1.2926208*hcgs1_15+0.043070372*hcgs1_16+0.04556742*hcgs1_17+-0.99922925*hcgs1_18+0.5443892*hcgs1_19+1.7640798*hcgs1_20+0.57941544*hcgs1_21+3.1087935*hcgs1_22+-0.946341*hcgs1_23+-0.936913*hcgs1_24+-1.4875553);\n",
        "hcgs2_2 = tanh(0.005477332*hcgs1_0+0.0053486465*hcgs1_1+0.047083233*hcgs1_2+0.5397506*hcgs1_3+-0.075822465*hcgs1_4+0.036040183*hcgs1_5+-0.21352278*hcgs1_6+0.89393115*hcgs1_7+-1.1001998*hcgs1_8+0.005473618*hcgs1_9+-1.1550146*hcgs1_10+-0.070924945*hcgs1_11+-0.16800475*hcgs1_12+-0.1525137*hcgs1_13+-0.06899515*hcgs1_14+-0.051954325*hcgs1_15+0.0054896707*hcgs1_16+0.0054276832*hcgs1_17+-1.158984*hcgs1_18+-0.026913341*hcgs1_19+0.21477942*hcgs1_20+-0.3370402*hcgs1_21+-0.12709796*hcgs1_22+0.5450149*hcgs1_23+-0.19750577*hcgs1_24+-0.45452434);\n",
        "hcgs2_3 = tanh(0.031963654*hcgs1_0+0.03035919*hcgs1_1+-0.26406923*hcgs1_2+2.4108412*hcgs1_3+0.6478258*hcgs1_4+-0.2639667*hcgs1_5+0.15820932*hcgs1_6+0.36013365*hcgs1_7+-0.07805458*hcgs1_8+0.032406412*hcgs1_9+0.15034798*hcgs1_10+0.04711759*hcgs1_11+0.51302767*hcgs1_12+-0.9524351*hcgs1_13+0.05383534*hcgs1_14+-2.1840389*hcgs1_15+0.033131413*hcgs1_16+0.030272223*hcgs1_17+-0.70434415*hcgs1_18+-1.3847973*hcgs1_19+0.055561617*hcgs1_20+-2.2977886*hcgs1_21+-1.2528052*hcgs1_22+1.6460723*hcgs1_23+0.52855104*hcgs1_24+0.20310849);\n",
        "hcgs2_4 = tanh(-0.004114148*hcgs1_0+-0.0036210776*hcgs1_1+-0.25540155*hcgs1_2+-0.18470578*hcgs1_3+-0.12067126*hcgs1_4+-0.7038924*hcgs1_5+0.21889566*hcgs1_6+-0.17658989*hcgs1_7+0.1299486*hcgs1_8+-0.0041041495*hcgs1_9+-0.4014189*hcgs1_10+0.20044085*hcgs1_11+-0.19608615*hcgs1_12+0.02296377*hcgs1_13+-0.013663749*hcgs1_14+0.4224307*hcgs1_15+-0.004211446*hcgs1_16+-0.003838226*hcgs1_17+-0.2911345*hcgs1_18+0.78450745*hcgs1_19+0.42747602*hcgs1_20+-0.07747369*hcgs1_21+-0.27040684*hcgs1_22+0.11331162*hcgs1_23+-0.20715222*hcgs1_24+-0.017163113);\n",
        "hcgs2_5 = tanh(0.011508604*hcgs1_0+0.011453037*hcgs1_1+1.7924933*hcgs1_2+-1.8155463*hcgs1_3+-0.25609693*hcgs1_4+-0.758906*hcgs1_5+-2.4812367*hcgs1_6+4.0513716*hcgs1_7+-2.5917294*hcgs1_8+0.011474304*hcgs1_9+-3.2454631*hcgs1_10+0.093522236*hcgs1_11+-1.3182486*hcgs1_12+-0.8882851*hcgs1_13+-0.9181794*hcgs1_14+1.3970197*hcgs1_15+0.011446857*hcgs1_16+0.011560462*hcgs1_17+-5.1778355*hcgs1_18+-0.2236407*hcgs1_19+2.0796807*hcgs1_20+5.9254537*hcgs1_21+-0.72559786*hcgs1_22+0.45175433*hcgs1_23+-3.1938994*hcgs1_24+0.9409381);\n",
        "hcgs2_6 = tanh(0.060208883*hcgs1_0+0.057784908*hcgs1_1+-0.4230708*hcgs1_2+0.31358114*hcgs1_3+0.5314662*hcgs1_4+-1.1043811*hcgs1_5+-0.30330408*hcgs1_6+0.5861162*hcgs1_7+-0.2730255*hcgs1_8+0.0608772*hcgs1_9+1.2918353*hcgs1_10+0.5573929*hcgs1_11+0.96025693*hcgs1_12+-1.5764415*hcgs1_13+-1.0071468*hcgs1_14+-2.9516594*hcgs1_15+0.061963964*hcgs1_16+0.057676923*hcgs1_17+-0.12257842*hcgs1_18+-1.6470038*hcgs1_19+0.9465945*hcgs1_20+-0.93217355*hcgs1_21+-2.438695*hcgs1_22+1.0968347*hcgs1_23+0.16981499*hcgs1_24+0.42413175);\n",
        "hcgs2_7 = tanh(-0.008120307*hcgs1_0+-0.008280249*hcgs1_1+0.17085399*hcgs1_2+1.1467992*hcgs1_3+2.214014*hcgs1_4+1.1809759*hcgs1_5+7.2768965*hcgs1_6+-4.8417068*hcgs1_7+5.984158*hcgs1_8+-0.0075962003*hcgs1_9+2.7736754*hcgs1_10+-1.9165568*hcgs1_11+0.4659055*hcgs1_12+-1.3562847*hcgs1_13+0.48391283*hcgs1_14+-2.8675776*hcgs1_15+-0.007374848*hcgs1_16+-0.00872389*hcgs1_17+-0.1205414*hcgs1_18+-1.7518774*hcgs1_19+-3.7752948*hcgs1_20+0.27624342*hcgs1_21+-1.239815*hcgs1_22+-0.11851393*hcgs1_23+4.030906*hcgs1_24+0.9663557);\n",
        "hcgs2_8 = tanh(-0.06382305*hcgs1_0+-0.07465671*hcgs1_1+-0.17580509*hcgs1_2+0.039555147*hcgs1_3+-0.3423838*hcgs1_4+0.40099683*hcgs1_5+0.08354397*hcgs1_6+0.049524635*hcgs1_7+0.010591255*hcgs1_8+-0.0655947*hcgs1_9+-0.09075449*hcgs1_10+0.21120805*hcgs1_11+0.18817192*hcgs1_12+-0.37621862*hcgs1_13+-0.11487423*hcgs1_14+0.025681255*hcgs1_15+-0.06386206*hcgs1_16+-0.068342805*hcgs1_17+-0.008454653*hcgs1_18+0.015086431*hcgs1_19+-0.13472116*hcgs1_20+0.025698638*hcgs1_21+0.0998438*hcgs1_22+0.20212452*hcgs1_23+0.05868889*hcgs1_24+-0.10602957);\n",
        "hcgs2_9 = tanh(0.019189388*hcgs1_0+0.01920859*hcgs1_1+-0.012194334*hcgs1_2+-1.4130874*hcgs1_3+0.30251437*hcgs1_4+-0.10520868*hcgs1_5+1.0161169*hcgs1_6+0.4738982*hcgs1_7+1.3726546*hcgs1_8+0.019165223*hcgs1_9+0.6629045*hcgs1_10+-0.17346822*hcgs1_11+-0.41499868*hcgs1_12+-0.2930857*hcgs1_13+-0.3305122*hcgs1_14+-0.82226783*hcgs1_15+0.01908325*hcgs1_16+0.019351097*hcgs1_17+0.09138516*hcgs1_18+-1.2453144*hcgs1_19+-0.88000125*hcgs1_20+0.48374525*hcgs1_21+-0.2261362*hcgs1_22+0.6680502*hcgs1_23+0.22479443*hcgs1_24+-0.056186553);\n",
        "hcgs2_10 = tanh(0.0064860974*hcgs1_0+0.00648306*hcgs1_1+0.48692945*hcgs1_2+2.3550978*hcgs1_3+0.43859667*hcgs1_4+0.27351967*hcgs1_5+0.90961814*hcgs1_6+-0.017722035*hcgs1_7+0.5930781*hcgs1_8+0.0064157885*hcgs1_9+0.4308463*hcgs1_10+-0.28399688*hcgs1_11+0.30260938*hcgs1_12+-0.39435783*hcgs1_13+-0.1002725*hcgs1_14+-2.053321*hcgs1_15+0.006337951*hcgs1_16+0.006645003*hcgs1_17+-1.2891791*hcgs1_18+0.05721846*hcgs1_19+-0.37843162*hcgs1_20+0.5648725*hcgs1_21+-0.6181118*hcgs1_22+1.1590537*hcgs1_23+-2.7134886*hcgs1_24+0.54398286);\n",
        "hcgs2_11 = tanh(0.14230382*hcgs1_0+0.123352334*hcgs1_1+0.042968668*hcgs1_2+0.047534492*hcgs1_3+0.010582293*hcgs1_4+-0.06643509*hcgs1_5+-0.13723876*hcgs1_6+-0.12684533*hcgs1_7+-0.040209897*hcgs1_8+0.13827254*hcgs1_9+-0.17713757*hcgs1_10+-0.35476053*hcgs1_11+-0.114496075*hcgs1_12+0.2810919*hcgs1_13+0.584066*hcgs1_14+0.017237866*hcgs1_15+0.14104304*hcgs1_16+0.13508707*hcgs1_17+-0.010557453*hcgs1_18+-0.1474484*hcgs1_19+-0.29602262*hcgs1_20+0.028033255*hcgs1_21+-0.22550379*hcgs1_22+-0.04313099*hcgs1_23+-0.11113166*hcgs1_24+-0.028289912);\n",
        "y_cgs = -0.090056084*hcgs2_0+-0.31736207*hcgs2_1+-0.24686596*hcgs2_2+-0.14114563*hcgs2_3+-0.03698175*hcgs2_4+-0.20639718*hcgs2_5+0.0935527*hcgs2_6+-0.028302789*hcgs2_7+0.0030376983*hcgs2_8+0.07618971*hcgs2_9+0.121289134*hcgs2_10+-1.5651603e-05*hcgs2_11+0.0065943017;\n",
        "\n",
        "\n",
        "h1_0 = tanh(0.0067332466*Vgs+0.0014811404*Vds+-0.089835726*Lg+0.09066404);\n",
        "h1_1 = tanh(0.11106648*Vgs+0.40663162*Vds+0.0077667693*Lg+-0.10081332);\n",
        "h1_2 = tanh(1.5560061*Vgs+2.696172*Vds+-0.2839149*Lg+-0.3486124);\n",
        "h1_3 = tanh(-2.0344324*Vgs+-0.28681883*Vds+0.8085338*Lg+0.0026162667);\n",
        "h1_4 = tanh(-0.03341659*Vgs+-0.15408932*Vds+-0.015416802*Lg+0.04370213);\n",
        "h1_5 = tanh(1.4746779*Vgs+0.33599785*Vds+-0.2669822*Lg+-0.42193142);\n",
        "h1_6 = tanh(-0.16598122*Vgs+-2.3765175*Vds+0.11762294*Lg+0.019988127);\n",
        "h1_7 = tanh(4.3143744*Vgs+-0.5703343*Vds+-0.3085829*Lg+-0.50043374);\n",
        "h1_8 = tanh(-0.4936719*Vgs+36.42876*Vds+0.32744452*Lg+0.24776502);\n",
        "h1_9 = tanh(0.4869826*Vgs+-0.13276859*Vds+0.28959593*Lg+-0.3828168);\n",
        "h1_10 = tanh(0.000804813*Vgs+0.0004923456*Vds+-0.41243985*Lg+0.41356155);\n",
        "h1_11 = tanh(2.6073072*Vgs+-0.56419706*Vds+-0.111921*Lg+-0.37480572);\n",
        "h1_12 = tanh(0.0020444687*Vgs+0.0014842966*Vds+0.21013871*Lg+-0.20708004);\n",
        "h1_13 = tanh(-2.6335895*Vgs+0.28439838*Vds+0.321686*Lg+0.41039804);\n",
        "h1_14 = tanh(0.0034599572*Vgs+0.0084134*Vds+-0.034978658*Lg+0.037748303);\n",
        "h1_15 = tanh(0.0025394042*Vgs+0.00083049946*Vds+-0.13070253*Lg+0.13411763);\n",
        "h1_16 = tanh(-2.1379406*Vgs+-0.57477415*Vds+0.19879702*Lg+0.6043196);\n",
        "h1_17 = tanh(-0.9647693*Vgs+3.0132833*Vds+0.13054319*Lg+0.38879853);\n",
        "h1_18 = tanh(-1.6564686*Vgs+-0.06445829*Vds+0.39810786*Lg+0.21570432);\n",
        "h1_19 = tanh(-0.120745726*Vgs+-0.37977916*Vds+-0.20842548*Lg+0.33059505);\n",
        "h1_20 = tanh(-0.5346294*Vgs+-0.24575421*Vds+0.42274392*Lg+-0.095065445);\n",
        "h1_21 = tanh(-0.17100386*Vgs+-0.36405364*Vds+-0.32086778*Lg+-0.45500055);\n",
        "h1_22 = tanh(-0.3456514*Vgs+-0.43013823*Vds+-0.14385213*Lg+0.4731826);\n",
        "h1_23 = tanh(0.28758252*Vgs+-0.55982685*Vds+0.44342747*Lg+-0.34977767);\n",
        "h1_24 = tanh(0.08224837*Vgs+0.35958192*Vds+0.059084654*Lg+-0.1397865);\n",
        "\n",
        "h2_0 = tanh(-0.8091375*h1_0+-0.43552017*h1_1+-0.8155236*h1_2+0.24130166*h1_3+0.14224924*h1_4+-0.62978196*h1_5+0.6101532*h1_6+-0.43199107*h1_7+-2.3122218*h1_8+-0.23163876*h1_9+0.068008065*h1_10+-0.03888623*h1_11+-0.014134661*h1_12+-0.06591678*h1_13+0.022440521*h1_14+-0.27041718*h1_15+0.090823404*h1_16+-0.26772383*h1_17+0.47534344*h1_18+0.73750144*h1_19+0.50188285*h1_20+0.3184862*h1_21+0.3608627*h1_22+0.79056877*h1_23+-0.037502155*h1_24+-0.24878933);\n",
        "h2_1 = tanh(-0.03261362*h1_0+-0.06024928*h1_1+-0.10186445*h1_2+0.018033225*h1_3+0.51713187*h1_4+0.022300791*h1_5+0.015979188*h1_6+0.056999434*h1_7+0.035186633*h1_8+0.25107002*h1_9+-0.012781056*h1_10+-0.3380685*h1_11+0.05043952*h1_12+-0.24957459*h1_13+0.024472937*h1_14+-0.08896992*h1_15+0.053890992*h1_16+-0.0101861*h1_17+-0.031921674*h1_18+0.17910303*h1_19+-0.03255347*h1_20+-0.38824677*h1_21+-0.14763784*h1_22+-0.24868536*h1_23+-0.024086645*h1_24+-0.18488966);\n",
        "h2_2 = tanh(0.12152682*h1_0+0.009792632*h1_1+-0.29032546*h1_2+0.5965219*h1_3+-0.021902572*h1_4+-0.5883061*h1_5+0.17799598*h1_6+-0.92929643*h1_7+0.32916892*h1_8+-0.2568552*h1_9+-0.067421295*h1_10+-1.1449344*h1_11+0.007887112*h1_12+0.276745*h1_13+-0.033592574*h1_14+0.1040227*h1_15+0.23562248*h1_16+-0.18375605*h1_17+1.0453725*h1_18+0.33611628*h1_19+0.6393779*h1_20+0.06726488*h1_21+0.7879763*h1_22+-0.14922209*h1_23+-0.18613423*h1_24+0.28223407);\n",
        "h2_3 = tanh(-0.18859175*h1_0+-0.9337733*h1_1+-0.938285*h1_2+0.7002026*h1_3+0.6061784*h1_4+-0.52529997*h1_5+0.71481675*h1_6+-0.38263503*h1_7+-0.5072332*h1_8+0.1263292*h1_9+0.04664359*h1_10+-0.5296314*h1_11+-0.033964746*h1_12+0.6460025*h1_13+-0.0231704*h1_14+-0.38591123*h1_15+0.0872077*h1_16+-0.5042024*h1_17+0.47931066*h1_18+0.50751704*h1_19+-0.063635096*h1_20+0.018421784*h1_21+0.08422998*h1_22+0.30641475*h1_23+-0.524482*h1_24+-0.26979908);\n",
        "h2_4 = tanh(-0.034289524*h1_0+-0.11348127*h1_1+0.48059276*h1_2+-1.123194*h1_3+0.12442129*h1_4+0.49476302*h1_5+0.0863118*h1_6+0.47754285*h1_7+0.2202004*h1_8+0.41435316*h1_9+0.028558401*h1_10+0.36081842*h1_11+-0.008486773*h1_12+-0.9903806*h1_13+0.015499473*h1_14+-0.063613005*h1_15+-1.4330287*h1_16+-0.74456465*h1_17+-0.5569409*h1_18+-0.019323817*h1_19+-0.14176102*h1_20+0.49098673*h1_21+-0.057230044*h1_22+0.515492*h1_23+-0.13265039*h1_24+-0.18507467);\n",
        "h2_5 = tanh(-0.09933434*h1_0+-0.15035452*h1_1+-0.5133741*h1_2+-0.47027865*h1_3+0.21104498*h1_4+0.14945817*h1_5+0.10640489*h1_6+0.1787739*h1_7+0.060723394*h1_8+-0.5602644*h1_9+-0.0019350805*h1_10+0.5330786*h1_11+0.012645531*h1_12+-0.42403966*h1_13+0.018393166*h1_14+-0.00062784646*h1_15+-0.0052586636*h1_16+1.0866524*h1_17+0.078084014*h1_18+0.059300236*h1_19+0.32314536*h1_20+0.49137965*h1_21+0.09876712*h1_22+-0.583638*h1_23+-0.1759503*h1_24+-0.18313222);\n",
        "h2_6 = tanh(-0.1285073*h1_0+-1.032602*h1_1+-1.224141*h1_2+0.14631094*h1_3+0.65741867*h1_4+-0.22908972*h1_5+0.63241625*h1_6+-0.88521755*h1_7+0.09097795*h1_8+0.09149886*h1_9+0.027507082*h1_10+-0.63233715*h1_11+-0.02061426*h1_12+0.1104531*h1_13+-0.012734477*h1_14+-0.34623924*h1_15+-0.18294835*h1_16+-0.045275174*h1_17+0.35704163*h1_18+0.5909152*h1_19+0.073851526*h1_20+0.49004716*h1_21+0.19812724*h1_22+0.3784807*h1_23+-0.6437184*h1_24+-0.3594689);\n",
        "h2_7 = tanh(0.119550616*h1_0+-0.035170615*h1_1+-0.60996103*h1_2+0.40164798*h1_3+-0.007347775*h1_4+-0.30736676*h1_5+-0.13570982*h1_6+0.9896757*h1_7+-4.0502048*h1_8+0.053264104*h1_9+-0.012164457*h1_10+0.48190302*h1_11+0.014136277*h1_12+-0.5162279*h1_13+0.014227704*h1_14+0.05181294*h1_15+0.7717682*h1_16+-0.111988194*h1_17+0.062060844*h1_18+-0.044897318*h1_19+-0.15565042*h1_20+0.24919312*h1_21+-0.07993477*h1_22+-0.09538691*h1_23+-0.0052977665*h1_24+-0.10968423);\n",
        "h2_8 = tanh(0.079706475*h1_0+-0.5814533*h1_1+0.6996235*h1_2+-0.16522586*h1_3+0.630409*h1_4+0.29656532*h1_5+-0.5331364*h1_6+0.114606544*h1_7+-0.21106683*h1_8+0.0467022*h1_9+-0.06535354*h1_10+-0.24001631*h1_11+0.059368264*h1_12+0.15451497*h1_13+0.053622272*h1_14+0.12981157*h1_15+0.5445277*h1_16+0.24295534*h1_17+-0.26532695*h1_18+0.56733835*h1_19+0.12550347*h1_20+0.048389148*h1_21+0.5622483*h1_22+0.6891362*h1_23+-0.76375955*h1_24+-0.2676767);\n",
        "h2_9 = tanh(-0.03992191*h1_0+0.09928779*h1_1+1.0056654*h1_2+-2.3181243*h1_3+-0.0916434*h1_4+2.143644*h1_5+0.19818373*h1_6+2.8020356*h1_7+-0.47644797*h1_8+0.24747562*h1_9+0.02024143*h1_10+1.3736787*h1_11+0.006931587*h1_12+-3.888161*h1_13+0.018778825*h1_14+-0.0041158814*h1_15+-1.5533967*h1_16+-0.71390355*h1_17+-2.8108795*h1_18+-0.24579133*h1_19+-1.128639*h1_20+0.2231992*h1_21+-1.2298777*h1_22+0.043120787*h1_23+0.403965*h1_24+-0.17281148);\n",
        "h2_10 = tanh(0.35093632*h1_0+-0.05374117*h1_1+0.16745348*h1_2+-0.19781992*h1_3+-0.07950959*h1_4+0.29579905*h1_5+-0.5073861*h1_6+-0.14164843*h1_7+0.70651436*h1_8+0.29345548*h1_9+-0.054172043*h1_10+-0.28257352*h1_11+0.033599235*h1_12+0.017732412*h1_13+0.02154612*h1_14+0.1586723*h1_15+-0.23733242*h1_16+0.43727452*h1_17+-0.084653825*h1_18+-0.054423757*h1_19+-0.36601782*h1_20+-0.07235579*h1_21+-0.03668117*h1_22+-0.05778476*h1_23+-0.020711297*h1_24+0.24233255);\n",
        "h2_11 = tanh(0.009613743*h1_0+-0.26873523*h1_1+-0.0937895*h1_2+-0.17233954*h1_3+0.2600588*h1_4+-0.41362444*h1_5+-0.13627699*h1_6+-0.03051018*h1_7+0.028052136*h1_8+0.06536644*h1_9+-0.029295161*h1_10+-0.017168272*h1_11+0.016685432*h1_12+-0.15576635*h1_13+0.08295777*h1_14+-0.16339928*h1_15+-0.069266655*h1_16+-0.07842072*h1_17+0.22933975*h1_18+-0.07057512*h1_19+0.172754*h1_20+-0.21173981*h1_21+-0.8436922*h1_22+0.28658*h1_23+-0.018899215*h1_24+-0.11145156);\n",
        "y = 0.13345447*h2_0+0.0023934222*h2_1+0.1793076*h2_2+0.51211137*h2_3+0.2699503*h2_4+0.11368197*h2_5+-0.5013421*h2_6+-0.61606425*h2_7+-0.037382632*h2_8+0.18645377*h2_9+0.32913932*h2_10+-0.00013747951*h2_11+-0.1796401;\n",
        "\n",
        "Cgd = pow(10, (y_cgd/normO_cgd + MinO_cgd))*W/15; //traning width was 15um\n",
        "Cgs = pow(10, (y_cgs/normO_cgs + MinO_cgs))*W/15;\n",
        "Cgg = Cgd+Cgs;\n",
        "\n",
        "Id = pow(10, (y/normI + MinI))*W*3;\n",
        "I(g, d) <+ Cgd*ddt(Vg-Vd) ;\n",
        "I(g, s) <+ Cgs*ddt(Vg-Vs) ;\n",
        "\n",
        "if (Vd >= Vs) begin\n",
        "\tI(d, s) <+ dir*Id;\n",
        "end\n",
        "\n",
        "else begin\n",
        "\tI(d, s) <+ dir*Id;\n",
        "end\n",
        "\n",
        "end\n",
        "endmodule\n",
        "\n",
        "\"\"\"\n",
        "print(verilog_code)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tTLQwbB_n8tW",
        "outputId": "ebe753a4-e975-46ab-9898-47927dc8f1b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "// VerilogA for ML_HV, ML_HV_emode, veriloga\n",
            "\n",
            "`include \"constants.vams\"\n",
            "`include \"disciplines.vams\"\n",
            "\n",
            "module ML_HV_emode_rev3(d, g, s);\n",
            "\n",
            "\n",
            "inout d, g, s;\n",
            "electrical d, g, s;\n",
            "\n",
            "//****** Parameters L and W ********\n",
            "parameter real W = 3;\n",
            "parameter real L = 0.6;\n",
            "parameter MinVg = -6.0 ;\n",
            "parameter normVg = 0.05555555555555555 ;\n",
            "parameter MinVd = 0.0 ;\n",
            "parameter normVd = 0.08333333333333333 ;\n",
            "parameter MinLg = 0 ;\n",
            "parameter normLg = 1.66666667 ;\n",
            "parameter MinI = -14.614393726401689 ;\n",
            "parameter normI =0.0975145951626748;\n",
            "parameter vth =0;\n",
            "parameter real T_stress = 1; //set on cadence as variable\n",
            "\n",
            "parameter MinVg_cgd = -6.0 ;\n",
            "parameter normVg_cgd = 0.05555555555555555 ;\n",
            "parameter MinVd_cgd = -6.0 ;\n",
            "parameter normVd_cgd = 0.05555555555555555 ;\n",
            "parameter MinLg_cgd = 0.6 ;\n",
            "parameter normLg_cgd = 0 ;\n",
            "parameter MinO_cgd = -13.926054483271312 ;\n",
            "parameter normO_cgd =1.4486107458852509 ;\n",
            "\n",
            "parameter MinVg_cgs = -6.0 ;\n",
            "parameter normVg_cgs = 0.05555555555555555 ;\n",
            "parameter MinVd_cgs = -6.0 ;\n",
            "parameter normVd_cgs = 0.05555555555555555 ;\n",
            "parameter MinLg_cgs = 0.6 ;\n",
            "parameter normLg_cgs = 0 ;\n",
            "parameter MinO_cgs = -13.08359343161434 ;\n",
            "parameter normO_cgs =2.886734269461609;\n",
            "\n",
            "\n",
            "parameter Mint_stress = 1 ;\n",
            "parameter normt_stress = 0.001001001001001001 ;\n",
            "parameter Mindelta_Vth = 0.0001376186203 ;\n",
            "parameter normdelta_Vth = 5.536926766020301 ;\n",
            "\n",
            "real hvth1_0, hvth1_1, hvth1_2, hvth1_3, hvth1_4, hvth1_5, hvth1_6, hvth1_7, hvth1_8, hvth1_9, hvth1_10, hvth1_11, hvth1_12, hvth1_13, hvth1_14, hvth1_15, hvth1_16, hvth1_17, hvth1_18, hvth1_19;\n",
            "real hvth2_0, hvth2_1, hvth2_2, hvth2_3, hvth2_4, hvth2_5, hvth2_6, hvth2_7, hvth2_8, hvth2_9;\n",
            "\n",
            "\n",
            "real Vg, Vd, Vs, Vgs, Vds, Lg, Id, Cgg, Cgs,Cgd, Vgd;\n",
            "real y, y_cgg, y_cgs, y_cgd, yvth;\n",
            "real Vgs_cgg, Vgs_cgs, Vgs_cgd;\n",
            "real Vds_cgg, Vds_cgs, Vds_cgd;\n",
            "real Lg_cgg, Lg_cgs, Lg_cgd;\n",
            "real Vgsraw, Vgdraw, dir;\n",
            "real Vdi, Vsi;\n",
            "real t_stress, v_ov, t_rec, clk_loops, temp, delta_Vth;\n",
            "\n",
            "\n",
            "real h1_0, h1_1, h1_2, h1_3, h1_4, h1_5, h1_6, h1_7, h1_8, h1_9, h1_10, h1_11, h1_12, h1_13, h1_14, h1_15, h1_16, h1_17, h1_18, h1_19, h1_20, h1_21, h1_22, h1_23, h1_24;\n",
            "real h2_0, h2_1, h2_2, h2_3, h2_4, h2_5, h2_6, h2_7, h2_8, h2_9, h2_10, h2_11;\n",
            "real hcgg1_0, hcgg1_1, hcgg1_2, hcgg1_3, hcgg1_4, hcgg1_5, hcgg1_6, hcgg1_7, hcgg1_8, hcgg1_9, hcgg1_10, hcgg1_11, hcgg1_12, hcgg1_13, hcgg1_14, hcgg1_15, hcgg1_16, hcgg1_17, hcgg1_18, hcgg1_19, hcgg1_20, hcgg1_21, hcgg1_22, hcgg1_23, hcgg1_24;\n",
            "real hcgg2_0, hcgg2_1, hcgg2_2, hcgg2_3, hcgg2_4, hcgg2_5, hcgg2_6, hcgg2_7, hcgg2_8, hcgg2_9, hcgg2_10, hcgg2_11;\n",
            "real hcgs1_0, hcgs1_1, hcgs1_2, hcgs1_3, hcgs1_4, hcgs1_5, hcgs1_6, hcgs1_7, hcgs1_8, hcgs1_9, hcgs1_10, hcgs1_11, hcgs1_12, hcgs1_13, hcgs1_14, hcgs1_15, hcgs1_16, hcgs1_17, hcgs1_18, hcgs1_19, hcgs1_20, hcgs1_21, hcgs1_22, hcgs1_23, hcgs1_24;\n",
            "real hcgs2_0, hcgs2_1, hcgs2_2, hcgs2_3, hcgs2_4, hcgs2_5, hcgs2_6, hcgs2_7, hcgs2_8, hcgs2_9, hcgs2_10, hcgs2_11;\n",
            "real hcgd1_0, hcgd1_1, hcgd1_2, hcgd1_3, hcgd1_4, hcgd1_5, hcgd1_6, hcgd1_7, hcgd1_8, hcgd1_9, hcgd1_10, hcgd1_11, hcgd1_12, hcgd1_13, hcgd1_14, hcgd1_15, hcgd1_16, hcgd1_17, hcgd1_18, hcgd1_19, hcgd1_20, hcgd1_21, hcgd1_22, hcgd1_23, hcgd1_24;\n",
            "real hcgd2_0, hcgd2_1, hcgd2_2, hcgd2_3, hcgd2_4, hcgd2_5, hcgd2_6, hcgd2_7, hcgd2_8, hcgd2_9, hcgd2_10, hcgd2_11;\n",
            "\n",
            "real Qgd, Qgs;\n",
            "\n",
            "analog begin\n",
            "\n",
            "t_stress = (T_stress - Mint_stress)*normt_stress;\n",
            "\n",
            "//******************** delta_Vth NN **********************************//\n",
            "\n",
            "hvth1_0 = tanh(-0.08902381*t_stress+-0.02877941);\n",
            "hvth1_1 = tanh(-0.9805543*t_stress+0.07654902);\n",
            "hvth1_2 = tanh(-0.3272974*t_stress+-0.017245384);\n",
            "hvth1_3 = tanh(-0.32462993*t_stress+0.057347808);\n",
            "hvth1_4 = tanh(0.170284*t_stress+-0.01620857);\n",
            "hvth1_5 = tanh(0.28962135*t_stress+-0.22551627);\n",
            "hvth1_6 = tanh(0.3075697*t_stress+-0.09936674);\n",
            "hvth1_7 = tanh(-0.46912596*t_stress+-0.23915449);\n",
            "hvth1_8 = tanh(0.23457381*t_stress+-0.22893316);\n",
            "hvth1_9 = tanh(-0.08504618*t_stress+0.102921605);\n",
            "hvth1_10 = tanh(0.20243207*t_stress+0.04040044);\n",
            "hvth1_11 = tanh(1.4148953*t_stress+-0.046939168);\n",
            "hvth1_12 = tanh(0.37571445*t_stress+-0.001599293);\n",
            "hvth1_13 = tanh(0.26412547*t_stress+0.046315055);\n",
            "hvth1_14 = tanh(0.5870343*t_stress+0.29707295);\n",
            "hvth1_15 = tanh(-1.1441946*t_stress+0.5972823);\n",
            "hvth1_16 = tanh(0.41617376*t_stress+0.027398039);\n",
            "hvth1_17 = tanh(-0.21481094*t_stress+-0.115722775);\n",
            "hvth1_18 = tanh(-0.05717216*t_stress+-0.032057196);\n",
            "hvth1_19 = tanh(0.24605636*t_stress+0.07019752);\n",
            "\n",
            "hvth2_0 = tanh(-0.37824032*hvth1_0+0.36063355*hvth1_1+-0.050843965*hvth1_2+0.18169983*hvth1_3+0.41081133*hvth1_4+-0.1631092*hvth1_5+0.008637005*hvth1_6+0.3337049*hvth1_7+0.16362588*hvth1_8+-0.22735497*hvth1_9+0.3377043*hvth1_10+0.36564746*hvth1_11+-0.30234447*hvth1_12+0.5410738*hvth1_13+0.47233745*hvth1_14+0.39219067*hvth1_15+0.48034814*hvth1_16+-0.44867763*hvth1_17+0.023469396*hvth1_18+0.05936132*hvth1_19+0.103330426);\n",
            "hvth2_1 = tanh(-0.2933684*hvth1_0+-0.2670041*hvth1_1+0.34114814*hvth1_2+-0.13680184*hvth1_3+0.061296202*hvth1_4+-0.21814762*hvth1_5+0.42253903*hvth1_6+0.24422003*hvth1_7+0.14768882*hvth1_8+0.3043634*hvth1_9+0.43590027*hvth1_10+-0.5593143*hvth1_11+-0.06439271*hvth1_12+0.4134954*hvth1_13+0.33554152*hvth1_14+-0.029979402*hvth1_15+0.14697897*hvth1_16+-0.10653279*hvth1_17+0.41802663*hvth1_18+-0.10778337*hvth1_19+0.03992685);\n",
            "hvth2_2 = tanh(0.49943414*hvth1_0+0.20643882*hvth1_1+0.16127238*hvth1_2+-0.067875385*hvth1_3+-0.36919546*hvth1_4+-0.14033571*hvth1_5+0.12968932*hvth1_6+0.44781142*hvth1_7+0.12507576*hvth1_8+0.29903397*hvth1_9+-0.5068058*hvth1_10+-0.07714777*hvth1_11+-0.12633473*hvth1_12+0.06457266*hvth1_13+0.18052492*hvth1_14+-0.37112617*hvth1_15+0.11328478*hvth1_16+-0.15488367*hvth1_17+0.31600004*hvth1_18+-0.03760944*hvth1_19+-0.07037728);\n",
            "hvth2_3 = tanh(0.3679182*hvth1_0+-0.1350676*hvth1_1+-0.1556064*hvth1_2+0.020984443*hvth1_3+0.23208079*hvth1_4+-0.07176552*hvth1_5+-0.35853276*hvth1_6+0.25367975*hvth1_7+-0.04196241*hvth1_8+0.48432723*hvth1_9+-0.1489709*hvth1_10+-0.4073151*hvth1_11+0.10960967*hvth1_12+-0.4049704*hvth1_13+0.13219115*hvth1_14+-0.39715335*hvth1_15+0.050715216*hvth1_16+-0.00801248*hvth1_17+0.026351286*hvth1_18+-0.4913571*hvth1_19+-0.08783956);\n",
            "hvth2_4 = tanh(0.1492943*hvth1_0+-0.6838092*hvth1_1+-0.5804646*hvth1_2+-0.53963304*hvth1_3+0.3138992*hvth1_4+-0.17161499*hvth1_5+0.7642205*hvth1_6+-0.3222178*hvth1_7+-0.15535371*hvth1_8+-0.40373302*hvth1_9+0.64041406*hvth1_10+0.706982*hvth1_11+0.55537665*hvth1_12+0.60347897*hvth1_13+0.43557456*hvth1_14+-0.64767337*hvth1_15+0.77902716*hvth1_16+-0.32928184*hvth1_17+0.1546812*hvth1_18+0.5444298*hvth1_19+-0.0019257085);\n",
            "hvth2_5 = tanh(0.47242337*hvth1_0+-0.16221137*hvth1_1+-0.2629348*hvth1_2+0.07522843*hvth1_3+0.31304973*hvth1_4+0.4407804*hvth1_5+0.14050591*hvth1_6+-0.24794002*hvth1_7+0.12571554*hvth1_8+0.4030857*hvth1_9+0.2745989*hvth1_10+0.22144255*hvth1_11+-0.33955196*hvth1_12+-0.17866239*hvth1_13+-0.47900677*hvth1_14+-0.021329284*hvth1_15+0.045665257*hvth1_16+-0.14639777*hvth1_17+-0.08840843*hvth1_18+-0.054598916*hvth1_19+-0.04554429);\n",
            "hvth2_6 = tanh(-0.035631787*hvth1_0+0.39734858*hvth1_1+-0.42248422*hvth1_2+-0.18481956*hvth1_3+0.092905596*hvth1_4+0.44449165*hvth1_5+-0.2491092*hvth1_6+0.1111984*hvth1_7+0.40610996*hvth1_8+-0.010597389*hvth1_9+0.046394642*hvth1_10+-0.30463338*hvth1_11+0.3163014*hvth1_12+0.18814068*hvth1_13+0.17918336*hvth1_14+0.40600494*hvth1_15+0.25108597*hvth1_16+-0.4200695*hvth1_17+-0.22369212*hvth1_18+-0.034397062*hvth1_19+0.058213577);\n",
            "hvth2_7 = tanh(-0.19706054*hvth1_0+0.37250963*hvth1_1+-0.08196783*hvth1_2+0.096218795*hvth1_3+0.17422457*hvth1_4+0.38772562*hvth1_5+0.328006*hvth1_6+-0.3319062*hvth1_7+-0.039911445*hvth1_8+0.1633089*hvth1_9+0.19494164*hvth1_10+-0.31046778*hvth1_11+0.36290973*hvth1_12+0.30403158*hvth1_13+-0.27639878*hvth1_14+-0.020246264*hvth1_15+-0.20217289*hvth1_16+-0.2934873*hvth1_17+0.22809516*hvth1_18+0.39915434*hvth1_19+0.0464693);\n",
            "hvth2_8 = tanh(0.23479077*hvth1_0+0.23899555*hvth1_1+-0.38676867*hvth1_2+0.067683704*hvth1_3+-0.5144899*hvth1_4+-0.07345886*hvth1_5+-0.43869048*hvth1_6+0.10569839*hvth1_7+-0.25792852*hvth1_8+-0.03737991*hvth1_9+-0.23404409*hvth1_10+0.5150217*hvth1_11+0.3184978*hvth1_12+-0.10984687*hvth1_13+0.10765941*hvth1_14+-0.048183482*hvth1_15+-0.33733535*hvth1_16+-0.071854*hvth1_17+0.30598986*hvth1_18+-0.46872184*hvth1_19+-0.034327153);\n",
            "hvth2_9 = tanh(0.3819428*hvth1_0+0.14135627*hvth1_1+0.25576162*hvth1_2+0.2222177*hvth1_3+-0.45351094*hvth1_4+-0.5380544*hvth1_5+-0.1663427*hvth1_6+-0.4186045*hvth1_7+-0.22610238*hvth1_8+0.5938841*hvth1_9+-0.24640417*hvth1_10+0.40944037*hvth1_11+-0.3121435*hvth1_12+-0.037786566*hvth1_13+0.42759386*hvth1_14+0.6176103*hvth1_15+-0.021448132*hvth1_16+-0.24869767*hvth1_17+0.06717177*hvth1_18+0.25640857*hvth1_19+0.009254798);\n",
            "\n",
            "yvth = 0.07077524*hvth2_0+0.33148006*hvth2_1+-0.2377276*hvth2_2+-0.18476911*hvth2_3+0.6573636*hvth2_4+-0.08442947*hvth2_5+1.2248384*hvth2_6+1.3210791*hvth2_7+-0.9271494*hvth2_8+-0.47908768*hvth2_9+0.06661722;\n",
            "\n",
            "delta_Vth = (yvth - Mindelta_Vth) * normdelta_Vth;\n",
            "$strobe(\"dvth=$g\",delta_Vth);\n",
            "\tVg = V(g);\n",
            "\tVs = V(s);\n",
            "\tVd = V(d);\n",
            "    Vgsraw = Vg-Vs ;\n",
            "    Vgdraw = Vg-Vd ;\n",
            "if (Vgsraw>=Vgdraw) begin\n",
            "\tVgs = ((Vg-Vs) - MinVg - vth - delta_Vth) * normVg ;\n",
            "\n",
            "\tVgs_cgs = ((Vg-Vs) - MinVg_cgs - vth - delta_Vth) * normVg_cgs ;\n",
            "\tVgs_cgd = ((Vg-Vs) - MinVg_cgd - vth - delta_Vth) * normVg_cgd ;\n",
            "\tdir = 1 ;\n",
            "end\n",
            "else begin\n",
            "\tVgs = ((Vg-Vd) - MinVg - vth - delta_Vth) * normVg ;\n",
            "\tVgs_cgs = ((Vg-Vd) - MinVg_cgs - vth - delta_Vth) * normVg_cgs ;\n",
            "\tVgs_cgd = ((Vg-Vd) - MinVg_cgd - vth - delta_Vth) * normVg_cgd ;\n",
            "\tdir = -1 ;\n",
            "end\n",
            "\n",
            "\tVds = (abs(Vd-Vs) - MinVd) * normVd ;\n",
            "\n",
            "\tVds_cgs = (abs(Vd-Vs) - MinVd_cgs) * normVd_cgs ;\n",
            "\tVds_cgd = (abs(Vd-Vs) - MinVd_cgd) * normVd_cgd ;\n",
            "\t\n",
            "\tLg = (L -MinLg)*normLg ;\n",
            "\n",
            "\tLg_cgs = (L -MinLg_cgs)*normLg_cgs ;\n",
            "\tLg_cgd = (L -MinLg_cgd)*normLg_cgd ;\n",
            "\n",
            "hcgd1_0 = tanh(-1.6088891*Vgs_cgd+6.3197856*Vds_cgd+-0.34359035*Lg_cgd+-1.2290763);\n",
            "hcgd1_1 = tanh(12.800741*Vgs_cgd+-0.7317432*Vds_cgd+-0.13210897*Lg_cgd+-0.9875373);\n",
            "hcgd1_2 = tanh(-0.0054054493*Vgs_cgd+-0.0007543899*Vds_cgd+0.23912339*Lg_cgd+0.0017988982);\n",
            "hcgd1_3 = tanh(0.22807486*Vgs_cgd+-0.5850707*Vds_cgd+0.1802489*Lg_cgd+-0.041721515);\n",
            "hcgd1_4 = tanh(-1.9484413*Vgs_cgd+0.5350963*Vds_cgd+0.094985865*Lg_cgd+0.38261548);\n",
            "hcgd1_5 = tanh(0.07684944*Vgs_cgd+0.73550975*Vds_cgd+0.24519779*Lg_cgd+-0.33061126);\n",
            "hcgd1_6 = tanh(-0.89280343*Vgs_cgd+-0.7184284*Vds_cgd+-0.3183667*Lg_cgd+0.8391025);\n",
            "hcgd1_7 = tanh(-0.39399716*Vgs_cgd+-0.48556596*Vds_cgd+0.03430429*Lg_cgd+-1.3344768);\n",
            "hcgd1_8 = tanh(-0.3645768*Vgs_cgd+0.28501382*Vds_cgd+0.21822546*Lg_cgd+0.05729416);\n",
            "hcgd1_9 = tanh(0.16371249*Vgs_cgd+-7.4085484*Vds_cgd+0.16751836*Lg_cgd+0.94246316);\n",
            "hcgd1_10 = tanh(-0.00981918*Vgs_cgd+1.5533135*Vds_cgd+0.3354165*Lg_cgd+-0.987391);\n",
            "hcgd1_11 = tanh(-0.8166236*Vgs_cgd+-1.5672101*Vds_cgd+-0.37568504*Lg_cgd+1.5318174);\n",
            "hcgd1_12 = tanh(0.2171772*Vgs_cgd+0.27744094*Vds_cgd+-0.3758668*Lg_cgd+0.22765432);\n",
            "hcgd1_13 = tanh(5.4901214*Vgs_cgd+-2.3752038*Vds_cgd+-0.37384644*Lg_cgd+-1.8590863);\n",
            "hcgd1_14 = tanh(0.0019571404*Vgs_cgd+-0.004179957*Vds_cgd+-0.19047788*Lg_cgd+-0.0013728535);\n",
            "hcgd1_15 = tanh(0.3415198*Vgs_cgd+-0.7726733*Vds_cgd+-0.2894067*Lg_cgd+0.28510723);\n",
            "hcgd1_16 = tanh(0.60382843*Vgs_cgd+0.29274306*Vds_cgd+0.3782456*Lg_cgd+-0.42503095);\n",
            "hcgd1_17 = tanh(-1.4589765*Vgs_cgd+-28.384256*Vds_cgd+0.14227134*Lg_cgd+0.27606094);\n",
            "hcgd1_18 = tanh(-0.925936*Vgs_cgd+-0.8781763*Vds_cgd+-0.015472701*Lg_cgd+1.0576844);\n",
            "hcgd1_19 = tanh(-0.24114211*Vgs_cgd+-0.23577656*Vds_cgd+-0.15397707*Lg_cgd+0.21749781);\n",
            "hcgd1_20 = tanh(-0.10808995*Vgs_cgd+-0.47354648*Vds_cgd+-0.4505364*Lg_cgd+0.20475943);\n",
            "hcgd1_21 = tanh(0.0030588252*Vgs_cgd+0.0031867356*Vds_cgd+0.36055082*Lg_cgd+0.0012428315);\n",
            "hcgd1_22 = tanh(-0.5569403*Vgs_cgd+0.49564502*Vds_cgd+-0.3408415*Lg_cgd+0.110713266);\n",
            "hcgd1_23 = tanh(-8.529932*Vgs_cgd+3.9473457*Vds_cgd+0.18583581*Lg_cgd+1.5164791);\n",
            "hcgd1_24 = tanh(-0.46144176*Vgs_cgd+0.17818846*Vds_cgd+0.40154326*Lg_cgd+0.013512757);\n",
            "hcgd2_0 = tanh(-1.2514079*hcgd1_0+1.8445047*hcgd1_1+-0.060258187*hcgd1_2+0.54575056*hcgd1_3+0.27076387*hcgd1_4+-0.7876322*hcgd1_5+0.77317226*hcgd1_6+-0.3889938*hcgd1_7+-0.21885906*hcgd1_8+-1.1528927*hcgd1_9+0.14252257*hcgd1_10+0.740463*hcgd1_11+0.08602427*hcgd1_12+0.7715963*hcgd1_13+-0.101067945*hcgd1_14+0.44884506*hcgd1_15+-0.2603153*hcgd1_16+-0.39788675*hcgd1_17+0.9237374*hcgd1_18+0.39531583*hcgd1_19+0.49464858*hcgd1_20+-0.12488924*hcgd1_21+-0.7705938*hcgd1_22+-0.86613214*hcgd1_23+-0.38659856*hcgd1_24+0.66808015);\n",
            "hcgd2_1 = tanh(-0.11062574*hcgd1_0+0.083067186*hcgd1_1+-0.037825696*hcgd1_2+0.20563497*hcgd1_3+0.33369613*hcgd1_4+0.0886166*hcgd1_5+0.09043529*hcgd1_6+0.030820973*hcgd1_7+0.33438814*hcgd1_8+-0.039903246*hcgd1_9+0.069336206*hcgd1_10+0.20417899*hcgd1_11+-0.082516894*hcgd1_12+0.024985574*hcgd1_13+0.048390143*hcgd1_14+0.06470794*hcgd1_15+0.02195431*hcgd1_16+0.09177701*hcgd1_17+-0.10868985*hcgd1_18+-0.6351576*hcgd1_19+-0.6435663*hcgd1_20+0.016601902*hcgd1_21+0.11345903*hcgd1_22+-0.106875926*hcgd1_23+-0.22184639*hcgd1_24+0.06259622);\n",
            "hcgd2_2 = tanh(0.9100889*hcgd1_0+0.8600478*hcgd1_1+-0.0020481474*hcgd1_2+0.07100766*hcgd1_3+-0.47693938*hcgd1_4+0.63060385*hcgd1_5+-1.7883018*hcgd1_6+-0.10218202*hcgd1_7+0.3747722*hcgd1_8+1.0218011*hcgd1_9+0.93496245*hcgd1_10+-1.7450128*hcgd1_11+-0.113168575*hcgd1_12+-0.66238743*hcgd1_13+-0.030654324*hcgd1_14+-0.92108375*hcgd1_15+1.0528872*hcgd1_16+0.033458687*hcgd1_17+-1.3917034*hcgd1_18+-0.35307285*hcgd1_19+-0.48046726*hcgd1_20+-0.014765772*hcgd1_21+0.23623382*hcgd1_22+-2.6411834*hcgd1_23+0.027243387*hcgd1_24+0.03591177);\n",
            "hcgd2_3 = tanh(0.62506354*hcgd1_0+0.44420892*hcgd1_1+-0.022703474*hcgd1_2+0.77745813*hcgd1_3+0.24668725*hcgd1_4+-0.39535686*hcgd1_5+-0.36517298*hcgd1_6+0.6084278*hcgd1_7+-0.3068207*hcgd1_8+-0.630984*hcgd1_9+-0.76083857*hcgd1_10+0.33673477*hcgd1_11+-0.38794822*hcgd1_12+0.8889784*hcgd1_13+-0.032760695*hcgd1_14+0.31031358*hcgd1_15+0.44298455*hcgd1_16+-1.8597292*hcgd1_17+0.1456184*hcgd1_18+0.29207367*hcgd1_19+0.32642275*hcgd1_20+-0.04083263*hcgd1_21+-0.8064821*hcgd1_22+-0.8100279*hcgd1_23+-0.40088394*hcgd1_24+-0.46441332);\n",
            "hcgd2_4 = tanh(1.8506681*hcgd1_0+-0.9750525*hcgd1_1+0.003659014*hcgd1_2+-0.6878417*hcgd1_3+-0.5547116*hcgd1_4+0.1308772*hcgd1_5+0.45871595*hcgd1_6+0.15744588*hcgd1_7+0.26718134*hcgd1_8+0.9983965*hcgd1_9+0.5910452*hcgd1_10+0.37368506*hcgd1_11+0.029024748*hcgd1_12+-1.3510306*hcgd1_13+0.0152292065*hcgd1_14+-0.35220924*hcgd1_15+-0.6619339*hcgd1_16+1.342967*hcgd1_17+0.46826208*hcgd1_18+-0.29376435*hcgd1_19+-0.3991243*hcgd1_20+-0.015383459*hcgd1_21+0.85965115*hcgd1_22+0.60075265*hcgd1_23+0.18427542*hcgd1_24+-0.32940418);\n",
            "hcgd2_5 = tanh(-0.8664879*hcgd1_0+0.44505116*hcgd1_1+0.013057195*hcgd1_2+-0.0031339983*hcgd1_3+0.19921027*hcgd1_4+-0.22984676*hcgd1_5+0.3673192*hcgd1_6+0.56390464*hcgd1_7+-0.034784332*hcgd1_8+-0.4111533*hcgd1_9+-0.118535414*hcgd1_10+0.5688084*hcgd1_11+-0.19346042*hcgd1_12+0.26538432*hcgd1_13+0.0061995205*hcgd1_14+-0.10955226*hcgd1_15+0.015352882*hcgd1_16+-0.16454482*hcgd1_17+0.32237643*hcgd1_18+0.088658795*hcgd1_19+0.20537783*hcgd1_20+0.0050276928*hcgd1_21+-0.015647996*hcgd1_22+-0.2880599*hcgd1_23+0.2548945*hcgd1_24+-0.16771647);\n",
            "hcgd2_6 = tanh(1.1495414*hcgd1_0+-0.22627151*hcgd1_1+0.015214143*hcgd1_2+0.27357554*hcgd1_3+-0.36256093*hcgd1_4+-0.92208475*hcgd1_5+0.24147213*hcgd1_6+0.38718736*hcgd1_7+-0.41560674*hcgd1_8+0.94879586*hcgd1_9+-0.5408847*hcgd1_10+0.32851028*hcgd1_11+0.040295456*hcgd1_12+-0.033835553*hcgd1_13+0.011123846*hcgd1_14+0.52912486*hcgd1_15+0.03981201*hcgd1_16+-0.30909863*hcgd1_17+0.3929471*hcgd1_18+0.32624614*hcgd1_19+0.8135803*hcgd1_20+0.009368981*hcgd1_21+-0.51490057*hcgd1_22+-0.45729175*hcgd1_23+0.26873454*hcgd1_24+-0.01673438);\n",
            "hcgd2_7 = tanh(-3.966802*hcgd1_0+0.042795368*hcgd1_1+0.054317914*hcgd1_2+-0.91404206*hcgd1_3+1.1362458*hcgd1_4+-0.07761897*hcgd1_5+1.5529021*hcgd1_6+-0.48278892*hcgd1_7+-0.5192229*hcgd1_8+-0.9233961*hcgd1_9+-0.73843503*hcgd1_10+2.1569836*hcgd1_11+0.7887896*hcgd1_12+1.366921*hcgd1_13+0.07727752*hcgd1_14+0.38954172*hcgd1_15+-0.47272664*hcgd1_16+-0.4188385*hcgd1_17+1.5436481*hcgd1_18+0.1914369*hcgd1_19+-0.024450772*hcgd1_20+0.010537377*hcgd1_21+0.051735442*hcgd1_22+2.4417806*hcgd1_23+-0.50142026*hcgd1_24+0.95721453);\n",
            "hcgd2_8 = tanh(-0.11731646*hcgd1_0+0.08615463*hcgd1_1+0.013967979*hcgd1_2+-0.3801745*hcgd1_3+0.26075113*hcgd1_4+0.05496182*hcgd1_5+0.26613086*hcgd1_6+0.053695574*hcgd1_7+-0.28042397*hcgd1_8+-0.03794526*hcgd1_9+-0.0068770945*hcgd1_10+0.30859926*hcgd1_11+-0.0229647*hcgd1_12+0.025560087*hcgd1_13+0.017060574*hcgd1_14+-0.25769773*hcgd1_15+0.06637153*hcgd1_16+0.07093841*hcgd1_17+-0.5513341*hcgd1_18+-0.19939162*hcgd1_19+0.08702792*hcgd1_20+0.09092291*hcgd1_21+0.31653336*hcgd1_22+-0.09402478*hcgd1_23+-0.2813343*hcgd1_24+-0.00752968);\n",
            "hcgd2_9 = tanh(0.110342234*hcgd1_0+-0.0826157*hcgd1_1+-0.14750138*hcgd1_2+-0.24709061*hcgd1_3+-0.25432354*hcgd1_4+-0.09893609*hcgd1_5+-0.015331019*hcgd1_6+0.19696374*hcgd1_7+-0.019382935*hcgd1_8+0.08544908*hcgd1_9+-0.09507841*hcgd1_10+0.11304415*hcgd1_11+0.2705397*hcgd1_12+-0.04126459*hcgd1_13+-0.1613513*hcgd1_14+0.3283118*hcgd1_15+-0.4909294*hcgd1_16+-0.066513576*hcgd1_17+-0.17823151*hcgd1_18+-0.2690935*hcgd1_19+-0.1837275*hcgd1_20+-0.11801412*hcgd1_21+-0.010364476*hcgd1_22+0.088502504*hcgd1_23+-0.10050535*hcgd1_24+-0.06808074);\n",
            "hcgd2_10 = tanh(-4.403549*hcgd1_0+-2.471523*hcgd1_1+-0.00089143997*hcgd1_2+-0.70052093*hcgd1_3+1.5336959*hcgd1_4+0.034311388*hcgd1_5+0.47960278*hcgd1_6+0.48460782*hcgd1_7+0.14602353*hcgd1_8+-2.0514007*hcgd1_9+0.85300875*hcgd1_10+0.6684621*hcgd1_11+-0.06943985*hcgd1_12+-0.34655583*hcgd1_13+0.024638627*hcgd1_14+-0.07107706*hcgd1_15+-0.6265861*hcgd1_16+-1.1899346*hcgd1_17+0.758323*hcgd1_18+0.078832895*hcgd1_19+-0.3047711*hcgd1_20+0.00015650672*hcgd1_21+0.56323916*hcgd1_22+0.5847528*hcgd1_23+0.07073806*hcgd1_24+-0.15086874);\n",
            "hcgd2_11 = tanh(0.47815424*hcgd1_0+0.048465684*hcgd1_1+-0.002923328*hcgd1_2+-0.37546745*hcgd1_3+0.66585714*hcgd1_4+0.09604421*hcgd1_5+-1.2400401*hcgd1_6+-1.5856158*hcgd1_7+-0.16395986*hcgd1_8+-0.12747197*hcgd1_9+0.029356737*hcgd1_10+0.10224382*hcgd1_11+0.76213807*hcgd1_12+-1.8363391*hcgd1_13+-0.008269408*hcgd1_14+0.2678933*hcgd1_15+0.46798977*hcgd1_16+-1.9006412*hcgd1_17+-0.4502235*hcgd1_18+-0.09107671*hcgd1_19+-0.2680099*hcgd1_20+-0.010051073*hcgd1_21+0.050625626*hcgd1_22+1.8481784*hcgd1_23+-0.6646883*hcgd1_24+0.9775716);\n",
            "y_cgd = -0.19379665*hcgd2_0+0.0061848215*hcgd2_1+-0.32997322*hcgd2_2+0.33700317*hcgd2_3+0.19574322*hcgd2_4+0.55552804*hcgd2_5+-0.18057454*hcgd2_6+-0.18055588*hcgd2_7+0.006111857*hcgd2_8+-0.006008425*hcgd2_9+-0.27025312*hcgd2_10+0.58404464*hcgd2_11+-0.059968084;\n",
            "\n",
            "hcgs1_0 = tanh(0.003866898*Vgs_cgs+-0.0061709154*Vds_cgs+0.07367021*Lg_cgs+0.005350336);\n",
            "hcgs1_1 = tanh(0.0038147226*Vgs_cgs+-0.006049334*Vds_cgs+-0.17519544*Lg_cgs+0.005365436);\n",
            "hcgs1_2 = tanh(-0.14925069*Vgs_cgs+0.31913087*Vds_cgs+-0.3709501*Lg_cgs+0.94486016);\n",
            "hcgs1_3 = tanh(-5.7050853*Vgs_cgs+3.0125842*Vds_cgs+-0.3576562*Lg_cgs+0.6098174);\n",
            "hcgs1_4 = tanh(0.73557264*Vgs_cgs+0.17232454*Vds_cgs+-0.408028*Lg_cgs+-0.41273043);\n",
            "hcgs1_5 = tanh(0.47328284*Vgs_cgs+-0.25681034*Vds_cgs+0.16284285*Lg_cgs+-0.033205096);\n",
            "hcgs1_6 = tanh(1.8763449*Vgs_cgs+-0.9621107*Vds_cgs+0.24538922*Lg_cgs+0.024004985);\n",
            "hcgs1_7 = tanh(-1.5798868*Vgs_cgs+1.9016485*Vds_cgs+-0.26374397*Lg_cgs+-1.0214508);\n",
            "hcgs1_8 = tanh(1.8300656*Vgs_cgs+-0.45609108*Vds_cgs+-0.11383694*Lg_cgs+-0.69372463);\n",
            "hcgs1_9 = tanh(0.0038994814*Vgs_cgs+-0.0062413956*Vds_cgs+0.16813117*Lg_cgs+0.005379745);\n",
            "hcgs1_10 = tanh(1.6561933*Vgs_cgs+-0.09678654*Vds_cgs+0.4255832*Lg_cgs+-1.0611489);\n",
            "hcgs1_11 = tanh(-0.28797054*Vgs_cgs+0.13389988*Vds_cgs+0.43625653*Lg_cgs+0.033546243);\n",
            "hcgs1_12 = tanh(0.8129892*Vgs_cgs+-0.029293621*Vds_cgs+0.1894619*Lg_cgs+-0.4490194);\n",
            "hcgs1_13 = tanh(0.008404021*Vgs_cgs+-0.5041153*Vds_cgs+0.29847372*Lg_cgs+-0.1161287);\n",
            "hcgs1_14 = tanh(0.32958084*Vgs_cgs+-0.30009958*Vds_cgs+-0.22462274*Lg_cgs+0.0048046284);\n",
            "hcgs1_15 = tanh(2.3854203*Vgs_cgs+-3.2975893*Vds_cgs+0.042711172*Lg_cgs+0.53147864);\n",
            "hcgs1_16 = tanh(0.0039317203*Vgs_cgs+-0.00632388*Vds_cgs+0.28598666*Lg_cgs+0.0053823735);\n",
            "hcgs1_17 = tanh(0.0037897201*Vgs_cgs+-0.005991189*Vds_cgs+-0.37188497*Lg_cgs+0.0053383214);\n",
            "hcgs1_18 = tanh(1.8098192*Vgs_cgs+-2.3450627*Vds_cgs+-0.15223776*Lg_cgs+1.3056793);\n",
            "hcgs1_19 = tanh(-0.01902712*Vgs_cgs+-2.2522266*Vds_cgs+0.42196727*Lg_cgs+1.2842451);\n",
            "hcgs1_20 = tanh(-1.3350194*Vgs_cgs+0.4994975*Vds_cgs+-0.17475094*Lg_cgs+0.18653236);\n",
            "hcgs1_21 = tanh(-4.302233*Vgs_cgs+-3.9769611*Vds_cgs+0.07871075*Lg_cgs+3.110047);\n",
            "hcgs1_22 = tanh(0.023444308*Vgs_cgs+-0.58594865*Vds_cgs+-0.02238914*Lg_cgs+-0.052440178);\n",
            "hcgs1_23 = tanh(0.15485905*Vgs_cgs+1.2359923*Vds_cgs+-0.15997769*Lg_cgs+-0.2019675);\n",
            "hcgs1_24 = tanh(-0.3575714*Vgs_cgs+-2.5208488*Vds_cgs+-0.1896365*Lg_cgs+2.4808133);\n",
            "hcgs2_0 = tanh(0.023202578*hcgs1_0+0.022973271*hcgs1_1+-0.06634169*hcgs1_2+0.7479778*hcgs1_3+-0.29115394*hcgs1_4+0.5281517*hcgs1_5+1.4476869*hcgs1_6+-0.57092524*hcgs1_7+1.342318*hcgs1_8+0.023113564*hcgs1_9+0.5039525*hcgs1_10+-0.28507996*hcgs1_11+0.33107176*hcgs1_12+0.59429955*hcgs1_13+-0.063477024*hcgs1_14+1.4652793*hcgs1_15+0.0230467*hcgs1_16+0.02331026*hcgs1_17+0.067940354*hcgs1_18+1.9354934*hcgs1_19+-0.6783489*hcgs1_20+-0.3415898*hcgs1_21+0.0735886*hcgs1_22+-1.0008856*hcgs1_23+0.009945778*hcgs1_24+-0.2625346);\n",
            "hcgs2_1 = tanh(0.044127226*hcgs1_0+0.045110825*hcgs1_1+-1.5823554*hcgs1_2+-2.1134062*hcgs1_3+0.279113*hcgs1_4+1.0231125*hcgs1_5+-0.47619388*hcgs1_6+0.64811915*hcgs1_7+-3.5616899*hcgs1_8+0.043715417*hcgs1_9+-0.96364224*hcgs1_10+0.93940836*hcgs1_11+0.90396035*hcgs1_12+2.3027606*hcgs1_13+2.184711*hcgs1_14+1.2926208*hcgs1_15+0.043070372*hcgs1_16+0.04556742*hcgs1_17+-0.99922925*hcgs1_18+0.5443892*hcgs1_19+1.7640798*hcgs1_20+0.57941544*hcgs1_21+3.1087935*hcgs1_22+-0.946341*hcgs1_23+-0.936913*hcgs1_24+-1.4875553);\n",
            "hcgs2_2 = tanh(0.005477332*hcgs1_0+0.0053486465*hcgs1_1+0.047083233*hcgs1_2+0.5397506*hcgs1_3+-0.075822465*hcgs1_4+0.036040183*hcgs1_5+-0.21352278*hcgs1_6+0.89393115*hcgs1_7+-1.1001998*hcgs1_8+0.005473618*hcgs1_9+-1.1550146*hcgs1_10+-0.070924945*hcgs1_11+-0.16800475*hcgs1_12+-0.1525137*hcgs1_13+-0.06899515*hcgs1_14+-0.051954325*hcgs1_15+0.0054896707*hcgs1_16+0.0054276832*hcgs1_17+-1.158984*hcgs1_18+-0.026913341*hcgs1_19+0.21477942*hcgs1_20+-0.3370402*hcgs1_21+-0.12709796*hcgs1_22+0.5450149*hcgs1_23+-0.19750577*hcgs1_24+-0.45452434);\n",
            "hcgs2_3 = tanh(0.031963654*hcgs1_0+0.03035919*hcgs1_1+-0.26406923*hcgs1_2+2.4108412*hcgs1_3+0.6478258*hcgs1_4+-0.2639667*hcgs1_5+0.15820932*hcgs1_6+0.36013365*hcgs1_7+-0.07805458*hcgs1_8+0.032406412*hcgs1_9+0.15034798*hcgs1_10+0.04711759*hcgs1_11+0.51302767*hcgs1_12+-0.9524351*hcgs1_13+0.05383534*hcgs1_14+-2.1840389*hcgs1_15+0.033131413*hcgs1_16+0.030272223*hcgs1_17+-0.70434415*hcgs1_18+-1.3847973*hcgs1_19+0.055561617*hcgs1_20+-2.2977886*hcgs1_21+-1.2528052*hcgs1_22+1.6460723*hcgs1_23+0.52855104*hcgs1_24+0.20310849);\n",
            "hcgs2_4 = tanh(-0.004114148*hcgs1_0+-0.0036210776*hcgs1_1+-0.25540155*hcgs1_2+-0.18470578*hcgs1_3+-0.12067126*hcgs1_4+-0.7038924*hcgs1_5+0.21889566*hcgs1_6+-0.17658989*hcgs1_7+0.1299486*hcgs1_8+-0.0041041495*hcgs1_9+-0.4014189*hcgs1_10+0.20044085*hcgs1_11+-0.19608615*hcgs1_12+0.02296377*hcgs1_13+-0.013663749*hcgs1_14+0.4224307*hcgs1_15+-0.004211446*hcgs1_16+-0.003838226*hcgs1_17+-0.2911345*hcgs1_18+0.78450745*hcgs1_19+0.42747602*hcgs1_20+-0.07747369*hcgs1_21+-0.27040684*hcgs1_22+0.11331162*hcgs1_23+-0.20715222*hcgs1_24+-0.017163113);\n",
            "hcgs2_5 = tanh(0.011508604*hcgs1_0+0.011453037*hcgs1_1+1.7924933*hcgs1_2+-1.8155463*hcgs1_3+-0.25609693*hcgs1_4+-0.758906*hcgs1_5+-2.4812367*hcgs1_6+4.0513716*hcgs1_7+-2.5917294*hcgs1_8+0.011474304*hcgs1_9+-3.2454631*hcgs1_10+0.093522236*hcgs1_11+-1.3182486*hcgs1_12+-0.8882851*hcgs1_13+-0.9181794*hcgs1_14+1.3970197*hcgs1_15+0.011446857*hcgs1_16+0.011560462*hcgs1_17+-5.1778355*hcgs1_18+-0.2236407*hcgs1_19+2.0796807*hcgs1_20+5.9254537*hcgs1_21+-0.72559786*hcgs1_22+0.45175433*hcgs1_23+-3.1938994*hcgs1_24+0.9409381);\n",
            "hcgs2_6 = tanh(0.060208883*hcgs1_0+0.057784908*hcgs1_1+-0.4230708*hcgs1_2+0.31358114*hcgs1_3+0.5314662*hcgs1_4+-1.1043811*hcgs1_5+-0.30330408*hcgs1_6+0.5861162*hcgs1_7+-0.2730255*hcgs1_8+0.0608772*hcgs1_9+1.2918353*hcgs1_10+0.5573929*hcgs1_11+0.96025693*hcgs1_12+-1.5764415*hcgs1_13+-1.0071468*hcgs1_14+-2.9516594*hcgs1_15+0.061963964*hcgs1_16+0.057676923*hcgs1_17+-0.12257842*hcgs1_18+-1.6470038*hcgs1_19+0.9465945*hcgs1_20+-0.93217355*hcgs1_21+-2.438695*hcgs1_22+1.0968347*hcgs1_23+0.16981499*hcgs1_24+0.42413175);\n",
            "hcgs2_7 = tanh(-0.008120307*hcgs1_0+-0.008280249*hcgs1_1+0.17085399*hcgs1_2+1.1467992*hcgs1_3+2.214014*hcgs1_4+1.1809759*hcgs1_5+7.2768965*hcgs1_6+-4.8417068*hcgs1_7+5.984158*hcgs1_8+-0.0075962003*hcgs1_9+2.7736754*hcgs1_10+-1.9165568*hcgs1_11+0.4659055*hcgs1_12+-1.3562847*hcgs1_13+0.48391283*hcgs1_14+-2.8675776*hcgs1_15+-0.007374848*hcgs1_16+-0.00872389*hcgs1_17+-0.1205414*hcgs1_18+-1.7518774*hcgs1_19+-3.7752948*hcgs1_20+0.27624342*hcgs1_21+-1.239815*hcgs1_22+-0.11851393*hcgs1_23+4.030906*hcgs1_24+0.9663557);\n",
            "hcgs2_8 = tanh(-0.06382305*hcgs1_0+-0.07465671*hcgs1_1+-0.17580509*hcgs1_2+0.039555147*hcgs1_3+-0.3423838*hcgs1_4+0.40099683*hcgs1_5+0.08354397*hcgs1_6+0.049524635*hcgs1_7+0.010591255*hcgs1_8+-0.0655947*hcgs1_9+-0.09075449*hcgs1_10+0.21120805*hcgs1_11+0.18817192*hcgs1_12+-0.37621862*hcgs1_13+-0.11487423*hcgs1_14+0.025681255*hcgs1_15+-0.06386206*hcgs1_16+-0.068342805*hcgs1_17+-0.008454653*hcgs1_18+0.015086431*hcgs1_19+-0.13472116*hcgs1_20+0.025698638*hcgs1_21+0.0998438*hcgs1_22+0.20212452*hcgs1_23+0.05868889*hcgs1_24+-0.10602957);\n",
            "hcgs2_9 = tanh(0.019189388*hcgs1_0+0.01920859*hcgs1_1+-0.012194334*hcgs1_2+-1.4130874*hcgs1_3+0.30251437*hcgs1_4+-0.10520868*hcgs1_5+1.0161169*hcgs1_6+0.4738982*hcgs1_7+1.3726546*hcgs1_8+0.019165223*hcgs1_9+0.6629045*hcgs1_10+-0.17346822*hcgs1_11+-0.41499868*hcgs1_12+-0.2930857*hcgs1_13+-0.3305122*hcgs1_14+-0.82226783*hcgs1_15+0.01908325*hcgs1_16+0.019351097*hcgs1_17+0.09138516*hcgs1_18+-1.2453144*hcgs1_19+-0.88000125*hcgs1_20+0.48374525*hcgs1_21+-0.2261362*hcgs1_22+0.6680502*hcgs1_23+0.22479443*hcgs1_24+-0.056186553);\n",
            "hcgs2_10 = tanh(0.0064860974*hcgs1_0+0.00648306*hcgs1_1+0.48692945*hcgs1_2+2.3550978*hcgs1_3+0.43859667*hcgs1_4+0.27351967*hcgs1_5+0.90961814*hcgs1_6+-0.017722035*hcgs1_7+0.5930781*hcgs1_8+0.0064157885*hcgs1_9+0.4308463*hcgs1_10+-0.28399688*hcgs1_11+0.30260938*hcgs1_12+-0.39435783*hcgs1_13+-0.1002725*hcgs1_14+-2.053321*hcgs1_15+0.006337951*hcgs1_16+0.006645003*hcgs1_17+-1.2891791*hcgs1_18+0.05721846*hcgs1_19+-0.37843162*hcgs1_20+0.5648725*hcgs1_21+-0.6181118*hcgs1_22+1.1590537*hcgs1_23+-2.7134886*hcgs1_24+0.54398286);\n",
            "hcgs2_11 = tanh(0.14230382*hcgs1_0+0.123352334*hcgs1_1+0.042968668*hcgs1_2+0.047534492*hcgs1_3+0.010582293*hcgs1_4+-0.06643509*hcgs1_5+-0.13723876*hcgs1_6+-0.12684533*hcgs1_7+-0.040209897*hcgs1_8+0.13827254*hcgs1_9+-0.17713757*hcgs1_10+-0.35476053*hcgs1_11+-0.114496075*hcgs1_12+0.2810919*hcgs1_13+0.584066*hcgs1_14+0.017237866*hcgs1_15+0.14104304*hcgs1_16+0.13508707*hcgs1_17+-0.010557453*hcgs1_18+-0.1474484*hcgs1_19+-0.29602262*hcgs1_20+0.028033255*hcgs1_21+-0.22550379*hcgs1_22+-0.04313099*hcgs1_23+-0.11113166*hcgs1_24+-0.028289912);\n",
            "y_cgs = -0.090056084*hcgs2_0+-0.31736207*hcgs2_1+-0.24686596*hcgs2_2+-0.14114563*hcgs2_3+-0.03698175*hcgs2_4+-0.20639718*hcgs2_5+0.0935527*hcgs2_6+-0.028302789*hcgs2_7+0.0030376983*hcgs2_8+0.07618971*hcgs2_9+0.121289134*hcgs2_10+-1.5651603e-05*hcgs2_11+0.0065943017;\n",
            "\n",
            "\n",
            "h1_0 = tanh(0.0067332466*Vgs+0.0014811404*Vds+-0.089835726*Lg+0.09066404);\n",
            "h1_1 = tanh(0.11106648*Vgs+0.40663162*Vds+0.0077667693*Lg+-0.10081332);\n",
            "h1_2 = tanh(1.5560061*Vgs+2.696172*Vds+-0.2839149*Lg+-0.3486124);\n",
            "h1_3 = tanh(-2.0344324*Vgs+-0.28681883*Vds+0.8085338*Lg+0.0026162667);\n",
            "h1_4 = tanh(-0.03341659*Vgs+-0.15408932*Vds+-0.015416802*Lg+0.04370213);\n",
            "h1_5 = tanh(1.4746779*Vgs+0.33599785*Vds+-0.2669822*Lg+-0.42193142);\n",
            "h1_6 = tanh(-0.16598122*Vgs+-2.3765175*Vds+0.11762294*Lg+0.019988127);\n",
            "h1_7 = tanh(4.3143744*Vgs+-0.5703343*Vds+-0.3085829*Lg+-0.50043374);\n",
            "h1_8 = tanh(-0.4936719*Vgs+36.42876*Vds+0.32744452*Lg+0.24776502);\n",
            "h1_9 = tanh(0.4869826*Vgs+-0.13276859*Vds+0.28959593*Lg+-0.3828168);\n",
            "h1_10 = tanh(0.000804813*Vgs+0.0004923456*Vds+-0.41243985*Lg+0.41356155);\n",
            "h1_11 = tanh(2.6073072*Vgs+-0.56419706*Vds+-0.111921*Lg+-0.37480572);\n",
            "h1_12 = tanh(0.0020444687*Vgs+0.0014842966*Vds+0.21013871*Lg+-0.20708004);\n",
            "h1_13 = tanh(-2.6335895*Vgs+0.28439838*Vds+0.321686*Lg+0.41039804);\n",
            "h1_14 = tanh(0.0034599572*Vgs+0.0084134*Vds+-0.034978658*Lg+0.037748303);\n",
            "h1_15 = tanh(0.0025394042*Vgs+0.00083049946*Vds+-0.13070253*Lg+0.13411763);\n",
            "h1_16 = tanh(-2.1379406*Vgs+-0.57477415*Vds+0.19879702*Lg+0.6043196);\n",
            "h1_17 = tanh(-0.9647693*Vgs+3.0132833*Vds+0.13054319*Lg+0.38879853);\n",
            "h1_18 = tanh(-1.6564686*Vgs+-0.06445829*Vds+0.39810786*Lg+0.21570432);\n",
            "h1_19 = tanh(-0.120745726*Vgs+-0.37977916*Vds+-0.20842548*Lg+0.33059505);\n",
            "h1_20 = tanh(-0.5346294*Vgs+-0.24575421*Vds+0.42274392*Lg+-0.095065445);\n",
            "h1_21 = tanh(-0.17100386*Vgs+-0.36405364*Vds+-0.32086778*Lg+-0.45500055);\n",
            "h1_22 = tanh(-0.3456514*Vgs+-0.43013823*Vds+-0.14385213*Lg+0.4731826);\n",
            "h1_23 = tanh(0.28758252*Vgs+-0.55982685*Vds+0.44342747*Lg+-0.34977767);\n",
            "h1_24 = tanh(0.08224837*Vgs+0.35958192*Vds+0.059084654*Lg+-0.1397865);\n",
            "\n",
            "h2_0 = tanh(-0.8091375*h1_0+-0.43552017*h1_1+-0.8155236*h1_2+0.24130166*h1_3+0.14224924*h1_4+-0.62978196*h1_5+0.6101532*h1_6+-0.43199107*h1_7+-2.3122218*h1_8+-0.23163876*h1_9+0.068008065*h1_10+-0.03888623*h1_11+-0.014134661*h1_12+-0.06591678*h1_13+0.022440521*h1_14+-0.27041718*h1_15+0.090823404*h1_16+-0.26772383*h1_17+0.47534344*h1_18+0.73750144*h1_19+0.50188285*h1_20+0.3184862*h1_21+0.3608627*h1_22+0.79056877*h1_23+-0.037502155*h1_24+-0.24878933);\n",
            "h2_1 = tanh(-0.03261362*h1_0+-0.06024928*h1_1+-0.10186445*h1_2+0.018033225*h1_3+0.51713187*h1_4+0.022300791*h1_5+0.015979188*h1_6+0.056999434*h1_7+0.035186633*h1_8+0.25107002*h1_9+-0.012781056*h1_10+-0.3380685*h1_11+0.05043952*h1_12+-0.24957459*h1_13+0.024472937*h1_14+-0.08896992*h1_15+0.053890992*h1_16+-0.0101861*h1_17+-0.031921674*h1_18+0.17910303*h1_19+-0.03255347*h1_20+-0.38824677*h1_21+-0.14763784*h1_22+-0.24868536*h1_23+-0.024086645*h1_24+-0.18488966);\n",
            "h2_2 = tanh(0.12152682*h1_0+0.009792632*h1_1+-0.29032546*h1_2+0.5965219*h1_3+-0.021902572*h1_4+-0.5883061*h1_5+0.17799598*h1_6+-0.92929643*h1_7+0.32916892*h1_8+-0.2568552*h1_9+-0.067421295*h1_10+-1.1449344*h1_11+0.007887112*h1_12+0.276745*h1_13+-0.033592574*h1_14+0.1040227*h1_15+0.23562248*h1_16+-0.18375605*h1_17+1.0453725*h1_18+0.33611628*h1_19+0.6393779*h1_20+0.06726488*h1_21+0.7879763*h1_22+-0.14922209*h1_23+-0.18613423*h1_24+0.28223407);\n",
            "h2_3 = tanh(-0.18859175*h1_0+-0.9337733*h1_1+-0.938285*h1_2+0.7002026*h1_3+0.6061784*h1_4+-0.52529997*h1_5+0.71481675*h1_6+-0.38263503*h1_7+-0.5072332*h1_8+0.1263292*h1_9+0.04664359*h1_10+-0.5296314*h1_11+-0.033964746*h1_12+0.6460025*h1_13+-0.0231704*h1_14+-0.38591123*h1_15+0.0872077*h1_16+-0.5042024*h1_17+0.47931066*h1_18+0.50751704*h1_19+-0.063635096*h1_20+0.018421784*h1_21+0.08422998*h1_22+0.30641475*h1_23+-0.524482*h1_24+-0.26979908);\n",
            "h2_4 = tanh(-0.034289524*h1_0+-0.11348127*h1_1+0.48059276*h1_2+-1.123194*h1_3+0.12442129*h1_4+0.49476302*h1_5+0.0863118*h1_6+0.47754285*h1_7+0.2202004*h1_8+0.41435316*h1_9+0.028558401*h1_10+0.36081842*h1_11+-0.008486773*h1_12+-0.9903806*h1_13+0.015499473*h1_14+-0.063613005*h1_15+-1.4330287*h1_16+-0.74456465*h1_17+-0.5569409*h1_18+-0.019323817*h1_19+-0.14176102*h1_20+0.49098673*h1_21+-0.057230044*h1_22+0.515492*h1_23+-0.13265039*h1_24+-0.18507467);\n",
            "h2_5 = tanh(-0.09933434*h1_0+-0.15035452*h1_1+-0.5133741*h1_2+-0.47027865*h1_3+0.21104498*h1_4+0.14945817*h1_5+0.10640489*h1_6+0.1787739*h1_7+0.060723394*h1_8+-0.5602644*h1_9+-0.0019350805*h1_10+0.5330786*h1_11+0.012645531*h1_12+-0.42403966*h1_13+0.018393166*h1_14+-0.00062784646*h1_15+-0.0052586636*h1_16+1.0866524*h1_17+0.078084014*h1_18+0.059300236*h1_19+0.32314536*h1_20+0.49137965*h1_21+0.09876712*h1_22+-0.583638*h1_23+-0.1759503*h1_24+-0.18313222);\n",
            "h2_6 = tanh(-0.1285073*h1_0+-1.032602*h1_1+-1.224141*h1_2+0.14631094*h1_3+0.65741867*h1_4+-0.22908972*h1_5+0.63241625*h1_6+-0.88521755*h1_7+0.09097795*h1_8+0.09149886*h1_9+0.027507082*h1_10+-0.63233715*h1_11+-0.02061426*h1_12+0.1104531*h1_13+-0.012734477*h1_14+-0.34623924*h1_15+-0.18294835*h1_16+-0.045275174*h1_17+0.35704163*h1_18+0.5909152*h1_19+0.073851526*h1_20+0.49004716*h1_21+0.19812724*h1_22+0.3784807*h1_23+-0.6437184*h1_24+-0.3594689);\n",
            "h2_7 = tanh(0.119550616*h1_0+-0.035170615*h1_1+-0.60996103*h1_2+0.40164798*h1_3+-0.007347775*h1_4+-0.30736676*h1_5+-0.13570982*h1_6+0.9896757*h1_7+-4.0502048*h1_8+0.053264104*h1_9+-0.012164457*h1_10+0.48190302*h1_11+0.014136277*h1_12+-0.5162279*h1_13+0.014227704*h1_14+0.05181294*h1_15+0.7717682*h1_16+-0.111988194*h1_17+0.062060844*h1_18+-0.044897318*h1_19+-0.15565042*h1_20+0.24919312*h1_21+-0.07993477*h1_22+-0.09538691*h1_23+-0.0052977665*h1_24+-0.10968423);\n",
            "h2_8 = tanh(0.079706475*h1_0+-0.5814533*h1_1+0.6996235*h1_2+-0.16522586*h1_3+0.630409*h1_4+0.29656532*h1_5+-0.5331364*h1_6+0.114606544*h1_7+-0.21106683*h1_8+0.0467022*h1_9+-0.06535354*h1_10+-0.24001631*h1_11+0.059368264*h1_12+0.15451497*h1_13+0.053622272*h1_14+0.12981157*h1_15+0.5445277*h1_16+0.24295534*h1_17+-0.26532695*h1_18+0.56733835*h1_19+0.12550347*h1_20+0.048389148*h1_21+0.5622483*h1_22+0.6891362*h1_23+-0.76375955*h1_24+-0.2676767);\n",
            "h2_9 = tanh(-0.03992191*h1_0+0.09928779*h1_1+1.0056654*h1_2+-2.3181243*h1_3+-0.0916434*h1_4+2.143644*h1_5+0.19818373*h1_6+2.8020356*h1_7+-0.47644797*h1_8+0.24747562*h1_9+0.02024143*h1_10+1.3736787*h1_11+0.006931587*h1_12+-3.888161*h1_13+0.018778825*h1_14+-0.0041158814*h1_15+-1.5533967*h1_16+-0.71390355*h1_17+-2.8108795*h1_18+-0.24579133*h1_19+-1.128639*h1_20+0.2231992*h1_21+-1.2298777*h1_22+0.043120787*h1_23+0.403965*h1_24+-0.17281148);\n",
            "h2_10 = tanh(0.35093632*h1_0+-0.05374117*h1_1+0.16745348*h1_2+-0.19781992*h1_3+-0.07950959*h1_4+0.29579905*h1_5+-0.5073861*h1_6+-0.14164843*h1_7+0.70651436*h1_8+0.29345548*h1_9+-0.054172043*h1_10+-0.28257352*h1_11+0.033599235*h1_12+0.017732412*h1_13+0.02154612*h1_14+0.1586723*h1_15+-0.23733242*h1_16+0.43727452*h1_17+-0.084653825*h1_18+-0.054423757*h1_19+-0.36601782*h1_20+-0.07235579*h1_21+-0.03668117*h1_22+-0.05778476*h1_23+-0.020711297*h1_24+0.24233255);\n",
            "h2_11 = tanh(0.009613743*h1_0+-0.26873523*h1_1+-0.0937895*h1_2+-0.17233954*h1_3+0.2600588*h1_4+-0.41362444*h1_5+-0.13627699*h1_6+-0.03051018*h1_7+0.028052136*h1_8+0.06536644*h1_9+-0.029295161*h1_10+-0.017168272*h1_11+0.016685432*h1_12+-0.15576635*h1_13+0.08295777*h1_14+-0.16339928*h1_15+-0.069266655*h1_16+-0.07842072*h1_17+0.22933975*h1_18+-0.07057512*h1_19+0.172754*h1_20+-0.21173981*h1_21+-0.8436922*h1_22+0.28658*h1_23+-0.018899215*h1_24+-0.11145156);\n",
            "y = 0.13345447*h2_0+0.0023934222*h2_1+0.1793076*h2_2+0.51211137*h2_3+0.2699503*h2_4+0.11368197*h2_5+-0.5013421*h2_6+-0.61606425*h2_7+-0.037382632*h2_8+0.18645377*h2_9+0.32913932*h2_10+-0.00013747951*h2_11+-0.1796401;\n",
            "\n",
            "Cgd = pow(10, (y_cgd/normO_cgd + MinO_cgd))*W/15; //traning width was 15um\n",
            "Cgs = pow(10, (y_cgs/normO_cgs + MinO_cgs))*W/15;\n",
            "Cgg = Cgd+Cgs;\n",
            "\n",
            "Id = pow(10, (y/normI + MinI))*W*3;\n",
            "I(g, d) <+ Cgd*ddt(Vg-Vd) ;\n",
            "I(g, s) <+ Cgs*ddt(Vg-Vs) ;\n",
            "\n",
            "if (Vd >= Vs) begin\n",
            "\tI(d, s) <+ dir*Id;\n",
            "end\n",
            "\n",
            "else begin\n",
            "\tI(d, s) <+ dir*Id;\n",
            "end\n",
            "\n",
            "end\n",
            "endmodule\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x7hNVdRh0E7z",
        "outputId": "a1fb0ac6-0a5b-4571-88e1-2fe5d42e1f94"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "// VerilogA for GB_lib, IWO_verliogA, veriloga\n",
            "//*******************************************************************************\n",
            "//* * School of Electrical and Computer Engineering, Georgia Institute of Technology\n",
            "//* PI: Prof. Shimeng Yu\n",
            "//* All rights reserved.\n",
            "//*\n",
            "//* Copyright of the model is maintained by the developers, and the model is distributed under\n",
            "//* the terms of the Creative Commons Attribution-NonCommercial 4.0 International Public License\n",
            "//* http://creativecommons.org/licenses/by-nc/4.0/legalcode.\n",
            "//*\n",
            "//* THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND\n",
            "//* ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\n",
            "//* WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n",
            "//* DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\n",
            "//* FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\n",
            "//* DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\n",
            "//* SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\n",
            "//* CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\n",
            "//* OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n",
            "//* OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n",
            "//*\n",
            "//* Developer:\n",
            "//*  Gihun Choe gchoe6@gatech.edu\n",
            "//********************************************************************************/\n",
            "\n",
            "`include \"constants.vams\"\n",
            "`include \"disciplines.vams\"\n",
            "\n",
            "\n",
            "module IWO_verliogA(d, g, s);\n",
            "        inout d, g, s;\n",
            "        electrical d, g, s;\n",
            "\n",
            "        //***** parameters L and W ******//\n",
            "        parameter real W = 0.1; //get parameter fom spectre\n",
            "        parameter real L = 0.05; //get parameter fom spectre\n",
            "        parameter real T_stress = 0.0001; //set on cadence as variable\n",
            "        parameter real T_rec = 0.001;     //set on cadence as variable\n",
            "        parameter real Clk_loops = 50;    //set on cadence as variable\n",
            "        parameter real V_ov = 1.7;        //set on cadence as variable\n",
            "        parameter real Temp = 25;  //set on cadence as variable\n",
            "\n",
            "        parameter MinVg = -1.0 ;\n",
            "        parameter normVg = 0.2222222222222222 ;\n",
            "        parameter MinVd = 0.01 ;\n",
            "        parameter normVd = 0.2949852507374631 ;\n",
            "        parameter MinLg = 0.05 ;\n",
            "        parameter normLg = 1.4285714285714286 ;\n",
            "        parameter MinO = 8.15e-15 ;\n",
            "        parameter normO =33613445378151.26;\n",
            "        parameter MinI = -23.98798356587402 ;\n",
            "        parameter normI =0.04615548498417793;\n",
            "\n",
            "        parameter Mint_stress = 1 ;\n",
            "        parameter normt_stress = 0.001001001001001001 ;\n",
            "        parameter Mindelta_Vth = 0.1338811912 ;\n",
            "        parameter normdelta_Vth = 7.643496178720076 ;\n",
            "\n",
            "        real hvth1_0, hvth1_1, hvth1_2, hvth1_3, hvth1_4, hvth1_5, hvth1_6, hvth1_7, hvth1_8, hvth1_9, hvth1_10, hvth1_11, hvth1_12, hvth1_13, hvth1_14, hvth1_15, hvth1_16, hvth1_17, hvth1_18, hvth1_19;\n",
            "        real hvth2_0, hvth2_1, hvth2_2, hvth2_3, hvth2_4, hvth2_5, hvth2_6, hvth2_7, hvth2_8, hvth2_9;\n",
            "\n",
            "        real Vg, Vd, Vs, Vgs, Vds, Lg, Id, Cgg, Cgsd, Vgd;\n",
            "        real Vgsraw, Vgdraw, dir;\n",
            "        real t_stress, v_ov, t_rec, clk_loops, temp, delta_Vth;\n",
            "\n",
            "        real h1_0, h1_1, h1_2, h1_3, h1_4, h1_5, h1_6, h1_7, h1_8, h1_9, h1_10, h1_11, h1_12, h1_13, h1_14, h1_15, h1_16, h1_17, h1_18, h1_19, h1_20, h1_21, h1_22, h1_23, h1_24;\n",
            "        real h2_0, h2_1, h2_2, h2_3, h2_4, h2_5, h2_6, h2_7, h2_8, h2_9, h2_10, h2_11, h2_12, h2_13, h2_14, h2_15, h2_16, y;\n",
            "        real hc1_0, hc1_1, hc1_2, hc1_3, hc1_4, hc1_5, hc1_6, hc1_7, hc1_8, hc1_9;\n",
            "        real hc1_10, hc1_11, hc1_12, hc1_13, hc1_14, hc1_15, hc1_16;\n",
            "        real hc2_0, hc2_1, hc2_2, hc2_3, hc2_4, hc2_5, hc2_6, hc2_7, hc2_8, hc2_9;\n",
            "        real hc2_10, hc2_11, hc2_12, hc2_13, hc2_14, hc2_15, hc2_16, yc, yvth;\n",
            "\n",
            "analog begin\n",
            "\n",
            "t_stress = (T_stress - Mint_stress)*normt_stress;\n",
            "\n",
            "//******************** delta_Vth NN **********************************//\n",
            "\n",
            "hvth1_0 = tanh(5.7647386*t_stress+0.48540172);\n",
            "hvth1_1 = tanh(0.38525197*t_stress+-0.07289814);\n",
            "hvth1_2 = tanh(-0.32286096*t_stress+0.09689432);\n",
            "hvth1_3 = tanh(-0.16950038*t_stress+0.014454721);\n",
            "hvth1_4 = tanh(0.15628646*t_stress+0.11375919);\n",
            "hvth1_5 = tanh(0.23828815*t_stress+-0.10353576);\n",
            "hvth1_6 = tanh(0.6046738*t_stress+-0.054057166);\n",
            "hvth1_7 = tanh(-0.092044145*t_stress+0.19634022);\n",
            "hvth1_8 = tanh(0.44190115*t_stress+-0.067294195);\n",
            "hvth1_9 = tanh(-0.20995724*t_stress+0.013192425);\n",
            "hvth1_10 = tanh(-0.5087854*t_stress+0.07794392);\n",
            "hvth1_11 = tanh(-0.40744516*t_stress+0.09584385);\n",
            "hvth1_12 = tanh(0.3010627*t_stress+-0.14065638);\n",
            "hvth1_13 = tanh(0.072046116*t_stress+-0.18140705);\n",
            "hvth1_14 = tanh(-0.5535074*t_stress+0.42590642);\n",
            "hvth1_15 = tanh(-0.00020826115*t_stress+-0.068295546);\n",
            "hvth1_16 = tanh(-0.15523441*t_stress+-0.10732198);\n",
            "hvth1_17 = tanh(-0.048792616*t_stress+-0.07019651);\n",
            "hvth1_18 = tanh(0.5111007*t_stress+-0.35923317);\n",
            "hvth1_19 = tanh(0.32603025*t_stress+-0.23393095);\n",
            "\n",
            "hvth2_0 = tanh(0.015660234*hvth1_0+-0.25394896*hvth1_1+-0.032009657*hvth1_2+-0.33710805*hvth1_3+-0.33312714*hvth1_4+0.043498244*hvth1_5+-0.37240052*hvth1_6+0.32132673*hvth1_7+0.2781878*hvth1_8+0.33936915*hvth1_9+0.051309165*hvth1_10+0.4404143*hvth1_11+0.43792498*hvth1_12+-0.089881994*hvth1_13+0.18210214*hvth1_14+-0.39629757*hvth1_15+-0.13833846*hvth1_16+0.046972238*hvth1_17+-0.24265075*hvth1_18+0.061062954*hvth1_19+0.053898938);\n",
            "hvth2_1 = tanh(-0.23036939*hvth1_0+0.27745017*hvth1_1+-0.030338975*hvth1_2+-0.39378327*hvth1_3+-0.19258437*hvth1_4+0.21238573*hvth1_5+-0.3708216*hvth1_6+0.27995133*hvth1_7+-0.44190142*hvth1_8+-0.031310305*hvth1_9+0.32129613*hvth1_10+0.2815101*hvth1_11+-0.06808714*hvth1_12+-0.30598393*hvth1_13+0.46908206*hvth1_14+-0.19565621*hvth1_15+-0.32040843*hvth1_16+-0.42600983*hvth1_17+-0.3888712*hvth1_18+-0.4405881*hvth1_19+0.061263777);\n",
            "hvth2_2 = tanh(0.1291172*hvth1_0+-0.20675837*hvth1_1+0.09629446*hvth1_2+-0.22967674*hvth1_3+0.3567697*hvth1_4+0.22345737*hvth1_5+-0.11807843*hvth1_6+-0.30533692*hvth1_7+-0.29882887*hvth1_8+-0.19123012*hvth1_9+0.42496908*hvth1_10+-0.12336621*hvth1_11+0.28715792*hvth1_12+0.16390803*hvth1_13+-0.27300775*hvth1_14+-0.19841349*hvth1_15+-0.13276285*hvth1_16+0.19518016*hvth1_17+-0.38239974*hvth1_18+-0.07332923*hvth1_19+0.040716734);\n",
            "hvth2_3 = tanh(-0.5909201*hvth1_0+0.32155797*hvth1_1+0.24177419*hvth1_2+-0.23851821*hvth1_3+-0.4757214*hvth1_4+-0.42064616*hvth1_5+0.15469414*hvth1_6+0.40514684*hvth1_7+-0.30713296*hvth1_8+0.3305534*hvth1_9+-0.28169695*hvth1_10+-0.13940963*hvth1_11+0.06511849*hvth1_12+0.16791591*hvth1_13+-0.16068037*hvth1_14+-0.15648803*hvth1_15+0.4800944*hvth1_16+0.32172346*hvth1_17+-0.2693486*hvth1_18+-0.19620566*hvth1_19+-0.035231445);\n",
            "hvth2_4 = tanh(-0.620505*hvth1_0+0.24934234*hvth1_1+-0.5093152*hvth1_2+-0.2619674*hvth1_3+-0.40698877*hvth1_4+0.41210732*hvth1_5+0.19491538*hvth1_6+-0.06662507*hvth1_7+0.5658649*hvth1_8+0.148295*hvth1_9+-0.53892213*hvth1_10+-0.3242818*hvth1_11+1.0698062*hvth1_12+0.66648823*hvth1_13+-1.2758291*hvth1_14+0.34647503*hvth1_15+0.31783825*hvth1_16+-0.03538063*hvth1_17+1.1696657*hvth1_18+1.7565093*hvth1_19+-0.305608);\n",
            "hvth2_5 = tanh(0.30714843*hvth1_0+-0.2505467*hvth1_1+0.37955177*hvth1_2+-0.12742479*hvth1_3+0.21503358*hvth1_4+-0.40592924*hvth1_5+-0.20808874*hvth1_6+0.6643736*hvth1_7+0.19315416*hvth1_8+0.114561744*hvth1_9+0.15944222*hvth1_10+0.25653172*hvth1_11+-0.34032595*hvth1_12+-0.32916787*hvth1_13+0.40763733*hvth1_14+0.10848248*hvth1_15+-0.35076502*hvth1_16+-0.4759938*hvth1_17+-0.6922618*hvth1_18+-0.8507297*hvth1_19+0.13345729);\n",
            "hvth2_6 = tanh(0.5476934*hvth1_0+0.030469894*hvth1_1+0.2060474*hvth1_2+0.2142246*hvth1_3+-0.3018381*hvth1_4+0.2775381*hvth1_5+-0.109109014*hvth1_6+-0.030637167*hvth1_7+0.18106034*hvth1_8+-0.36141196*hvth1_9+0.097291015*hvth1_10+-0.20773186*hvth1_11+-0.042468738*hvth1_12+0.19633642*hvth1_13+-0.15511356*hvth1_14+0.35691926*hvth1_15+-0.08209184*hvth1_16+0.33671457*hvth1_17+-0.04761894*hvth1_18+-0.35298976*hvth1_19+0.046065476);\n",
            "hvth2_7 = tanh(-0.06919702*hvth1_0+0.15043844*hvth1_1+0.42857102*hvth1_2+0.28022274*hvth1_3+-0.42794165*hvth1_4+0.08731942*hvth1_5+0.31545204*hvth1_6+-0.29850867*hvth1_7+0.15515916*hvth1_8+0.044068467*hvth1_9+-0.24808201*hvth1_10+0.20796198*hvth1_11+0.40620986*hvth1_12+0.13524604*hvth1_13+-0.15850142*hvth1_14+0.14922251*hvth1_15+-0.2640195*hvth1_16+0.4726488*hvth1_17+0.19361697*hvth1_18+-0.009510106*hvth1_19+-0.057470787);\n",
            "hvth2_8 = tanh(0.36502245*hvth1_0+-0.2590835*hvth1_1+0.39329097*hvth1_2+0.053694468*hvth1_3+-0.040928334*hvth1_4+-0.35671663*hvth1_5+-0.014835431*hvth1_6+0.2232648*hvth1_7+0.29714307*hvth1_8+0.18555437*hvth1_9+0.3146754*hvth1_10+-0.2975361*hvth1_11+-0.06868758*hvth1_12+-0.20261012*hvth1_13+-0.40037477*hvth1_14+-0.063908*hvth1_15+-0.0560676*hvth1_16+0.11081772*hvth1_17+0.27904797*hvth1_18+0.04556402*hvth1_19+0.07602377);\n",
            "hvth2_9 = tanh(0.23505636*hvth1_0+-0.31137058*hvth1_1+0.86015993*hvth1_2+0.07274245*hvth1_3+-0.32266733*hvth1_4+-0.14096372*hvth1_5+-0.7108863*hvth1_6+-0.30789322*hvth1_7+-0.85494477*hvth1_8+-0.06513376*hvth1_9+0.65265805*hvth1_10+0.76861066*hvth1_11+-1.5586225*hvth1_12+0.08678662*hvth1_13+0.9954604*hvth1_14+-0.06995699*hvth1_15+0.1400858*hvth1_16+-0.03144032*hvth1_17+-0.468318*hvth1_18+-1.3921238*hvth1_19+0.00811575);\n",
            "\n",
            "yvth = 0.45023876*hvth2_0+0.59534264*hvth2_1+0.44379938*hvth2_2+-0.8880583*hvth2_3+0.4149087*hvth2_4+-0.40795714*hvth2_5+0.20554432*hvth2_6+-0.72165364*hvth2_7+0.76366603*hvth2_8+-0.27383897*hvth2_9+0.069368996;\n",
            "\n",
            "delta_Vth = (yvth - Mindelta_Vth) * normdelta_Vth;\n",
            "$strobe(\"dvth=$g\",delta_Vth);\n",
            "\n",
            "\n",
            "        Vg = V(g) ;\n",
            "        Vs = V(s) ;\n",
            "        Vd = V(d) ;\n",
            "        Vgsraw = Vg-Vs ;\n",
            "        Vgdraw = Vg-Vd ;\n",
            "\n",
            "if (Vgsraw >= Vgdraw) begin\n",
            "        Vgs = ((Vg-Vs) - MinVg) * normVg ;\n",
            "        dir = 1;\n",
            "end\n",
            "\n",
            "else begin\n",
            "        Vgs = ((Vg-Vd) - MinVg) * normVg ;\n",
            "        dir = -1;\n",
            "end\n",
            "\n",
            "        Vds = (abs(Vd-Vs) - MinVd) * normVd ;\n",
            "        Vgs = Vgs - delta_Vth ; // BTI Vth variation\n",
            "        Lg = (L -MinLg)*normLg ;\n",
            "\n",
            "\n",
            "\n",
            "//******************** C-V NN **********************************//\n",
            "hc1_0 = tanh(-0.99871427*Vgs+-0.16952373*Vds+0.32118186*Lg+0.41485423);\n",
            "hc1_1 = tanh(0.31587568*Vgs+0.13397887*Vds+-0.4541538*Lg+-0.3630942);\n",
            "hc1_2 = tanh(-0.76281804*Vgs+0.09352969*Vds+1.1961353*Lg+0.3904977);\n",
            "hc1_3 = tanh(-1.115087*Vgs+0.85752946*Vds+-0.11746484*Lg+0.5500279);\n",
            "hc1_4 = tanh(1.0741386*Vgs+0.82041687*Vds+0.19092631*Lg+-0.4009425);\n",
            "hc1_5 = tanh(-0.47921795*Vgs+-0.8749933*Vds+-0.054768667*Lg+0.62785167);\n",
            "hc1_6 = tanh(0.5449184*Vgs+-4.409165*Vds+-0.072947875*Lg+-0.31324536);\n",
            "hc1_7 = tanh(-2.9224303*Vgs+2.7675478*Vds+0.08862238*Lg+0.6493558);\n",
            "hc1_8 = tanh(0.65050656*Vgs+-0.29751927*Vds+0.1571876*Lg+-0.38088372);\n",
            "hc1_9 = tanh(-0.30384183*Vgs+0.5649165*Vds+2.6806898*Lg+0.3197917);\n",
            "hc1_10 = tanh(-0.095988505*Vgs+2.0158541*Vds+-0.42972717*Lg+-0.30388466);\n",
            "hc1_11 = tanh(6.7699738*Vgs+-0.07234483*Vds+-0.013545353*Lg+-1.3694142);\n",
            "hc1_12 = tanh(-0.3404029*Vgs+0.0443459*Vds+0.89597*Lg+0.069993004);\n",
            "hc1_13 = tanh(0.62300175*Vgs+-0.29515797*Vds+1.6753465*Lg+-0.6520838);\n",
            "hc1_14 = tanh(0.37957156*Vgs+0.2237372*Vds+0.08591952*Lg+0.13126835);\n",
            "hc1_15 = tanh(0.19949242*Vgs+-0.26481664*Vds+-0.41059187*Lg+-0.40832308);\n",
            "hc1_16 = tanh(0.98966587*Vgs+-0.24259183*Vds+0.36584845*Lg+-0.8024042);\n",
            "\n",
            "hc2_0 = tanh(-0.91327864*hc1_0+0.4696781*hc1_1+0.3302202*hc1_2+0.11393868*hc1_3+0.45070222*hc1_4+-0.2894044*hc1_5+0.55066*hc1_6+-1.6242687*hc1_7+-0.38140613*hc1_8+0.032771554*hc1_9+0.17647126);\n",
            "hc2_1 = tanh(-1.1663305*hc1_0+-0.523984*hc1_1+-0.90804136*hc1_2+-0.7418044*hc1_3+1.456171*hc1_4+-0.16802542*hc1_5+0.8235596*hc1_6+-2.2246246*hc1_7+-0.40805355*hc1_8+0.7207601*hc1_9+0.4729169);\n",
            "hc2_2 = tanh(-0.69065374*hc1_0+0.40205315*hc1_1+-0.49410668*hc1_2+0.8681325*hc1_3+0.471351*hc1_4+0.46939445*hc1_5+0.45568785*hc1_6+-0.92935294*hc1_7+-0.8209646*hc1_8+0.1158967*hc1_9+-0.075798474);\n",
            "hc2_3 = tanh(0.11535446*hc1_0+-0.06296927*hc1_1+-0.6740435*hc1_2+0.7428315*hc1_3+0.05890677*hc1_4+0.9579441*hc1_5+-0.037319*hc1_6+-0.18491426*hc1_7+-0.02981994*hc1_8+0.038347963*hc1_9+0.039531134);\n",
            "hc2_4 = tanh(0.37817463*hc1_0+-0.6811279*hc1_1+1.1388369*hc1_2+0.19983096*hc1_3+-0.20415118*hc1_4+1.3022176*hc1_5+0.22571652*hc1_6+0.1690611*hc1_7+-0.56475276*hc1_8+-0.4069731*hc1_9+0.99962974);\n",
            "hc2_5 = tanh(0.032873478*hc1_0+-0.05209407*hc1_1+-0.010839908*hc1_2+-0.13892579*hc1_3+-0.050480977*hc1_4+0.0127089145*hc1_5+0.0052771433*hc1_6+0.02029242*hc1_7+-0.08705659*hc1_8+0.0254766*hc1_9+0.025135752);\n",
            "hc2_6 = tanh(-0.34639204*hc1_0+0.06937975*hc1_1+0.18671949*hc1_2+-0.18912783*hc1_3+0.1312504*hc1_4+0.4627272*hc1_5+-0.42590702*hc1_6+-0.10966313*hc1_7+0.66083515*hc1_8+-0.050718334*hc1_9+0.08234678);\n",
            "hc2_7 = tanh(0.90656275*hc1_0+-0.037281644*hc1_1+0.77237594*hc1_2+1.4710428*hc1_3+0.13597831*hc1_4+-0.059844542*hc1_5+-0.7801535*hc1_6+3.7814677*hc1_7+-0.5976644*hc1_8+0.2721995*hc1_9+0.023777716);\n",
            "hc2_8 = tanh(0.39720625*hc1_0+-0.45262313*hc1_1+0.19873238*hc1_2+0.9750888*hc1_3+-0.9427992*hc1_4+0.4487432*hc1_5+-0.3372945*hc1_6+0.33729544*hc1_7+-0.1667088*hc1_8+-0.5707525*hc1_9+0.27954483);\n",
            "hc2_9 = tanh(0.28551984*hc1_0+-0.68350387*hc1_1+0.9916423*hc1_2+-0.8254094*hc1_3+0.09875706*hc1_4+0.47609732*hc1_5+-0.058662917*hc1_6+0.09181381*hc1_7+0.09592329*hc1_8+1.3940467*hc1_9+0.3865768);\n",
            "hc2_10 = tanh(0.098737516*hc1_0+0.060473576*hc1_1+0.42824662*hc1_2+0.15018038*hc1_3+0.082621895*hc1_4+-0.00019039502*hc1_5+-0.3321634*hc1_6+0.7936295*hc1_7+-0.041197542*hc1_8+0.6530619*hc1_9+0.1338804);\n",
            "hc2_11 = tanh(-0.3585284*hc1_0+-0.09956017*hc1_1+0.17224246*hc1_2+-0.016925728*hc1_3+-0.46462816*hc1_4+-0.5649022*hc1_5+1.251695*hc1_6+-0.4303161*hc1_7+0.48546878*hc1_8+0.22958975*hc1_9+-0.27899802);\n",
            "hc2_12 = tanh(0.8565631*hc1_0+-0.7622999*hc1_1+0.32367912*hc1_2+1.4776785*hc1_3+0.2712369*hc1_4+0.2275511*hc1_5+0.39908803*hc1_6+4.2305493*hc1_7+-0.3467536*hc1_8+0.41231114*hc1_9+0.47123823);\n",
            "hc2_13 = tanh(-0.26411077*hc1_0+-0.17583853*hc1_1+0.045439184*hc1_2+-0.13801138*hc1_3+0.03278085*hc1_4+-0.45625108*hc1_5+-0.17905861*hc1_6+0.3060186*hc1_7+-0.3361926*hc1_8+-0.050055273*hc1_9+0.17996444);\n",
            "hc2_14 = tanh(0.016094366*hc1_0+-0.013196736*hc1_1+0.4124856*hc1_2+0.22926371*hc1_3+-0.35071182*hc1_4+-0.34217188*hc1_5+-0.69466996*hc1_6+1.0563152*hc1_7+-0.18019852*hc1_8+0.061871335*hc1_9+0.09762555);\n",
            "hc2_15 = tanh(-0.18970782*hc1_0+0.3624813*hc1_1+-1.3419824*hc1_2+0.103635244*hc1_3+-0.14595217*hc1_4+-0.1899393*hc1_5+0.176524*hc1_6+-0.4428012*hc1_7+-0.39544868*hc1_8+-0.45783517*hc1_9+0.1884755);\n",
            "hc2_16 = tanh(-0.1585831*hc1_0+0.035894837*hc1_1+-0.14261873*hc1_2+0.25914294*hc1_3+0.040607046*hc1_4+0.11555795*hc1_5+0.0022548323*hc1_6+-0.002149359*hc1_7+0.07067297*hc1_8+0.019578662*hc1_9+0.16657573);\n",
            "yc = 0.17503543*hc2_0+-0.15556754*hc2_1+-0.125455*hc2_2+-0.07502612*hc2_3+-0.16671115*hc2_4+0.29854503;\n",
            "\n",
            "Cgg = (yc / normO + MinO)*W;\n",
            "Cgsd = Cgg/2 ;\n",
            "\n",
            "//******************** I-V NN **********************************//\n",
            "h1_0 = tanh(0.0023826673*Vgs+-0.0009636134*Vds+0.006639037*Lg+-0.0004692053);\n",
            "h1_1 = tanh(-7.8007555*Vgs+2.639791*Vds+-0.025273597*Lg+-1.1747514);\n",
            "h1_2 = tanh(1.9884652*Vgs+0.62111*Vds+-0.11525345*Lg+-0.14575638);\n",
            "h1_3 = tanh(0.10625471*Vgs+0.09442053*Vds+-0.052119628*Lg+1.1568379);\n",
            "h1_4 = tanh(4.058087*Vgs+-0.7568158*Vds+0.008070099*Lg+-0.4820452);\n",
            "h1_5 = tanh(-1.3065948*Vgs+-0.0492497*Vds+-0.081622526*Lg+0.20351954);\n",
            "h1_6 = tanh(-2.9507525*Vgs+-0.91150546*Vds+-0.06477873*Lg+0.09646383);\n",
            "h1_7 = tanh(-0.5886177*Vgs+0.63788587*Vds+-0.13710928*Lg+0.30260038);\n",
            "h1_8 = tanh(-0.4311161*Vgs+-0.7250705*Vds+-0.06134645*Lg+0.44054285);\n",
            "h1_9 = tanh(0.012132638*Vgs+-0.42545322*Vds+-0.66478956*Lg+0.13182367);\n",
            "h1_10 = tanh(3.289041e-05*Vgs+0.0011367714*Vds+-0.0051904405*Lg+5.4112927e-05);\n",
            "h1_11 = tanh(-0.6007774*Vgs+0.09259224*Vds+0.2170182*Lg+-0.2908748);\n",
            "h1_12 = tanh(0.28018302*Vgs+1.3014064*Vds+-0.13490067*Lg+-0.22006753);\n",
            "h1_13 = tanh(0.01864017*Vgs+-13.0928755*Vds+-0.0037254083*Lg+-0.10935691);\n",
            "h1_14 = tanh(-0.0049708197*Vgs+0.0022613218*Vds+-0.014408149*Lg+0.0011340647);\n",
            "h1_15 = tanh(-0.094654046*Vgs+-0.4174114*Vds+0.18892045*Lg+0.05248958);\n",
            "h1_16 = tanh(-0.25090316*Vgs+-0.11111481*Vds+-0.009133225*Lg+0.08377026);\n",
            "h1_17 = tanh(0.9275306*Vgs+0.40686756*Vds+0.14127344*Lg+-0.059246097);\n",
            "h1_18 = tanh(-0.27084363*Vgs+0.07915911*Vds+-10.836302*Lg+-1.1535788);\n",
            "h1_19 = tanh(1.6852072*Vgs+-0.24846223*Vds+0.049734037*Lg+-0.19625053);\n",
            "\n",
            "h2_0 = tanh(1.1139417*h1_0+-0.40033522*h1_1+0.920759*h1_2+0.47607598*h1_3+0.3978683*h1_4+0.8008105*h1_5+-0.40445566*h1_6+0.8668027*h1_7+0.23365597*h1_8+-0.28254375*h1_9+-0.63985884*h1_10+-0.62523574*h1_11+0.8338383*h1_12+-2.0540943*h1_13+1.0749483*h1_14+-1.1075479*h1_15+0.60926706*h1_16+1.1118286*h1_17+-0.8917559*h1_18+0.029025732*h1_19+0.6622382);\n",
            "h2_1 = tanh(0.37195024*h1_0+-0.761445*h1_1+0.6514153*h1_2+0.6211995*h1_3+-0.063539445*h1_4+0.31260127*h1_5+-0.3529579*h1_6+0.50422627*h1_7+0.18943883*h1_8+-0.38029787*h1_9+-0.3464503*h1_10+-0.48301366*h1_11+0.081978925*h1_12+-0.08305682*h1_13+-0.08420117*h1_14+-0.8029313*h1_15+-0.37052512*h1_16+0.4553502*h1_17+-0.71959645*h1_18+-0.17320661*h1_19+0.6566257);\n",
            "h2_2 = tanh(-0.021933522*h1_0+-0.017245224*h1_1+0.030417712*h1_2+0.2967218*h1_3+-0.048668377*h1_4+0.029245213*h1_5+0.011756416*h1_6+0.004705658*h1_7+-0.042515088*h1_8+0.010462476*h1_9+0.001331279*h1_10+0.1694541*h1_11+-0.010949079*h1_12+-0.004144526*h1_13+0.04856694*h1_14+-0.010465159*h1_15+0.24588406*h1_16+-0.028521152*h1_17+0.12161568*h1_18+0.1753255*h1_19+-0.09125355);\n",
            "h2_3 = tanh(-0.025546636*h1_0+-3.2702503*h1_1+0.46812135*h1_2+-0.25135994*h1_3+3.0725436*h1_4+0.22513995*h1_5+-1.0327209*h1_6+-0.56274736*h1_7+0.4726107*h1_8+0.38182434*h1_9+0.016403453*h1_10+-0.09128962*h1_11+0.20997792*h1_12+-0.4951615*h1_13+0.026481293*h1_14+0.6085288*h1_15+-0.09078652*h1_16+0.36613584*h1_17+0.26257026*h1_18+0.39794916*h1_19+-0.52920526);\n",
            "h2_4 = tanh(-0.44392154*h1_0+-0.5650475*h1_1+0.91716766*h1_2+0.96703196*h1_3+0.4597048*h1_4+1.452921*h1_5+-0.8115141*h1_6+0.92326456*h1_7+0.2862403*h1_8+-0.9441585*h1_9+0.22188367*h1_10+-1.0092766*h1_11+0.5495966*h1_12+0.44870532*h1_13+0.48705962*h1_14+-0.6957133*h1_15+0.27711377*h1_16+1.0138507*h1_17+-0.34337458*h1_18+-0.21548931*h1_19+0.42685974);\n",
            "h2_5 = tanh(0.0003710325*h1_0+0.5746112*h1_1+0.4144258*h1_2+0.04459103*h1_3+1.2373503*h1_4+0.12786175*h1_5+-0.16099685*h1_6+0.9826207*h1_7+-0.09701193*h1_8+-0.4379135*h1_9+-0.0035542254*h1_10+0.04205593*h1_11+0.65820116*h1_12+0.3810506*h1_13+0.0004259507*h1_14+-0.21782044*h1_15+-0.057544652*h1_16+0.24420041*h1_17+-0.10138292*h1_18+-0.2387106*h1_19+0.867847);\n",
            "h2_6 = tanh(-0.0077049686*h1_0+-0.05349668*h1_1+-0.11536639*h1_2+0.06141029*h1_3+-0.034344573*h1_4+0.21672213*h1_5+-0.09835135*h1_6+-0.25849992*h1_7+0.10576329*h1_8+-0.14288934*h1_9+0.027846925*h1_10+-0.20104973*h1_11+0.24784875*h1_12+0.03396087*h1_13+-0.016039118*h1_14+-0.03147038*h1_15+-0.023109786*h1_16+-0.118931994*h1_17+0.18256636*h1_18+0.09981429*h1_19+0.079372324);\n",
            "h2_7 = tanh(-0.009369999*h1_0+-2.4468672*h1_1+-0.41433397*h1_2+-0.6289744*h1_3+-1.8285819*h1_4+0.12905455*h1_5+0.83588725*h1_6+0.11491322*h1_7+0.3871084*h1_8+0.07153052*h1_9+0.013530584*h1_10+0.40614852*h1_11+0.10340061*h1_12+-0.04473306*h1_13+0.0075287875*h1_14+0.5593612*h1_15+0.34401885*h1_16+-0.5538749*h1_17+0.36615464*h1_18+-0.2666981*h1_19+0.20194086);\n",
            "h2_8 = tanh(-0.0017045025*h1_0+-1.1388719*h1_1+0.25932005*h1_2+-0.6482845*h1_3+2.263395*h1_4+0.115192235*h1_5+-0.40946937*h1_6+-0.038117886*h1_7+-0.03703431*h1_8+0.05965774*h1_9+0.0032721795*h1_10+0.2561857*h1_11+0.47324425*h1_12+-0.25200802*h1_13+0.0017081021*h1_14+0.29819545*h1_15+0.061945435*h1_16+0.13516657*h1_17+0.5409335*h1_18+-0.17400871*h1_19+0.11659722);\n",
            "h2_9 = tanh(0.014866875*h1_0+1.0399909*h1_1+1.0108095*h1_2+-0.0918755*h1_3+-0.7634763*h1_4+-0.49749368*h1_5+-0.7020006*h1_6+-0.16384888*h1_7+-0.07833025*h1_8+-0.23745225*h1_9+-0.023095092*h1_10+-0.29244414*h1_11+0.6255917*h1_12+0.41413808*h1_13+-0.019823061*h1_14+-1.0593235*h1_15+-0.42221433*h1_16+0.98025715*h1_17+-0.71444243*h1_18+0.84347373*h1_19+0.28510216);\n",
            "h2_10 = tanh(-0.013515852*h1_0+0.68840384*h1_1+-0.030184174*h1_2+-0.48098114*h1_3+0.08609854*h1_4+-0.50878614*h1_5+0.26547763*h1_6+-0.7151076*h1_7+0.111288495*h1_8+0.53379977*h1_9+0.022258151*h1_10+0.16971351*h1_11+-1.2486296*h1_12+0.9649942*h1_13+0.0028906881*h1_14+0.31582054*h1_15+0.24213083*h1_16+-0.27242163*h1_17+0.11330636*h1_18+0.17038865*h1_19+-0.42089102);\n",
            "h2_11 = tanh(-0.009949424*h1_0+-0.6704206*h1_1+0.13177924*h1_2+0.07316155*h1_3+-0.28207502*h1_4+-0.32401362*h1_5+0.020746209*h1_6+-0.05517531*h1_7+0.10340257*h1_8+0.29381263*h1_9+0.020497978*h1_10+0.031984854*h1_11+0.079552434*h1_12+-7.7884445*h1_13+0.01224806*h1_14+0.8700812*h1_15+-0.16562358*h1_16+0.14370379*h1_17+0.2389181*h1_18+-0.19771136*h1_19+0.33039534);\n",
            "h2_12 = tanh(-0.002431529*h1_0+0.30154988*h1_1+-0.45072728*h1_2+0.27823612*h1_3+-0.25173753*h1_4+-0.58188903*h1_5+0.39836177*h1_6+-0.060329597*h1_7+0.1667837*h1_8+-0.09419194*h1_9+0.0108116735*h1_10+-0.3581154*h1_11+0.15690842*h1_12+-0.41209573*h1_13+-0.0070331707*h1_14+-0.23976392*h1_15+0.16620094*h1_16+-0.034617994*h1_17+0.32754526*h1_18+0.8490298*h1_19+0.026407415);\n",
            "h2_13 = tanh(-0.7385622*h1_0+-0.25174448*h1_1+2.090295*h1_2+0.67499936*h1_3+0.07392262*h1_4+0.5413692*h1_5+-0.8287433*h1_6+2.2810504*h1_7+1.1917778*h1_8+1.3719273*h1_9+0.25762329*h1_10+-2.1111712*h1_11+1.6987114*h1_12+-0.437825*h1_13+1.7649944*h1_14+-2.2254016*h1_15+-2.3555171*h1_16+-0.7732424*h1_17+-1.0756916*h1_18+-0.1367373*h1_19+1.5085722);\n",
            "h2_14 = tanh(-0.002535408*h1_0+-4.2686224*h1_1+-0.045537975*h1_2+0.08043166*h1_3+1.7274952*h1_4+-0.1260494*h1_5+-0.23011239*h1_6+-0.21025337*h1_7+0.48537356*h1_8+0.39049447*h1_9+-0.006565428*h1_10+-0.293028*h1_11+-1.1253971*h1_12+-0.2890831*h1_13+0.0042421576*h1_14+0.4230598*h1_15+-0.17869289*h1_16+-0.034056116*h1_17+0.61135995*h1_18+0.6345532*h1_19+0.73144835);\n",
            "h2_15 = tanh(-0.5239093*h1_0+0.49806875*h1_1+0.53555894*h1_2+1.2645539*h1_3+1.011548*h1_4+0.39209503*h1_5+-0.78194916*h1_6+0.94610345*h1_7+0.9121405*h1_8+-1.4693716*h1_9+-0.58336824*h1_10+-1.1661471*h1_11+-0.16158457*h1_12+0.058512915*h1_13+1.3358645*h1_14+0.5742125*h1_15+-1.3325212*h1_16+1.1804186*h1_17+-1.4877121*h1_18+0.6357802*h1_19+1.1328585);\n",
            "h2_16 = tanh(0.0095406*h1_0+-0.53232694*h1_1+0.1535685*h1_2+0.26108417*h1_3+-0.08375187*h1_4+0.3462123*h1_5+-0.61755395*h1_6+0.0015145333*h1_7+-0.108780764*h1_8+0.2555477*h1_9+0.00023940884*h1_10+-0.26577523*h1_11+-0.27689674*h1_12+0.43049082*h1_13+-0.025424613*h1_14+0.24634649*h1_15+-0.22579487*h1_16+0.37249807*h1_17+-0.8058662*h1_18+-0.77451354*h1_19+0.5821276);\n",
            "y = -0.17525196*h2_0+-0.22995844*h2_1+0.0026147955*h2_2+0.091129646*h2_3+-0.4534783*h2_4+0.4119419*h2_5+-1.4205361e-05*h2_6+-0.11359831*h2_7+0.1762025*h2_8+-0.1002102*h2_9+-0.37161925*h2_10+0.2683793*h2_11+0.051897053*h2_12+0.19354156*h2_13+-0.10134394*h2_14+0.3937854*h2_15+-0.3296008*h2_16+0.3220947;\n",
            "\n",
            "        Id = pow(10, (y/normI + MinI)) ;\n",
            "\n",
            "if (Id <= 1e-15) begin //limit\n",
            "        Id = 1e-15;\n",
            "        //Id = Id;\n",
            "end\n",
            "else begin\n",
            "        Id = Id;\n",
            "end  //limit end\n",
            "\n",
            "\n",
            "        I(g, d) <+ Cgsd*ddt(Vg-Vd) ;\n",
            "        I(g, s) <+ Cgsd*ddt(Vg-Vs) ;\n",
            "\n",
            "if (Vgsraw >= Vgdraw) begin\n",
            "        I(d, s) <+ dir*Id*W ;\n",
            "\n",
            "end\n",
            "\n",
            "else begin\n",
            "        I(d, s) <+ dir*Id*W ;\n",
            "\n",
            "end\n",
            "\n",
            "end\n",
            "\n",
            "endmodule\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "with torch.no_grad():\n",
        "\n",
        "    output = []\n",
        "    # Iterate over the dataloader for training data\n",
        "    for i, data in enumerate(testdataloader, 0):\n",
        "        inputs, targets = data\n",
        "        inputs, targets = inputs.float(), targets.float()\n",
        "        targets = targets.reshape((targets.shape[0],1))\n",
        "\n",
        "        #zero the gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        #perform forward pass\n",
        "        outputs = model(inputs)\n",
        "        output.append(outputs)\n",
        "# Process is complete.\n",
        "#print('Training process has finished.')\n",
        "\n",
        "# Extract the weights and biases from the model\n",
        "weights_1 = model.fc1.weight.detach().numpy()\n",
        "bias_1 = model.fc1.bias.detach().numpy()\n",
        "weights_2 = model.fc2.weight.detach().numpy()\n",
        "bias_2 = model.fc2.bias.detach().numpy()\n",
        "weights_3 = model.fc3.weight.detach().numpy()\n",
        "bias_3 = model.fc3.bias.detach().numpy()\n",
        "\n",
        "def generate_variable_declarations(weights_shape, layer_prefix):\n",
        "    declarations = \"\"\n",
        "    num_neurons = weights_shape  # Number of neurons is determined by the first dimension of the weights matrix\n",
        "    layer_declarations = \", \".join([f\"{layer_prefix}_{i}\" for i in range(num_neurons)]) + \";\"\n",
        "    declarations += layer_declarations\n",
        "    return declarations\n",
        "\n",
        "# Use the function to generate declarations for each layer\n",
        "h1_declarations = generate_variable_declarations(weights_1.shape[0], \"hvth1\")\n",
        "h2_declarations = generate_variable_declarations(weights_2.shape[0], \"hvth2\")\n",
        "\n",
        "verilog_code = \"\"\"\n",
        "// VerilogA for GB_lib, IWO_verliogA, veriloga\n",
        "//*******************************************************************************\n",
        "//* * School of Electrical and Computer Engineering, Georgia Institute of Technology\n",
        "//* PI: Prof. Shimeng Yu\n",
        "//* All rights reserved.\n",
        "//*\n",
        "//* Copyright of the model is maintained by the developers, and the model is distributed under\n",
        "//* the terms of the Creative Commons Attribution-NonCommercial 4.0 International Public License\n",
        "//* http://creativecommons.org/licenses/by-nc/4.0/legalcode.\n",
        "//*\n",
        "//* THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND\n",
        "//* ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\n",
        "//* WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n",
        "//* DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\n",
        "//* FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\n",
        "//* DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\n",
        "//* SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\n",
        "//* CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\n",
        "//* OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n",
        "//* OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n",
        "//*\n",
        "//* Developer:\n",
        "//*  Gihun Choe gchoe6@gatech.edu\n",
        "//********************************************************************************/\n",
        "\n",
        "`include \"constants.vams\"\n",
        "`include \"disciplines.vams\"\n",
        "\n",
        "\n",
        "module IWO_verliogA(d, g, s);\n",
        "        inout d, g, s;\n",
        "        electrical d, g, s;\n",
        "\n",
        "        //***** parameters L and W ******//\n",
        "        parameter real W = 0.1; //get parameter fom spectre\n",
        "        parameter real L = 0.05; //get parameter fom spectre\n",
        "        parameter real T_stress = 0.0001; //set on cadence as variable\n",
        "        parameter real T_rec = 0.001;     //set on cadence as variable\n",
        "        parameter real Clk_loops = 50;    //set on cadence as variable\n",
        "        parameter real V_ov = 1.7;        //set on cadence as variable\n",
        "        parameter real Temp = 25;  //set on cadence as variable\n",
        "\n",
        "        parameter MinVg = -1.0 ;\n",
        "        parameter normVg = 0.2222222222222222 ;\n",
        "        parameter MinVd = 0.01 ;\n",
        "        parameter normVd = 0.2949852507374631 ;\n",
        "        parameter MinLg = 0.05 ;\n",
        "        parameter normLg = 1.4285714285714286 ;\n",
        "        parameter MinO = 8.15e-15 ;\n",
        "        parameter normO =33613445378151.26;\n",
        "        parameter MinI = -23.98798356587402 ;\n",
        "        parameter normI =0.04615548498417793;\n",
        "\n",
        "        parameter Mint_stress = {} ;\n",
        "        parameter normt_stress = {} ;\n",
        "        parameter Mindelta_Vth = {} ;\n",
        "        parameter normdelta_Vth = {} ;\n",
        "\n",
        "        real {}\n",
        "        real {}\n",
        "\n",
        "        real Vg, Vd, Vs, Vgs, Vds, Lg, Id, Cgg, Cgsd, Vgd;\n",
        "        real Vgsraw, Vgdraw, dir;\n",
        "        real t_stress, v_ov, t_rec, clk_loops, temp, delta_Vth;\n",
        "\n",
        "        real h1_0, h1_1, h1_2, h1_3, h1_4, h1_5, h1_6, h1_7, h1_8, h1_9, h1_10, h1_11, h1_12, h1_13, h1_14, h1_15, h1_16, h1_17, h1_18, h1_19, h1_20, h1_21, h1_22, h1_23, h1_24;\n",
        "        real h2_0, h2_1, h2_2, h2_3, h2_4, h2_5, h2_6, h2_7, h2_8, h2_9, h2_10, h2_11, h2_12, h2_13, h2_14, h2_15, h2_16, y;\n",
        "        real hc1_0, hc1_1, hc1_2, hc1_3, hc1_4, hc1_5, hc1_6, hc1_7, hc1_8, hc1_9;\n",
        "        real hc1_10, hc1_11, hc1_12, hc1_13, hc1_14, hc1_15, hc1_16;\n",
        "        real hc2_0, hc2_1, hc2_2, hc2_3, hc2_4, hc2_5, hc2_6, hc2_7, hc2_8, hc2_9;\n",
        "        real hc2_10, hc2_11, hc2_12, hc2_13, hc2_14, hc2_15, hc2_16, yc, yvth;\n",
        "\n",
        "analog begin\n",
        "\n",
        "t_stress = (T_stress - Mint_stress)*normt_stress;\n",
        "\n",
        "//******************** delta_Vth NN **********************************//\n",
        "\n",
        "\"\"\".format(Mint_stress, normt_stress, Mindelta_Vth, normdelta_Vth, h1_declarations, h2_declarations)\n",
        "# V_ov = (V_ov - MinV_ov)*normV_ov ;\n",
        "# t_stress = (T_stress - Mint_stress)*normt_stress ;\n",
        "\n",
        "# Create the Verilog-A code for the 1st hidden layer\n",
        "for i in range(n1):\n",
        "    inputs = [\"t_stress\"]\n",
        "    inputs = [\"*\".join([str(weights_1[i][j]), inp]) for j, inp in enumerate(inputs)]\n",
        "    inputs = \"+\".join(inputs)\n",
        "    inputs = \"+\".join([inputs, str(bias_1[i])])\n",
        "    verilog_code += \"hvth1_{} = tanh({});\\n\".format(i, inputs)\n",
        "verilog_code += \"\\n\"\n",
        "\n",
        "# Create the Verilog-A code for the 2nd hidden layer\n",
        "for i in range(n2):\n",
        "    inputs = [\"hvth1_{}\".format(j) for j in range(n1)]\n",
        "    inputs = [\"*\".join([str(weights_2[i][j]), inp]) for j, inp in enumerate(inputs)]\n",
        "    inputs = \"+\".join(inputs)\n",
        "    inputs = \"+\".join([inputs, str(bias_2[i])])\n",
        "    verilog_code += \"hvth2_{} = tanh({});\\n\".format(i, inputs)\n",
        "verilog_code += \"\\n\"\n",
        "\n",
        "# Create the Verilog-A code for the output layer\n",
        "inputs = [\"hvth2_{}\".format(i) for i in range(n2)]\n",
        "inputs = [\"*\".join([str(weights_3[0][i]), inp]) for i, inp in enumerate(inputs)]\n",
        "inputs = \"+\".join(inputs)\n",
        "inputs = \"+\".join([inputs, str(bias_3[0])])\n",
        "verilog_code += \"yvth = {};\\n\\n\".format(inputs)\n",
        "verilog_code += \"delta_Vth = (yvth - Mindelta_Vth) * normdelta_Vth;\\n\"\n",
        "verilog_code += \"\"\"$strobe(\"dvth=$g\",delta_Vth);\"\"\"\n",
        "verilog_code += \"\"\"\n",
        "\n",
        "\n",
        "        Vg = V(g) ;\n",
        "        Vs = V(s) ;\n",
        "        Vd = V(d) ;\n",
        "        Vgsraw = Vg-Vs ;\n",
        "        Vgdraw = Vg-Vd ;\n",
        "\n",
        "if (Vgsraw >= Vgdraw) begin\n",
        "        Vgs = ((Vg-Vs) - MinVg) * normVg ;\n",
        "        dir = 1;\n",
        "end\n",
        "\n",
        "else begin\n",
        "        Vgs = ((Vg-Vd) - MinVg) * normVg ;\n",
        "        dir = -1;\n",
        "end\n",
        "\n",
        "        Vds = (abs(Vd-Vs) - MinVd) * normVd ;\n",
        "        Vgs = Vgs - delta_Vth ; // BTI Vth variation\n",
        "        Lg = (L -MinLg)*normLg ;\n",
        "\n",
        "\n",
        "\n",
        "//******************** C-V NN **********************************//\n",
        "hc1_0 = tanh(-0.99871427*Vgs+-0.16952373*Vds+0.32118186*Lg+0.41485423);\n",
        "hc1_1 = tanh(0.31587568*Vgs+0.13397887*Vds+-0.4541538*Lg+-0.3630942);\n",
        "hc1_2 = tanh(-0.76281804*Vgs+0.09352969*Vds+1.1961353*Lg+0.3904977);\n",
        "hc1_3 = tanh(-1.115087*Vgs+0.85752946*Vds+-0.11746484*Lg+0.5500279);\n",
        "hc1_4 = tanh(1.0741386*Vgs+0.82041687*Vds+0.19092631*Lg+-0.4009425);\n",
        "hc1_5 = tanh(-0.47921795*Vgs+-0.8749933*Vds+-0.054768667*Lg+0.62785167);\n",
        "hc1_6 = tanh(0.5449184*Vgs+-4.409165*Vds+-0.072947875*Lg+-0.31324536);\n",
        "hc1_7 = tanh(-2.9224303*Vgs+2.7675478*Vds+0.08862238*Lg+0.6493558);\n",
        "hc1_8 = tanh(0.65050656*Vgs+-0.29751927*Vds+0.1571876*Lg+-0.38088372);\n",
        "hc1_9 = tanh(-0.30384183*Vgs+0.5649165*Vds+2.6806898*Lg+0.3197917);\n",
        "hc1_10 = tanh(-0.095988505*Vgs+2.0158541*Vds+-0.42972717*Lg+-0.30388466);\n",
        "hc1_11 = tanh(6.7699738*Vgs+-0.07234483*Vds+-0.013545353*Lg+-1.3694142);\n",
        "hc1_12 = tanh(-0.3404029*Vgs+0.0443459*Vds+0.89597*Lg+0.069993004);\n",
        "hc1_13 = tanh(0.62300175*Vgs+-0.29515797*Vds+1.6753465*Lg+-0.6520838);\n",
        "hc1_14 = tanh(0.37957156*Vgs+0.2237372*Vds+0.08591952*Lg+0.13126835);\n",
        "hc1_15 = tanh(0.19949242*Vgs+-0.26481664*Vds+-0.41059187*Lg+-0.40832308);\n",
        "hc1_16 = tanh(0.98966587*Vgs+-0.24259183*Vds+0.36584845*Lg+-0.8024042);\n",
        "\n",
        "hc2_0 = tanh(-0.91327864*hc1_0+0.4696781*hc1_1+0.3302202*hc1_2+0.11393868*hc1_3+0.45070222*hc1_4+-0.2894044*hc1_5+0.55066*hc1_6+-1.6242687*hc1_7+-0.38140613*hc1_8+0.032771554*hc1_9+0.17647126);\n",
        "hc2_1 = tanh(-1.1663305*hc1_0+-0.523984*hc1_1+-0.90804136*hc1_2+-0.7418044*hc1_3+1.456171*hc1_4+-0.16802542*hc1_5+0.8235596*hc1_6+-2.2246246*hc1_7+-0.40805355*hc1_8+0.7207601*hc1_9+0.4729169);\n",
        "hc2_2 = tanh(-0.69065374*hc1_0+0.40205315*hc1_1+-0.49410668*hc1_2+0.8681325*hc1_3+0.471351*hc1_4+0.46939445*hc1_5+0.45568785*hc1_6+-0.92935294*hc1_7+-0.8209646*hc1_8+0.1158967*hc1_9+-0.075798474);\n",
        "hc2_3 = tanh(0.11535446*hc1_0+-0.06296927*hc1_1+-0.6740435*hc1_2+0.7428315*hc1_3+0.05890677*hc1_4+0.9579441*hc1_5+-0.037319*hc1_6+-0.18491426*hc1_7+-0.02981994*hc1_8+0.038347963*hc1_9+0.039531134);\n",
        "hc2_4 = tanh(0.37817463*hc1_0+-0.6811279*hc1_1+1.1388369*hc1_2+0.19983096*hc1_3+-0.20415118*hc1_4+1.3022176*hc1_5+0.22571652*hc1_6+0.1690611*hc1_7+-0.56475276*hc1_8+-0.4069731*hc1_9+0.99962974);\n",
        "hc2_5 = tanh(0.032873478*hc1_0+-0.05209407*hc1_1+-0.010839908*hc1_2+-0.13892579*hc1_3+-0.050480977*hc1_4+0.0127089145*hc1_5+0.0052771433*hc1_6+0.02029242*hc1_7+-0.08705659*hc1_8+0.0254766*hc1_9+0.025135752);\n",
        "hc2_6 = tanh(-0.34639204*hc1_0+0.06937975*hc1_1+0.18671949*hc1_2+-0.18912783*hc1_3+0.1312504*hc1_4+0.4627272*hc1_5+-0.42590702*hc1_6+-0.10966313*hc1_7+0.66083515*hc1_8+-0.050718334*hc1_9+0.08234678);\n",
        "hc2_7 = tanh(0.90656275*hc1_0+-0.037281644*hc1_1+0.77237594*hc1_2+1.4710428*hc1_3+0.13597831*hc1_4+-0.059844542*hc1_5+-0.7801535*hc1_6+3.7814677*hc1_7+-0.5976644*hc1_8+0.2721995*hc1_9+0.023777716);\n",
        "hc2_8 = tanh(0.39720625*hc1_0+-0.45262313*hc1_1+0.19873238*hc1_2+0.9750888*hc1_3+-0.9427992*hc1_4+0.4487432*hc1_5+-0.3372945*hc1_6+0.33729544*hc1_7+-0.1667088*hc1_8+-0.5707525*hc1_9+0.27954483);\n",
        "hc2_9 = tanh(0.28551984*hc1_0+-0.68350387*hc1_1+0.9916423*hc1_2+-0.8254094*hc1_3+0.09875706*hc1_4+0.47609732*hc1_5+-0.058662917*hc1_6+0.09181381*hc1_7+0.09592329*hc1_8+1.3940467*hc1_9+0.3865768);\n",
        "hc2_10 = tanh(0.098737516*hc1_0+0.060473576*hc1_1+0.42824662*hc1_2+0.15018038*hc1_3+0.082621895*hc1_4+-0.00019039502*hc1_5+-0.3321634*hc1_6+0.7936295*hc1_7+-0.041197542*hc1_8+0.6530619*hc1_9+0.1338804);\n",
        "hc2_11 = tanh(-0.3585284*hc1_0+-0.09956017*hc1_1+0.17224246*hc1_2+-0.016925728*hc1_3+-0.46462816*hc1_4+-0.5649022*hc1_5+1.251695*hc1_6+-0.4303161*hc1_7+0.48546878*hc1_8+0.22958975*hc1_9+-0.27899802);\n",
        "hc2_12 = tanh(0.8565631*hc1_0+-0.7622999*hc1_1+0.32367912*hc1_2+1.4776785*hc1_3+0.2712369*hc1_4+0.2275511*hc1_5+0.39908803*hc1_6+4.2305493*hc1_7+-0.3467536*hc1_8+0.41231114*hc1_9+0.47123823);\n",
        "hc2_13 = tanh(-0.26411077*hc1_0+-0.17583853*hc1_1+0.045439184*hc1_2+-0.13801138*hc1_3+0.03278085*hc1_4+-0.45625108*hc1_5+-0.17905861*hc1_6+0.3060186*hc1_7+-0.3361926*hc1_8+-0.050055273*hc1_9+0.17996444);\n",
        "hc2_14 = tanh(0.016094366*hc1_0+-0.013196736*hc1_1+0.4124856*hc1_2+0.22926371*hc1_3+-0.35071182*hc1_4+-0.34217188*hc1_5+-0.69466996*hc1_6+1.0563152*hc1_7+-0.18019852*hc1_8+0.061871335*hc1_9+0.09762555);\n",
        "hc2_15 = tanh(-0.18970782*hc1_0+0.3624813*hc1_1+-1.3419824*hc1_2+0.103635244*hc1_3+-0.14595217*hc1_4+-0.1899393*hc1_5+0.176524*hc1_6+-0.4428012*hc1_7+-0.39544868*hc1_8+-0.45783517*hc1_9+0.1884755);\n",
        "hc2_16 = tanh(-0.1585831*hc1_0+0.035894837*hc1_1+-0.14261873*hc1_2+0.25914294*hc1_3+0.040607046*hc1_4+0.11555795*hc1_5+0.0022548323*hc1_6+-0.002149359*hc1_7+0.07067297*hc1_8+0.019578662*hc1_9+0.16657573);\n",
        "yc = 0.17503543*hc2_0+-0.15556754*hc2_1+-0.125455*hc2_2+-0.07502612*hc2_3+-0.16671115*hc2_4+0.29854503;\n",
        "\n",
        "Cgg = (yc / normO + MinO)*W;\n",
        "Cgsd = Cgg/2 ;\n",
        "\n",
        "//******************** I-V NN **********************************//\n",
        "h1_0 = tanh(0.0023826673*Vgs+-0.0009636134*Vds+0.006639037*Lg+-0.0004692053);\n",
        "h1_1 = tanh(-7.8007555*Vgs+2.639791*Vds+-0.025273597*Lg+-1.1747514);\n",
        "h1_2 = tanh(1.9884652*Vgs+0.62111*Vds+-0.11525345*Lg+-0.14575638);\n",
        "h1_3 = tanh(0.10625471*Vgs+0.09442053*Vds+-0.052119628*Lg+1.1568379);\n",
        "h1_4 = tanh(4.058087*Vgs+-0.7568158*Vds+0.008070099*Lg+-0.4820452);\n",
        "h1_5 = tanh(-1.3065948*Vgs+-0.0492497*Vds+-0.081622526*Lg+0.20351954);\n",
        "h1_6 = tanh(-2.9507525*Vgs+-0.91150546*Vds+-0.06477873*Lg+0.09646383);\n",
        "h1_7 = tanh(-0.5886177*Vgs+0.63788587*Vds+-0.13710928*Lg+0.30260038);\n",
        "h1_8 = tanh(-0.4311161*Vgs+-0.7250705*Vds+-0.06134645*Lg+0.44054285);\n",
        "h1_9 = tanh(0.012132638*Vgs+-0.42545322*Vds+-0.66478956*Lg+0.13182367);\n",
        "h1_10 = tanh(3.289041e-05*Vgs+0.0011367714*Vds+-0.0051904405*Lg+5.4112927e-05);\n",
        "h1_11 = tanh(-0.6007774*Vgs+0.09259224*Vds+0.2170182*Lg+-0.2908748);\n",
        "h1_12 = tanh(0.28018302*Vgs+1.3014064*Vds+-0.13490067*Lg+-0.22006753);\n",
        "h1_13 = tanh(0.01864017*Vgs+-13.0928755*Vds+-0.0037254083*Lg+-0.10935691);\n",
        "h1_14 = tanh(-0.0049708197*Vgs+0.0022613218*Vds+-0.014408149*Lg+0.0011340647);\n",
        "h1_15 = tanh(-0.094654046*Vgs+-0.4174114*Vds+0.18892045*Lg+0.05248958);\n",
        "h1_16 = tanh(-0.25090316*Vgs+-0.11111481*Vds+-0.009133225*Lg+0.08377026);\n",
        "h1_17 = tanh(0.9275306*Vgs+0.40686756*Vds+0.14127344*Lg+-0.059246097);\n",
        "h1_18 = tanh(-0.27084363*Vgs+0.07915911*Vds+-10.836302*Lg+-1.1535788);\n",
        "h1_19 = tanh(1.6852072*Vgs+-0.24846223*Vds+0.049734037*Lg+-0.19625053);\n",
        "\n",
        "h2_0 = tanh(1.1139417*h1_0+-0.40033522*h1_1+0.920759*h1_2+0.47607598*h1_3+0.3978683*h1_4+0.8008105*h1_5+-0.40445566*h1_6+0.8668027*h1_7+0.23365597*h1_8+-0.28254375*h1_9+-0.63985884*h1_10+-0.62523574*h1_11+0.8338383*h1_12+-2.0540943*h1_13+1.0749483*h1_14+-1.1075479*h1_15+0.60926706*h1_16+1.1118286*h1_17+-0.8917559*h1_18+0.029025732*h1_19+0.6622382);\n",
        "h2_1 = tanh(0.37195024*h1_0+-0.761445*h1_1+0.6514153*h1_2+0.6211995*h1_3+-0.063539445*h1_4+0.31260127*h1_5+-0.3529579*h1_6+0.50422627*h1_7+0.18943883*h1_8+-0.38029787*h1_9+-0.3464503*h1_10+-0.48301366*h1_11+0.081978925*h1_12+-0.08305682*h1_13+-0.08420117*h1_14+-0.8029313*h1_15+-0.37052512*h1_16+0.4553502*h1_17+-0.71959645*h1_18+-0.17320661*h1_19+0.6566257);\n",
        "h2_2 = tanh(-0.021933522*h1_0+-0.017245224*h1_1+0.030417712*h1_2+0.2967218*h1_3+-0.048668377*h1_4+0.029245213*h1_5+0.011756416*h1_6+0.004705658*h1_7+-0.042515088*h1_8+0.010462476*h1_9+0.001331279*h1_10+0.1694541*h1_11+-0.010949079*h1_12+-0.004144526*h1_13+0.04856694*h1_14+-0.010465159*h1_15+0.24588406*h1_16+-0.028521152*h1_17+0.12161568*h1_18+0.1753255*h1_19+-0.09125355);\n",
        "h2_3 = tanh(-0.025546636*h1_0+-3.2702503*h1_1+0.46812135*h1_2+-0.25135994*h1_3+3.0725436*h1_4+0.22513995*h1_5+-1.0327209*h1_6+-0.56274736*h1_7+0.4726107*h1_8+0.38182434*h1_9+0.016403453*h1_10+-0.09128962*h1_11+0.20997792*h1_12+-0.4951615*h1_13+0.026481293*h1_14+0.6085288*h1_15+-0.09078652*h1_16+0.36613584*h1_17+0.26257026*h1_18+0.39794916*h1_19+-0.52920526);\n",
        "h2_4 = tanh(-0.44392154*h1_0+-0.5650475*h1_1+0.91716766*h1_2+0.96703196*h1_3+0.4597048*h1_4+1.452921*h1_5+-0.8115141*h1_6+0.92326456*h1_7+0.2862403*h1_8+-0.9441585*h1_9+0.22188367*h1_10+-1.0092766*h1_11+0.5495966*h1_12+0.44870532*h1_13+0.48705962*h1_14+-0.6957133*h1_15+0.27711377*h1_16+1.0138507*h1_17+-0.34337458*h1_18+-0.21548931*h1_19+0.42685974);\n",
        "h2_5 = tanh(0.0003710325*h1_0+0.5746112*h1_1+0.4144258*h1_2+0.04459103*h1_3+1.2373503*h1_4+0.12786175*h1_5+-0.16099685*h1_6+0.9826207*h1_7+-0.09701193*h1_8+-0.4379135*h1_9+-0.0035542254*h1_10+0.04205593*h1_11+0.65820116*h1_12+0.3810506*h1_13+0.0004259507*h1_14+-0.21782044*h1_15+-0.057544652*h1_16+0.24420041*h1_17+-0.10138292*h1_18+-0.2387106*h1_19+0.867847);\n",
        "h2_6 = tanh(-0.0077049686*h1_0+-0.05349668*h1_1+-0.11536639*h1_2+0.06141029*h1_3+-0.034344573*h1_4+0.21672213*h1_5+-0.09835135*h1_6+-0.25849992*h1_7+0.10576329*h1_8+-0.14288934*h1_9+0.027846925*h1_10+-0.20104973*h1_11+0.24784875*h1_12+0.03396087*h1_13+-0.016039118*h1_14+-0.03147038*h1_15+-0.023109786*h1_16+-0.118931994*h1_17+0.18256636*h1_18+0.09981429*h1_19+0.079372324);\n",
        "h2_7 = tanh(-0.009369999*h1_0+-2.4468672*h1_1+-0.41433397*h1_2+-0.6289744*h1_3+-1.8285819*h1_4+0.12905455*h1_5+0.83588725*h1_6+0.11491322*h1_7+0.3871084*h1_8+0.07153052*h1_9+0.013530584*h1_10+0.40614852*h1_11+0.10340061*h1_12+-0.04473306*h1_13+0.0075287875*h1_14+0.5593612*h1_15+0.34401885*h1_16+-0.5538749*h1_17+0.36615464*h1_18+-0.2666981*h1_19+0.20194086);\n",
        "h2_8 = tanh(-0.0017045025*h1_0+-1.1388719*h1_1+0.25932005*h1_2+-0.6482845*h1_3+2.263395*h1_4+0.115192235*h1_5+-0.40946937*h1_6+-0.038117886*h1_7+-0.03703431*h1_8+0.05965774*h1_9+0.0032721795*h1_10+0.2561857*h1_11+0.47324425*h1_12+-0.25200802*h1_13+0.0017081021*h1_14+0.29819545*h1_15+0.061945435*h1_16+0.13516657*h1_17+0.5409335*h1_18+-0.17400871*h1_19+0.11659722);\n",
        "h2_9 = tanh(0.014866875*h1_0+1.0399909*h1_1+1.0108095*h1_2+-0.0918755*h1_3+-0.7634763*h1_4+-0.49749368*h1_5+-0.7020006*h1_6+-0.16384888*h1_7+-0.07833025*h1_8+-0.23745225*h1_9+-0.023095092*h1_10+-0.29244414*h1_11+0.6255917*h1_12+0.41413808*h1_13+-0.019823061*h1_14+-1.0593235*h1_15+-0.42221433*h1_16+0.98025715*h1_17+-0.71444243*h1_18+0.84347373*h1_19+0.28510216);\n",
        "h2_10 = tanh(-0.013515852*h1_0+0.68840384*h1_1+-0.030184174*h1_2+-0.48098114*h1_3+0.08609854*h1_4+-0.50878614*h1_5+0.26547763*h1_6+-0.7151076*h1_7+0.111288495*h1_8+0.53379977*h1_9+0.022258151*h1_10+0.16971351*h1_11+-1.2486296*h1_12+0.9649942*h1_13+0.0028906881*h1_14+0.31582054*h1_15+0.24213083*h1_16+-0.27242163*h1_17+0.11330636*h1_18+0.17038865*h1_19+-0.42089102);\n",
        "h2_11 = tanh(-0.009949424*h1_0+-0.6704206*h1_1+0.13177924*h1_2+0.07316155*h1_3+-0.28207502*h1_4+-0.32401362*h1_5+0.020746209*h1_6+-0.05517531*h1_7+0.10340257*h1_8+0.29381263*h1_9+0.020497978*h1_10+0.031984854*h1_11+0.079552434*h1_12+-7.7884445*h1_13+0.01224806*h1_14+0.8700812*h1_15+-0.16562358*h1_16+0.14370379*h1_17+0.2389181*h1_18+-0.19771136*h1_19+0.33039534);\n",
        "h2_12 = tanh(-0.002431529*h1_0+0.30154988*h1_1+-0.45072728*h1_2+0.27823612*h1_3+-0.25173753*h1_4+-0.58188903*h1_5+0.39836177*h1_6+-0.060329597*h1_7+0.1667837*h1_8+-0.09419194*h1_9+0.0108116735*h1_10+-0.3581154*h1_11+0.15690842*h1_12+-0.41209573*h1_13+-0.0070331707*h1_14+-0.23976392*h1_15+0.16620094*h1_16+-0.034617994*h1_17+0.32754526*h1_18+0.8490298*h1_19+0.026407415);\n",
        "h2_13 = tanh(-0.7385622*h1_0+-0.25174448*h1_1+2.090295*h1_2+0.67499936*h1_3+0.07392262*h1_4+0.5413692*h1_5+-0.8287433*h1_6+2.2810504*h1_7+1.1917778*h1_8+1.3719273*h1_9+0.25762329*h1_10+-2.1111712*h1_11+1.6987114*h1_12+-0.437825*h1_13+1.7649944*h1_14+-2.2254016*h1_15+-2.3555171*h1_16+-0.7732424*h1_17+-1.0756916*h1_18+-0.1367373*h1_19+1.5085722);\n",
        "h2_14 = tanh(-0.002535408*h1_0+-4.2686224*h1_1+-0.045537975*h1_2+0.08043166*h1_3+1.7274952*h1_4+-0.1260494*h1_5+-0.23011239*h1_6+-0.21025337*h1_7+0.48537356*h1_8+0.39049447*h1_9+-0.006565428*h1_10+-0.293028*h1_11+-1.1253971*h1_12+-0.2890831*h1_13+0.0042421576*h1_14+0.4230598*h1_15+-0.17869289*h1_16+-0.034056116*h1_17+0.61135995*h1_18+0.6345532*h1_19+0.73144835);\n",
        "h2_15 = tanh(-0.5239093*h1_0+0.49806875*h1_1+0.53555894*h1_2+1.2645539*h1_3+1.011548*h1_4+0.39209503*h1_5+-0.78194916*h1_6+0.94610345*h1_7+0.9121405*h1_8+-1.4693716*h1_9+-0.58336824*h1_10+-1.1661471*h1_11+-0.16158457*h1_12+0.058512915*h1_13+1.3358645*h1_14+0.5742125*h1_15+-1.3325212*h1_16+1.1804186*h1_17+-1.4877121*h1_18+0.6357802*h1_19+1.1328585);\n",
        "h2_16 = tanh(0.0095406*h1_0+-0.53232694*h1_1+0.1535685*h1_2+0.26108417*h1_3+-0.08375187*h1_4+0.3462123*h1_5+-0.61755395*h1_6+0.0015145333*h1_7+-0.108780764*h1_8+0.2555477*h1_9+0.00023940884*h1_10+-0.26577523*h1_11+-0.27689674*h1_12+0.43049082*h1_13+-0.025424613*h1_14+0.24634649*h1_15+-0.22579487*h1_16+0.37249807*h1_17+-0.8058662*h1_18+-0.77451354*h1_19+0.5821276);\n",
        "y = -0.17525196*h2_0+-0.22995844*h2_1+0.0026147955*h2_2+0.091129646*h2_3+-0.4534783*h2_4+0.4119419*h2_5+-1.4205361e-05*h2_6+-0.11359831*h2_7+0.1762025*h2_8+-0.1002102*h2_9+-0.37161925*h2_10+0.2683793*h2_11+0.051897053*h2_12+0.19354156*h2_13+-0.10134394*h2_14+0.3937854*h2_15+-0.3296008*h2_16+0.3220947;\n",
        "\n",
        "        Id = pow(10, (y/normI + MinI)) ;\n",
        "\n",
        "if (Id <= 1e-15) begin //limit\n",
        "        Id = 1e-15;\n",
        "        //Id = Id;\n",
        "end\n",
        "else begin\n",
        "        Id = Id;\n",
        "end  //limit end\n",
        "\n",
        "\n",
        "        I(g, d) <+ Cgsd*ddt(Vg-Vd) ;\n",
        "        I(g, s) <+ Cgsd*ddt(Vg-Vs) ;\n",
        "\n",
        "if (Vgsraw >= Vgdraw) begin\n",
        "        I(d, s) <+ dir*Id*W ;\n",
        "\n",
        "end\n",
        "\n",
        "else begin\n",
        "        I(d, s) <+ dir*Id*W ;\n",
        "\n",
        "end\n",
        "\n",
        "end\n",
        "\n",
        "endmodule\n",
        "\n",
        "\"\"\"\n",
        "print(verilog_code)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "6Tw_20-o48MH",
        "outputId": "bdec6423-c163-4596-a645-97006643ebbd"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'Mint_rec' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-93-3bca65f909cc>\u001b[0m in \u001b[0;36m<cell line: 38>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0;34m//\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;34m**\u001b[0m \u001b[0mdelta_Vth\u001b[0m \u001b[0mNN\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m \"\"\".format(Mint_stress, normt_stress, Mint_rec, normt_rec, Minclk_loops, normclk_loops, MinV_ov, normV_ov, Mintemperature, normtemperature, Mindelta_Vth, normdelta_Vth, h1_declarations, h2_declarations)\n\u001b[0m\u001b[1;32m    130\u001b[0m \u001b[0;31m# V_ov = (V_ov - MinV_ov)*normV_ov ;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0;31m# t_stress = (T_stress - Mint_stress)*normt_stress ;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'Mint_rec' is not defined"
          ]
        }
      ],
      "source": [
        "with torch.no_grad():\n",
        "\n",
        "    output = []\n",
        "    # Iterate over the dataloader for training data\n",
        "    for i, data in enumerate(testdataloader, 0):\n",
        "        inputs, targets = data\n",
        "        inputs, targets = inputs.float(), targets.float()\n",
        "        targets = targets.reshape((targets.shape[0],1))\n",
        "\n",
        "        #zero the gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        #perform forward pass\n",
        "        outputs = model(inputs)\n",
        "        output.append(outputs)\n",
        "# Process is complete.\n",
        "#print('Training process has finished.')\n",
        "\n",
        "# Extract the weights and biases from the model\n",
        "weights_1 = model.fc1.weight.detach().numpy()\n",
        "bias_1 = model.fc1.bias.detach().numpy()\n",
        "weights_2 = model.fc2.weight.detach().numpy()\n",
        "bias_2 = model.fc2.bias.detach().numpy()\n",
        "weights_3 = model.fc3.weight.detach().numpy()\n",
        "bias_3 = model.fc3.bias.detach().numpy()\n",
        "\n",
        "def generate_variable_declarations(weights_shape, layer_prefix):\n",
        "    declarations = \"\"\n",
        "    num_neurons = weights_shape  # Number of neurons is determined by the first dimension of the weights matrix\n",
        "    layer_declarations = \", \".join([f\"{layer_prefix}_{i}\" for i in range(num_neurons)]) + \";\"\n",
        "    declarations += layer_declarations\n",
        "    return declarations\n",
        "\n",
        "# Use the function to generate declarations for each layer\n",
        "h1_declarations = generate_variable_declarations(weights_1.shape[0], \"hvth1\")\n",
        "h2_declarations = generate_variable_declarations(weights_2.shape[0], \"hvth2\")\n",
        "\n",
        "verilog_code = \"\"\"\n",
        "// VerilogA for GB_lib, IWO_verliogA, veriloga\n",
        "//*******************************************************************************\n",
        "//* * School of Electrical and Computer Engineering, Georgia Institute of Technology\n",
        "//* PI: Prof. Shimeng Yu\n",
        "//* All rights reserved.\n",
        "//*\n",
        "//* Copyright of the model is maintained by the developers, and the model is distributed under\n",
        "//* the terms of the Creative Commons Attribution-NonCommercial 4.0 International Public License\n",
        "//* http://creativecommons.org/licenses/by-nc/4.0/legalcode.\n",
        "//*\n",
        "//* THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND\n",
        "//* ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\n",
        "//* WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n",
        "//* DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\n",
        "//* FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\n",
        "//* DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\n",
        "//* SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\n",
        "//* CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\n",
        "//* OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n",
        "//* OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n",
        "//*\n",
        "//* Developer:\n",
        "//*  Gihun Choe gchoe6@gatech.edu\n",
        "//********************************************************************************/\n",
        "\n",
        "`include \"constants.vams\"\n",
        "`include \"disciplines.vams\"\n",
        "\n",
        "\n",
        "module IWO_verliogA(d, g, s);\n",
        "        inout d, g, s;\n",
        "        electrical d, g, s;\n",
        "\n",
        "        //***** parameters L and W ******//\n",
        "        parameter real W = 0.1; //get parameter fom spectre\n",
        "        parameter real L = 0.05; //get parameter fom spectre\n",
        "        parameter real T_stress = 0.0001; //set on cadence as variable\n",
        "        parameter real T_rec = 0.001;     //set on cadence as variable\n",
        "        parameter real Clk_loops = 50;    //set on cadence as variable\n",
        "        parameter real V_ov = 1.7;        //set on cadence as variable\n",
        "        parameter real Temp = 25;  //set on cadence as variable\n",
        "\n",
        "        parameter MinVg = -1.0 ;\n",
        "        parameter normVg = 0.2222222222222222 ;\n",
        "        parameter MinVd = 0.01 ;\n",
        "        parameter normVd = 0.2949852507374631 ;\n",
        "        parameter MinLg = 0.05 ;\n",
        "        parameter normLg = 1.4285714285714286 ;\n",
        "        parameter MinO = 8.15e-15 ;\n",
        "        parameter normO =33613445378151.26;\n",
        "        parameter MinI = -23.98798356587402 ;\n",
        "        parameter normI =0.04615548498417793;\n",
        "\n",
        "        parameter Mint_stress = {} ;\n",
        "        parameter normt_stress = {} ;\n",
        "        parameter Mint_rec = {} ;\n",
        "        parameter normt_rec = {} ;\n",
        "        parameter Minclk_loops = {} ;\n",
        "        parameter normclk_loops = {} ;\n",
        "        parameter Minv_ov = {} ;\n",
        "        parameter normv_ov = {} ;\n",
        "        parameter Mintemperature = {} ;\n",
        "        parameter normtemperature = {} ;\n",
        "        parameter Mindelta_Vth = {} ;\n",
        "        parameter normdelta_Vth = {} ;\n",
        "\n",
        "        real {}\n",
        "        real {}\n",
        "\n",
        "        real Vg, Vd, Vs, Vgs, Vds, Lg, Id, Cgg, Cgsd, Vgd;\n",
        "        real Vgsraw, Vgdraw, dir;\n",
        "        real t_stress, v_ov, t_rec, clk_loops, temp, delta_Vth;\n",
        "\n",
        "        real h1_0, h1_1, h1_2, h1_3, h1_4, h1_5, h1_6, h1_7, h1_8, h1_9, h1_10, h1_11, h1_12, h1_13, h1_14, h1_15, h1_16, h1_17, h1_18, h1_19, h1_20, h1_21, h1_22, h1_23, h1_24;\n",
        "        real h2_0, h2_1, h2_2, h2_3, h2_4, h2_5, h2_6, h2_7, h2_8, h2_9, h2_10, h2_11, h2_12, h2_13, h2_14, h2_15, h2_16, y;\n",
        "        real hc1_0, hc1_1, hc1_2, hc1_3, hc1_4, hc1_5, hc1_6, hc1_7, hc1_8, hc1_9;\n",
        "        real hc1_10, hc1_11, hc1_12, hc1_13, hc1_14, hc1_15, hc1_16;\n",
        "        real hc2_0, hc2_1, hc2_2, hc2_3, hc2_4, hc2_5, hc2_6, hc2_7, hc2_8, hc2_9;\n",
        "        real hc2_10, hc2_11, hc2_12, hc2_13, hc2_14, hc2_15, hc2_16, yc, yvth;\n",
        "\n",
        "analog begin\n",
        "\n",
        "t_stress = (T_stress - Mint_stress)*normt_stress;\n",
        "v_ov = (V_ov - Minv_ov)*normv_ov;\n",
        "t_rec = (T_rec - Mint_rec)*normt_rec ;\n",
        "clk_loops = (Clk_loops - Minclk_loops)*normclk_loops ;\n",
        "temp = (Temp - Mintemperature)*normtemperature ;\n",
        "\n",
        "//******************** delta_Vth NN **********************************//\n",
        "\n",
        "\"\"\".format(Mint_stress, normt_stress, Mint_rec, normt_rec, Minclk_loops, normclk_loops, MinV_ov, normV_ov, Mintemperature, normtemperature, Mindelta_Vth, normdelta_Vth, h1_declarations, h2_declarations)\n",
        "# V_ov = (V_ov - MinV_ov)*normV_ov ;\n",
        "# t_stress = (T_stress - Mint_stress)*normt_stress ;\n",
        "\n",
        "# Create the Verilog-A code for the 1st hidden layer\n",
        "for i in range(n1):\n",
        "    inputs = [\"t_stress\", \"t_rec\", \"clk_loops\", \"v_ov\", \"temp\"]\n",
        "    inputs = [\"*\".join([str(weights_1[i][j]), inp]) for j, inp in enumerate(inputs)]\n",
        "    inputs = \"+\".join(inputs)\n",
        "    inputs = \"+\".join([inputs, str(bias_1[i])])\n",
        "    verilog_code += \"hvth1_{} = tanh({});\\n\".format(i, inputs)\n",
        "verilog_code += \"\\n\"\n",
        "\n",
        "# Create the Verilog-A code for the 2nd hidden layer\n",
        "for i in range(n2):\n",
        "    inputs = [\"hvth1_{}\".format(j) for j in range(n1)]\n",
        "    inputs = [\"*\".join([str(weights_2[i][j]), inp]) for j, inp in enumerate(inputs)]\n",
        "    inputs = \"+\".join(inputs)\n",
        "    inputs = \"+\".join([inputs, str(bias_2[i])])\n",
        "    verilog_code += \"hvth2_{} = tanh({});\\n\".format(i, inputs)\n",
        "verilog_code += \"\\n\"\n",
        "\n",
        "# Create the Verilog-A code for the output layer\n",
        "inputs = [\"hvth2_{}\".format(i) for i in range(n2)]\n",
        "inputs = [\"*\".join([str(weights_3[0][i]), inp]) for i, inp in enumerate(inputs)]\n",
        "inputs = \"+\".join(inputs)\n",
        "inputs = \"+\".join([inputs, str(bias_3[0])])\n",
        "verilog_code += \"yvth = {};\\n\\n\".format(inputs)\n",
        "verilog_code += \"delta_Vth = (yvth - Mindelta_Vth) * normdelta_Vth;\\n\"\n",
        "verilog_code += \"\"\"$strobe(\"dvth=$g\",delta_Vth);\"\"\"\n",
        "verilog_code += \"\"\"\n",
        "\n",
        "\n",
        "        Vg = V(g) ;\n",
        "        Vs = V(s) ;\n",
        "        Vd = V(d) ;\n",
        "        Vgsraw = Vg-Vs ;\n",
        "        Vgdraw = Vg-Vd ;\n",
        "\n",
        "if (Vgsraw >= Vgdraw) begin\n",
        "        Vgs = ((Vg-Vs) - MinVg) * normVg ;\n",
        "        dir = 1;\n",
        "end\n",
        "\n",
        "else begin\n",
        "        Vgs = ((Vg-Vd) - MinVg) * normVg ;\n",
        "        dir = -1;\n",
        "end\n",
        "\n",
        "        Vds = (abs(Vd-Vs) - MinVd) * normVd ;\n",
        "        Vgs = Vgs - delta_Vth ; // BTI Vth variation\n",
        "        Lg = (L -MinLg)*normLg ;\n",
        "\n",
        "\n",
        "\n",
        "//******************** C-V NN **********************************//\n",
        "hc1_0 = tanh(-0.99871427*Vgs+-0.16952373*Vds+0.32118186*Lg+0.41485423);\n",
        "hc1_1 = tanh(0.31587568*Vgs+0.13397887*Vds+-0.4541538*Lg+-0.3630942);\n",
        "hc1_2 = tanh(-0.76281804*Vgs+0.09352969*Vds+1.1961353*Lg+0.3904977);\n",
        "hc1_3 = tanh(-1.115087*Vgs+0.85752946*Vds+-0.11746484*Lg+0.5500279);\n",
        "hc1_4 = tanh(1.0741386*Vgs+0.82041687*Vds+0.19092631*Lg+-0.4009425);\n",
        "hc1_5 = tanh(-0.47921795*Vgs+-0.8749933*Vds+-0.054768667*Lg+0.62785167);\n",
        "hc1_6 = tanh(0.5449184*Vgs+-4.409165*Vds+-0.072947875*Lg+-0.31324536);\n",
        "hc1_7 = tanh(-2.9224303*Vgs+2.7675478*Vds+0.08862238*Lg+0.6493558);\n",
        "hc1_8 = tanh(0.65050656*Vgs+-0.29751927*Vds+0.1571876*Lg+-0.38088372);\n",
        "hc1_9 = tanh(-0.30384183*Vgs+0.5649165*Vds+2.6806898*Lg+0.3197917);\n",
        "hc1_10 = tanh(-0.095988505*Vgs+2.0158541*Vds+-0.42972717*Lg+-0.30388466);\n",
        "hc1_11 = tanh(6.7699738*Vgs+-0.07234483*Vds+-0.013545353*Lg+-1.3694142);\n",
        "hc1_12 = tanh(-0.3404029*Vgs+0.0443459*Vds+0.89597*Lg+0.069993004);\n",
        "hc1_13 = tanh(0.62300175*Vgs+-0.29515797*Vds+1.6753465*Lg+-0.6520838);\n",
        "hc1_14 = tanh(0.37957156*Vgs+0.2237372*Vds+0.08591952*Lg+0.13126835);\n",
        "hc1_15 = tanh(0.19949242*Vgs+-0.26481664*Vds+-0.41059187*Lg+-0.40832308);\n",
        "hc1_16 = tanh(0.98966587*Vgs+-0.24259183*Vds+0.36584845*Lg+-0.8024042);\n",
        "\n",
        "hc2_0 = tanh(-0.91327864*hc1_0+0.4696781*hc1_1+0.3302202*hc1_2+0.11393868*hc1_3+0.45070222*hc1_4+-0.2894044*hc1_5+0.55066*hc1_6+-1.6242687*hc1_7+-0.38140613*hc1_8+0.032771554*hc1_9+0.17647126);\n",
        "hc2_1 = tanh(-1.1663305*hc1_0+-0.523984*hc1_1+-0.90804136*hc1_2+-0.7418044*hc1_3+1.456171*hc1_4+-0.16802542*hc1_5+0.8235596*hc1_6+-2.2246246*hc1_7+-0.40805355*hc1_8+0.7207601*hc1_9+0.4729169);\n",
        "hc2_2 = tanh(-0.69065374*hc1_0+0.40205315*hc1_1+-0.49410668*hc1_2+0.8681325*hc1_3+0.471351*hc1_4+0.46939445*hc1_5+0.45568785*hc1_6+-0.92935294*hc1_7+-0.8209646*hc1_8+0.1158967*hc1_9+-0.075798474);\n",
        "hc2_3 = tanh(0.11535446*hc1_0+-0.06296927*hc1_1+-0.6740435*hc1_2+0.7428315*hc1_3+0.05890677*hc1_4+0.9579441*hc1_5+-0.037319*hc1_6+-0.18491426*hc1_7+-0.02981994*hc1_8+0.038347963*hc1_9+0.039531134);\n",
        "hc2_4 = tanh(0.37817463*hc1_0+-0.6811279*hc1_1+1.1388369*hc1_2+0.19983096*hc1_3+-0.20415118*hc1_4+1.3022176*hc1_5+0.22571652*hc1_6+0.1690611*hc1_7+-0.56475276*hc1_8+-0.4069731*hc1_9+0.99962974);\n",
        "hc2_5 = tanh(0.032873478*hc1_0+-0.05209407*hc1_1+-0.010839908*hc1_2+-0.13892579*hc1_3+-0.050480977*hc1_4+0.0127089145*hc1_5+0.0052771433*hc1_6+0.02029242*hc1_7+-0.08705659*hc1_8+0.0254766*hc1_9+0.025135752);\n",
        "hc2_6 = tanh(-0.34639204*hc1_0+0.06937975*hc1_1+0.18671949*hc1_2+-0.18912783*hc1_3+0.1312504*hc1_4+0.4627272*hc1_5+-0.42590702*hc1_6+-0.10966313*hc1_7+0.66083515*hc1_8+-0.050718334*hc1_9+0.08234678);\n",
        "hc2_7 = tanh(0.90656275*hc1_0+-0.037281644*hc1_1+0.77237594*hc1_2+1.4710428*hc1_3+0.13597831*hc1_4+-0.059844542*hc1_5+-0.7801535*hc1_6+3.7814677*hc1_7+-0.5976644*hc1_8+0.2721995*hc1_9+0.023777716);\n",
        "hc2_8 = tanh(0.39720625*hc1_0+-0.45262313*hc1_1+0.19873238*hc1_2+0.9750888*hc1_3+-0.9427992*hc1_4+0.4487432*hc1_5+-0.3372945*hc1_6+0.33729544*hc1_7+-0.1667088*hc1_8+-0.5707525*hc1_9+0.27954483);\n",
        "hc2_9 = tanh(0.28551984*hc1_0+-0.68350387*hc1_1+0.9916423*hc1_2+-0.8254094*hc1_3+0.09875706*hc1_4+0.47609732*hc1_5+-0.058662917*hc1_6+0.09181381*hc1_7+0.09592329*hc1_8+1.3940467*hc1_9+0.3865768);\n",
        "hc2_10 = tanh(0.098737516*hc1_0+0.060473576*hc1_1+0.42824662*hc1_2+0.15018038*hc1_3+0.082621895*hc1_4+-0.00019039502*hc1_5+-0.3321634*hc1_6+0.7936295*hc1_7+-0.041197542*hc1_8+0.6530619*hc1_9+0.1338804);\n",
        "hc2_11 = tanh(-0.3585284*hc1_0+-0.09956017*hc1_1+0.17224246*hc1_2+-0.016925728*hc1_3+-0.46462816*hc1_4+-0.5649022*hc1_5+1.251695*hc1_6+-0.4303161*hc1_7+0.48546878*hc1_8+0.22958975*hc1_9+-0.27899802);\n",
        "hc2_12 = tanh(0.8565631*hc1_0+-0.7622999*hc1_1+0.32367912*hc1_2+1.4776785*hc1_3+0.2712369*hc1_4+0.2275511*hc1_5+0.39908803*hc1_6+4.2305493*hc1_7+-0.3467536*hc1_8+0.41231114*hc1_9+0.47123823);\n",
        "hc2_13 = tanh(-0.26411077*hc1_0+-0.17583853*hc1_1+0.045439184*hc1_2+-0.13801138*hc1_3+0.03278085*hc1_4+-0.45625108*hc1_5+-0.17905861*hc1_6+0.3060186*hc1_7+-0.3361926*hc1_8+-0.050055273*hc1_9+0.17996444);\n",
        "hc2_14 = tanh(0.016094366*hc1_0+-0.013196736*hc1_1+0.4124856*hc1_2+0.22926371*hc1_3+-0.35071182*hc1_4+-0.34217188*hc1_5+-0.69466996*hc1_6+1.0563152*hc1_7+-0.18019852*hc1_8+0.061871335*hc1_9+0.09762555);\n",
        "hc2_15 = tanh(-0.18970782*hc1_0+0.3624813*hc1_1+-1.3419824*hc1_2+0.103635244*hc1_3+-0.14595217*hc1_4+-0.1899393*hc1_5+0.176524*hc1_6+-0.4428012*hc1_7+-0.39544868*hc1_8+-0.45783517*hc1_9+0.1884755);\n",
        "hc2_16 = tanh(-0.1585831*hc1_0+0.035894837*hc1_1+-0.14261873*hc1_2+0.25914294*hc1_3+0.040607046*hc1_4+0.11555795*hc1_5+0.0022548323*hc1_6+-0.002149359*hc1_7+0.07067297*hc1_8+0.019578662*hc1_9+0.16657573);\n",
        "yc = 0.17503543*hc2_0+-0.15556754*hc2_1+-0.125455*hc2_2+-0.07502612*hc2_3+-0.16671115*hc2_4+0.29854503;\n",
        "\n",
        "Cgg = (yc / normO + MinO)*W;\n",
        "Cgsd = Cgg/2 ;\n",
        "\n",
        "//******************** I-V NN **********************************//\n",
        "h1_0 = tanh(0.0023826673*Vgs+-0.0009636134*Vds+0.006639037*Lg+-0.0004692053);\n",
        "h1_1 = tanh(-7.8007555*Vgs+2.639791*Vds+-0.025273597*Lg+-1.1747514);\n",
        "h1_2 = tanh(1.9884652*Vgs+0.62111*Vds+-0.11525345*Lg+-0.14575638);\n",
        "h1_3 = tanh(0.10625471*Vgs+0.09442053*Vds+-0.052119628*Lg+1.1568379);\n",
        "h1_4 = tanh(4.058087*Vgs+-0.7568158*Vds+0.008070099*Lg+-0.4820452);\n",
        "h1_5 = tanh(-1.3065948*Vgs+-0.0492497*Vds+-0.081622526*Lg+0.20351954);\n",
        "h1_6 = tanh(-2.9507525*Vgs+-0.91150546*Vds+-0.06477873*Lg+0.09646383);\n",
        "h1_7 = tanh(-0.5886177*Vgs+0.63788587*Vds+-0.13710928*Lg+0.30260038);\n",
        "h1_8 = tanh(-0.4311161*Vgs+-0.7250705*Vds+-0.06134645*Lg+0.44054285);\n",
        "h1_9 = tanh(0.012132638*Vgs+-0.42545322*Vds+-0.66478956*Lg+0.13182367);\n",
        "h1_10 = tanh(3.289041e-05*Vgs+0.0011367714*Vds+-0.0051904405*Lg+5.4112927e-05);\n",
        "h1_11 = tanh(-0.6007774*Vgs+0.09259224*Vds+0.2170182*Lg+-0.2908748);\n",
        "h1_12 = tanh(0.28018302*Vgs+1.3014064*Vds+-0.13490067*Lg+-0.22006753);\n",
        "h1_13 = tanh(0.01864017*Vgs+-13.0928755*Vds+-0.0037254083*Lg+-0.10935691);\n",
        "h1_14 = tanh(-0.0049708197*Vgs+0.0022613218*Vds+-0.014408149*Lg+0.0011340647);\n",
        "h1_15 = tanh(-0.094654046*Vgs+-0.4174114*Vds+0.18892045*Lg+0.05248958);\n",
        "h1_16 = tanh(-0.25090316*Vgs+-0.11111481*Vds+-0.009133225*Lg+0.08377026);\n",
        "h1_17 = tanh(0.9275306*Vgs+0.40686756*Vds+0.14127344*Lg+-0.059246097);\n",
        "h1_18 = tanh(-0.27084363*Vgs+0.07915911*Vds+-10.836302*Lg+-1.1535788);\n",
        "h1_19 = tanh(1.6852072*Vgs+-0.24846223*Vds+0.049734037*Lg+-0.19625053);\n",
        "\n",
        "h2_0 = tanh(1.1139417*h1_0+-0.40033522*h1_1+0.920759*h1_2+0.47607598*h1_3+0.3978683*h1_4+0.8008105*h1_5+-0.40445566*h1_6+0.8668027*h1_7+0.23365597*h1_8+-0.28254375*h1_9+-0.63985884*h1_10+-0.62523574*h1_11+0.8338383*h1_12+-2.0540943*h1_13+1.0749483*h1_14+-1.1075479*h1_15+0.60926706*h1_16+1.1118286*h1_17+-0.8917559*h1_18+0.029025732*h1_19+0.6622382);\n",
        "h2_1 = tanh(0.37195024*h1_0+-0.761445*h1_1+0.6514153*h1_2+0.6211995*h1_3+-0.063539445*h1_4+0.31260127*h1_5+-0.3529579*h1_6+0.50422627*h1_7+0.18943883*h1_8+-0.38029787*h1_9+-0.3464503*h1_10+-0.48301366*h1_11+0.081978925*h1_12+-0.08305682*h1_13+-0.08420117*h1_14+-0.8029313*h1_15+-0.37052512*h1_16+0.4553502*h1_17+-0.71959645*h1_18+-0.17320661*h1_19+0.6566257);\n",
        "h2_2 = tanh(-0.021933522*h1_0+-0.017245224*h1_1+0.030417712*h1_2+0.2967218*h1_3+-0.048668377*h1_4+0.029245213*h1_5+0.011756416*h1_6+0.004705658*h1_7+-0.042515088*h1_8+0.010462476*h1_9+0.001331279*h1_10+0.1694541*h1_11+-0.010949079*h1_12+-0.004144526*h1_13+0.04856694*h1_14+-0.010465159*h1_15+0.24588406*h1_16+-0.028521152*h1_17+0.12161568*h1_18+0.1753255*h1_19+-0.09125355);\n",
        "h2_3 = tanh(-0.025546636*h1_0+-3.2702503*h1_1+0.46812135*h1_2+-0.25135994*h1_3+3.0725436*h1_4+0.22513995*h1_5+-1.0327209*h1_6+-0.56274736*h1_7+0.4726107*h1_8+0.38182434*h1_9+0.016403453*h1_10+-0.09128962*h1_11+0.20997792*h1_12+-0.4951615*h1_13+0.026481293*h1_14+0.6085288*h1_15+-0.09078652*h1_16+0.36613584*h1_17+0.26257026*h1_18+0.39794916*h1_19+-0.52920526);\n",
        "h2_4 = tanh(-0.44392154*h1_0+-0.5650475*h1_1+0.91716766*h1_2+0.96703196*h1_3+0.4597048*h1_4+1.452921*h1_5+-0.8115141*h1_6+0.92326456*h1_7+0.2862403*h1_8+-0.9441585*h1_9+0.22188367*h1_10+-1.0092766*h1_11+0.5495966*h1_12+0.44870532*h1_13+0.48705962*h1_14+-0.6957133*h1_15+0.27711377*h1_16+1.0138507*h1_17+-0.34337458*h1_18+-0.21548931*h1_19+0.42685974);\n",
        "h2_5 = tanh(0.0003710325*h1_0+0.5746112*h1_1+0.4144258*h1_2+0.04459103*h1_3+1.2373503*h1_4+0.12786175*h1_5+-0.16099685*h1_6+0.9826207*h1_7+-0.09701193*h1_8+-0.4379135*h1_9+-0.0035542254*h1_10+0.04205593*h1_11+0.65820116*h1_12+0.3810506*h1_13+0.0004259507*h1_14+-0.21782044*h1_15+-0.057544652*h1_16+0.24420041*h1_17+-0.10138292*h1_18+-0.2387106*h1_19+0.867847);\n",
        "h2_6 = tanh(-0.0077049686*h1_0+-0.05349668*h1_1+-0.11536639*h1_2+0.06141029*h1_3+-0.034344573*h1_4+0.21672213*h1_5+-0.09835135*h1_6+-0.25849992*h1_7+0.10576329*h1_8+-0.14288934*h1_9+0.027846925*h1_10+-0.20104973*h1_11+0.24784875*h1_12+0.03396087*h1_13+-0.016039118*h1_14+-0.03147038*h1_15+-0.023109786*h1_16+-0.118931994*h1_17+0.18256636*h1_18+0.09981429*h1_19+0.079372324);\n",
        "h2_7 = tanh(-0.009369999*h1_0+-2.4468672*h1_1+-0.41433397*h1_2+-0.6289744*h1_3+-1.8285819*h1_4+0.12905455*h1_5+0.83588725*h1_6+0.11491322*h1_7+0.3871084*h1_8+0.07153052*h1_9+0.013530584*h1_10+0.40614852*h1_11+0.10340061*h1_12+-0.04473306*h1_13+0.0075287875*h1_14+0.5593612*h1_15+0.34401885*h1_16+-0.5538749*h1_17+0.36615464*h1_18+-0.2666981*h1_19+0.20194086);\n",
        "h2_8 = tanh(-0.0017045025*h1_0+-1.1388719*h1_1+0.25932005*h1_2+-0.6482845*h1_3+2.263395*h1_4+0.115192235*h1_5+-0.40946937*h1_6+-0.038117886*h1_7+-0.03703431*h1_8+0.05965774*h1_9+0.0032721795*h1_10+0.2561857*h1_11+0.47324425*h1_12+-0.25200802*h1_13+0.0017081021*h1_14+0.29819545*h1_15+0.061945435*h1_16+0.13516657*h1_17+0.5409335*h1_18+-0.17400871*h1_19+0.11659722);\n",
        "h2_9 = tanh(0.014866875*h1_0+1.0399909*h1_1+1.0108095*h1_2+-0.0918755*h1_3+-0.7634763*h1_4+-0.49749368*h1_5+-0.7020006*h1_6+-0.16384888*h1_7+-0.07833025*h1_8+-0.23745225*h1_9+-0.023095092*h1_10+-0.29244414*h1_11+0.6255917*h1_12+0.41413808*h1_13+-0.019823061*h1_14+-1.0593235*h1_15+-0.42221433*h1_16+0.98025715*h1_17+-0.71444243*h1_18+0.84347373*h1_19+0.28510216);\n",
        "h2_10 = tanh(-0.013515852*h1_0+0.68840384*h1_1+-0.030184174*h1_2+-0.48098114*h1_3+0.08609854*h1_4+-0.50878614*h1_5+0.26547763*h1_6+-0.7151076*h1_7+0.111288495*h1_8+0.53379977*h1_9+0.022258151*h1_10+0.16971351*h1_11+-1.2486296*h1_12+0.9649942*h1_13+0.0028906881*h1_14+0.31582054*h1_15+0.24213083*h1_16+-0.27242163*h1_17+0.11330636*h1_18+0.17038865*h1_19+-0.42089102);\n",
        "h2_11 = tanh(-0.009949424*h1_0+-0.6704206*h1_1+0.13177924*h1_2+0.07316155*h1_3+-0.28207502*h1_4+-0.32401362*h1_5+0.020746209*h1_6+-0.05517531*h1_7+0.10340257*h1_8+0.29381263*h1_9+0.020497978*h1_10+0.031984854*h1_11+0.079552434*h1_12+-7.7884445*h1_13+0.01224806*h1_14+0.8700812*h1_15+-0.16562358*h1_16+0.14370379*h1_17+0.2389181*h1_18+-0.19771136*h1_19+0.33039534);\n",
        "h2_12 = tanh(-0.002431529*h1_0+0.30154988*h1_1+-0.45072728*h1_2+0.27823612*h1_3+-0.25173753*h1_4+-0.58188903*h1_5+0.39836177*h1_6+-0.060329597*h1_7+0.1667837*h1_8+-0.09419194*h1_9+0.0108116735*h1_10+-0.3581154*h1_11+0.15690842*h1_12+-0.41209573*h1_13+-0.0070331707*h1_14+-0.23976392*h1_15+0.16620094*h1_16+-0.034617994*h1_17+0.32754526*h1_18+0.8490298*h1_19+0.026407415);\n",
        "h2_13 = tanh(-0.7385622*h1_0+-0.25174448*h1_1+2.090295*h1_2+0.67499936*h1_3+0.07392262*h1_4+0.5413692*h1_5+-0.8287433*h1_6+2.2810504*h1_7+1.1917778*h1_8+1.3719273*h1_9+0.25762329*h1_10+-2.1111712*h1_11+1.6987114*h1_12+-0.437825*h1_13+1.7649944*h1_14+-2.2254016*h1_15+-2.3555171*h1_16+-0.7732424*h1_17+-1.0756916*h1_18+-0.1367373*h1_19+1.5085722);\n",
        "h2_14 = tanh(-0.002535408*h1_0+-4.2686224*h1_1+-0.045537975*h1_2+0.08043166*h1_3+1.7274952*h1_4+-0.1260494*h1_5+-0.23011239*h1_6+-0.21025337*h1_7+0.48537356*h1_8+0.39049447*h1_9+-0.006565428*h1_10+-0.293028*h1_11+-1.1253971*h1_12+-0.2890831*h1_13+0.0042421576*h1_14+0.4230598*h1_15+-0.17869289*h1_16+-0.034056116*h1_17+0.61135995*h1_18+0.6345532*h1_19+0.73144835);\n",
        "h2_15 = tanh(-0.5239093*h1_0+0.49806875*h1_1+0.53555894*h1_2+1.2645539*h1_3+1.011548*h1_4+0.39209503*h1_5+-0.78194916*h1_6+0.94610345*h1_7+0.9121405*h1_8+-1.4693716*h1_9+-0.58336824*h1_10+-1.1661471*h1_11+-0.16158457*h1_12+0.058512915*h1_13+1.3358645*h1_14+0.5742125*h1_15+-1.3325212*h1_16+1.1804186*h1_17+-1.4877121*h1_18+0.6357802*h1_19+1.1328585);\n",
        "h2_16 = tanh(0.0095406*h1_0+-0.53232694*h1_1+0.1535685*h1_2+0.26108417*h1_3+-0.08375187*h1_4+0.3462123*h1_5+-0.61755395*h1_6+0.0015145333*h1_7+-0.108780764*h1_8+0.2555477*h1_9+0.00023940884*h1_10+-0.26577523*h1_11+-0.27689674*h1_12+0.43049082*h1_13+-0.025424613*h1_14+0.24634649*h1_15+-0.22579487*h1_16+0.37249807*h1_17+-0.8058662*h1_18+-0.77451354*h1_19+0.5821276);\n",
        "y = -0.17525196*h2_0+-0.22995844*h2_1+0.0026147955*h2_2+0.091129646*h2_3+-0.4534783*h2_4+0.4119419*h2_5+-1.4205361e-05*h2_6+-0.11359831*h2_7+0.1762025*h2_8+-0.1002102*h2_9+-0.37161925*h2_10+0.2683793*h2_11+0.051897053*h2_12+0.19354156*h2_13+-0.10134394*h2_14+0.3937854*h2_15+-0.3296008*h2_16+0.3220947;\n",
        "\n",
        "        Id = pow(10, (y/normI + MinI)) ;\n",
        "\n",
        "if (Id <= 1e-15) begin //limit\n",
        "        Id = 1e-15;\n",
        "        //Id = Id;\n",
        "end\n",
        "else begin\n",
        "        Id = Id;\n",
        "end  //limit end\n",
        "\n",
        "\n",
        "        I(g, d) <+ Cgsd*ddt(Vg-Vd) ;\n",
        "        I(g, s) <+ Cgsd*ddt(Vg-Vs) ;\n",
        "\n",
        "if (Vgsraw >= Vgdraw) begin\n",
        "        I(d, s) <+ dir*Id*W ;\n",
        "\n",
        "end\n",
        "\n",
        "else begin\n",
        "        I(d, s) <+ dir*Id*W ;\n",
        "\n",
        "end\n",
        "\n",
        "end\n",
        "\n",
        "endmodule\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "print(verilog_code)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 576
        },
        "id": "jlLiTNqU9gKw",
        "outputId": "b37692fd-6518-4c80-a194-46c8339ad64e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1.10036881607686\n",
            "0.04321379542061602\n",
            "(29, 1, 1)\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predicted delta_V: 0.4370737671852112V\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-26-0ddd2f296d2b>:19: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  print(\"Predicted delta_V: {}V\".format(float(pred_y)))\n"
          ]
        }
      ],
      "source": [
        "output1 = np.array(output)/normdelta_Vth + Mindelta_Vth\n",
        "ytest1 = np.array(y_test)/normdelta_Vth + Mindelta_Vth\n",
        "print(normdelta_Vth)\n",
        "print(Mindelta_Vth)\n",
        "print(np.shape(output1))\n",
        "\n",
        "plt.scatter(output1, ytest1)\n",
        "a = [min(ytest1), max(ytest1)]\n",
        "b = [min(ytest1), max(ytest1)]\n",
        "plt.plot(a,b,'k')\n",
        "plt.scatter(ytest1,output1)\n",
        "plt.xlabel(\"TCAD [A/um]\")\n",
        "plt.ylabel(\"ML-Prediction [A/um]\")\n",
        "plt.show()\n",
        "\n",
        "tp = [0.0001, 0.0001, 1000, 1.7, 25] # [t_st, t_rec, clk_loops, V_ov, temperature]\n",
        "new_var = torch.FloatTensor([(tp[0]-Mint_stress)*normt_stress, (tp[1]-Mint_rec)*normt_rec, (tp[2]-Minclk_loops)*normclk_loops, (tp[3]-MinV_ov)*normV_ov, (tp[4] - Mintemperature)* normtemperature])\n",
        "pred_y=model(new_var).data.numpy()\n",
        "print(\"Predicted delta_V: {}V\".format(float(pred_y)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mCmumHX7By8e"
      },
      "source": [
        "    \n",
        "    ===========================================\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_svAlxE1RRYo"
      },
      "source": [
        "**Layer 1**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "_UsVAZBFRUtV",
        "outputId": "45e5e7b9-cc28-4417-f825-40ce7f599caf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss (epoch:    1): 1.20045621\n",
            "Loss (epoch:    2): 1.15505596\n",
            "Loss (epoch:    3): 1.11135498\n",
            "Loss (epoch:    4): 1.06911688\n",
            "Loss (epoch:    5): 1.02834294\n",
            "Loss (epoch:    6): 0.98902388\n",
            "Loss (epoch:    7): 0.95113640\n",
            "Loss (epoch:    8): 0.91464892\n",
            "Loss (epoch:    9): 0.87952644\n",
            "Loss (epoch:   10): 0.84573260\n",
            "Loss (epoch:   11): 0.81323153\n",
            "Loss (epoch:   12): 0.78198754\n",
            "Loss (epoch:   13): 0.75196593\n",
            "Loss (epoch:   14): 0.72313262\n",
            "Loss (epoch:   15): 0.69545401\n",
            "Loss (epoch:   16): 0.66889699\n",
            "Loss (epoch:   17): 0.64342882\n",
            "Loss (epoch:   18): 0.61901712\n",
            "Loss (epoch:   19): 0.59562984\n",
            "Loss (epoch:   20): 0.57323520\n",
            "Loss (epoch:   21): 0.55180176\n",
            "Loss (epoch:   22): 0.53129842\n",
            "Loss (epoch:   23): 0.51169426\n",
            "Loss (epoch:   24): 0.49295860\n",
            "Loss (epoch:   25): 0.47506112\n",
            "Loss (epoch:   26): 0.45797173\n",
            "Loss (epoch:   27): 0.44166082\n",
            "Loss (epoch:   28): 0.42609887\n",
            "Loss (epoch:   29): 0.41125691\n",
            "Loss (epoch:   30): 0.39710635\n",
            "Loss (epoch:   31): 0.38361906\n",
            "Loss (epoch:   32): 0.37076722\n",
            "Loss (epoch:   33): 0.35852364\n",
            "Loss (epoch:   34): 0.34686167\n",
            "Loss (epoch:   35): 0.33575511\n",
            "Loss (epoch:   36): 0.32517836\n",
            "Loss (epoch:   37): 0.31510637\n",
            "Loss (epoch:   38): 0.30551489\n",
            "Loss (epoch:   39): 0.29638031\n",
            "Loss (epoch:   40): 0.28767964\n",
            "Loss (epoch:   41): 0.27939065\n",
            "Loss (epoch:   42): 0.27149173\n",
            "Loss (epoch:   43): 0.26396220\n",
            "Loss (epoch:   44): 0.25678220\n",
            "Loss (epoch:   45): 0.24993244\n",
            "Loss (epoch:   46): 0.24339451\n",
            "Loss (epoch:   47): 0.23715082\n",
            "Loss (epoch:   48): 0.23118449\n",
            "Loss (epoch:   49): 0.22547951\n",
            "Loss (epoch:   50): 0.22002056\n",
            "Loss (epoch:   51): 0.21479322\n",
            "Loss (epoch:   52): 0.20978373\n",
            "Loss (epoch:   53): 0.20497906\n",
            "Loss (epoch:   54): 0.20036697\n",
            "Loss (epoch:   55): 0.19593586\n",
            "Loss (epoch:   56): 0.19167491\n",
            "Loss (epoch:   57): 0.18757383\n",
            "Loss (epoch:   58): 0.18362302\n",
            "Loss (epoch:   59): 0.17981353\n",
            "Loss (epoch:   60): 0.17613691\n",
            "Loss (epoch:   61): 0.17258538\n",
            "Loss (epoch:   62): 0.16915160\n",
            "Loss (epoch:   63): 0.16582879\n",
            "Loss (epoch:   64): 0.16261049\n",
            "Loss (epoch:   65): 0.15949086\n",
            "Loss (epoch:   66): 0.15646446\n",
            "Loss (epoch:   67): 0.15352622\n",
            "Loss (epoch:   68): 0.15067139\n",
            "Loss (epoch:   69): 0.14789557\n",
            "Loss (epoch:   70): 0.14519474\n",
            "Loss (epoch:   71): 0.14256521\n",
            "Loss (epoch:   72): 0.14000343\n",
            "Loss (epoch:   73): 0.13750627\n",
            "Loss (epoch:   74): 0.13507073\n",
            "Loss (epoch:   75): 0.13269406\n",
            "Loss (epoch:   76): 0.13037375\n",
            "Loss (epoch:   77): 0.12810742\n",
            "Loss (epoch:   78): 0.12589294\n",
            "Loss (epoch:   79): 0.12372829\n",
            "Loss (epoch:   80): 0.12161157\n",
            "Loss (epoch:   81): 0.11954108\n",
            "Loss (epoch:   82): 0.11751516\n",
            "Loss (epoch:   83): 0.11553242\n",
            "Loss (epoch:   84): 0.11359141\n",
            "Loss (epoch:   85): 0.11169082\n",
            "Loss (epoch:   86): 0.10982949\n",
            "Loss (epoch:   87): 0.10800628\n",
            "Loss (epoch:   88): 0.10622014\n",
            "Loss (epoch:   89): 0.10447008\n",
            "Loss (epoch:   90): 0.10275520\n",
            "Loss (epoch:   91): 0.10107462\n",
            "Loss (epoch:   92): 0.09942750\n",
            "Loss (epoch:   93): 0.09781309\n",
            "Loss (epoch:   94): 0.09623062\n",
            "Loss (epoch:   95): 0.09467944\n",
            "Loss (epoch:   96): 0.09315886\n",
            "Loss (epoch:   97): 0.09166827\n",
            "Loss (epoch:   98): 0.09020703\n",
            "Loss (epoch:   99): 0.08877462\n",
            "Loss (epoch:  100): 0.08737046\n",
            "Loss (epoch:  101): 0.08599402\n",
            "Loss (epoch:  102): 0.08464480\n",
            "Loss (epoch:  103): 0.08332229\n",
            "Loss (epoch:  104): 0.08202605\n",
            "Loss (epoch:  105): 0.08075561\n",
            "Loss (epoch:  106): 0.07951049\n",
            "Loss (epoch:  107): 0.07829029\n",
            "Loss (epoch:  108): 0.07709459\n",
            "Loss (epoch:  109): 0.07592295\n",
            "Loss (epoch:  110): 0.07477499\n",
            "Loss (epoch:  111): 0.07365031\n",
            "Loss (epoch:  112): 0.07254854\n",
            "Loss (epoch:  113): 0.07146929\n",
            "Loss (epoch:  114): 0.07041217\n",
            "Loss (epoch:  115): 0.06937687\n",
            "Loss (epoch:  116): 0.06836301\n",
            "Loss (epoch:  117): 0.06737024\n",
            "Loss (epoch:  118): 0.06639822\n",
            "Loss (epoch:  119): 0.06544660\n",
            "Loss (epoch:  120): 0.06451506\n",
            "Loss (epoch:  121): 0.06360326\n",
            "Loss (epoch:  122): 0.06271088\n",
            "Loss (epoch:  123): 0.06183761\n",
            "Loss (epoch:  124): 0.06098311\n",
            "Loss (epoch:  125): 0.06014709\n",
            "Loss (epoch:  126): 0.05932923\n",
            "Loss (epoch:  127): 0.05852922\n",
            "Loss (epoch:  128): 0.05774678\n",
            "Loss (epoch:  129): 0.05698160\n",
            "Loss (epoch:  130): 0.05623340\n",
            "Loss (epoch:  131): 0.05550189\n",
            "Loss (epoch:  132): 0.05478675\n",
            "Loss (epoch:  133): 0.05408773\n",
            "Loss (epoch:  134): 0.05340454\n",
            "Loss (epoch:  135): 0.05273690\n",
            "Loss (epoch:  136): 0.05208454\n",
            "Loss (epoch:  137): 0.05144719\n",
            "Loss (epoch:  138): 0.05082459\n",
            "Loss (epoch:  139): 0.05021646\n",
            "Loss (epoch:  140): 0.04962255\n",
            "Loss (epoch:  141): 0.04904259\n",
            "Loss (epoch:  142): 0.04847633\n",
            "Loss (epoch:  143): 0.04792352\n",
            "Loss (epoch:  144): 0.04738389\n",
            "Loss (epoch:  145): 0.04685723\n",
            "Loss (epoch:  146): 0.04634326\n",
            "Loss (epoch:  147): 0.04584178\n",
            "Loss (epoch:  148): 0.04535252\n",
            "Loss (epoch:  149): 0.04487524\n",
            "Loss (epoch:  150): 0.04440973\n",
            "Loss (epoch:  151): 0.04395575\n",
            "Loss (epoch:  152): 0.04351309\n",
            "Loss (epoch:  153): 0.04308149\n",
            "Loss (epoch:  154): 0.04266077\n",
            "Loss (epoch:  155): 0.04225070\n",
            "Loss (epoch:  156): 0.04185105\n",
            "Loss (epoch:  157): 0.04146161\n",
            "Loss (epoch:  158): 0.04108220\n",
            "Loss (epoch:  159): 0.04071256\n",
            "Loss (epoch:  160): 0.04035253\n",
            "Loss (epoch:  161): 0.04000190\n",
            "Loss (epoch:  162): 0.03966046\n",
            "Loss (epoch:  163): 0.03932803\n",
            "Loss (epoch:  164): 0.03900441\n",
            "Loss (epoch:  165): 0.03868942\n",
            "Loss (epoch:  166): 0.03838286\n",
            "Loss (epoch:  167): 0.03808454\n",
            "Loss (epoch:  168): 0.03779429\n",
            "Loss (epoch:  169): 0.03751194\n",
            "Loss (epoch:  170): 0.03723729\n",
            "Loss (epoch:  171): 0.03697019\n",
            "Loss (epoch:  172): 0.03671047\n",
            "Loss (epoch:  173): 0.03645795\n",
            "Loss (epoch:  174): 0.03621246\n",
            "Loss (epoch:  175): 0.03597385\n",
            "Loss (epoch:  176): 0.03574196\n",
            "Loss (epoch:  177): 0.03551663\n",
            "Loss (epoch:  178): 0.03529770\n",
            "Loss (epoch:  179): 0.03508503\n",
            "Loss (epoch:  180): 0.03487846\n",
            "Loss (epoch:  181): 0.03467785\n",
            "Loss (epoch:  182): 0.03448305\n",
            "Loss (epoch:  183): 0.03429393\n",
            "Loss (epoch:  184): 0.03411034\n",
            "Loss (epoch:  185): 0.03393216\n",
            "Loss (epoch:  186): 0.03375923\n",
            "Loss (epoch:  187): 0.03359143\n",
            "Loss (epoch:  188): 0.03342864\n",
            "Loss (epoch:  189): 0.03327073\n",
            "Loss (epoch:  190): 0.03311757\n",
            "Loss (epoch:  191): 0.03296904\n",
            "Loss (epoch:  192): 0.03282503\n",
            "Loss (epoch:  193): 0.03268541\n",
            "Loss (epoch:  194): 0.03255007\n",
            "Loss (epoch:  195): 0.03241889\n",
            "Loss (epoch:  196): 0.03229177\n",
            "Loss (epoch:  197): 0.03216860\n",
            "Loss (epoch:  198): 0.03204928\n",
            "Loss (epoch:  199): 0.03193369\n",
            "Loss (epoch:  200): 0.03182174\n",
            "Loss (epoch:  201): 0.03171334\n",
            "Loss (epoch:  202): 0.03160837\n",
            "Loss (epoch:  203): 0.03150675\n",
            "Loss (epoch:  204): 0.03140839\n",
            "Loss (epoch:  205): 0.03131319\n",
            "Loss (epoch:  206): 0.03122107\n",
            "Loss (epoch:  207): 0.03113193\n",
            "Loss (epoch:  208): 0.03104569\n",
            "Loss (epoch:  209): 0.03096227\n",
            "Loss (epoch:  210): 0.03088159\n",
            "Loss (epoch:  211): 0.03080357\n",
            "Loss (epoch:  212): 0.03072814\n",
            "Loss (epoch:  213): 0.03065520\n",
            "Loss (epoch:  214): 0.03058470\n",
            "Loss (epoch:  215): 0.03051656\n",
            "Loss (epoch:  216): 0.03045070\n",
            "Loss (epoch:  217): 0.03038707\n",
            "Loss (epoch:  218): 0.03032559\n",
            "Loss (epoch:  219): 0.03026619\n",
            "Loss (epoch:  220): 0.03020882\n",
            "Loss (epoch:  221): 0.03015341\n",
            "Loss (epoch:  222): 0.03009990\n",
            "Loss (epoch:  223): 0.03004824\n",
            "Loss (epoch:  224): 0.02999836\n",
            "Loss (epoch:  225): 0.02995021\n",
            "Loss (epoch:  226): 0.02990373\n",
            "Loss (epoch:  227): 0.02985887\n",
            "Loss (epoch:  228): 0.02981558\n",
            "Loss (epoch:  229): 0.02977382\n",
            "Loss (epoch:  230): 0.02973351\n",
            "Loss (epoch:  231): 0.02969464\n",
            "Loss (epoch:  232): 0.02965713\n",
            "Loss (epoch:  233): 0.02962095\n",
            "Loss (epoch:  234): 0.02958607\n",
            "Loss (epoch:  235): 0.02955243\n",
            "Loss (epoch:  236): 0.02951999\n",
            "Loss (epoch:  237): 0.02948870\n",
            "Loss (epoch:  238): 0.02945854\n",
            "Loss (epoch:  239): 0.02942947\n",
            "Loss (epoch:  240): 0.02940145\n",
            "Loss (epoch:  241): 0.02937443\n",
            "Loss (epoch:  242): 0.02934839\n",
            "Loss (epoch:  243): 0.02932329\n",
            "Loss (epoch:  244): 0.02929910\n",
            "Loss (epoch:  245): 0.02927579\n",
            "Loss (epoch:  246): 0.02925334\n",
            "Loss (epoch:  247): 0.02923169\n",
            "Loss (epoch:  248): 0.02921083\n",
            "Loss (epoch:  249): 0.02919073\n",
            "Loss (epoch:  250): 0.02917136\n",
            "Loss (epoch:  251): 0.02915270\n",
            "Loss (epoch:  252): 0.02913472\n",
            "Loss (epoch:  253): 0.02911740\n",
            "Loss (epoch:  254): 0.02910070\n",
            "Loss (epoch:  255): 0.02908461\n",
            "Loss (epoch:  256): 0.02906911\n",
            "Loss (epoch:  257): 0.02905417\n",
            "Loss (epoch:  258): 0.02903977\n",
            "Loss (epoch:  259): 0.02902589\n",
            "Loss (epoch:  260): 0.02901252\n",
            "Loss (epoch:  261): 0.02899963\n",
            "Loss (epoch:  262): 0.02898720\n",
            "Loss (epoch:  263): 0.02897522\n",
            "Loss (epoch:  264): 0.02896366\n",
            "Loss (epoch:  265): 0.02895252\n",
            "Loss (epoch:  266): 0.02894178\n",
            "Loss (epoch:  267): 0.02893141\n",
            "Loss (epoch:  268): 0.02892141\n",
            "Loss (epoch:  269): 0.02891177\n",
            "Loss (epoch:  270): 0.02890245\n",
            "Loss (epoch:  271): 0.02889346\n",
            "Loss (epoch:  272): 0.02888479\n",
            "Loss (epoch:  273): 0.02887640\n",
            "Loss (epoch:  274): 0.02886831\n",
            "Loss (epoch:  275): 0.02886048\n",
            "Loss (epoch:  276): 0.02885292\n",
            "Loss (epoch:  277): 0.02884562\n",
            "Loss (epoch:  278): 0.02883855\n",
            "Loss (epoch:  279): 0.02883171\n",
            "Loss (epoch:  280): 0.02882510\n",
            "Loss (epoch:  281): 0.02881869\n",
            "Loss (epoch:  282): 0.02881249\n",
            "Loss (epoch:  283): 0.02880648\n",
            "Loss (epoch:  284): 0.02880066\n",
            "Loss (epoch:  285): 0.02879501\n",
            "Loss (epoch:  286): 0.02878954\n",
            "Loss (epoch:  287): 0.02878423\n",
            "Loss (epoch:  288): 0.02877907\n",
            "Loss (epoch:  289): 0.02877405\n",
            "Loss (epoch:  290): 0.02876918\n",
            "Loss (epoch:  291): 0.02876445\n",
            "Loss (epoch:  292): 0.02875985\n",
            "Loss (epoch:  293): 0.02875536\n",
            "Loss (epoch:  294): 0.02875099\n",
            "Loss (epoch:  295): 0.02874674\n",
            "Loss (epoch:  296): 0.02874259\n",
            "Loss (epoch:  297): 0.02873855\n",
            "Loss (epoch:  298): 0.02873460\n",
            "Loss (epoch:  299): 0.02873075\n",
            "Loss (epoch:  300): 0.02872698\n",
            "Loss (epoch:  301): 0.02872330\n",
            "Loss (epoch:  302): 0.02871969\n",
            "Loss (epoch:  303): 0.02871616\n",
            "Loss (epoch:  304): 0.02871271\n",
            "Loss (epoch:  305): 0.02870931\n",
            "Loss (epoch:  306): 0.02870599\n",
            "Loss (epoch:  307): 0.02870273\n",
            "Loss (epoch:  308): 0.02869952\n",
            "Loss (epoch:  309): 0.02869637\n",
            "Loss (epoch:  310): 0.02869328\n",
            "Loss (epoch:  311): 0.02869023\n",
            "Loss (epoch:  312): 0.02868723\n",
            "Loss (epoch:  313): 0.02868428\n",
            "Loss (epoch:  314): 0.02868137\n",
            "Loss (epoch:  315): 0.02867849\n",
            "Loss (epoch:  316): 0.02867566\n",
            "Loss (epoch:  317): 0.02867286\n",
            "Loss (epoch:  318): 0.02867009\n",
            "Loss (epoch:  319): 0.02866736\n",
            "Loss (epoch:  320): 0.02866465\n",
            "Loss (epoch:  321): 0.02866198\n",
            "Loss (epoch:  322): 0.02865933\n",
            "Loss (epoch:  323): 0.02865670\n",
            "Loss (epoch:  324): 0.02865410\n",
            "Loss (epoch:  325): 0.02865152\n",
            "Loss (epoch:  326): 0.02864895\n",
            "Loss (epoch:  327): 0.02864641\n",
            "Loss (epoch:  328): 0.02864389\n",
            "Loss (epoch:  329): 0.02864138\n",
            "Loss (epoch:  330): 0.02863889\n",
            "Loss (epoch:  331): 0.02863641\n",
            "Loss (epoch:  332): 0.02863394\n",
            "Loss (epoch:  333): 0.02863149\n",
            "Loss (epoch:  334): 0.02862904\n",
            "Loss (epoch:  335): 0.02862661\n",
            "Loss (epoch:  336): 0.02862419\n",
            "Loss (epoch:  337): 0.02862178\n",
            "Loss (epoch:  338): 0.02861937\n",
            "Loss (epoch:  339): 0.02861697\n",
            "Loss (epoch:  340): 0.02861458\n",
            "Loss (epoch:  341): 0.02861219\n",
            "Loss (epoch:  342): 0.02860981\n",
            "Loss (epoch:  343): 0.02860743\n",
            "Loss (epoch:  344): 0.02860506\n",
            "Loss (epoch:  345): 0.02860269\n",
            "Loss (epoch:  346): 0.02860032\n",
            "Loss (epoch:  347): 0.02859795\n",
            "Loss (epoch:  348): 0.02859559\n",
            "Loss (epoch:  349): 0.02859323\n",
            "Loss (epoch:  350): 0.02859086\n",
            "Loss (epoch:  351): 0.02858851\n",
            "Loss (epoch:  352): 0.02858614\n",
            "Loss (epoch:  353): 0.02858379\n",
            "Loss (epoch:  354): 0.02858142\n",
            "Loss (epoch:  355): 0.02857906\n",
            "Loss (epoch:  356): 0.02857670\n",
            "Loss (epoch:  357): 0.02857433\n",
            "Loss (epoch:  358): 0.02857196\n",
            "Loss (epoch:  359): 0.02856959\n",
            "Loss (epoch:  360): 0.02856723\n",
            "Loss (epoch:  361): 0.02856485\n",
            "Loss (epoch:  362): 0.02856247\n",
            "Loss (epoch:  363): 0.02856010\n",
            "Loss (epoch:  364): 0.02855771\n",
            "Loss (epoch:  365): 0.02855533\n",
            "Loss (epoch:  366): 0.02855294\n",
            "Loss (epoch:  367): 0.02855054\n",
            "Loss (epoch:  368): 0.02854815\n",
            "Loss (epoch:  369): 0.02854575\n",
            "Loss (epoch:  370): 0.02854334\n",
            "Loss (epoch:  371): 0.02854094\n",
            "Loss (epoch:  372): 0.02853852\n",
            "Loss (epoch:  373): 0.02853610\n",
            "Loss (epoch:  374): 0.02853368\n",
            "Loss (epoch:  375): 0.02853125\n",
            "Loss (epoch:  376): 0.02852882\n",
            "Loss (epoch:  377): 0.02852638\n",
            "Loss (epoch:  378): 0.02852394\n",
            "Loss (epoch:  379): 0.02852150\n",
            "Loss (epoch:  380): 0.02851905\n",
            "Loss (epoch:  381): 0.02851659\n",
            "Loss (epoch:  382): 0.02851413\n",
            "Loss (epoch:  383): 0.02851166\n",
            "Loss (epoch:  384): 0.02850919\n",
            "Loss (epoch:  385): 0.02850672\n",
            "Loss (epoch:  386): 0.02850423\n",
            "Loss (epoch:  387): 0.02850174\n",
            "Loss (epoch:  388): 0.02849925\n",
            "Loss (epoch:  389): 0.02849675\n",
            "Loss (epoch:  390): 0.02849424\n",
            "Loss (epoch:  391): 0.02849174\n",
            "Loss (epoch:  392): 0.02848922\n",
            "Loss (epoch:  393): 0.02848670\n",
            "Loss (epoch:  394): 0.02848417\n",
            "Loss (epoch:  395): 0.02848163\n",
            "Loss (epoch:  396): 0.02847910\n",
            "Loss (epoch:  397): 0.02847655\n",
            "Loss (epoch:  398): 0.02847400\n",
            "Loss (epoch:  399): 0.02847144\n",
            "Loss (epoch:  400): 0.02846888\n",
            "Loss (epoch:  401): 0.02846631\n",
            "Loss (epoch:  402): 0.02846374\n",
            "Loss (epoch:  403): 0.02846116\n",
            "Loss (epoch:  404): 0.02845857\n",
            "Loss (epoch:  405): 0.02845599\n",
            "Loss (epoch:  406): 0.02845339\n",
            "Loss (epoch:  407): 0.02845078\n",
            "Loss (epoch:  408): 0.02844818\n",
            "Loss (epoch:  409): 0.02844556\n",
            "Loss (epoch:  410): 0.02844294\n",
            "Loss (epoch:  411): 0.02844031\n",
            "Loss (epoch:  412): 0.02843768\n",
            "Loss (epoch:  413): 0.02843504\n",
            "Loss (epoch:  414): 0.02843240\n",
            "Loss (epoch:  415): 0.02842975\n",
            "Loss (epoch:  416): 0.02842710\n",
            "Loss (epoch:  417): 0.02842444\n",
            "Loss (epoch:  418): 0.02842177\n",
            "Loss (epoch:  419): 0.02841910\n",
            "Loss (epoch:  420): 0.02841642\n",
            "Loss (epoch:  421): 0.02841374\n",
            "Loss (epoch:  422): 0.02841105\n",
            "Loss (epoch:  423): 0.02840835\n",
            "Loss (epoch:  424): 0.02840565\n",
            "Loss (epoch:  425): 0.02840294\n",
            "Loss (epoch:  426): 0.02840023\n",
            "Loss (epoch:  427): 0.02839751\n",
            "Loss (epoch:  428): 0.02839479\n",
            "Loss (epoch:  429): 0.02839206\n",
            "Loss (epoch:  430): 0.02838933\n",
            "Loss (epoch:  431): 0.02838658\n",
            "Loss (epoch:  432): 0.02838384\n",
            "Loss (epoch:  433): 0.02838108\n",
            "Loss (epoch:  434): 0.02837833\n",
            "Loss (epoch:  435): 0.02837556\n",
            "Loss (epoch:  436): 0.02837280\n",
            "Loss (epoch:  437): 0.02837002\n",
            "Loss (epoch:  438): 0.02836724\n",
            "Loss (epoch:  439): 0.02836446\n",
            "Loss (epoch:  440): 0.02836166\n",
            "Loss (epoch:  441): 0.02835887\n",
            "Loss (epoch:  442): 0.02835607\n",
            "Loss (epoch:  443): 0.02835326\n",
            "Loss (epoch:  444): 0.02835045\n",
            "Loss (epoch:  445): 0.02834763\n",
            "Loss (epoch:  446): 0.02834481\n",
            "Loss (epoch:  447): 0.02834198\n",
            "Loss (epoch:  448): 0.02833914\n",
            "Loss (epoch:  449): 0.02833631\n",
            "Loss (epoch:  450): 0.02833346\n",
            "Loss (epoch:  451): 0.02833061\n",
            "Loss (epoch:  452): 0.02832775\n",
            "Loss (epoch:  453): 0.02832489\n",
            "Loss (epoch:  454): 0.02832203\n",
            "Loss (epoch:  455): 0.02831916\n",
            "Loss (epoch:  456): 0.02831628\n",
            "Loss (epoch:  457): 0.02831340\n",
            "Loss (epoch:  458): 0.02831051\n",
            "Loss (epoch:  459): 0.02830762\n",
            "Loss (epoch:  460): 0.02830473\n",
            "Loss (epoch:  461): 0.02830183\n",
            "Loss (epoch:  462): 0.02829892\n",
            "Loss (epoch:  463): 0.02829601\n",
            "Loss (epoch:  464): 0.02829309\n",
            "Loss (epoch:  465): 0.02829017\n",
            "Loss (epoch:  466): 0.02828724\n",
            "Loss (epoch:  467): 0.02828431\n",
            "Loss (epoch:  468): 0.02828138\n",
            "Loss (epoch:  469): 0.02827844\n",
            "Loss (epoch:  470): 0.02827549\n",
            "Loss (epoch:  471): 0.02827254\n",
            "Loss (epoch:  472): 0.02826958\n",
            "Loss (epoch:  473): 0.02826662\n",
            "Loss (epoch:  474): 0.02826366\n",
            "Loss (epoch:  475): 0.02826069\n",
            "Loss (epoch:  476): 0.02825772\n",
            "Loss (epoch:  477): 0.02825474\n",
            "Loss (epoch:  478): 0.02825175\n",
            "Loss (epoch:  479): 0.02824876\n",
            "Loss (epoch:  480): 0.02824577\n",
            "Loss (epoch:  481): 0.02824278\n",
            "Loss (epoch:  482): 0.02823977\n",
            "Loss (epoch:  483): 0.02823677\n",
            "Loss (epoch:  484): 0.02823376\n",
            "Loss (epoch:  485): 0.02823074\n",
            "Loss (epoch:  486): 0.02822772\n",
            "Loss (epoch:  487): 0.02822470\n",
            "Loss (epoch:  488): 0.02822167\n",
            "Loss (epoch:  489): 0.02821864\n",
            "Loss (epoch:  490): 0.02821560\n",
            "Loss (epoch:  491): 0.02821256\n",
            "Loss (epoch:  492): 0.02820951\n",
            "Loss (epoch:  493): 0.02820646\n",
            "Loss (epoch:  494): 0.02820341\n",
            "Loss (epoch:  495): 0.02820035\n",
            "Loss (epoch:  496): 0.02819729\n",
            "Loss (epoch:  497): 0.02819422\n",
            "Loss (epoch:  498): 0.02819115\n",
            "Loss (epoch:  499): 0.02818807\n",
            "Loss (epoch:  500): 0.02818500\n",
            "Loss (epoch:  501): 0.02818191\n",
            "Loss (epoch:  502): 0.02817883\n",
            "Loss (epoch:  503): 0.02817574\n",
            "Loss (epoch:  504): 0.02817264\n",
            "Loss (epoch:  505): 0.02816955\n",
            "Loss (epoch:  506): 0.02816644\n",
            "Loss (epoch:  507): 0.02816334\n",
            "Loss (epoch:  508): 0.02816023\n",
            "Loss (epoch:  509): 0.02815711\n",
            "Loss (epoch:  510): 0.02815400\n",
            "Loss (epoch:  511): 0.02815087\n",
            "Loss (epoch:  512): 0.02814775\n",
            "Loss (epoch:  513): 0.02814462\n",
            "Loss (epoch:  514): 0.02814149\n",
            "Loss (epoch:  515): 0.02813836\n",
            "Loss (epoch:  516): 0.02813521\n",
            "Loss (epoch:  517): 0.02813208\n",
            "Loss (epoch:  518): 0.02812893\n",
            "Loss (epoch:  519): 0.02812578\n",
            "Loss (epoch:  520): 0.02812263\n",
            "Loss (epoch:  521): 0.02811947\n",
            "Loss (epoch:  522): 0.02811631\n",
            "Loss (epoch:  523): 0.02811315\n",
            "Loss (epoch:  524): 0.02810999\n",
            "Loss (epoch:  525): 0.02810681\n",
            "Loss (epoch:  526): 0.02810364\n",
            "Loss (epoch:  527): 0.02810047\n",
            "Loss (epoch:  528): 0.02809729\n",
            "Loss (epoch:  529): 0.02809410\n",
            "Loss (epoch:  530): 0.02809092\n",
            "Loss (epoch:  531): 0.02808774\n",
            "Loss (epoch:  532): 0.02808454\n",
            "Loss (epoch:  533): 0.02808135\n",
            "Loss (epoch:  534): 0.02807815\n",
            "Loss (epoch:  535): 0.02807495\n",
            "Loss (epoch:  536): 0.02807175\n",
            "Loss (epoch:  537): 0.02806854\n",
            "Loss (epoch:  538): 0.02806533\n",
            "Loss (epoch:  539): 0.02806212\n",
            "Loss (epoch:  540): 0.02805891\n",
            "Loss (epoch:  541): 0.02805569\n",
            "Loss (epoch:  542): 0.02805247\n",
            "Loss (epoch:  543): 0.02804925\n",
            "Loss (epoch:  544): 0.02804603\n",
            "Loss (epoch:  545): 0.02804280\n",
            "Loss (epoch:  546): 0.02803957\n",
            "Loss (epoch:  547): 0.02803634\n",
            "Loss (epoch:  548): 0.02803310\n",
            "Loss (epoch:  549): 0.02802987\n",
            "Loss (epoch:  550): 0.02802663\n",
            "Loss (epoch:  551): 0.02802338\n",
            "Loss (epoch:  552): 0.02802014\n",
            "Loss (epoch:  553): 0.02801689\n",
            "Loss (epoch:  554): 0.02801364\n",
            "Loss (epoch:  555): 0.02801039\n",
            "Loss (epoch:  556): 0.02800714\n",
            "Loss (epoch:  557): 0.02800388\n",
            "Loss (epoch:  558): 0.02800062\n",
            "Loss (epoch:  559): 0.02799736\n",
            "Loss (epoch:  560): 0.02799410\n",
            "Loss (epoch:  561): 0.02799084\n",
            "Loss (epoch:  562): 0.02798757\n",
            "Loss (epoch:  563): 0.02798431\n",
            "Loss (epoch:  564): 0.02798104\n",
            "Loss (epoch:  565): 0.02797776\n",
            "Loss (epoch:  566): 0.02797449\n",
            "Loss (epoch:  567): 0.02797121\n",
            "Loss (epoch:  568): 0.02796794\n",
            "Loss (epoch:  569): 0.02796466\n",
            "Loss (epoch:  570): 0.02796138\n",
            "Loss (epoch:  571): 0.02795809\n",
            "Loss (epoch:  572): 0.02795481\n",
            "Loss (epoch:  573): 0.02795153\n",
            "Loss (epoch:  574): 0.02794824\n",
            "Loss (epoch:  575): 0.02794495\n",
            "Loss (epoch:  576): 0.02794166\n",
            "Loss (epoch:  577): 0.02793837\n",
            "Loss (epoch:  578): 0.02793508\n",
            "Loss (epoch:  579): 0.02793178\n",
            "Loss (epoch:  580): 0.02792848\n",
            "Loss (epoch:  581): 0.02792519\n",
            "Loss (epoch:  582): 0.02792189\n",
            "Loss (epoch:  583): 0.02791859\n",
            "Loss (epoch:  584): 0.02791529\n",
            "Loss (epoch:  585): 0.02791198\n",
            "Loss (epoch:  586): 0.02790868\n",
            "Loss (epoch:  587): 0.02790538\n",
            "Loss (epoch:  588): 0.02790207\n",
            "Loss (epoch:  589): 0.02789876\n",
            "Loss (epoch:  590): 0.02789545\n",
            "Loss (epoch:  591): 0.02789214\n",
            "Loss (epoch:  592): 0.02788884\n",
            "Loss (epoch:  593): 0.02788552\n",
            "Loss (epoch:  594): 0.02788222\n",
            "Loss (epoch:  595): 0.02787890\n",
            "Loss (epoch:  596): 0.02787559\n",
            "Loss (epoch:  597): 0.02787227\n",
            "Loss (epoch:  598): 0.02786896\n",
            "Loss (epoch:  599): 0.02786564\n",
            "Loss (epoch:  600): 0.02786233\n",
            "Loss (epoch:  601): 0.02785902\n",
            "Loss (epoch:  602): 0.02785570\n",
            "Loss (epoch:  603): 0.02785238\n",
            "Loss (epoch:  604): 0.02784906\n",
            "Loss (epoch:  605): 0.02784574\n",
            "Loss (epoch:  606): 0.02784242\n",
            "Loss (epoch:  607): 0.02783910\n",
            "Loss (epoch:  608): 0.02783578\n",
            "Loss (epoch:  609): 0.02783246\n",
            "Loss (epoch:  610): 0.02782914\n",
            "Loss (epoch:  611): 0.02782582\n",
            "Loss (epoch:  612): 0.02782250\n",
            "Loss (epoch:  613): 0.02781918\n",
            "Loss (epoch:  614): 0.02781586\n",
            "Loss (epoch:  615): 0.02781254\n",
            "Loss (epoch:  616): 0.02780921\n",
            "Loss (epoch:  617): 0.02780589\n",
            "Loss (epoch:  618): 0.02780257\n",
            "Loss (epoch:  619): 0.02779925\n",
            "Loss (epoch:  620): 0.02779592\n",
            "Loss (epoch:  621): 0.02779260\n",
            "Loss (epoch:  622): 0.02778928\n",
            "Loss (epoch:  623): 0.02778596\n",
            "Loss (epoch:  624): 0.02778264\n",
            "Loss (epoch:  625): 0.02777932\n",
            "Loss (epoch:  626): 0.02777599\n",
            "Loss (epoch:  627): 0.02777267\n",
            "Loss (epoch:  628): 0.02776935\n",
            "Loss (epoch:  629): 0.02776604\n",
            "Loss (epoch:  630): 0.02776271\n",
            "Loss (epoch:  631): 0.02775939\n",
            "Loss (epoch:  632): 0.02775608\n",
            "Loss (epoch:  633): 0.02775275\n",
            "Loss (epoch:  634): 0.02774944\n",
            "Loss (epoch:  635): 0.02774612\n",
            "Loss (epoch:  636): 0.02774280\n",
            "Loss (epoch:  637): 0.02773949\n",
            "Loss (epoch:  638): 0.02773617\n",
            "Loss (epoch:  639): 0.02773285\n",
            "Loss (epoch:  640): 0.02772954\n",
            "Loss (epoch:  641): 0.02772622\n",
            "Loss (epoch:  642): 0.02772291\n",
            "Loss (epoch:  643): 0.02771960\n",
            "Loss (epoch:  644): 0.02771628\n",
            "Loss (epoch:  645): 0.02771297\n",
            "Loss (epoch:  646): 0.02770966\n",
            "Loss (epoch:  647): 0.02770635\n",
            "Loss (epoch:  648): 0.02770305\n",
            "Loss (epoch:  649): 0.02769974\n",
            "Loss (epoch:  650): 0.02769643\n",
            "Loss (epoch:  651): 0.02769312\n",
            "Loss (epoch:  652): 0.02768982\n",
            "Loss (epoch:  653): 0.02768651\n",
            "Loss (epoch:  654): 0.02768321\n",
            "Loss (epoch:  655): 0.02767991\n",
            "Loss (epoch:  656): 0.02767661\n",
            "Loss (epoch:  657): 0.02767331\n",
            "Loss (epoch:  658): 0.02767001\n",
            "Loss (epoch:  659): 0.02766671\n",
            "Loss (epoch:  660): 0.02766342\n",
            "Loss (epoch:  661): 0.02766012\n",
            "Loss (epoch:  662): 0.02765683\n",
            "Loss (epoch:  663): 0.02765354\n",
            "Loss (epoch:  664): 0.02765025\n",
            "Loss (epoch:  665): 0.02764696\n",
            "Loss (epoch:  666): 0.02764367\n",
            "Loss (epoch:  667): 0.02764039\n",
            "Loss (epoch:  668): 0.02763710\n",
            "Loss (epoch:  669): 0.02763382\n",
            "Loss (epoch:  670): 0.02763054\n",
            "Loss (epoch:  671): 0.02762725\n",
            "Loss (epoch:  672): 0.02762398\n",
            "Loss (epoch:  673): 0.02762070\n",
            "Loss (epoch:  674): 0.02761743\n",
            "Loss (epoch:  675): 0.02761416\n",
            "Loss (epoch:  676): 0.02761088\n",
            "Loss (epoch:  677): 0.02760761\n",
            "Loss (epoch:  678): 0.02760434\n",
            "Loss (epoch:  679): 0.02760108\n",
            "Loss (epoch:  680): 0.02759781\n",
            "Loss (epoch:  681): 0.02759455\n",
            "Loss (epoch:  682): 0.02759129\n",
            "Loss (epoch:  683): 0.02758803\n",
            "Loss (epoch:  684): 0.02758477\n",
            "Loss (epoch:  685): 0.02758151\n",
            "Loss (epoch:  686): 0.02757826\n",
            "Loss (epoch:  687): 0.02757501\n",
            "Loss (epoch:  688): 0.02757175\n",
            "Loss (epoch:  689): 0.02756851\n",
            "Loss (epoch:  690): 0.02756526\n",
            "Loss (epoch:  691): 0.02756202\n",
            "Loss (epoch:  692): 0.02755877\n",
            "Loss (epoch:  693): 0.02755554\n",
            "Loss (epoch:  694): 0.02755230\n",
            "Loss (epoch:  695): 0.02754907\n",
            "Loss (epoch:  696): 0.02754583\n",
            "Loss (epoch:  697): 0.02754260\n",
            "Loss (epoch:  698): 0.02753937\n",
            "Loss (epoch:  699): 0.02753614\n",
            "Loss (epoch:  700): 0.02753292\n",
            "Loss (epoch:  701): 0.02752970\n",
            "Loss (epoch:  702): 0.02752648\n",
            "Loss (epoch:  703): 0.02752326\n",
            "Loss (epoch:  704): 0.02752005\n",
            "Loss (epoch:  705): 0.02751683\n",
            "Loss (epoch:  706): 0.02751362\n",
            "Loss (epoch:  707): 0.02751042\n",
            "Loss (epoch:  708): 0.02750721\n",
            "Loss (epoch:  709): 0.02750401\n",
            "Loss (epoch:  710): 0.02750081\n",
            "Loss (epoch:  711): 0.02749761\n",
            "Loss (epoch:  712): 0.02749441\n",
            "Loss (epoch:  713): 0.02749122\n",
            "Loss (epoch:  714): 0.02748803\n",
            "Loss (epoch:  715): 0.02748484\n",
            "Loss (epoch:  716): 0.02748166\n",
            "Loss (epoch:  717): 0.02747848\n",
            "Loss (epoch:  718): 0.02747530\n",
            "Loss (epoch:  719): 0.02747212\n",
            "Loss (epoch:  720): 0.02746894\n",
            "Loss (epoch:  721): 0.02746577\n",
            "Loss (epoch:  722): 0.02746260\n",
            "Loss (epoch:  723): 0.02745944\n",
            "Loss (epoch:  724): 0.02745627\n",
            "Loss (epoch:  725): 0.02745311\n",
            "Loss (epoch:  726): 0.02744995\n",
            "Loss (epoch:  727): 0.02744679\n",
            "Loss (epoch:  728): 0.02744364\n",
            "Loss (epoch:  729): 0.02744050\n",
            "Loss (epoch:  730): 0.02743735\n",
            "Loss (epoch:  731): 0.02743420\n",
            "Loss (epoch:  732): 0.02743106\n",
            "Loss (epoch:  733): 0.02742792\n",
            "Loss (epoch:  734): 0.02742479\n",
            "Loss (epoch:  735): 0.02742165\n",
            "Loss (epoch:  736): 0.02741852\n",
            "Loss (epoch:  737): 0.02741540\n",
            "Loss (epoch:  738): 0.02741227\n",
            "Loss (epoch:  739): 0.02740916\n",
            "Loss (epoch:  740): 0.02740604\n",
            "Loss (epoch:  741): 0.02740292\n",
            "Loss (epoch:  742): 0.02739981\n",
            "Loss (epoch:  743): 0.02739671\n",
            "Loss (epoch:  744): 0.02739360\n",
            "Loss (epoch:  745): 0.02739050\n",
            "Loss (epoch:  746): 0.02738739\n",
            "Loss (epoch:  747): 0.02738430\n",
            "Loss (epoch:  748): 0.02738120\n",
            "Loss (epoch:  749): 0.02737812\n",
            "Loss (epoch:  750): 0.02737503\n",
            "Loss (epoch:  751): 0.02737195\n",
            "Loss (epoch:  752): 0.02736887\n",
            "Loss (epoch:  753): 0.02736579\n",
            "Loss (epoch:  754): 0.02736271\n",
            "Loss (epoch:  755): 0.02735964\n",
            "Loss (epoch:  756): 0.02735658\n",
            "Loss (epoch:  757): 0.02735351\n",
            "Loss (epoch:  758): 0.02735045\n",
            "Loss (epoch:  759): 0.02734739\n",
            "Loss (epoch:  760): 0.02734434\n",
            "Loss (epoch:  761): 0.02734129\n",
            "Loss (epoch:  762): 0.02733824\n",
            "Loss (epoch:  763): 0.02733520\n",
            "Loss (epoch:  764): 0.02733215\n",
            "Loss (epoch:  765): 0.02732911\n",
            "Loss (epoch:  766): 0.02732608\n",
            "Loss (epoch:  767): 0.02732305\n",
            "Loss (epoch:  768): 0.02732002\n",
            "Loss (epoch:  769): 0.02731700\n",
            "Loss (epoch:  770): 0.02731398\n",
            "Loss (epoch:  771): 0.02731096\n",
            "Loss (epoch:  772): 0.02730795\n",
            "Loss (epoch:  773): 0.02730494\n",
            "Loss (epoch:  774): 0.02730193\n",
            "Loss (epoch:  775): 0.02729892\n",
            "Loss (epoch:  776): 0.02729593\n",
            "Loss (epoch:  777): 0.02729293\n",
            "Loss (epoch:  778): 0.02728994\n",
            "Loss (epoch:  779): 0.02728695\n",
            "Loss (epoch:  780): 0.02728396\n",
            "Loss (epoch:  781): 0.02728098\n",
            "Loss (epoch:  782): 0.02727800\n",
            "Loss (epoch:  783): 0.02727502\n",
            "Loss (epoch:  784): 0.02727205\n",
            "Loss (epoch:  785): 0.02726908\n",
            "Loss (epoch:  786): 0.02726612\n",
            "Loss (epoch:  787): 0.02726316\n",
            "Loss (epoch:  788): 0.02726020\n",
            "Loss (epoch:  789): 0.02725725\n",
            "Loss (epoch:  790): 0.02725429\n",
            "Loss (epoch:  791): 0.02725135\n",
            "Loss (epoch:  792): 0.02724840\n",
            "Loss (epoch:  793): 0.02724547\n",
            "Loss (epoch:  794): 0.02724253\n",
            "Loss (epoch:  795): 0.02723960\n",
            "Loss (epoch:  796): 0.02723667\n",
            "Loss (epoch:  797): 0.02723374\n",
            "Loss (epoch:  798): 0.02723082\n",
            "Loss (epoch:  799): 0.02722791\n",
            "Loss (epoch:  800): 0.02722499\n",
            "Loss (epoch:  801): 0.02722208\n",
            "Loss (epoch:  802): 0.02721917\n",
            "Loss (epoch:  803): 0.02721627\n",
            "Loss (epoch:  804): 0.02721337\n",
            "Loss (epoch:  805): 0.02721048\n",
            "Loss (epoch:  806): 0.02720758\n",
            "Loss (epoch:  807): 0.02720470\n",
            "Loss (epoch:  808): 0.02720181\n",
            "Loss (epoch:  809): 0.02719893\n",
            "Loss (epoch:  810): 0.02719605\n",
            "Loss (epoch:  811): 0.02719318\n",
            "Loss (epoch:  812): 0.02719031\n",
            "Loss (epoch:  813): 0.02718744\n",
            "Loss (epoch:  814): 0.02718458\n",
            "Loss (epoch:  815): 0.02718172\n",
            "Loss (epoch:  816): 0.02717887\n",
            "Loss (epoch:  817): 0.02717602\n",
            "Loss (epoch:  818): 0.02717318\n",
            "Loss (epoch:  819): 0.02717033\n",
            "Loss (epoch:  820): 0.02716749\n",
            "Loss (epoch:  821): 0.02716465\n",
            "Loss (epoch:  822): 0.02716182\n",
            "Loss (epoch:  823): 0.02715899\n",
            "Loss (epoch:  824): 0.02715617\n",
            "Loss (epoch:  825): 0.02715335\n",
            "Loss (epoch:  826): 0.02715053\n",
            "Loss (epoch:  827): 0.02714772\n",
            "Loss (epoch:  828): 0.02714491\n",
            "Loss (epoch:  829): 0.02714211\n",
            "Loss (epoch:  830): 0.02713931\n",
            "Loss (epoch:  831): 0.02713650\n",
            "Loss (epoch:  832): 0.02713372\n",
            "Loss (epoch:  833): 0.02713092\n",
            "Loss (epoch:  834): 0.02712814\n",
            "Loss (epoch:  835): 0.02712536\n",
            "Loss (epoch:  836): 0.02712258\n",
            "Loss (epoch:  837): 0.02711980\n",
            "Loss (epoch:  838): 0.02711703\n",
            "Loss (epoch:  839): 0.02711427\n",
            "Loss (epoch:  840): 0.02711150\n",
            "Loss (epoch:  841): 0.02710874\n",
            "Loss (epoch:  842): 0.02710599\n",
            "Loss (epoch:  843): 0.02710324\n",
            "Loss (epoch:  844): 0.02710049\n",
            "Loss (epoch:  845): 0.02709775\n",
            "Loss (epoch:  846): 0.02709501\n",
            "Loss (epoch:  847): 0.02709227\n",
            "Loss (epoch:  848): 0.02708954\n",
            "Loss (epoch:  849): 0.02708681\n",
            "Loss (epoch:  850): 0.02708409\n",
            "Loss (epoch:  851): 0.02708137\n",
            "Loss (epoch:  852): 0.02707865\n",
            "Loss (epoch:  853): 0.02707594\n",
            "Loss (epoch:  854): 0.02707323\n",
            "Loss (epoch:  855): 0.02707053\n",
            "Loss (epoch:  856): 0.02706783\n",
            "Loss (epoch:  857): 0.02706513\n",
            "Loss (epoch:  858): 0.02706243\n",
            "Loss (epoch:  859): 0.02705975\n",
            "Loss (epoch:  860): 0.02705706\n",
            "Loss (epoch:  861): 0.02705438\n",
            "Loss (epoch:  862): 0.02705170\n",
            "Loss (epoch:  863): 0.02704903\n",
            "Loss (epoch:  864): 0.02704636\n",
            "Loss (epoch:  865): 0.02704369\n",
            "Loss (epoch:  866): 0.02704103\n",
            "Loss (epoch:  867): 0.02703837\n",
            "Loss (epoch:  868): 0.02703572\n",
            "Loss (epoch:  869): 0.02703307\n",
            "Loss (epoch:  870): 0.02703043\n",
            "Loss (epoch:  871): 0.02702778\n",
            "Loss (epoch:  872): 0.02702514\n",
            "Loss (epoch:  873): 0.02702251\n",
            "Loss (epoch:  874): 0.02701988\n",
            "Loss (epoch:  875): 0.02701725\n",
            "Loss (epoch:  876): 0.02701463\n",
            "Loss (epoch:  877): 0.02701201\n",
            "Loss (epoch:  878): 0.02700939\n",
            "Loss (epoch:  879): 0.02700678\n",
            "Loss (epoch:  880): 0.02700417\n",
            "Loss (epoch:  881): 0.02700157\n",
            "Loss (epoch:  882): 0.02699897\n",
            "Loss (epoch:  883): 0.02699638\n",
            "Loss (epoch:  884): 0.02699378\n",
            "Loss (epoch:  885): 0.02699120\n",
            "Loss (epoch:  886): 0.02698861\n",
            "Loss (epoch:  887): 0.02698603\n",
            "Loss (epoch:  888): 0.02698345\n",
            "Loss (epoch:  889): 0.02698088\n",
            "Loss (epoch:  890): 0.02697831\n",
            "Loss (epoch:  891): 0.02697575\n",
            "Loss (epoch:  892): 0.02697319\n",
            "Loss (epoch:  893): 0.02697063\n",
            "Loss (epoch:  894): 0.02696808\n",
            "Loss (epoch:  895): 0.02696553\n",
            "Loss (epoch:  896): 0.02696299\n",
            "Loss (epoch:  897): 0.02696045\n",
            "Loss (epoch:  898): 0.02695791\n",
            "Loss (epoch:  899): 0.02695538\n",
            "Loss (epoch:  900): 0.02695284\n",
            "Loss (epoch:  901): 0.02695032\n",
            "Loss (epoch:  902): 0.02694780\n",
            "Loss (epoch:  903): 0.02694528\n",
            "Loss (epoch:  904): 0.02694277\n",
            "Loss (epoch:  905): 0.02694026\n",
            "Loss (epoch:  906): 0.02693775\n",
            "Loss (epoch:  907): 0.02693525\n",
            "Loss (epoch:  908): 0.02693275\n",
            "Loss (epoch:  909): 0.02693025\n",
            "Loss (epoch:  910): 0.02692776\n",
            "Loss (epoch:  911): 0.02692527\n",
            "Loss (epoch:  912): 0.02692279\n",
            "Loss (epoch:  913): 0.02692031\n",
            "Loss (epoch:  914): 0.02691784\n",
            "Loss (epoch:  915): 0.02691537\n",
            "Loss (epoch:  916): 0.02691290\n",
            "Loss (epoch:  917): 0.02691043\n",
            "Loss (epoch:  918): 0.02690798\n",
            "Loss (epoch:  919): 0.02690552\n",
            "Loss (epoch:  920): 0.02690307\n",
            "Loss (epoch:  921): 0.02690062\n",
            "Loss (epoch:  922): 0.02689817\n",
            "Loss (epoch:  923): 0.02689573\n",
            "Loss (epoch:  924): 0.02689329\n",
            "Loss (epoch:  925): 0.02689086\n",
            "Loss (epoch:  926): 0.02688843\n",
            "Loss (epoch:  927): 0.02688600\n",
            "Loss (epoch:  928): 0.02688358\n",
            "Loss (epoch:  929): 0.02688117\n",
            "Loss (epoch:  930): 0.02687875\n",
            "Loss (epoch:  931): 0.02687634\n",
            "Loss (epoch:  932): 0.02687394\n",
            "Loss (epoch:  933): 0.02687153\n",
            "Loss (epoch:  934): 0.02686913\n",
            "Loss (epoch:  935): 0.02686674\n",
            "Loss (epoch:  936): 0.02686435\n",
            "Loss (epoch:  937): 0.02686196\n",
            "Loss (epoch:  938): 0.02685957\n",
            "Loss (epoch:  939): 0.02685720\n",
            "Loss (epoch:  940): 0.02685482\n",
            "Loss (epoch:  941): 0.02685245\n",
            "Loss (epoch:  942): 0.02685008\n",
            "Loss (epoch:  943): 0.02684771\n",
            "Loss (epoch:  944): 0.02684535\n",
            "Loss (epoch:  945): 0.02684299\n",
            "Loss (epoch:  946): 0.02684064\n",
            "Loss (epoch:  947): 0.02683829\n",
            "Loss (epoch:  948): 0.02683594\n",
            "Loss (epoch:  949): 0.02683360\n",
            "Loss (epoch:  950): 0.02683126\n",
            "Loss (epoch:  951): 0.02682893\n",
            "Loss (epoch:  952): 0.02682660\n",
            "Loss (epoch:  953): 0.02682427\n",
            "Loss (epoch:  954): 0.02682194\n",
            "Loss (epoch:  955): 0.02681963\n",
            "Loss (epoch:  956): 0.02681731\n",
            "Loss (epoch:  957): 0.02681500\n",
            "Loss (epoch:  958): 0.02681269\n",
            "Loss (epoch:  959): 0.02681038\n",
            "Loss (epoch:  960): 0.02680808\n",
            "Loss (epoch:  961): 0.02680578\n",
            "Loss (epoch:  962): 0.02680349\n",
            "Loss (epoch:  963): 0.02680119\n",
            "Loss (epoch:  964): 0.02679891\n",
            "Loss (epoch:  965): 0.02679663\n",
            "Loss (epoch:  966): 0.02679435\n",
            "Loss (epoch:  967): 0.02679207\n",
            "Loss (epoch:  968): 0.02678980\n",
            "Loss (epoch:  969): 0.02678753\n",
            "Loss (epoch:  970): 0.02678527\n",
            "Loss (epoch:  971): 0.02678301\n",
            "Loss (epoch:  972): 0.02678075\n",
            "Loss (epoch:  973): 0.02677849\n",
            "Loss (epoch:  974): 0.02677624\n",
            "Loss (epoch:  975): 0.02677399\n",
            "Loss (epoch:  976): 0.02677175\n",
            "Loss (epoch:  977): 0.02676952\n",
            "Loss (epoch:  978): 0.02676728\n",
            "Loss (epoch:  979): 0.02676505\n",
            "Loss (epoch:  980): 0.02676282\n",
            "Loss (epoch:  981): 0.02676059\n",
            "Loss (epoch:  982): 0.02675837\n",
            "Loss (epoch:  983): 0.02675615\n",
            "Loss (epoch:  984): 0.02675394\n",
            "Loss (epoch:  985): 0.02675173\n",
            "Loss (epoch:  986): 0.02674952\n",
            "Loss (epoch:  987): 0.02674732\n",
            "Loss (epoch:  988): 0.02674513\n",
            "Loss (epoch:  989): 0.02674293\n",
            "Loss (epoch:  990): 0.02674073\n",
            "Loss (epoch:  991): 0.02673855\n",
            "Loss (epoch:  992): 0.02673636\n",
            "Loss (epoch:  993): 0.02673418\n",
            "Loss (epoch:  994): 0.02673200\n",
            "Loss (epoch:  995): 0.02672982\n",
            "Loss (epoch:  996): 0.02672765\n",
            "Loss (epoch:  997): 0.02672549\n",
            "Loss (epoch:  998): 0.02672332\n",
            "Loss (epoch:  999): 0.02672116\n",
            "Loss (epoch: 1000): 0.02671900\n"
          ]
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAksAAAGwCAYAAAC5ACFFAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABCWklEQVR4nO3deXxU9b3/8fdMJpksZJJAlkkk7EhAQBAkBmmlhcrWWpTaYqOCPx5QFawLFqFWq1LEutUrWqi3rleUq161SBUbAffIahAQIiqSsCQBQjIJIeuc3x9JBoZlCGGSk5m8no/HeWTmnO858zmnD5x3v+d7vmMxDMMQAAAATslqdgEAAABtGWEJAADAB8ISAACAD4QlAAAAHwhLAAAAPhCWAAAAfCAsAQAA+GAzu4Bg4Ha7tW/fPkVHR8tisZhdDgAAaALDMFRWVqaUlBRZrafvPyIs+cG+ffuUmppqdhkAAKAZ8vPz1blz59NuJyz5QXR0tKT6i+1wOEyuBgAANIXL5VJqaqrne/x0CEt+0HjrzeFwEJYAAAgwZxpCwwBvAAAAHwhLAAAAPhCWAAAAfCAsAQAA+EBYAgAA8IGwBAAA4ANhCQAAwAfCEgAAgA+EJQAAAB8ISwAAAD4QlgAAAHwgLAEAAPhAWGrDKqprlVtQpqraOrNLAQCg3SIstWE/fniNxjzxsXYWlptdCgAA7RZhqQ1L7RgpScovrjC5EgAA2i/CUhvWpSEs7SYsAQBgGsJSG9YYlvIISwAAmIaw1IZxGw4AAPMRltqwrvQsAQBgOsJSG9alU31Y2nv4qGrr3CZXAwBA+0RYasOSosMVFmJVrdvQ/tJKs8sBAKBdIiy1YVarRZ07RkjiVhwAAGYhLLVxPBEHAIC5CEttHIO8AQAwF2GpjUslLAEAYCrCUhvnuQ13iLAEAIAZAi4sPf300+rWrZvCw8OVnp6udevW+Wz/+uuvKy0tTeHh4RowYIDeffddr+2GYejee+9VcnKyIiIiNHr0aO3cubMlT+GsNE4fQM8SAADmCKiw9L//+7+644479Oc//1mbNm3ShRdeqDFjxqioqOiU7T///HNdc801mjZtmr788ktNnDhREydO1NatWz1tHn74YT355JNasmSJ1q5dq6ioKI0ZM0aVlW3jUf3UuPqwVHq0RqUVNSZXAwBA+2MxDMMwu4imSk9P18UXX6ynnnpKkuR2u5WamqpbbrlFc+fOPan9b37zGx05ckQrVqzwrLvkkks0aNAgLVmyRIZhKCUlRbNnz9add94pSSotLVVSUpJeeOEFTZ48uUl1uVwuxcTEqLS0VA6Hww9n6m3oXz7QwfIqrbhlhPqfF+P34wMA0B419fs7YHqWqqurtXHjRo0ePdqzzmq1avTo0crOzj7lPtnZ2V7tJWnMmDGe9rt27VJBQYFXm5iYGKWnp5/2mJJUVVUll8vltbSkLsy1BACAaQImLB08eFB1dXVKSkryWp+UlKSCgoJT7lNQUOCzfePfszmmJC1cuFAxMTGeJTU19azP52w0DvLezSBvAABaXcCEpbZk3rx5Ki0t9Sz5+fkt+nlMTAkAgHkCJizFx8crJCREhYWFXusLCwvldDpPuY/T6fTZvvHv2RxTkux2uxwOh9fSkhrnWsonLAEA0OoCJiyFhYVpyJAhWrVqlWed2+3WqlWrlJGRccp9MjIyvNpLUlZWlqd99+7d5XQ6vdq4XC6tXbv2tMc0Q9dOUZLoWQIAwAw2sws4G3fccYemTJmioUOHatiwYXriiSd05MgR3XDDDZKk66+/Xuedd54WLlwoSbr11lt12WWX6bHHHtOECRO0bNkybdiwQc8884wkyWKx6LbbbtNf/vIX9e7dW927d9c999yjlJQUTZw40azTPEnjbbi9JUdVW+eWLSRgMi4AAAEvoMLSb37zGx04cED33nuvCgoKNGjQIK1cudIzQDsvL09W67EgMXz4cL3yyiv605/+pD/+8Y/q3bu33n77bfXv39/TZs6cOTpy5IhmzJihkpISjRgxQitXrlR4eHirn9/pJEbbFWazqrrWrX0llZ6JKgEAQMsLqHmW2qqWnmdJkkY99qG+O3BEL09L14je8S3yGQAAtCdBN89Se8cTcQAAmIOwFCAY5A0AgDkISwGC6QMAADAHYSlAeGbxLj5iciUAALQvhKUA4RmzxE+eAADQqghLASK14cd0XZW1Kq2oMbkaAADaD8JSgIgMsykh2i6JQd4AALQmwlIAYfoAAABaH2EpgDDIGwCA1kdYCiBMHwAAQOsjLAUQbsMBAND6CEsBpGsnwhIAAK2NsBRAGnuW9pVUqqbObXI1AAC0D4SlAJLQwS67zao6t6F9JUfNLgcAgHaBsBRArFaLZ5A3t+IAAGgdhKUAwyBvAABaF2EpwBCWAABoXYSlANOFuZYAAGhVhKUA45nF+xBhCQCA1kBYCjBdGudaOlQhwzBMrgYAgOBHWAowqXH1YamsqlalR2tMrgYAgOBHWAowEWEhSoy2S2KQNwAArYGwFIB4Ig4AgNZDWApADPIGAKD1EJYCUCrTBwAA0GoISwGI23AAALQewlIA6tqJsAQAQGshLAWgxp6lfSVHVVPnNrkaAACCG2EpACVE22W3WeU2pL2Hj5pdDgAAQY2wFIAsFgvjlgAAaCWEpQBFWAIAoHUQlgJU42/EMX0AAAAti7AUoOhZAgCgdRCWAhSzeAMA0DoISwGqy3GzeBuGYXI1AAAEL8JSgGr8yZOyqlqVVNSYXA0AAMGLsBSgwkNDlOSwS2LcEgAALYmwFMAY5A0AQMsjLAWwVMISAAAtjrAUwDw9SzwRBwBAiwmYsFRcXKzMzEw5HA7FxsZq2rRpKi8v99n+lltuUZ8+fRQREaEuXbro97//vUpLS73aWSyWk5Zly5a19On4RddO9CwBANDSbGYX0FSZmZnav3+/srKyVFNToxtuuEEzZszQK6+8csr2+/bt0759+/Too4+qX79+2r17t2688Ubt27dPb7zxhlfb559/XmPHjvW8j42NbclT8RvGLAEA0PICIixt375dK1eu1Pr16zV06FBJ0qJFizR+/Hg9+uijSklJOWmf/v376//+7/8873v27KkFCxbo2muvVW1trWy2Y6ceGxsrp9PZ8ifiZ41jlvaXHlV1rVthtoDpKAQAIGAExLdrdna2YmNjPUFJkkaPHi2r1aq1a9c2+TilpaVyOBxeQUmSZs6cqfj4eA0bNkzPPffcGSd5rKqqksvl8lrMkNDBrvBQq9yGtLfkqCk1AAAQ7AIiLBUUFCgxMdFrnc1mU8eOHVVQUNCkYxw8eFDz58/XjBkzvNY/8MADeu2115SVlaVJkybp5ptv1qJFi3wea+HChYqJifEsqampZ3dCfmKxWLgVBwBACzM1LM2dO/eUA6yPX3bs2HHOn+NyuTRhwgT169dP9913n9e2e+65R5deeqkGDx6su+66S3PmzNEjjzzi83jz5s1TaWmpZ8nPzz/nGpurS8coSYQlAABaiqljlmbPnq2pU6f6bNOjRw85nU4VFRV5ra+trVVxcfEZxxqVlZVp7Nixio6O1ltvvaXQ0FCf7dPT0zV//nxVVVXJbrefso3dbj/tttZ2/G/EAQAA/zM1LCUkJCghIeGM7TIyMlRSUqKNGzdqyJAhkqTVq1fL7XYrPT39tPu5XC6NGTNGdrtdy5cvV3h4+Bk/KycnR3FxcW0mDJ1Jl44RkphrCQCAlhIQT8P17dtXY8eO1fTp07VkyRLV1NRo1qxZmjx5sudJuL1792rUqFF66aWXNGzYMLlcLl1++eWqqKjQyy+/7DUQOyEhQSEhIXrnnXdUWFioSy65ROHh4crKytKDDz6oO++808zTPStdGuZa2k3PEgAALSIgwpIkLV26VLNmzdKoUaNktVo1adIkPfnkk57tNTU1ys3NVUVFfWjYtGmT50m5Xr16eR1r165d6tatm0JDQ/X000/r9ttvl2EY6tWrlx5//HFNnz699U7sHB1/G84wDFksFpMrAgAguFiMMz0njzNyuVyKiYnxTE3Qmipr6pR2z0pJ0qZ7fqaOUWGt+vkAAASqpn5/B8TUATi98NAQOR31Y7F4Ig4AAP8jLAUB5loCAKDlEJaCQOPPnuQdOmJyJQAABB/CUhCgZwkAgJZDWAoCXTsRlgAAaCmEpSCQ6pk+gB/TBQDA3whLQaDxNty+0qOqrnWbXA0AAMGFsBQE4juEKSI0RIYh7TnMrTgAAPyJsBQELBYLg7wBAGghhKUg0fgbcfmEJQAA/IqwFCToWQIAoGUQloIEYQkAgJZBWAoSjWFp9yHCEgAA/kRYChLH5lqqkGEYJlcDAEDwICwFic5xEbJYpCPVdSo+Um12OQAABA3CUpAIDw2R0xEuiXFLAAD4E2EpiKQyyBsAAL8jLAURzxNxDPIGAMBvCEtBhOkDAADwP8JSEOnaibAEAIC/EZaCSCpzLQEA4HeEpSDSIz5KklTgqlRFda3J1QAAEBwIS0EkNjJMcZGhkqRdB4+YXA0AAMGBsBRkeiR0kCR9f4CwBACAPxCWgkzjrTjCEgAA/kFYCjLdExrC0sFykysBACA4EJaCTI/4+ttwjFkCAMA/CEtBpmfCsdtwhmGYXA0AAIGPsBRkunSKlNUilVfV6kBZldnlAAAQ8AhLQcZuC1HnuPrJKb/nVhwAAOeMsBSEeiTwRBwAAP5CWApCjYO8vz/AE3EAAJwrwlIQOjZ9AD1LAACcK8JSEOrZMDEl0wcAAHDuCEtBqPEnT/KKK1Rd6za5GgAAAhthKQglOeyKDAtRndtQXnGF2eUAABDQCEtByGKxqDu34gAA8AvCUpBqvBXHE3EAAJwbwlKQ6tHQs/QdYQkAgHNCWApSvZPqe5Z2FhGWAAA4FwETloqLi5WZmSmHw6HY2FhNmzZN5eW+g8DIkSNlsVi8lhtvvNGrTV5eniZMmKDIyEglJibqD3/4g2pra1vyVFpF78RoSdK3heX8oC4AAOfAZnYBTZWZman9+/crKytLNTU1uuGGGzRjxgy98sorPvebPn26HnjgAc/7yMhIz+u6ujpNmDBBTqdTn3/+ufbv36/rr79eoaGhevDBB1vsXFpDt/hIhVgtKquqVYGrUskxEWaXBABAQAqInqXt27dr5cqV+uc//6n09HSNGDFCixYt0rJly7Rv3z6f+0ZGRsrpdHoWh8Ph2faf//xHX3/9tV5++WUNGjRI48aN0/z58/X000+rurq6pU+rRdltIerWqT4Y7izkVhwAAM0VEGEpOztbsbGxGjp0qGfd6NGjZbVatXbtWp/7Ll26VPHx8erfv7/mzZuniopj8w5lZ2drwIABSkpK8qwbM2aMXC6Xtm3bdtpjVlVVyeVyeS1tUeOtOMYtAQDQfAFxG66goECJiYle62w2mzp27KiCgoLT7vfb3/5WXbt2VUpKir766ivdddddys3N1Ztvvuk57vFBSZLnva/jLly4UPfff39zT6fV9E7qoJXbpJ2FZWaXAgBAwDI1LM2dO1d//etffbbZvn17s48/Y8YMz+sBAwYoOTlZo0aN0nfffaeePXs2+7jz5s3THXfc4XnvcrmUmpra7OO1lN5J9CwBAHCuTA1Ls2fP1tSpU3226dGjh5xOp4qKirzW19bWqri4WE6ns8mfl56eLkn69ttv1bNnTzmdTq1bt86rTWFhoST5PK7dbpfdbm/y55qld2LD9AGFZTIMQxaLxeSKAAAIPKaGpYSEBCUkJJyxXUZGhkpKSrRx40YNGTJEkrR69Wq53W5PAGqKnJwcSVJycrLnuAsWLFBRUZHnNl9WVpYcDof69et3lmfT9nSPj5LVIrkqa1VUVqUkR7jZJQEAEHACYoB33759NXbsWE2fPl3r1q3TZ599plmzZmny5MlKSUmRJO3du1dpaWmenqLvvvtO8+fP18aNG/XDDz9o+fLluv766/XjH/9YAwcOlCRdfvnl6tevn6677jpt3rxZ77//vv70pz9p5syZAdFzdCbhoSHq1ql+Jm+eiAMAoHkCIixJ9U+1paWladSoURo/frxGjBihZ555xrO9pqZGubm5nqfdwsLC9MEHH+jyyy9XWlqaZs+erUmTJumdd97x7BMSEqIVK1YoJCREGRkZuvbaa3X99dd7zcsU6Ho13oorYpA3AADNYTGY3vmcuVwuxcTEqLS01Gsep7bgkfd36Ok13+maYV208KoBZpcDAECb0dTv74DpWULznN/4RBzTBwAA0CyEpSDXODFlbsMTcQAA4OwQloJcz8Qo2awWlVXWal9ppdnlAAAQcAhLQc5uC/EM8t6+r23+LAsAAG0ZYakd6JtcP2ht+37CEgAAZ4uw1A6kOevHLe0oYJA3AABni7DUDtCzBABA8xGW2oG05PqepV2HjuhodZ3J1QAAEFgIS+1AYnS44juEyTDqpxAAAABNR1hqJ9Kc9bfidnArDgCAs0JYaif6NtyKY9wSAABnh7DUTjT2LG3niTgAAM4KYamdOP6JOH72BACApiMstRO9EjsoNKT+Z0/2HD5qdjkAAAQMwlI7EWaz6vyk+nFLW/eWmlwNAACBg7DUjgw4L0aStIWwBABAkxGW2pEBnQlLAACcLcJSO3J8zxKDvAEAaBrCUjvSxxmt0BCLSipqGOQNAEATEZbaEbstRH2cDPIGAOBsEJbamcZbcV8RlgAAaBLCUjvTvyEs0bMEAEDTEJbamYHnxUpikDcAAE1FWGpnznd2YJA3AABngbDUzhw/yPurPdyKAwDgTAhL7dCFnWMlSTn5h80tBACAAEBYaocu6hInSfoyr8TcQgAACACEpXZocJdYSfXTB1TXus0tBgCANo6w1A51j49SbGSoqmvd2r7fZXY5AAC0aYSldshisWhwaqwkaVMe45YAAPCFsNROMW4JAICmISy1U4MbwxJPxAEA4FOzwlJ+fr727Nnjeb9u3TrddttteuaZZ/xWGFrWhakxslik/OKjOlBWZXY5AAC0Wc0KS7/97W+1Zs0aSVJBQYF+9rOfad26dbr77rv1wAMP+LVAtIzo8FCdn1g/OeWXjFsCAOC0mhWWtm7dqmHDhkmSXnvtNfXv31+ff/65li5dqhdeeMGf9aEFNU4hsIlxSwAAnFazwlJNTY3sdrsk6YMPPtAVV1whSUpLS9P+/fv9Vx1a1JCu9eOWNvxQbHIlAAC0Xc0KSxdccIGWLFmiTz75RFlZWRo7dqwkad++ferUqZNfC0TLGda9oyRp854SVdbUmVwNAABtU7PC0l//+lf94x//0MiRI3XNNdfowgsvlCQtX77cc3sObV+XjpFKcthVU2cwhQAAAKdha85OI0eO1MGDB+VyuRQXF+dZP2PGDEVGRvqtOLQsi8WiYd076Z3N+7RuV7EyetIrCADAiZrVs3T06FFVVVV5gtLu3bv1xBNPKDc3V4mJiX4tEC2r8VbcesYtAQBwSs0KS7/85S/10ksvSZJKSkqUnp6uxx57TBMnTtTixYv9WmCj4uJiZWZmyuFwKDY2VtOmTVN5eflp2//www+yWCynXF5//XVPu1NtX7ZsWYucQ1uU3hCWNu4+rJo6flQXAIATNSssbdq0ST/60Y8kSW+88YaSkpK0e/duvfTSS3ryySf9WmCjzMxMbdu2TVlZWVqxYoU+/vhjzZgx47TtU1NTtX//fq/l/vvvV4cOHTRu3Divts8//7xXu4kTJ7bIObRFvRI6KC4yVEdr6rR1b6nZ5QAA0OY0a8xSRUWFoqPrJzT8z3/+o6uuukpWq1WXXHKJdu/e7dcCJWn79u1auXKl1q9fr6FDh0qSFi1apPHjx+vRRx9VSkrKSfuEhITI6XR6rXvrrbf061//Wh06dPBaHxsbe1Lb9sJqtWhot47K+rpQ63YVe34GBQAA1GtWz1KvXr309ttvKz8/X++//74uv/xySVJRUZEcDodfC5Sk7OxsxcbGeoKSJI0ePVpWq1Vr165t0jE2btyonJwcTZs27aRtM2fOVHx8vIYNG6bnnntOhmH4PFZVVZVcLpfXEsgab8Wt28W4JQAATtSssHTvvffqzjvvVLdu3TRs2DBlZGRIqu9lGjx4sF8LlOp/UuXEgeM2m00dO3ZUQUFBk47x7LPPqm/fvho+fLjX+gceeECvvfaasrKyNGnSJN18881atGiRz2MtXLhQMTExniU1NfXsTqiNaRzkve6HYtW5fQdFAADam2aFpV/96lfKy8vThg0b9P7773vWjxo1Sn/729+afJy5c+eedhB247Jjx47mlOjl6NGjeuWVV07Zq3TPPffo0ksv1eDBg3XXXXdpzpw5euSRR3web968eSotLfUs+fn551yjmfolOxQVFqKyylrlFpSZXQ4AAG1Ks8YsSZLT6ZTT6dSePXskSZ07dz7rCSlnz56tqVOn+mzTo0cPOZ1OFRUVea2vra1VcXFxk8YavfHGG6qoqND1119/xrbp6emaP3++qqqqPD/pciK73X7abYHIFmLV0G4d9dE3B5T9/SH1S/H/rVQAAAJVs3qW3G63HnjgAcXExKhr167q2rWrYmNjNX/+fLndTX/8PCEhQWlpaT6XsLAwZWRkqKSkRBs3bvTsu3r1arndbqWnp5/xc5599lldccUVSkhIOGPbnJwcxcXFBVUYaorhDRNSfv7tQZMrAQCgbWlWz9Ldd9+tZ599Vg899JAuvfRSSdKnn36q++67T5WVlVqwYIFfi+zbt6/Gjh2r6dOna8mSJaqpqdGsWbM0efJkz5Nwe/fu1ahRo/TSSy959XB9++23+vjjj/Xuu++edNx33nlHhYWFuuSSSxQeHq6srCw9+OCDuvPOO/1afyC4tFe8JOmL7w+pps6t0JBm5WgAAIJOs8LSiy++qH/+85+64oorPOsGDhyo8847TzfffLPfw5IkLV26VLNmzdKoUaNktVo1adIkrzmdampqlJubq4qKCq/9nnvuOXXu3NnzxN7xQkND9fTTT+v222+XYRjq1auXHn/8cU2fPt3v9bd1/ZIdiosM1eGKGn21p0RDunY0uyQAANoEi3Gm5+RPITw8XF999ZXOP/98r/W5ubkaNGiQjh496rcCA4HL5VJMTIxKS0tbZOqE1jJz6Sb9e8t+3T76fN06urfZ5QAA0KKa+v3drHstF154oZ566qmT1j/11FMaOHBgcw6JNmB4r/pxS599x7glAAAaNes23MMPP6wJEybogw8+8MyxlJ2drfz8/FOODUJgGNEwbunLvMOqqK5VZFizH5YEACBoNKtn6bLLLtM333yjK6+8UiUlJSopKdFVV12lbdu26X/+53/8XSNaSZeOkTovNkI1dQazeQMA0KBZY5ZOZ/PmzbroootUV1fnr0MGhGAZsyRJc97YrNc27NH0H3XX3RP6mV0OAAAtpkXHLCF4NU4h8Nm3h0yuBACAtoGwBC/De9aHpa/3u3SovMrkagAAMB9hCV4Sou1Kc0ZLkj7/jt4lAADO6nGnq666yuf2kpKSc6kFbcSlveK1o6BMn+w8oF9cmGJ2OQAAmOqswlJMTMwZtzflx2rRtl12foKe/XSXPvrmgAzDkMViMbskAABMc1Zh6fnnn2+pOtCGDOveUeGhVhW6qpRbWKY0Z2A/4QcAwLlgzBJOEh4aoowe9bN5f5h7wORqAAAwF2EJp3TZ+QmSpI8ISwCAdo6whFO6rE+iJGnD7mKVV9WaXA0AAOYhLOGUusdHqWunSNXUGfr8W35YFwDQfhGWcFqeW3HfcCsOANB+EZZwWseHJT/+hCAAAAGFsITTyujZSWEhVu05fFTfHThidjkAAJiCsITTigyzaVj3jpK4FQcAaL8IS/BpZB/GLQEA2jfCEnxqHLf0xfeHdLS6zuRqAABofYQl+NQrsYPOi41Qda1bn3/HFAIAgPaHsASfLBaLRvWtn6By1Y4ik6sBAKD1EZZwRj9Nqw9Lq7cXMYUAAKDdISzhjC7p0UmRYSEqcFVq2z6X2eUAANCqCEs4o/DQEI3oFS9JWs2tOABAO0NYQpMwbgkA0F4RltAkP+lTH5Y255eoqKzS5GoAAGg9hCU0SaIjXAM7x0iSPtzBBJUAgPaDsIQmG5WWJElataPQ5EoAAGg9hCU0WeO4pU92HlRlDbN5AwDaB8ISmuyCFIeSHHZVVNdp7a5is8sBAKBVEJbQZBaL5bgJKrkVBwBoHwhLOCvHxi0xmzcAoH0gLOGsXNorXnabVXsOH9U3heVmlwMAQIsjLOGsRISFaHjPTpKkD7gVBwBoBwhLOGuj+tbfiiMsAQDaA8ISztrP+tWHpS/zSlTkYjZvAEBwIyzhrCU5wjUoNVaS9MF2fisOABDcCEtolsbepf98XWByJQAAtCzCEpplzAX1Yenzbw+prLLG5GoAAGg5AROWFixYoOHDhysyMlKxsbFN2scwDN17771KTk5WRESERo8erZ07d3q1KS4uVmZmphwOh2JjYzVt2jSVl/NI/Jn0TOigHvFRqq5z66Nv+GFdAEDwCpiwVF1drauvvlo33XRTk/d5+OGH9eSTT2rJkiVau3atoqKiNGbMGFVWHhuUnJmZqW3btikrK0srVqzQxx9/rBkzZrTEKQQVi8WinzX0Lv1nG0/FAQCCl8UIsGmYX3jhBd12220qKSnx2c4wDKWkpGj27Nm68847JUmlpaVKSkrSCy+8oMmTJ2v79u3q16+f1q9fr6FDh0qSVq5cqfHjx2vPnj1KSUk55bGrqqpUVVXlee9yuZSamqrS0lI5HA7/nGgA2Lj7sCYt/lzR4TZt/NPPFGYLmOwNAIBcLpdiYmLO+P0dtN9uu3btUkFBgUaPHu1ZFxMTo/T0dGVnZ0uSsrOzFRsb6wlKkjR69GhZrVatXbv2tMdeuHChYmJiPEtqamrLnUgbNjg1VvEd7CqrrNXaXYfMLgcAgBYRtGGpoKD+Ka2kpCSv9UlJSZ5tBQUFSkxM9Npus9nUsWNHT5tTmTdvnkpLSz1Lfn6+n6sPDFarRT/rV3/9uBUHAAhWpoaluXPnymKx+Fx27NhhZomnZLfb5XA4vJb26vJ+TklS1teFcrsD6o4uAABNYjPzw2fPnq2pU6f6bNOjR49mHdvprP8SLywsVHJysmd9YWGhBg0a5GlTVOQ9qWJtba2Ki4s9+8O3jJ6dFBUWogJXpbbsLdWFDZNVAgAQLEwNSwkJCUpISGiRY3fv3l1Op1OrVq3yhCOXy6W1a9d6nqjLyMhQSUmJNm7cqCFDhkiSVq9eLbfbrfT09BapK9iEh4ZoZJ9E/XvLfmV9XUhYAgAEnYAZs5SXl6ecnBzl5eWprq5OOTk5ysnJ8ZoTKS0tTW+99Zak+kfbb7vtNv3lL3/R8uXLtWXLFl1//fVKSUnRxIkTJUl9+/bV2LFjNX36dK1bt06fffaZZs2apcmTJ5/2STicjNm8AQDBzNSepbNx77336sUXX/S8Hzx4sCRpzZo1GjlypCQpNzdXpaWlnjZz5szRkSNHNGPGDJWUlGjEiBFauXKlwsPDPW2WLl2qWbNmadSoUbJarZo0aZKefPLJ1jmpIPGTPomyWS36prBcuw4eUff4KLNLAgDAbwJunqW2qKnzNASza/+5Vp9+e1B/HJ+mGT/uaXY5AACcUbufZwmt63Jm8wYABCnCEvyicdzSxrzDKiqrPENrAAACB2EJfpEcE6ELU2NlGPQuAQCCC2EJfjOuf/3cVO9t3W9yJQAA+A9hCX7TGJa++L5YxUeqTa4GAAD/ICzBb7p2ilK/ZIfq3IaymHMJABAkCEvwq/EDGm/FEZYAAMGBsAS/Gtu//nf4Pvv2oEqP1phcDQAA546wBL/qldhB5yd1UE2doVXbeSoOABD4CEvwu8bepXe3cCsOABD4CEvwu8ZxSx/vPKDyqlqTqwEA4NwQluB3fZKi1T0+StW1bq3eUWR2OQAAnBPCEvzOYrF45lxayQSVAIAAR1hCixjXMG5pzY4DOlpdZ3I1AAA0H2EJLaL/eQ51jovQ0Zo6ffQNt+IAAIGLsIQWcfytOJ6KAwAEMsISWsy4AfW34lZtL1RlDbfiAACBibCEFjOoc6zOi43Qkeo6fZjLrTgAQGAiLKHFWK0W/Xxgfe/SO5t5Kg4AEJgIS2hRv7gwRZK0akchE1QCAAISYQkt6oIUh7rHR6myxs1vxQEAAhJhCS3KYrHoFw234pbn7DO5GgAAzh5hCS2u8VbcxzsPqKSi2uRqAAA4O4QltLjeSdFKc0arps7Q+9uYcwkAEFgIS2gVjb1LPBUHAAg0hCW0isYpBD7/7qAOlFWZXA0AAE1HWEKr6NopShemxsptSO9tpXcJABA4CEtoNb/wTFDJU3EAgMBBWEKr+fnAFFks0vofDmtvyVGzywEAoEkIS2g1zphwXdytoyRpBb1LAIAAQVhCq/rloPqn4t76cq/JlQAA0DSEJbSqnw9IUViIVTsKyvT1PpfZ5QAAcEaEJbSqmMhQjeqbKEl6c9Mek6sBAODMCEtodVdd1FmS9HbOPtXWuU2uBgAA3whLaHWXnZ+gjlFhOlhepU+/PWh2OQAA+ERYQqsLs1k9cy69uYmB3gCAto2wBFM03op7f1uByiprTK4GAIDTIyzBFAM7x6hnQpSqat16b2uB2eUAAHBahCWYwmKxeHqXeCoOANCWEZZgmomDz5MkffF9sfKLK0yuBgCAUwuYsLRgwQINHz5ckZGRio2NPWP7mpoa3XXXXRowYICioqKUkpKi66+/Xvv2ef/MRrdu3WSxWLyWhx56qIXOAsc7LzZCI3rFS5Je35BvcjUAAJxawISl6upqXX311brpppua1L6iokKbNm3SPffco02bNunNN99Ubm6urrjiipPaPvDAA9q/f79nueWWW/xdPk7jNxenSpJe27CHOZcAAG2SzewCmur++++XJL3wwgtNah8TE6OsrCyvdU899ZSGDRumvLw8denSxbM+OjpaTqezybVUVVWpqqrK897l4mc7muvyC5IUFxmqAlelPt55QD9NSzK7JAAAvARMz5I/lJaWymKxnHQb76GHHlKnTp00ePBgPfLII6qtrfV5nIULFyomJsazpKamtmDVwc1uC/EM9H51HbfiAABtT7sJS5WVlbrrrrt0zTXXyOFweNb//ve/17Jly7RmzRr97ne/04MPPqg5c+b4PNa8efNUWlrqWfLz+ZI/F5MbbsWt3lGkIlelydUAAODN1LA0d+7ckwZXn7js2LHjnD+npqZGv/71r2UYhhYvXuy17Y477tDIkSM1cOBA3XjjjXrssce0aNEir9tsJ7Lb7XI4HF4Lmq93UrSGdI1TndvQ6xuZRgAA0LaYOmZp9uzZmjp1qs82PXr0OKfPaAxKu3fv1urVq88YbNLT01VbW6sffvhBffr0OafPRtP95uJUbdx9WK9tyNdNl/WU1WoxuyQAACSZHJYSEhKUkJDQYsdvDEo7d+7UmjVr1KlTpzPuk5OTI6vVqsTExBarCyf7+cBkPfDO19p9qEJf7Dqk4T3jzS4JAABJATRmKS8vTzk5OcrLy1NdXZ1ycnKUk5Oj8vJyT5u0tDS99dZbkuqD0q9+9Stt2LBBS5cuVV1dnQoKClRQUKDq6mpJUnZ2tp544glt3rxZ33//vZYuXarbb79d1157reLi4kw5z/YqMsymKwalSGKgNwCgbQmYqQPuvfdevfjii573gwcPliStWbNGI0eOlCTl5uaqtLRUkrR3714tX75ckjRo0CCvYzXuY7fbtWzZMt13332qqqpS9+7ddfvtt+uOO+5o+RPCSX47rIteWZun97bsV9GEvkp0hJtdEgAAshiGYZhdRKBzuVyKiYlRaWkpg73P0aTFn2vj7sO6dVRv3f6z880uBwAQxJr6/R0wt+HQPkwZ3k2S9Mq6PFXXMqM3AMB8hCW0KWMvcCox2q4DZVV6b+t+s8sBAICwhLYlzGbVb9Prf4rmxc9/MLcYAABEWEIb9Nv0LgoNsWhTXom27Ck1uxwAQDtHWEKbkxgdrvEDkiVJL9C7BAAwGWEJbVLjQO93Nu/j9+IAAKYiLKFNuqhLnIZ2jVN1nVvP07sEADARYQlt1u8u6ylJevmL3SqrrDG5GgBAe0VYQps1Ki1RPROiVFZZq2X8BAoAwCSEJbRZVqtFv/txfe/Ss5/uYpJKAIApCEto0345OEWJ0XYVuCr1r5y9ZpcDAGiHCEto0+y2EP2/Ed0lSX//8DvV1tG7BABoXYQltHnXXtJVcZGh2nXwiJZv3md2OQCAdoawhDavg92m6T/uIUlatPpbepcAAK2KsISAMCWjm6d36V859C4BAFoPYQkBIcpu04yGJ+MWrd5J7xIAoNUQlhAwrs/oqo5RYfrhUIXe2LjH7HIAAO0EYQkBI8pu080j63uXHs/6RhXVtSZXBABoDwhLCCjXZXRVascIFZVV6Z+f7DK7HABAO0BYQkCx20I0Z0yaJGnJR9+pqKzS5IoAAMGOsISA8/OBybowNVYV1XV64oOdZpcDAAhyhCUEHIvForvH95UkLVuXp617S02uCAAQzAhLCEjDunfUhIHJchvSn97eKrfbMLskAECQIiwhYN37837qYLcpJ79Er67PM7scAECQIiwhYCU5wnXHz86XJP31vR06WF5lckUAgGBEWEJAuz6jq/olO+SqrNX8FV+bXQ4AIAgRlhDQbCFWPXjVAFkt0r9y9undLfvNLgkAEGQISwh4g1JjdfPIXpKku9/aoiIXcy8BAPyHsISg8PtRvdUv2aHDFTWa++YWGQZPxwEA/IOwhKAQZrPqb78ZpLAQq1bvKNL/fLHb7JIAAEGCsISg0ccZrTlj+0iS5q/4Wjn5JeYWBAAICoQlBJVpI7pr7AVO1dQZuvnljSo+Um12SQCAAEdYQlCxWCx65OqB6h4fpX2llbp56UZV17rNLgsAEMAISwg60eGhWnLtEHWw2/TF98Wa++ZXDPgGADQbYQlBqY8zWk9nXqQQq0VvbtqrJz7YaXZJAIAARVhC0Lrs/ATN/2V/SdJ/rdqp//74e5MrAgAEIsISgtpv07todsPvxy14d7ue/2yXyRUBAAINYQlB75ZRvXXLT+tn+L7/na/19JpvGcMEAGgywhLahTt+dr5m/aQ+MD3yfq7uW75NdW4CEwDgzAImLC1YsEDDhw9XZGSkYmNjm7TP1KlTZbFYvJaxY8d6tSkuLlZmZqYcDodiY2M1bdo0lZeXt8AZwEwWi0V3jumjP/+inywW6cXs3Zr24nqVVtSYXRoAoI0LmLBUXV2tq6++WjfddNNZ7Td27Fjt37/fs7z66qte2zMzM7Vt2zZlZWVpxYoV+vjjjzVjxgx/lo425IZLu2vRNYMVHmrVh7kH9IunPtXWvaVmlwUAaMNsZhfQVPfff78k6YUXXjir/ex2u5xO5ym3bd++XStXrtT69es1dOhQSdKiRYs0fvx4Pfroo0pJSTmnmtE2/XxgirrHR+l3/7NRecUVuvLvn+n3P+2tm0b2lC0kYP7/AwCglQT9N8OHH36oxMRE9enTRzfddJMOHTrk2Zadna3Y2FhPUJKk0aNHy2q1au3atac9ZlVVlVwul9eCwHJBSoxW3DJCYy5IUk2doceyvtFViz/Xlj30MgEAvAV1WBo7dqxeeuklrVq1Sn/961/10Ucfady4caqrq5MkFRQUKDEx0Wsfm82mjh07qqCg4LTHXbhwoWJiYjxLampqi54HWkZsZJiWXDtET/xmkBzhNn21p1S/eOpTzX5tswpdlWaXBwBoI0wNS3Pnzj1pAPaJy44dO5p9/MmTJ+uKK67QgAEDNHHiRK1YsULr16/Xhx9+eE51z5s3T6WlpZ4lPz//nI4H81gsFk0cfJ6y7rhMVw4+T5L0f5v26McPr9G9/9qq/OIKkysEAJjN1DFLs2fP1tSpU3226dGjh98+r0ePHoqPj9e3336rUaNGyel0qqioyKtNbW2tiouLTzvOSaofB2W32/1WF8yX5AjX334zSFOGd9P8FV9r4+7Deil7t5auzdPY/k5dc3EXDe/ZSVarxexSAQCtzNSwlJCQoISEhFb7vD179ujQoUNKTk6WJGVkZKikpEQbN27UkCFDJEmrV6+W2+1Wenp6q9WFtmNQaqzeuDFD2d8d0t8//E6ffntQ//5qv/791X51jovQhIHJGnOBU4M6xxKcAKCdsBgBMpVxXl6eiouLtXz5cj3yyCP65JNPJEm9evVShw4dJElpaWlauHChrrzySpWXl+v+++/XpEmT5HQ69d1332nOnDkqKyvTli1bPD1D48aNU2FhoZYsWaKamhrdcMMNGjp0qF555ZUm1+ZyuRQTE6PS0lI5HA7/nzxMs3Vvqf53fb7eztmrsspaz/rEaLtG9IpXeo+OGta9k7p1ipTFQngCgEDS1O/vgAlLU6dO1YsvvnjS+jVr1mjkyJGS6sefPP/885o6daqOHj2qiRMn6ssvv1RJSYlSUlJ0+eWXa/78+UpKSvLsX1xcrFmzZumdd96R1WrVpEmT9OSTT3oCWFMQloLf0eo6rdpRqP9sK9SaHUUqq6r12h7fIUx9kx3ql+xQWnK0+iQ51KVTpDrYA2Z2DgBod4IuLLVlhKX2paq2Tut3HdYX3x/Sul3FyskvUXWd+5Rt4yJDldoxUp3jIuR0RKhThzDFdwhTpyi7OnUIU8eoMHWw29Qh3Ca7LaSVzwQA2jfCUisiLLVvlTV1+nq/Szv2l2lHgUvb97u0s6hcJWf5UyqhIRZF2W314cluU5TdpvBQq8JCrLLbQhRms8pus8oealVYSEjDX6vnr81qke2kvxbZrBaFWK2e17YTXodYLQoNsTT8rX9fv9163P4WhVqtjNMCEFSa+v3NPQLgHIWHhuiiLnG6qEuc1/qyyhrlFx/VnsMVyj98VAfKqnSovEqHjlTrUHmVDpZX63BFtSqq6+f9qqkzVFJRc9YhqzVZLFLo8WGrIZSFhtSvawxVp9reGMpsVqtCQiwKbdjeGNRs1sbXVk+744/jWdcY5hoD4Ymf71lfv87zmce1a9wntOG4jDcD4AthCWgh0eGh6pcSqn4pvnsb69yGjlTXqryyVkeqalVWVf/3SFWtqmrdnqW61q2q2jpV1bhVXedu+Hvsfa3bUG2dW3Vuo+G1oVq3u+Hvsdee7cdva9i/7rj93KfoczYMqbrOrYZ8FzSOhafjQpSt4bX1uNcntvF67f0+LKQx5NWvD7NZPYEwzGb1hLpQW33PYGOQCzvN59hCGtvVh0ICHtB6CEuAyUKsFjnCQ+UIDzW7FC/u40PVieGrzlCNuz6Y1dSdHLpq6hq31e/jeV3nVo3bUJ2n3bF9TgxvNccd78Tg1/iZde76Os70+Sfuf6L6z6/T0bbbqeelsYcvNKQ+bNWHuuNee4Wtk8NdWEhjr9uZAuDx4bHhM45/fVzoCzvF6+P3D+EWLgIYYQnAKVmtFoVZLQoLsl9FMgzDE6Rq3G7V1LrrX9e5G5aTX1c3hLNTtamubQhotQ3vG0Nhw+v6YFe/T3XD+trjXp/0vrY+2FUfV9eJAc+rhy9AuvmsFnn1jp342rs37uRQ1+SAd6pQR8DDOSIsAWhXLJbGMU9ShALjCUTDMI4FpxOClne4OzHI1QevxhBWfZrgdmIgPDEcnhgAa0/52Q2B0V3/uu6EgOc2pOqG28mBoqkBL/Q0oe5MAe/4cXfHxu8dG5/nGefX0HMYcsI4P8+t3OMezAj1jAk8Nj6PW7bnjrAEAG2cxWJRmK2+9yNQuBtukZ4Yohp74M7Um3embSf2xh1//Fp3Q6/eCa8be+mqj3tdU3tszF8wBLxTCTnuQQivBy2OC2PHnp49OYx5AtwZQtvxxwk94fjHf67Nc4yGY5/w3rPd6v2UbkK0XaEh5vwbICwBAPzOarXIbg2R3SYpQH5Ks3EMXlN63KobwtupetxO7H2rbghtXq9rj437azxm/Ri8hrF3dcfGC9b31LmPjRWsM045pq+m7tQzAdU1BMGqAA99q2Zfpp4JTZ8w2p8ISwAAqL4HJsQaovDQwLg9eyqehy7cRkOoOvEBh2MB66QHIRqDV+ODGO7GQHjc6xPa1bq9H7o4/kELz8MajWGvoZ3bLa/j1LmPPQjieSr3xKd03YZsJo4hIywBABAkGgMf/CtwboADAACYgLAEAADgA2EJAADAB8ISAACAD4QlAAAAHwhLAAAAPhCWAAAAfCAsAQAA+EBYAgAA8IGwBAAA4ANhCQAAwAfCEgAAgA+EJQAAAB8ISwAAAD7YzC4gGBiGIUlyuVwmVwIAAJqq8Xu78Xv8dAhLflBWViZJSk1NNbkSAABwtsrKyhQTE3Pa7RbjTHEKZ+R2u7Vv3z5FR0fLYrH47bgul0upqanKz8+Xw+Hw23HhjevcerjWrYPr3Dq4zq2npa61YRgqKytTSkqKrNbTj0yiZ8kPrFarOnfu3GLHdzgc/ENsBVzn1sO1bh1c59bBdW49LXGtffUoNWKANwAAgA+EJQAAAB8IS22Y3W7Xn//8Z9ntdrNLCWpc59bDtW4dXOfWwXVuPWZfawZ4AwAA+EDPEgAAgA+EJQAAAB8ISwAAAD4QlgAAAHwgLLVhTz/9tLp166bw8HClp6dr3bp1ZpcUMBYuXKiLL75Y0dHRSkxM1MSJE5Wbm+vVprKyUjNnzlSnTp3UoUMHTZo0SYWFhV5t8vLyNGHCBEVGRioxMVF/+MMfVFtb25qnElAeeughWSwW3XbbbZ51XGf/2bt3r6699lp16tRJERERGjBggDZs2ODZbhiG7r33XiUnJysiIkKjR4/Wzp07vY5RXFyszMxMORwOxcbGatq0aSovL2/tU2mz6urqdM8996h79+6KiIhQz549NX/+fK/fDuM6N8/HH3+sX/ziF0pJSZHFYtHbb7/ttd1f1/Wrr77Sj370I4WHhys1NVUPP/zwuRdvoE1atmyZERYWZjz33HPGtm3bjOnTpxuxsbFGYWGh2aUFhDFjxhjPP/+8sXXrViMnJ8cYP3680aVLF6O8vNzT5sYbbzRSU1ONVatWGRs2bDAuueQSY/jw4Z7ttbW1Rv/+/Y3Ro0cbX375pfHuu+8a8fHxxrx588w4pTZv3bp1Rrdu3YyBAwcat956q2c919k/iouLja5duxpTp0411q5da3z//ffG+++/b3z77beeNg899JARExNjvP3228bmzZuNK664wujevbtx9OhRT5uxY8caF154ofHFF18Yn3zyidGrVy/jmmuuMeOU2qQFCxYYnTp1MlasWGHs2rXLeP31140OHToY//Vf/+Vpw3Vunnfffde4++67jTfffNOQZLz11lte2/1xXUtLS42kpCQjMzPT2Lp1q/Hqq68aERERxj/+8Y9zqp2w1EYNGzbMmDlzpud9XV2dkZKSYixcuNDEqgJXUVGRIcn46KOPDMMwjJKSEiM0NNR4/fXXPW22b99uSDKys7MNw6j/h221Wo2CggJPm8WLFxsOh8Ooqqpq3RNo48rKyozevXsbWVlZxmWXXeYJS1xn/7nrrruMESNGnHa72+02nE6n8cgjj3jWlZSUGHa73Xj11VcNwzCMr7/+2pBkrF+/3tPmvffeMywWi7F3796WKz6ATJgwwfh//+//ea276qqrjMzMTMMwuM7+cmJY8td1/fvf/27ExcV5/bfjrrvuMvr06XNO9XIbrg2qrq7Wxo0bNXr0aM86q9Wq0aNHKzs728TKAldpaakkqWPHjpKkjRs3qqamxusap6WlqUuXLp5rnJ2drQEDBigpKcnTZsyYMXK5XNq2bVsrVt/2zZw5UxMmTPC6nhLX2Z+WL1+uoUOH6uqrr1ZiYqIGDx6s//7v//Zs37VrlwoKCryudUxMjNLT072udWxsrIYOHeppM3r0aFmtVq1du7b1TqYNGz58uFatWqVvvvlGkrR582Z9+umnGjdunCSuc0vx13XNzs7Wj3/8Y4WFhXnajBkzRrm5uTp8+HCz6+OHdNuggwcPqq6uzuvLQ5KSkpK0Y8cOk6oKXG63W7fddpsuvfRS9e/fX5JUUFCgsLAwxcbGerVNSkpSQUGBp82p/jdo3IZ6y5Yt06ZNm7R+/fqTtnGd/ef777/X4sWLdccdd+iPf/yj1q9fr9///vcKCwvTlClTPNfqVNfy+GudmJjotd1ms6ljx45c6wZz586Vy+VSWlqaQkJCVFdXpwULFigzM1OSuM4txF/XtaCgQN27dz/pGI3b4uLimlUfYQlBb+bMmdq6das+/fRTs0sJOvn5+br11luVlZWl8PBws8sJam63W0OHDtWDDz4oSRo8eLC2bt2qJUuWaMqUKSZXFzxee+01LV26VK+88oouuOAC5eTk6LbbblNKSgrXuR3jNlwbFB8fr5CQkJOeGCosLJTT6TSpqsA0a9YsrVixQmvWrFHnzp09651Op6qrq1VSUuLV/vhr7HQ6T/m/QeM21N9mKyoq0kUXXSSbzSabzaaPPvpITz75pGw2m5KSkrjOfpKcnKx+/fp5revbt6/y8vIkHbtWvv674XQ6VVRU5LW9trZWxcXFXOsGf/jDHzR37lxNnjxZAwYM0HXXXafbb79dCxculMR1bin+uq4t9d8TwlIbFBYWpiFDhmjVqlWedW63W6tWrVJGRoaJlQUOwzA0a9YsvfXWW1q9evVJ3bJDhgxRaGio1zXOzc1VXl6e5xpnZGRoy5YtXv84s7Ky5HA4TvrSaq9GjRqlLVu2KCcnx7MMHTpUmZmZntdcZ/+49NJLT5r+4ptvvlHXrl0lSd27d5fT6fS61i6XS2vXrvW61iUlJdq4caOnzerVq+V2u5Went4KZ9H2VVRUyGr1/moMCQmR2+2WxHVuKf66rhkZGfr4449VU1PjaZOVlaU+ffo0+xacJKYOaKuWLVtm2O1244UXXjC+/vprY8aMGUZsbKzXE0M4vZtuusmIiYkxPvzwQ2P//v2epaKiwtPmxhtvNLp06WKsXr3a2LBhg5GRkWFkZGR4tjc+0n755ZcbOTk5xsqVK42EhAQeaT+D45+GMwyus7+sW7fOsNlsxoIFC4ydO3caS5cuNSIjI42XX37Z0+ahhx4yYmNjjX/961/GV199Zfzyl7885aPXgwcPNtauXWt8+umnRu/evdv9I+3HmzJlinHeeed5pg548803jfj4eGPOnDmeNlzn5ikrKzO+/PJL48svvzQkGY8//rjx5ZdfGrt37zYMwz/XtaSkxEhKSjKuu+46Y+vWrcayZcuMyMhIpg4IZosWLTK6dOlihIWFGcOGDTO++OILs0sKGJJOuTz//POeNkePHjVuvvlmIy4uzoiMjDSuvPJKY//+/V7H+eGHH4xx48YZERERRnx8vDF79myjpqamlc8msJwYlrjO/vPOO+8Y/fv3N+x2u5GWlmY888wzXtvdbrdxzz33GElJSYbdbjdGjRpl5ObmerU5dOiQcc011xgdOnQwHA6HccMNNxhlZWWteRptmsvlMm699VajS5cuRnh4uNGjRw/j7rvv9noUnevcPGvWrDnlf5enTJliGIb/ruvmzZuNESNGGHa73TjvvPOMhx566JxrtxjGcdOSAgAAwAtjlgAAAHwgLAEAAPhAWAIAAPCBsAQAAOADYQkAAMAHwhIAAIAPhCUAAAAfCEsAAAA+EJYAoAVYLBa9/fbbZpcBwA8ISwCCztSpU2WxWE5axo4da3ZpAAKQzewCAKAljB07Vs8//7zXOrvdblI1AAIZPUsAgpLdbpfT6fRa4uLiJNXfIlu8eLHGjRuniIgI9ejRQ2+88YbX/lu2bNFPf/pTRUREqFOnTpoxY4bKy8u92jz33HO64IILZLfblZycrFmzZnltP3jwoK688kpFRkaqd+/eWr58ecueNIAWQVgC0C7dc889mjRpkjZv3qzMzExNnjxZ27dvlyQdOXJEY8aMUVxcnNavX6/XX39dH3zwgVcYWrx4sWbOnKkZM2Zoy5YtWr58uXr16uX1Gffff79+/etf66uvvtL48eOVmZmp4uLiVj1PAH5gAECQmTJlihESEmJERUV5LQsWLDAMwzAkGTfeeKPXPunp6cZNN91kGIZhPPPMM0ZcXJxRXl7u2f7vf//bsFqtRkFBgWEYhpGSkmLcfffdp61BkvGnP/3J8768vNyQZLz33nt+O08ArYMxSwCC0k9+8hMtXrzYa13Hjh09rzMyMry2ZWRkKCcnR5K0fft2XXjhhYqKivJsv/TSS+V2u5WbmyuLxaJ9+/Zp1KhRPmsYOHCg53VUVJQcDoeKioqae0oATEJYAhCUoqKiTrot5i8RERFNahcaGur13mKxyO12t0RJAFoQY5YAtEtffPHFSe/79u0rSerbt682b96sI0eOeLZ/9tlnslqt6tOnj6Kjo9WtWzetWrWqVWsGYA56lgAEpaqqKhUUFHits9lsio+PlyS9/vrrGjp0qEaMGKGlS5dq3bp1evbZZyVJmZmZ+vOf/6wpU6bovvvu04EDB3TLLbfouuuuU1JSkiTpvvvu04033qjExESNGzdOZWVl+uyzz3TLLbe07okCaHGEJQBBaeXKlUpOTvZa16dPH+3YsUNS/ZNqy5Yt080336zk5GS9+uqr6tevnyQpMjJS77//vm699VZdfPHFioyM1KRJk/T44497jjVlyhRVVlbqb3/7m+68807Fx8frV7/6VeudIIBWYzEMwzC7CABoTRaLRW+99ZYmTpxodikAAgBjlgAAAHwgLAEAAPjAmCUA7Q6jDwCcDXqWAAAAfCAsAQAA+EBYAgAA8IGwBAAA4ANhCQAAwAfCEgAAgA+EJQAAAB8ISwAAAD78fzFdJMkIig9OAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from re import L\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import random\n",
        "\n",
        "from torch import optim\n",
        "from torch.utils import data\n",
        "from torch.optim import lr_scheduler\n",
        "from torchvision import datasets, models, transforms\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import statistics\n",
        "import datetime\n",
        "import os\n",
        "import csv\n",
        "import math\n",
        "import time\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_tensor = torch.tensor(X, dtype=torch.float64)\n",
        "Y_tensor = torch.tensor(Y, dtype=torch.float64)\n",
        "x_train, x_test, y_train, y_test = train_test_split(X_tensor, Y_tensor, test_size=0.1, random_state=41)\n",
        "dataset = TensorDataset(x_train, y_train)\n",
        "dataloader = DataLoader(dataset, batch_size = 32)\n",
        "testdataloader = DataLoader(TensorDataset(x_test, y_test))\n",
        "\n",
        "n1 = 20\n",
        "#n2 = 10\n",
        "\n",
        "# Define the neural network class\n",
        "class MLP(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MLP, self).__init__()\n",
        "        self.fc1 = torch.nn.Linear(5, n1)\n",
        "        self.fc2 = torch.nn.Linear(n1, n2)\n",
        "        self.fc3 = torch.nn.Linear(n2, 1)\n",
        "        self.fc4 = torch.nn.Linear(n1, 1)\n",
        "        self.dropout = torch.nn.Dropout(0.1)\n",
        "        self.tanh = torch.nn.Tanh()\n",
        "        self.bn1 = torch.nn.BatchNorm1d(n1)\n",
        "        self.bn2 = torch.nn.BatchNorm1d(n2)\n",
        "        self.bn3 = torch.nn.BatchNorm1d(1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        #x = self.bn1(x)\n",
        "        x = self.tanh(x)\n",
        "        #x = self.dropout(x)\n",
        "        #x = self.fc2(x)\n",
        "        #x = self.bn2(x)\n",
        "        #x = self.tanh(x)\n",
        "        #x = self.dropout(x)\n",
        "        #x = self.fc3(x)\n",
        "        #x = self.bn3(x)\n",
        "        x = self.fc4(x)\n",
        "        return x\n",
        "\n",
        "# Create an instance of the MLP class\n",
        "model = MLP()\n",
        "\n",
        "def initialize_weights(m):\n",
        "    if isinstance(m, torch.nn.Linear):\n",
        "        torch.nn.init.xavier_uniform_(m.weight)\n",
        "        if m.bias is not None:\n",
        "            torch.nn.init.constant_(m.bias, 0)\n",
        "\n",
        "model.apply(initialize_weights)\n",
        "\n",
        "loss_function = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5)\n",
        "#torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "nb_epochs = 1000\n",
        "MLoss = []\n",
        "for epoch in range(0, nb_epochs):\n",
        "\n",
        "    current_loss = 0.0\n",
        "    losses = []\n",
        "    # Iterate over the dataloader for training data\n",
        "    for i, data in enumerate(dataloader, 0):\n",
        "        inputs, targets = data\n",
        "        inputs, targets = inputs.float(), targets.float()\n",
        "        targets = targets.reshape((targets.shape[0],1))\n",
        "        #zero the gradients\n",
        "        optimizer.zero_grad()\n",
        "        #perform forward pass\n",
        "        outputs = model(inputs)\n",
        "        L_weight = 3\n",
        "        #compute loss\n",
        "        batch_loss = []\n",
        "        for j in range(inputs.size(0)):\n",
        "            input_j = inputs[j].reshape((1, inputs.shape[1]))\n",
        "            if input_j[0,0]>0.3:\n",
        "                batch_loss.append(L_weight*loss_function(outputs[j], targets[j]))\n",
        "            else:\n",
        "                batch_loss.append(loss_function(outputs[j], targets[j]))\n",
        "        loss = torch.stack(batch_loss).mean()\n",
        "        losses.append(loss.item())\n",
        "        #perform backward pass\n",
        "        loss.backward()\n",
        "        #perform optimization\n",
        "        optimizer.step()\n",
        "        # Print statistics\n",
        "\n",
        "    mean_loss = sum(losses)/len(losses)\n",
        "    scheduler.step(mean_loss)\n",
        "\n",
        "    print('Loss (epoch: %4d): %.8f' %(epoch+1, mean_loss))\n",
        "    current_loss = 0.0\n",
        "    MLoss.append(mean_loss)\n",
        "\n",
        "# Process is complete.\n",
        "#print('Training process has finished.')\n",
        "\n",
        "torch.save(model, 'IWO_idvg.pt')\n",
        "torch.save(model.state_dict(), 'IWO_idvg_state_dict.pt')\n",
        "\n",
        "####### loss vs. epoch #######\n",
        "xloss = list(range(0, nb_epochs))\n",
        "plt.plot(xloss, np.log10(MLoss))\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bbMxJW3-Ras7",
        "outputId": "bf846a1d-77da-49e6-857a-ca608c61e728"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "// VerilogA for GB_lib, IWO_verliogA, veriloga\n",
            "//*******************************************************************************\n",
            "//* * School of Electrical and Computer Engineering, Georgia Institute of Technology\n",
            "//* PI: Prof. Shimeng Yu\n",
            "//* All rights reserved.\n",
            "//*\n",
            "//* Copyright of the model is maintained by the developers, and the model is distributed under\n",
            "//* the terms of the Creative Commons Attribution-NonCommercial 4.0 International Public License\n",
            "//* http://creativecommons.org/licenses/by-nc/4.0/legalcode.\n",
            "//*\n",
            "//* THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND\n",
            "//* ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\n",
            "//* WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n",
            "//* DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\n",
            "//* FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\n",
            "//* DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\n",
            "//* SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\n",
            "//* CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\n",
            "//* OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n",
            "//* OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n",
            "//*\n",
            "//* Developer:\n",
            "//*  Gihun Choe gchoe6@gatech.edu\n",
            "//********************************************************************************/\n",
            "\n",
            "`include \"constants.vams\"\n",
            "`include \"disciplines.vams\"\n",
            "\n",
            "\n",
            "module IWO_verliogA(d, g, s);\n",
            "        inout d, g, s;\n",
            "        electrical d, g, s;\n",
            "\n",
            "        //***** parameters L and W ******//\n",
            "        parameter real W = 0.1; //get parameter fom spectre\n",
            "        parameter real L = 0.05; //get parameter fom spectre\n",
            "        parameter real T_stress = 0.0001; //set on cadence as variable\n",
            "        parameter real T_rec = 0.001;     //set on cadence as variable\n",
            "        parameter real Clk_loops = 50;    //set on cadence as variable\n",
            "        parameter real V_ov = 1.7;        //set on cadence as variable\n",
            "        parameter real Temp = 25;  //set on cadence as variable\n",
            "\n",
            "        parameter MinVg = -1.0 ;\n",
            "        parameter normVg = 0.2222222222222222 ;\n",
            "        parameter MinVd = 0.01 ;\n",
            "        parameter normVd = 0.2949852507374631 ;\n",
            "        parameter MinLg = 0.05 ;\n",
            "        parameter normLg = 1.4285714285714286 ;\n",
            "        parameter MinO = 8.15e-15 ;\n",
            "        parameter normO =33613445378151.26;\n",
            "        parameter MinI = -23.98798356587402 ;\n",
            "        parameter normI =0.04615548498417793;\n",
            "\n",
            "        parameter Mint_stress = 0.0001 ;\n",
            "        parameter normt_stress = 1111.111111111111 ;\n",
            "        parameter Mint_rec = 0.0001 ;\n",
            "        parameter normt_rec = 0.07629452739355005 ;\n",
            "        parameter Minclk_loops = 100.0 ;\n",
            "        parameter normclk_loops = 0.0011111111111111111 ;\n",
            "        parameter Minv_ov = 1.0 ;\n",
            "        parameter normv_ov = 1.4285714285714286 ;\n",
            "        parameter Mintemperature = 25.0 ;\n",
            "        parameter normtemperature = 0.016666666666666666 ;\n",
            "        parameter Mindelta_Vth = 0.04321379542061602 ;\n",
            "        parameter normdelta_Vth = 1.10036881607686 ;\n",
            "\n",
            "        real hvth1_0, hvth1_1, hvth1_2, hvth1_3, hvth1_4, hvth1_5, hvth1_6, hvth1_7, hvth1_8, hvth1_9, hvth1_10, hvth1_11, hvth1_12, hvth1_13, hvth1_14, hvth1_15, hvth1_16, hvth1_17, hvth1_18, hvth1_19;\n",
            "        real hvth2_0, hvth2_1, hvth2_2, hvth2_3, hvth2_4, hvth2_5, hvth2_6, hvth2_7, hvth2_8, hvth2_9;\n",
            "\n",
            "        real Vg, Vd, Vs, Vgs, Vds, Lg, Id, Cgg, Cgsd, Vgd;\n",
            "        real Vgsraw, Vgdraw, dir;\n",
            "        real t_stress, v_ov, t_rec, clk_loops, temp, delta_Vth;\n",
            "\n",
            "        real h1_0, h1_1, h1_2, h1_3, h1_4, h1_5, h1_6, h1_7, h1_8, h1_9, h1_10, h1_11, h1_12, h1_13, h1_14, h1_15, h1_16, h1_17, h1_18, h1_19, h1_20, h1_21, h1_22, h1_23, h1_24;\n",
            "        real h2_0, h2_1, h2_2, h2_3, h2_4, h2_5, h2_6, h2_7, h2_8, h2_9, h2_10, h2_11, h2_12, h2_13, h2_14, h2_15, h2_16, y;\n",
            "        real hc1_0, hc1_1, hc1_2, hc1_3, hc1_4, hc1_5, hc1_6, hc1_7, hc1_8, hc1_9;\n",
            "        real hc1_10, hc1_11, hc1_12, hc1_13, hc1_14, hc1_15, hc1_16;\n",
            "        real hc2_0, hc2_1, hc2_2, hc2_3, hc2_4, hc2_5, hc2_6, hc2_7, hc2_8, hc2_9;\n",
            "        real hc2_10, hc2_11, hc2_12, hc2_13, hc2_14, hc2_15, hc2_16, yc, yvth;\n",
            "\n",
            "analog begin\n",
            "\n",
            "t_stress = (T_stress - Mint_stress)*normt_stress;\n",
            "v_ov = (V_ov - Minv_ov)*normv_ov;\n",
            "t_rec = (T_rec - Mint_rec)*normt_rec ;\n",
            "clk_loops = (Clk_loops - Minclk_loops)*normclk_loops ;\n",
            "temp = (Temp - Mintemperature)*normtemperature ;\n",
            "\n",
            "//******************** delta_Vth NN **********************************//\n",
            "\n",
            "hvth1_0 = tanh(0.272744*t_stress+-0.23056228*t_rec+0.596634*clk_loops+-0.24343318*v_ov+-0.13230233*temp+0.048297387);\n",
            "hvth1_1 = tanh(0.245181*t_stress+0.21064457*t_rec+-0.07516779*clk_loops+0.40903565*v_ov+0.3082128*temp+0.059504695);\n",
            "hvth1_2 = tanh(0.32406178*t_stress+0.0961214*t_rec+0.01575127*clk_loops+-0.009446217*v_ov+-0.23892635*temp+-0.028230919);\n",
            "hvth1_3 = tanh(-0.36218816*t_stress+0.1578473*t_rec+-0.27259*clk_loops+-0.052113354*v_ov+0.11496024*temp+0.06155471);\n",
            "hvth1_4 = tanh(-0.13945356*t_stress+-0.021458378*t_rec+-0.40719503*clk_loops+-0.03505379*v_ov+-0.34112245*temp+0.035970967);\n",
            "hvth1_5 = tanh(0.42872298*t_stress+-0.45401916*t_rec+0.29580662*clk_loops+0.14504935*v_ov+-0.09959237*temp+-0.06627877);\n",
            "hvth1_6 = tanh(0.23034811*t_stress+-0.35950583*t_rec+0.11901722*clk_loops+0.11415793*v_ov+0.13242759*temp+0.05198286);\n",
            "hvth1_7 = tanh(0.06712985*t_stress+-0.062374733*t_rec+-0.28906003*clk_loops+0.024265185*v_ov+-0.06899681*temp+-0.034705576);\n",
            "hvth1_8 = tanh(-0.3795196*t_stress+0.043707807*t_rec+0.17164321*clk_loops+0.09797319*v_ov+0.06505784*temp+-0.031417992);\n",
            "hvth1_9 = tanh(0.28494462*t_stress+0.36375752*t_rec+0.093011215*clk_loops+0.31761396*v_ov+-0.5277346*temp+-0.078400396);\n",
            "hvth1_10 = tanh(-0.013580938*t_stress+-0.43107393*t_rec+0.28892088*clk_loops+0.22873798*v_ov+0.25575206*temp+-0.023186358);\n",
            "hvth1_11 = tanh(0.2528077*t_stress+-0.20586371*t_rec+0.13156222*clk_loops+-0.3816571*v_ov+0.30369946*temp+-0.04340843);\n",
            "hvth1_12 = tanh(0.18532023*t_stress+0.24751016*t_rec+0.08201023*clk_loops+-0.25347483*v_ov+-0.4244604*temp+0.004803747);\n",
            "hvth1_13 = tanh(-0.022441879*t_stress+0.033056706*t_rec+-0.38571253*clk_loops+-0.23792677*v_ov+0.3504729*temp+-0.07746615);\n",
            "hvth1_14 = tanh(0.10063724*t_stress+-0.45871386*t_rec+0.23451798*clk_loops+-0.03935696*v_ov+0.036486935*temp+0.037222292);\n",
            "hvth1_15 = tanh(-0.16819571*t_stress+-0.4405411*t_rec+-0.114860095*clk_loops+0.0030280957*v_ov+0.30049363*temp+-0.043794256);\n",
            "hvth1_16 = tanh(-0.06282347*t_stress+0.030551529*t_rec+-0.51957816*clk_loops+-0.47829595*v_ov+0.3119373*temp+-0.096701674);\n",
            "hvth1_17 = tanh(0.10372053*t_stress+0.084576525*t_rec+-0.22177602*clk_loops+-0.26837832*v_ov+0.54554504*temp+0.09300423);\n",
            "hvth1_18 = tanh(-0.14441638*t_stress+-0.41340104*t_rec+-0.05324316*clk_loops+0.32211384*v_ov+0.18434624*temp+0.03877211);\n",
            "hvth1_19 = tanh(-0.42406812*t_stress+-0.40645334*t_rec+0.015821397*clk_loops+0.39822432*v_ov+0.011161399*temp+0.019219667);\n",
            "\n",
            "yvth = 0.36008114*hvth1_0+0.45734045*hvth1_1+-0.18510826*hvth1_2+0.4497314*hvth1_3+0.3650353*hvth1_4+-0.36643317*hvth1_5+0.20361336*hvth1_6+-0.580298*hvth1_7+-0.029452145*hvth1_8+-0.21224296*hvth1_9+-0.311336*hvth1_10+-0.12667896*hvth1_11+0.0036873992*hvth1_12+-0.4226459*hvth1_13+0.42364526*hvth1_14+-0.04215827*hvth1_15+-0.05251629*hvth1_16+0.24671972*hvth1_17+0.016773598*hvth1_18+0.296929*hvth1_19+0.04334759;\n",
            "\n",
            "delta_Vth = (yvth - Mindelta_Vth) * normdelta_Vth;\n",
            "\n",
            "\n",
            "        Vg = V(g) ;\n",
            "        Vs = V(s) ;\n",
            "        Vd = V(d) ;\n",
            "        Vgsraw = Vg-Vs ;\n",
            "        Vgdraw = Vg-Vd ;\n",
            "\n",
            "if (Vgsraw >= Vgdraw) begin\n",
            "        Vgs = ((Vg-Vs) - MinVg) * normVg ;\n",
            "        dir = 1;\n",
            "end\n",
            "\n",
            "else begin\n",
            "        Vgs = ((Vg-Vd) - MinVg) * normVg ;\n",
            "        dir = -1;\n",
            "end\n",
            "\n",
            "        Vds = (abs(Vd-Vs) - MinVd) * normVd ;\n",
            "        Vgs = Vgs - delta_Vth ; // BTI Vth variation\n",
            "        Lg = (L -MinLg)*normLg ;\n",
            "\n",
            "\n",
            "\n",
            "//******************** C-V NN **********************************//\n",
            "hc1_0 = tanh(-0.99871427*Vgs+-0.16952373*Vds+0.32118186*Lg+0.41485423);\n",
            "hc1_1 = tanh(0.31587568*Vgs+0.13397887*Vds+-0.4541538*Lg+-0.3630942);\n",
            "hc1_2 = tanh(-0.76281804*Vgs+0.09352969*Vds+1.1961353*Lg+0.3904977);\n",
            "hc1_3 = tanh(-1.115087*Vgs+0.85752946*Vds+-0.11746484*Lg+0.5500279);\n",
            "hc1_4 = tanh(1.0741386*Vgs+0.82041687*Vds+0.19092631*Lg+-0.4009425);\n",
            "hc1_5 = tanh(-0.47921795*Vgs+-0.8749933*Vds+-0.054768667*Lg+0.62785167);\n",
            "hc1_6 = tanh(0.5449184*Vgs+-4.409165*Vds+-0.072947875*Lg+-0.31324536);\n",
            "hc1_7 = tanh(-2.9224303*Vgs+2.7675478*Vds+0.08862238*Lg+0.6493558);\n",
            "hc1_8 = tanh(0.65050656*Vgs+-0.29751927*Vds+0.1571876*Lg+-0.38088372);\n",
            "hc1_9 = tanh(-0.30384183*Vgs+0.5649165*Vds+2.6806898*Lg+0.3197917);\n",
            "hc1_10 = tanh(-0.095988505*Vgs+2.0158541*Vds+-0.42972717*Lg+-0.30388466);\n",
            "hc1_11 = tanh(6.7699738*Vgs+-0.07234483*Vds+-0.013545353*Lg+-1.3694142);\n",
            "hc1_12 = tanh(-0.3404029*Vgs+0.0443459*Vds+0.89597*Lg+0.069993004);\n",
            "hc1_13 = tanh(0.62300175*Vgs+-0.29515797*Vds+1.6753465*Lg+-0.6520838);\n",
            "hc1_14 = tanh(0.37957156*Vgs+0.2237372*Vds+0.08591952*Lg+0.13126835);\n",
            "hc1_15 = tanh(0.19949242*Vgs+-0.26481664*Vds+-0.41059187*Lg+-0.40832308);\n",
            "hc1_16 = tanh(0.98966587*Vgs+-0.24259183*Vds+0.36584845*Lg+-0.8024042);\n",
            "\n",
            "hc2_0 = tanh(-0.91327864*hc1_0+0.4696781*hc1_1+0.3302202*hc1_2+0.11393868*hc1_3+0.45070222*hc1_4+-0.2894044*hc1_5+0.55066*hc1_6+-1.6242687*hc1_7+-0.38140613*hc1_8+0.032771554*hc1_9+0.17647126);\n",
            "hc2_1 = tanh(-1.1663305*hc1_0+-0.523984*hc1_1+-0.90804136*hc1_2+-0.7418044*hc1_3+1.456171*hc1_4+-0.16802542*hc1_5+0.8235596*hc1_6+-2.2246246*hc1_7+-0.40805355*hc1_8+0.7207601*hc1_9+0.4729169);\n",
            "hc2_2 = tanh(-0.69065374*hc1_0+0.40205315*hc1_1+-0.49410668*hc1_2+0.8681325*hc1_3+0.471351*hc1_4+0.46939445*hc1_5+0.45568785*hc1_6+-0.92935294*hc1_7+-0.8209646*hc1_8+0.1158967*hc1_9+-0.075798474);\n",
            "hc2_3 = tanh(0.11535446*hc1_0+-0.06296927*hc1_1+-0.6740435*hc1_2+0.7428315*hc1_3+0.05890677*hc1_4+0.9579441*hc1_5+-0.037319*hc1_6+-0.18491426*hc1_7+-0.02981994*hc1_8+0.038347963*hc1_9+0.039531134);\n",
            "hc2_4 = tanh(0.37817463*hc1_0+-0.6811279*hc1_1+1.1388369*hc1_2+0.19983096*hc1_3+-0.20415118*hc1_4+1.3022176*hc1_5+0.22571652*hc1_6+0.1690611*hc1_7+-0.56475276*hc1_8+-0.4069731*hc1_9+0.99962974);\n",
            "hc2_5 = tanh(0.032873478*hc1_0+-0.05209407*hc1_1+-0.010839908*hc1_2+-0.13892579*hc1_3+-0.050480977*hc1_4+0.0127089145*hc1_5+0.0052771433*hc1_6+0.02029242*hc1_7+-0.08705659*hc1_8+0.0254766*hc1_9+0.025135752);\n",
            "hc2_6 = tanh(-0.34639204*hc1_0+0.06937975*hc1_1+0.18671949*hc1_2+-0.18912783*hc1_3+0.1312504*hc1_4+0.4627272*hc1_5+-0.42590702*hc1_6+-0.10966313*hc1_7+0.66083515*hc1_8+-0.050718334*hc1_9+0.08234678);\n",
            "hc2_7 = tanh(0.90656275*hc1_0+-0.037281644*hc1_1+0.77237594*hc1_2+1.4710428*hc1_3+0.13597831*hc1_4+-0.059844542*hc1_5+-0.7801535*hc1_6+3.7814677*hc1_7+-0.5976644*hc1_8+0.2721995*hc1_9+0.023777716);\n",
            "hc2_8 = tanh(0.39720625*hc1_0+-0.45262313*hc1_1+0.19873238*hc1_2+0.9750888*hc1_3+-0.9427992*hc1_4+0.4487432*hc1_5+-0.3372945*hc1_6+0.33729544*hc1_7+-0.1667088*hc1_8+-0.5707525*hc1_9+0.27954483);\n",
            "hc2_9 = tanh(0.28551984*hc1_0+-0.68350387*hc1_1+0.9916423*hc1_2+-0.8254094*hc1_3+0.09875706*hc1_4+0.47609732*hc1_5+-0.058662917*hc1_6+0.09181381*hc1_7+0.09592329*hc1_8+1.3940467*hc1_9+0.3865768);\n",
            "hc2_10 = tanh(0.098737516*hc1_0+0.060473576*hc1_1+0.42824662*hc1_2+0.15018038*hc1_3+0.082621895*hc1_4+-0.00019039502*hc1_5+-0.3321634*hc1_6+0.7936295*hc1_7+-0.041197542*hc1_8+0.6530619*hc1_9+0.1338804);\n",
            "hc2_11 = tanh(-0.3585284*hc1_0+-0.09956017*hc1_1+0.17224246*hc1_2+-0.016925728*hc1_3+-0.46462816*hc1_4+-0.5649022*hc1_5+1.251695*hc1_6+-0.4303161*hc1_7+0.48546878*hc1_8+0.22958975*hc1_9+-0.27899802);\n",
            "hc2_12 = tanh(0.8565631*hc1_0+-0.7622999*hc1_1+0.32367912*hc1_2+1.4776785*hc1_3+0.2712369*hc1_4+0.2275511*hc1_5+0.39908803*hc1_6+4.2305493*hc1_7+-0.3467536*hc1_8+0.41231114*hc1_9+0.47123823);\n",
            "hc2_13 = tanh(-0.26411077*hc1_0+-0.17583853*hc1_1+0.045439184*hc1_2+-0.13801138*hc1_3+0.03278085*hc1_4+-0.45625108*hc1_5+-0.17905861*hc1_6+0.3060186*hc1_7+-0.3361926*hc1_8+-0.050055273*hc1_9+0.17996444);\n",
            "hc2_14 = tanh(0.016094366*hc1_0+-0.013196736*hc1_1+0.4124856*hc1_2+0.22926371*hc1_3+-0.35071182*hc1_4+-0.34217188*hc1_5+-0.69466996*hc1_6+1.0563152*hc1_7+-0.18019852*hc1_8+0.061871335*hc1_9+0.09762555);\n",
            "hc2_15 = tanh(-0.18970782*hc1_0+0.3624813*hc1_1+-1.3419824*hc1_2+0.103635244*hc1_3+-0.14595217*hc1_4+-0.1899393*hc1_5+0.176524*hc1_6+-0.4428012*hc1_7+-0.39544868*hc1_8+-0.45783517*hc1_9+0.1884755);\n",
            "hc2_16 = tanh(-0.1585831*hc1_0+0.035894837*hc1_1+-0.14261873*hc1_2+0.25914294*hc1_3+0.040607046*hc1_4+0.11555795*hc1_5+0.0022548323*hc1_6+-0.002149359*hc1_7+0.07067297*hc1_8+0.019578662*hc1_9+0.16657573);\n",
            "yc = 0.17503543*hc2_0+-0.15556754*hc2_1+-0.125455*hc2_2+-0.07502612*hc2_3+-0.16671115*hc2_4+0.29854503;\n",
            "\n",
            "Cgg = (yc / normO + MinO)*W;\n",
            "Cgsd = Cgg/2 ;\n",
            "\n",
            "//******************** I-V NN **********************************//\n",
            "h1_0 = tanh(0.0023826673*Vgs+-0.0009636134*Vds+0.006639037*Lg+-0.0004692053);\n",
            "h1_1 = tanh(-7.8007555*Vgs+2.639791*Vds+-0.025273597*Lg+-1.1747514);\n",
            "h1_2 = tanh(1.9884652*Vgs+0.62111*Vds+-0.11525345*Lg+-0.14575638);\n",
            "h1_3 = tanh(0.10625471*Vgs+0.09442053*Vds+-0.052119628*Lg+1.1568379);\n",
            "h1_4 = tanh(4.058087*Vgs+-0.7568158*Vds+0.008070099*Lg+-0.4820452);\n",
            "h1_5 = tanh(-1.3065948*Vgs+-0.0492497*Vds+-0.081622526*Lg+0.20351954);\n",
            "h1_6 = tanh(-2.9507525*Vgs+-0.91150546*Vds+-0.06477873*Lg+0.09646383);\n",
            "h1_7 = tanh(-0.5886177*Vgs+0.63788587*Vds+-0.13710928*Lg+0.30260038);\n",
            "h1_8 = tanh(-0.4311161*Vgs+-0.7250705*Vds+-0.06134645*Lg+0.44054285);\n",
            "h1_9 = tanh(0.012132638*Vgs+-0.42545322*Vds+-0.66478956*Lg+0.13182367);\n",
            "h1_10 = tanh(3.289041e-05*Vgs+0.0011367714*Vds+-0.0051904405*Lg+5.4112927e-05);\n",
            "h1_11 = tanh(-0.6007774*Vgs+0.09259224*Vds+0.2170182*Lg+-0.2908748);\n",
            "h1_12 = tanh(0.28018302*Vgs+1.3014064*Vds+-0.13490067*Lg+-0.22006753);\n",
            "h1_13 = tanh(0.01864017*Vgs+-13.0928755*Vds+-0.0037254083*Lg+-0.10935691);\n",
            "h1_14 = tanh(-0.0049708197*Vgs+0.0022613218*Vds+-0.014408149*Lg+0.0011340647);\n",
            "h1_15 = tanh(-0.094654046*Vgs+-0.4174114*Vds+0.18892045*Lg+0.05248958);\n",
            "h1_16 = tanh(-0.25090316*Vgs+-0.11111481*Vds+-0.009133225*Lg+0.08377026);\n",
            "h1_17 = tanh(0.9275306*Vgs+0.40686756*Vds+0.14127344*Lg+-0.059246097);\n",
            "h1_18 = tanh(-0.27084363*Vgs+0.07915911*Vds+-10.836302*Lg+-1.1535788);\n",
            "h1_19 = tanh(1.6852072*Vgs+-0.24846223*Vds+0.049734037*Lg+-0.19625053);\n",
            "\n",
            "h2_0 = tanh(1.1139417*h1_0+-0.40033522*h1_1+0.920759*h1_2+0.47607598*h1_3+0.3978683*h1_4+0.8008105*h1_5+-0.40445566*h1_6+0.8668027*h1_7+0.23365597*h1_8+-0.28254375*h1_9+-0.63985884*h1_10+-0.62523574*h1_11+0.8338383*h1_12+-2.0540943*h1_13+1.0749483*h1_14+-1.1075479*h1_15+0.60926706*h1_16+1.1118286*h1_17+-0.8917559*h1_18+0.029025732*h1_19+0.6622382);\n",
            "h2_1 = tanh(0.37195024*h1_0+-0.761445*h1_1+0.6514153*h1_2+0.6211995*h1_3+-0.063539445*h1_4+0.31260127*h1_5+-0.3529579*h1_6+0.50422627*h1_7+0.18943883*h1_8+-0.38029787*h1_9+-0.3464503*h1_10+-0.48301366*h1_11+0.081978925*h1_12+-0.08305682*h1_13+-0.08420117*h1_14+-0.8029313*h1_15+-0.37052512*h1_16+0.4553502*h1_17+-0.71959645*h1_18+-0.17320661*h1_19+0.6566257);\n",
            "h2_2 = tanh(-0.021933522*h1_0+-0.017245224*h1_1+0.030417712*h1_2+0.2967218*h1_3+-0.048668377*h1_4+0.029245213*h1_5+0.011756416*h1_6+0.004705658*h1_7+-0.042515088*h1_8+0.010462476*h1_9+0.001331279*h1_10+0.1694541*h1_11+-0.010949079*h1_12+-0.004144526*h1_13+0.04856694*h1_14+-0.010465159*h1_15+0.24588406*h1_16+-0.028521152*h1_17+0.12161568*h1_18+0.1753255*h1_19+-0.09125355);\n",
            "h2_3 = tanh(-0.025546636*h1_0+-3.2702503*h1_1+0.46812135*h1_2+-0.25135994*h1_3+3.0725436*h1_4+0.22513995*h1_5+-1.0327209*h1_6+-0.56274736*h1_7+0.4726107*h1_8+0.38182434*h1_9+0.016403453*h1_10+-0.09128962*h1_11+0.20997792*h1_12+-0.4951615*h1_13+0.026481293*h1_14+0.6085288*h1_15+-0.09078652*h1_16+0.36613584*h1_17+0.26257026*h1_18+0.39794916*h1_19+-0.52920526);\n",
            "h2_4 = tanh(-0.44392154*h1_0+-0.5650475*h1_1+0.91716766*h1_2+0.96703196*h1_3+0.4597048*h1_4+1.452921*h1_5+-0.8115141*h1_6+0.92326456*h1_7+0.2862403*h1_8+-0.9441585*h1_9+0.22188367*h1_10+-1.0092766*h1_11+0.5495966*h1_12+0.44870532*h1_13+0.48705962*h1_14+-0.6957133*h1_15+0.27711377*h1_16+1.0138507*h1_17+-0.34337458*h1_18+-0.21548931*h1_19+0.42685974);\n",
            "h2_5 = tanh(0.0003710325*h1_0+0.5746112*h1_1+0.4144258*h1_2+0.04459103*h1_3+1.2373503*h1_4+0.12786175*h1_5+-0.16099685*h1_6+0.9826207*h1_7+-0.09701193*h1_8+-0.4379135*h1_9+-0.0035542254*h1_10+0.04205593*h1_11+0.65820116*h1_12+0.3810506*h1_13+0.0004259507*h1_14+-0.21782044*h1_15+-0.057544652*h1_16+0.24420041*h1_17+-0.10138292*h1_18+-0.2387106*h1_19+0.867847);\n",
            "h2_6 = tanh(-0.0077049686*h1_0+-0.05349668*h1_1+-0.11536639*h1_2+0.06141029*h1_3+-0.034344573*h1_4+0.21672213*h1_5+-0.09835135*h1_6+-0.25849992*h1_7+0.10576329*h1_8+-0.14288934*h1_9+0.027846925*h1_10+-0.20104973*h1_11+0.24784875*h1_12+0.03396087*h1_13+-0.016039118*h1_14+-0.03147038*h1_15+-0.023109786*h1_16+-0.118931994*h1_17+0.18256636*h1_18+0.09981429*h1_19+0.079372324);\n",
            "h2_7 = tanh(-0.009369999*h1_0+-2.4468672*h1_1+-0.41433397*h1_2+-0.6289744*h1_3+-1.8285819*h1_4+0.12905455*h1_5+0.83588725*h1_6+0.11491322*h1_7+0.3871084*h1_8+0.07153052*h1_9+0.013530584*h1_10+0.40614852*h1_11+0.10340061*h1_12+-0.04473306*h1_13+0.0075287875*h1_14+0.5593612*h1_15+0.34401885*h1_16+-0.5538749*h1_17+0.36615464*h1_18+-0.2666981*h1_19+0.20194086);\n",
            "h2_8 = tanh(-0.0017045025*h1_0+-1.1388719*h1_1+0.25932005*h1_2+-0.6482845*h1_3+2.263395*h1_4+0.115192235*h1_5+-0.40946937*h1_6+-0.038117886*h1_7+-0.03703431*h1_8+0.05965774*h1_9+0.0032721795*h1_10+0.2561857*h1_11+0.47324425*h1_12+-0.25200802*h1_13+0.0017081021*h1_14+0.29819545*h1_15+0.061945435*h1_16+0.13516657*h1_17+0.5409335*h1_18+-0.17400871*h1_19+0.11659722);\n",
            "h2_9 = tanh(0.014866875*h1_0+1.0399909*h1_1+1.0108095*h1_2+-0.0918755*h1_3+-0.7634763*h1_4+-0.49749368*h1_5+-0.7020006*h1_6+-0.16384888*h1_7+-0.07833025*h1_8+-0.23745225*h1_9+-0.023095092*h1_10+-0.29244414*h1_11+0.6255917*h1_12+0.41413808*h1_13+-0.019823061*h1_14+-1.0593235*h1_15+-0.42221433*h1_16+0.98025715*h1_17+-0.71444243*h1_18+0.84347373*h1_19+0.28510216);\n",
            "h2_10 = tanh(-0.013515852*h1_0+0.68840384*h1_1+-0.030184174*h1_2+-0.48098114*h1_3+0.08609854*h1_4+-0.50878614*h1_5+0.26547763*h1_6+-0.7151076*h1_7+0.111288495*h1_8+0.53379977*h1_9+0.022258151*h1_10+0.16971351*h1_11+-1.2486296*h1_12+0.9649942*h1_13+0.0028906881*h1_14+0.31582054*h1_15+0.24213083*h1_16+-0.27242163*h1_17+0.11330636*h1_18+0.17038865*h1_19+-0.42089102);\n",
            "h2_11 = tanh(-0.009949424*h1_0+-0.6704206*h1_1+0.13177924*h1_2+0.07316155*h1_3+-0.28207502*h1_4+-0.32401362*h1_5+0.020746209*h1_6+-0.05517531*h1_7+0.10340257*h1_8+0.29381263*h1_9+0.020497978*h1_10+0.031984854*h1_11+0.079552434*h1_12+-7.7884445*h1_13+0.01224806*h1_14+0.8700812*h1_15+-0.16562358*h1_16+0.14370379*h1_17+0.2389181*h1_18+-0.19771136*h1_19+0.33039534);\n",
            "h2_12 = tanh(-0.002431529*h1_0+0.30154988*h1_1+-0.45072728*h1_2+0.27823612*h1_3+-0.25173753*h1_4+-0.58188903*h1_5+0.39836177*h1_6+-0.060329597*h1_7+0.1667837*h1_8+-0.09419194*h1_9+0.0108116735*h1_10+-0.3581154*h1_11+0.15690842*h1_12+-0.41209573*h1_13+-0.0070331707*h1_14+-0.23976392*h1_15+0.16620094*h1_16+-0.034617994*h1_17+0.32754526*h1_18+0.8490298*h1_19+0.026407415);\n",
            "h2_13 = tanh(-0.7385622*h1_0+-0.25174448*h1_1+2.090295*h1_2+0.67499936*h1_3+0.07392262*h1_4+0.5413692*h1_5+-0.8287433*h1_6+2.2810504*h1_7+1.1917778*h1_8+1.3719273*h1_9+0.25762329*h1_10+-2.1111712*h1_11+1.6987114*h1_12+-0.437825*h1_13+1.7649944*h1_14+-2.2254016*h1_15+-2.3555171*h1_16+-0.7732424*h1_17+-1.0756916*h1_18+-0.1367373*h1_19+1.5085722);\n",
            "h2_14 = tanh(-0.002535408*h1_0+-4.2686224*h1_1+-0.045537975*h1_2+0.08043166*h1_3+1.7274952*h1_4+-0.1260494*h1_5+-0.23011239*h1_6+-0.21025337*h1_7+0.48537356*h1_8+0.39049447*h1_9+-0.006565428*h1_10+-0.293028*h1_11+-1.1253971*h1_12+-0.2890831*h1_13+0.0042421576*h1_14+0.4230598*h1_15+-0.17869289*h1_16+-0.034056116*h1_17+0.61135995*h1_18+0.6345532*h1_19+0.73144835);\n",
            "h2_15 = tanh(-0.5239093*h1_0+0.49806875*h1_1+0.53555894*h1_2+1.2645539*h1_3+1.011548*h1_4+0.39209503*h1_5+-0.78194916*h1_6+0.94610345*h1_7+0.9121405*h1_8+-1.4693716*h1_9+-0.58336824*h1_10+-1.1661471*h1_11+-0.16158457*h1_12+0.058512915*h1_13+1.3358645*h1_14+0.5742125*h1_15+-1.3325212*h1_16+1.1804186*h1_17+-1.4877121*h1_18+0.6357802*h1_19+1.1328585);\n",
            "h2_16 = tanh(0.0095406*h1_0+-0.53232694*h1_1+0.1535685*h1_2+0.26108417*h1_3+-0.08375187*h1_4+0.3462123*h1_5+-0.61755395*h1_6+0.0015145333*h1_7+-0.108780764*h1_8+0.2555477*h1_9+0.00023940884*h1_10+-0.26577523*h1_11+-0.27689674*h1_12+0.43049082*h1_13+-0.025424613*h1_14+0.24634649*h1_15+-0.22579487*h1_16+0.37249807*h1_17+-0.8058662*h1_18+-0.77451354*h1_19+0.5821276);\n",
            "y = -0.17525196*h2_0+-0.22995844*h2_1+0.0026147955*h2_2+0.091129646*h2_3+-0.4534783*h2_4+0.4119419*h2_5+-1.4205361e-05*h2_6+-0.11359831*h2_7+0.1762025*h2_8+-0.1002102*h2_9+-0.37161925*h2_10+0.2683793*h2_11+0.051897053*h2_12+0.19354156*h2_13+-0.10134394*h2_14+0.3937854*h2_15+-0.3296008*h2_16+0.3220947;\n",
            "\n",
            "        Id = pow(10, (y/normI + MinI)) ;\n",
            "\n",
            "if (Id <= 1e-15) begin //limit\n",
            "        Id = 1e-15;\n",
            "        //Id = Id;\n",
            "end\n",
            "else begin\n",
            "        Id = Id;\n",
            "end  //limit end\n",
            "\n",
            "\n",
            "        I(g, d) <+ Cgsd*ddt(Vg-Vd) ;\n",
            "        I(g, s) <+ Cgsd*ddt(Vg-Vs) ;\n",
            "\n",
            "if (Vgsraw >= Vgdraw) begin\n",
            "        I(d, s) <+ dir*Id*W ;\n",
            "\n",
            "end\n",
            "\n",
            "else begin\n",
            "        I(d, s) <+ dir*Id*W ;\n",
            "\n",
            "end\n",
            "\n",
            "end\n",
            "\n",
            "endmodule\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "with torch.no_grad():\n",
        "\n",
        "    output = []\n",
        "    # Iterate over the dataloader for training data\n",
        "    for i, data in enumerate(testdataloader, 0):\n",
        "        inputs, targets = data\n",
        "        inputs, targets = inputs.float(), targets.float()\n",
        "        targets = targets.reshape((targets.shape[0],1))\n",
        "\n",
        "        #zero the gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        #perform forward pass\n",
        "        outputs = model(inputs)\n",
        "        output.append(outputs)\n",
        "# Process is complete.\n",
        "#print('Training process has finished.')\n",
        "\n",
        "# Extract the weights and biases from the model\n",
        "weights_1 = model.fc1.weight.detach().numpy()\n",
        "bias_1 = model.fc1.bias.detach().numpy()\n",
        "#weights_2 = model.fc2.weight.detach().numpy()\n",
        "#bias_2 = model.fc2.bias.detach().numpy()\n",
        "#weights_3 = model.fc3.weight.detach().numpy()\n",
        "#bias_3 = model.fc3.bias.detach().numpy()\n",
        "weights_4 = model.fc4.weight.detach().numpy()\n",
        "bias_4 = model.fc4.bias.detach().numpy()\n",
        "\n",
        "def generate_variable_declarations(weights_shape, layer_prefix):\n",
        "    declarations = \"\"\n",
        "    num_neurons = weights_shape  # Number of neurons is determined by the first dimension of the weights matrix\n",
        "    layer_declarations = \", \".join([f\"{layer_prefix}_{i}\" for i in range(num_neurons)]) + \";\"\n",
        "    declarations += layer_declarations\n",
        "    return declarations\n",
        "\n",
        "# Use the function to generate declarations for each layer\n",
        "h1_declarations = generate_variable_declarations(weights_1.shape[0], \"hvth1\")\n",
        "#2_declarations = generate_variable_declarations(weights_2.shape[0], \"hvth2\")\n",
        "\n",
        "verilog_code = \"\"\"\n",
        "// VerilogA for GB_lib, IWO_verliogA, veriloga\n",
        "//*******************************************************************************\n",
        "//* * School of Electrical and Computer Engineering, Georgia Institute of Technology\n",
        "//* PI: Prof. Shimeng Yu\n",
        "//* All rights reserved.\n",
        "//*\n",
        "//* Copyright of the model is maintained by the developers, and the model is distributed under\n",
        "//* the terms of the Creative Commons Attribution-NonCommercial 4.0 International Public License\n",
        "//* http://creativecommons.org/licenses/by-nc/4.0/legalcode.\n",
        "//*\n",
        "//* THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND\n",
        "//* ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\n",
        "//* WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n",
        "//* DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\n",
        "//* FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\n",
        "//* DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\n",
        "//* SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\n",
        "//* CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\n",
        "//* OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n",
        "//* OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n",
        "//*\n",
        "//* Developer:\n",
        "//*  Gihun Choe gchoe6@gatech.edu\n",
        "//********************************************************************************/\n",
        "\n",
        "`include \"constants.vams\"\n",
        "`include \"disciplines.vams\"\n",
        "\n",
        "\n",
        "module IWO_verliogA(d, g, s);\n",
        "        inout d, g, s;\n",
        "        electrical d, g, s;\n",
        "\n",
        "        //***** parameters L and W ******//\n",
        "        parameter real W = 0.1; //get parameter fom spectre\n",
        "        parameter real L = 0.05; //get parameter fom spectre\n",
        "        parameter real T_stress = 0.0001; //set on cadence as variable\n",
        "        parameter real T_rec = 0.001;     //set on cadence as variable\n",
        "        parameter real Clk_loops = 50;    //set on cadence as variable\n",
        "        parameter real V_ov = 1.7;        //set on cadence as variable\n",
        "        parameter real Temp = 25;  //set on cadence as variable\n",
        "\n",
        "        parameter MinVg = -1.0 ;\n",
        "        parameter normVg = 0.2222222222222222 ;\n",
        "        parameter MinVd = 0.01 ;\n",
        "        parameter normVd = 0.2949852507374631 ;\n",
        "        parameter MinLg = 0.05 ;\n",
        "        parameter normLg = 1.4285714285714286 ;\n",
        "        parameter MinO = 8.15e-15 ;\n",
        "        parameter normO =33613445378151.26;\n",
        "        parameter MinI = -23.98798356587402 ;\n",
        "        parameter normI =0.04615548498417793;\n",
        "\n",
        "        parameter Mint_stress = {} ;\n",
        "        parameter normt_stress = {} ;\n",
        "        parameter Mint_rec = {} ;\n",
        "        parameter normt_rec = {} ;\n",
        "        parameter Minclk_loops = {} ;\n",
        "        parameter normclk_loops = {} ;\n",
        "        parameter Minv_ov = {} ;\n",
        "        parameter normv_ov = {} ;\n",
        "        parameter Mintemperature = {} ;\n",
        "        parameter normtemperature = {} ;\n",
        "        parameter Mindelta_Vth = {} ;\n",
        "        parameter normdelta_Vth = {} ;\n",
        "\n",
        "        real {}\n",
        "        real {}\n",
        "\n",
        "        real Vg, Vd, Vs, Vgs, Vds, Lg, Id, Cgg, Cgsd, Vgd;\n",
        "        real Vgsraw, Vgdraw, dir;\n",
        "        real t_stress, v_ov, t_rec, clk_loops, temp, delta_Vth;\n",
        "\n",
        "        real h1_0, h1_1, h1_2, h1_3, h1_4, h1_5, h1_6, h1_7, h1_8, h1_9, h1_10, h1_11, h1_12, h1_13, h1_14, h1_15, h1_16, h1_17, h1_18, h1_19, h1_20, h1_21, h1_22, h1_23, h1_24;\n",
        "        real h2_0, h2_1, h2_2, h2_3, h2_4, h2_5, h2_6, h2_7, h2_8, h2_9, h2_10, h2_11, h2_12, h2_13, h2_14, h2_15, h2_16, y;\n",
        "        real hc1_0, hc1_1, hc1_2, hc1_3, hc1_4, hc1_5, hc1_6, hc1_7, hc1_8, hc1_9;\n",
        "        real hc1_10, hc1_11, hc1_12, hc1_13, hc1_14, hc1_15, hc1_16;\n",
        "        real hc2_0, hc2_1, hc2_2, hc2_3, hc2_4, hc2_5, hc2_6, hc2_7, hc2_8, hc2_9;\n",
        "        real hc2_10, hc2_11, hc2_12, hc2_13, hc2_14, hc2_15, hc2_16, yc, yvth;\n",
        "\n",
        "analog begin\n",
        "\n",
        "t_stress = (T_stress - Mint_stress)*normt_stress;\n",
        "v_ov = (V_ov - Minv_ov)*normv_ov;\n",
        "t_rec = (T_rec - Mint_rec)*normt_rec ;\n",
        "clk_loops = (Clk_loops - Minclk_loops)*normclk_loops ;\n",
        "temp = (Temp - Mintemperature)*normtemperature ;\n",
        "\n",
        "//******************** delta_Vth NN **********************************//\n",
        "\n",
        "\"\"\".format(Mint_stress, normt_stress, Mint_rec, normt_rec, Minclk_loops, normclk_loops, MinV_ov, normV_ov, Mintemperature, normtemperature, Mindelta_Vth, normdelta_Vth, h1_declarations, h2_declarations)\n",
        "# V_ov = (V_ov - MinV_ov)*normV_ov ;\n",
        "# t_stress = (T_stress - Mint_stress)*normt_stress ;\n",
        "\n",
        "# Create the Verilog-A code for the 1st hidden layer\n",
        "for i in range(n1):\n",
        "    inputs = [\"t_stress\", \"t_rec\", \"clk_loops\", \"v_ov\", \"temp\"]\n",
        "    inputs = [\"*\".join([str(weights_1[i][j]), inp]) for j, inp in enumerate(inputs)]\n",
        "    inputs = \"+\".join(inputs)\n",
        "    inputs = \"+\".join([inputs, str(bias_1[i])])\n",
        "    verilog_code += \"hvth1_{} = tanh({});\\n\".format(i, inputs)\n",
        "verilog_code += \"\\n\"\n",
        "\n",
        "\"\"\"\n",
        "# Create the Verilog-A code for the 2nd hidden layer\n",
        "for i in range(n2):\n",
        "    inputs = [\"h1_{}\".format(j) for j in range(n1)]\n",
        "    inputs = [\"*\".join([str(weights_2[i][j]), inp]) for j, inp in enumerate(inputs)]\n",
        "    inputs = \"+\".join(inputs)\n",
        "    inputs = \"+\".join([inputs, str(bias_2[i])])\n",
        "    verilog_code += \"hvth2_{} = tanh({});\\n\".format(i, inputs)\n",
        "verilog_code += \"\\n\"\n",
        "\"\"\"\n",
        "# Create the Verilog-A code for the output layer\n",
        "inputs = [\"hvth1_{}\".format(i) for i in range(n1)]\n",
        "inputs = [\"*\".join([str(weights_4[0][i]), inp]) for i, inp in enumerate(inputs)]\n",
        "inputs = \"+\".join(inputs)\n",
        "inputs = \"+\".join([inputs, str(bias_4[0])])\n",
        "verilog_code += \"yvth = {};\\n\\n\".format(inputs)\n",
        "verilog_code += \"delta_Vth = (yvth - Mindelta_Vth) * normdelta_Vth;\"\n",
        "verilog_code += \"\"\"\n",
        "\n",
        "\n",
        "        Vg = V(g) ;\n",
        "        Vs = V(s) ;\n",
        "        Vd = V(d) ;\n",
        "        Vgsraw = Vg-Vs ;\n",
        "        Vgdraw = Vg-Vd ;\n",
        "\n",
        "if (Vgsraw >= Vgdraw) begin\n",
        "        Vgs = ((Vg-Vs) - MinVg) * normVg ;\n",
        "        dir = 1;\n",
        "end\n",
        "\n",
        "else begin\n",
        "        Vgs = ((Vg-Vd) - MinVg) * normVg ;\n",
        "        dir = -1;\n",
        "end\n",
        "\n",
        "        Vds = (abs(Vd-Vs) - MinVd) * normVd ;\n",
        "        Vgs = Vgs - delta_Vth ; // BTI Vth variation\n",
        "        Lg = (L -MinLg)*normLg ;\n",
        "\n",
        "\n",
        "\n",
        "//******************** C-V NN **********************************//\n",
        "hc1_0 = tanh(-0.99871427*Vgs+-0.16952373*Vds+0.32118186*Lg+0.41485423);\n",
        "hc1_1 = tanh(0.31587568*Vgs+0.13397887*Vds+-0.4541538*Lg+-0.3630942);\n",
        "hc1_2 = tanh(-0.76281804*Vgs+0.09352969*Vds+1.1961353*Lg+0.3904977);\n",
        "hc1_3 = tanh(-1.115087*Vgs+0.85752946*Vds+-0.11746484*Lg+0.5500279);\n",
        "hc1_4 = tanh(1.0741386*Vgs+0.82041687*Vds+0.19092631*Lg+-0.4009425);\n",
        "hc1_5 = tanh(-0.47921795*Vgs+-0.8749933*Vds+-0.054768667*Lg+0.62785167);\n",
        "hc1_6 = tanh(0.5449184*Vgs+-4.409165*Vds+-0.072947875*Lg+-0.31324536);\n",
        "hc1_7 = tanh(-2.9224303*Vgs+2.7675478*Vds+0.08862238*Lg+0.6493558);\n",
        "hc1_8 = tanh(0.65050656*Vgs+-0.29751927*Vds+0.1571876*Lg+-0.38088372);\n",
        "hc1_9 = tanh(-0.30384183*Vgs+0.5649165*Vds+2.6806898*Lg+0.3197917);\n",
        "hc1_10 = tanh(-0.095988505*Vgs+2.0158541*Vds+-0.42972717*Lg+-0.30388466);\n",
        "hc1_11 = tanh(6.7699738*Vgs+-0.07234483*Vds+-0.013545353*Lg+-1.3694142);\n",
        "hc1_12 = tanh(-0.3404029*Vgs+0.0443459*Vds+0.89597*Lg+0.069993004);\n",
        "hc1_13 = tanh(0.62300175*Vgs+-0.29515797*Vds+1.6753465*Lg+-0.6520838);\n",
        "hc1_14 = tanh(0.37957156*Vgs+0.2237372*Vds+0.08591952*Lg+0.13126835);\n",
        "hc1_15 = tanh(0.19949242*Vgs+-0.26481664*Vds+-0.41059187*Lg+-0.40832308);\n",
        "hc1_16 = tanh(0.98966587*Vgs+-0.24259183*Vds+0.36584845*Lg+-0.8024042);\n",
        "\n",
        "hc2_0 = tanh(-0.91327864*hc1_0+0.4696781*hc1_1+0.3302202*hc1_2+0.11393868*hc1_3+0.45070222*hc1_4+-0.2894044*hc1_5+0.55066*hc1_6+-1.6242687*hc1_7+-0.38140613*hc1_8+0.032771554*hc1_9+0.17647126);\n",
        "hc2_1 = tanh(-1.1663305*hc1_0+-0.523984*hc1_1+-0.90804136*hc1_2+-0.7418044*hc1_3+1.456171*hc1_4+-0.16802542*hc1_5+0.8235596*hc1_6+-2.2246246*hc1_7+-0.40805355*hc1_8+0.7207601*hc1_9+0.4729169);\n",
        "hc2_2 = tanh(-0.69065374*hc1_0+0.40205315*hc1_1+-0.49410668*hc1_2+0.8681325*hc1_3+0.471351*hc1_4+0.46939445*hc1_5+0.45568785*hc1_6+-0.92935294*hc1_7+-0.8209646*hc1_8+0.1158967*hc1_9+-0.075798474);\n",
        "hc2_3 = tanh(0.11535446*hc1_0+-0.06296927*hc1_1+-0.6740435*hc1_2+0.7428315*hc1_3+0.05890677*hc1_4+0.9579441*hc1_5+-0.037319*hc1_6+-0.18491426*hc1_7+-0.02981994*hc1_8+0.038347963*hc1_9+0.039531134);\n",
        "hc2_4 = tanh(0.37817463*hc1_0+-0.6811279*hc1_1+1.1388369*hc1_2+0.19983096*hc1_3+-0.20415118*hc1_4+1.3022176*hc1_5+0.22571652*hc1_6+0.1690611*hc1_7+-0.56475276*hc1_8+-0.4069731*hc1_9+0.99962974);\n",
        "hc2_5 = tanh(0.032873478*hc1_0+-0.05209407*hc1_1+-0.010839908*hc1_2+-0.13892579*hc1_3+-0.050480977*hc1_4+0.0127089145*hc1_5+0.0052771433*hc1_6+0.02029242*hc1_7+-0.08705659*hc1_8+0.0254766*hc1_9+0.025135752);\n",
        "hc2_6 = tanh(-0.34639204*hc1_0+0.06937975*hc1_1+0.18671949*hc1_2+-0.18912783*hc1_3+0.1312504*hc1_4+0.4627272*hc1_5+-0.42590702*hc1_6+-0.10966313*hc1_7+0.66083515*hc1_8+-0.050718334*hc1_9+0.08234678);\n",
        "hc2_7 = tanh(0.90656275*hc1_0+-0.037281644*hc1_1+0.77237594*hc1_2+1.4710428*hc1_3+0.13597831*hc1_4+-0.059844542*hc1_5+-0.7801535*hc1_6+3.7814677*hc1_7+-0.5976644*hc1_8+0.2721995*hc1_9+0.023777716);\n",
        "hc2_8 = tanh(0.39720625*hc1_0+-0.45262313*hc1_1+0.19873238*hc1_2+0.9750888*hc1_3+-0.9427992*hc1_4+0.4487432*hc1_5+-0.3372945*hc1_6+0.33729544*hc1_7+-0.1667088*hc1_8+-0.5707525*hc1_9+0.27954483);\n",
        "hc2_9 = tanh(0.28551984*hc1_0+-0.68350387*hc1_1+0.9916423*hc1_2+-0.8254094*hc1_3+0.09875706*hc1_4+0.47609732*hc1_5+-0.058662917*hc1_6+0.09181381*hc1_7+0.09592329*hc1_8+1.3940467*hc1_9+0.3865768);\n",
        "hc2_10 = tanh(0.098737516*hc1_0+0.060473576*hc1_1+0.42824662*hc1_2+0.15018038*hc1_3+0.082621895*hc1_4+-0.00019039502*hc1_5+-0.3321634*hc1_6+0.7936295*hc1_7+-0.041197542*hc1_8+0.6530619*hc1_9+0.1338804);\n",
        "hc2_11 = tanh(-0.3585284*hc1_0+-0.09956017*hc1_1+0.17224246*hc1_2+-0.016925728*hc1_3+-0.46462816*hc1_4+-0.5649022*hc1_5+1.251695*hc1_6+-0.4303161*hc1_7+0.48546878*hc1_8+0.22958975*hc1_9+-0.27899802);\n",
        "hc2_12 = tanh(0.8565631*hc1_0+-0.7622999*hc1_1+0.32367912*hc1_2+1.4776785*hc1_3+0.2712369*hc1_4+0.2275511*hc1_5+0.39908803*hc1_6+4.2305493*hc1_7+-0.3467536*hc1_8+0.41231114*hc1_9+0.47123823);\n",
        "hc2_13 = tanh(-0.26411077*hc1_0+-0.17583853*hc1_1+0.045439184*hc1_2+-0.13801138*hc1_3+0.03278085*hc1_4+-0.45625108*hc1_5+-0.17905861*hc1_6+0.3060186*hc1_7+-0.3361926*hc1_8+-0.050055273*hc1_9+0.17996444);\n",
        "hc2_14 = tanh(0.016094366*hc1_0+-0.013196736*hc1_1+0.4124856*hc1_2+0.22926371*hc1_3+-0.35071182*hc1_4+-0.34217188*hc1_5+-0.69466996*hc1_6+1.0563152*hc1_7+-0.18019852*hc1_8+0.061871335*hc1_9+0.09762555);\n",
        "hc2_15 = tanh(-0.18970782*hc1_0+0.3624813*hc1_1+-1.3419824*hc1_2+0.103635244*hc1_3+-0.14595217*hc1_4+-0.1899393*hc1_5+0.176524*hc1_6+-0.4428012*hc1_7+-0.39544868*hc1_8+-0.45783517*hc1_9+0.1884755);\n",
        "hc2_16 = tanh(-0.1585831*hc1_0+0.035894837*hc1_1+-0.14261873*hc1_2+0.25914294*hc1_3+0.040607046*hc1_4+0.11555795*hc1_5+0.0022548323*hc1_6+-0.002149359*hc1_7+0.07067297*hc1_8+0.019578662*hc1_9+0.16657573);\n",
        "yc = 0.17503543*hc2_0+-0.15556754*hc2_1+-0.125455*hc2_2+-0.07502612*hc2_3+-0.16671115*hc2_4+0.29854503;\n",
        "\n",
        "Cgg = (yc / normO + MinO)*W;\n",
        "Cgsd = Cgg/2 ;\n",
        "\n",
        "//******************** I-V NN **********************************//\n",
        "h1_0 = tanh(0.0023826673*Vgs+-0.0009636134*Vds+0.006639037*Lg+-0.0004692053);\n",
        "h1_1 = tanh(-7.8007555*Vgs+2.639791*Vds+-0.025273597*Lg+-1.1747514);\n",
        "h1_2 = tanh(1.9884652*Vgs+0.62111*Vds+-0.11525345*Lg+-0.14575638);\n",
        "h1_3 = tanh(0.10625471*Vgs+0.09442053*Vds+-0.052119628*Lg+1.1568379);\n",
        "h1_4 = tanh(4.058087*Vgs+-0.7568158*Vds+0.008070099*Lg+-0.4820452);\n",
        "h1_5 = tanh(-1.3065948*Vgs+-0.0492497*Vds+-0.081622526*Lg+0.20351954);\n",
        "h1_6 = tanh(-2.9507525*Vgs+-0.91150546*Vds+-0.06477873*Lg+0.09646383);\n",
        "h1_7 = tanh(-0.5886177*Vgs+0.63788587*Vds+-0.13710928*Lg+0.30260038);\n",
        "h1_8 = tanh(-0.4311161*Vgs+-0.7250705*Vds+-0.06134645*Lg+0.44054285);\n",
        "h1_9 = tanh(0.012132638*Vgs+-0.42545322*Vds+-0.66478956*Lg+0.13182367);\n",
        "h1_10 = tanh(3.289041e-05*Vgs+0.0011367714*Vds+-0.0051904405*Lg+5.4112927e-05);\n",
        "h1_11 = tanh(-0.6007774*Vgs+0.09259224*Vds+0.2170182*Lg+-0.2908748);\n",
        "h1_12 = tanh(0.28018302*Vgs+1.3014064*Vds+-0.13490067*Lg+-0.22006753);\n",
        "h1_13 = tanh(0.01864017*Vgs+-13.0928755*Vds+-0.0037254083*Lg+-0.10935691);\n",
        "h1_14 = tanh(-0.0049708197*Vgs+0.0022613218*Vds+-0.014408149*Lg+0.0011340647);\n",
        "h1_15 = tanh(-0.094654046*Vgs+-0.4174114*Vds+0.18892045*Lg+0.05248958);\n",
        "h1_16 = tanh(-0.25090316*Vgs+-0.11111481*Vds+-0.009133225*Lg+0.08377026);\n",
        "h1_17 = tanh(0.9275306*Vgs+0.40686756*Vds+0.14127344*Lg+-0.059246097);\n",
        "h1_18 = tanh(-0.27084363*Vgs+0.07915911*Vds+-10.836302*Lg+-1.1535788);\n",
        "h1_19 = tanh(1.6852072*Vgs+-0.24846223*Vds+0.049734037*Lg+-0.19625053);\n",
        "\n",
        "h2_0 = tanh(1.1139417*h1_0+-0.40033522*h1_1+0.920759*h1_2+0.47607598*h1_3+0.3978683*h1_4+0.8008105*h1_5+-0.40445566*h1_6+0.8668027*h1_7+0.23365597*h1_8+-0.28254375*h1_9+-0.63985884*h1_10+-0.62523574*h1_11+0.8338383*h1_12+-2.0540943*h1_13+1.0749483*h1_14+-1.1075479*h1_15+0.60926706*h1_16+1.1118286*h1_17+-0.8917559*h1_18+0.029025732*h1_19+0.6622382);\n",
        "h2_1 = tanh(0.37195024*h1_0+-0.761445*h1_1+0.6514153*h1_2+0.6211995*h1_3+-0.063539445*h1_4+0.31260127*h1_5+-0.3529579*h1_6+0.50422627*h1_7+0.18943883*h1_8+-0.38029787*h1_9+-0.3464503*h1_10+-0.48301366*h1_11+0.081978925*h1_12+-0.08305682*h1_13+-0.08420117*h1_14+-0.8029313*h1_15+-0.37052512*h1_16+0.4553502*h1_17+-0.71959645*h1_18+-0.17320661*h1_19+0.6566257);\n",
        "h2_2 = tanh(-0.021933522*h1_0+-0.017245224*h1_1+0.030417712*h1_2+0.2967218*h1_3+-0.048668377*h1_4+0.029245213*h1_5+0.011756416*h1_6+0.004705658*h1_7+-0.042515088*h1_8+0.010462476*h1_9+0.001331279*h1_10+0.1694541*h1_11+-0.010949079*h1_12+-0.004144526*h1_13+0.04856694*h1_14+-0.010465159*h1_15+0.24588406*h1_16+-0.028521152*h1_17+0.12161568*h1_18+0.1753255*h1_19+-0.09125355);\n",
        "h2_3 = tanh(-0.025546636*h1_0+-3.2702503*h1_1+0.46812135*h1_2+-0.25135994*h1_3+3.0725436*h1_4+0.22513995*h1_5+-1.0327209*h1_6+-0.56274736*h1_7+0.4726107*h1_8+0.38182434*h1_9+0.016403453*h1_10+-0.09128962*h1_11+0.20997792*h1_12+-0.4951615*h1_13+0.026481293*h1_14+0.6085288*h1_15+-0.09078652*h1_16+0.36613584*h1_17+0.26257026*h1_18+0.39794916*h1_19+-0.52920526);\n",
        "h2_4 = tanh(-0.44392154*h1_0+-0.5650475*h1_1+0.91716766*h1_2+0.96703196*h1_3+0.4597048*h1_4+1.452921*h1_5+-0.8115141*h1_6+0.92326456*h1_7+0.2862403*h1_8+-0.9441585*h1_9+0.22188367*h1_10+-1.0092766*h1_11+0.5495966*h1_12+0.44870532*h1_13+0.48705962*h1_14+-0.6957133*h1_15+0.27711377*h1_16+1.0138507*h1_17+-0.34337458*h1_18+-0.21548931*h1_19+0.42685974);\n",
        "h2_5 = tanh(0.0003710325*h1_0+0.5746112*h1_1+0.4144258*h1_2+0.04459103*h1_3+1.2373503*h1_4+0.12786175*h1_5+-0.16099685*h1_6+0.9826207*h1_7+-0.09701193*h1_8+-0.4379135*h1_9+-0.0035542254*h1_10+0.04205593*h1_11+0.65820116*h1_12+0.3810506*h1_13+0.0004259507*h1_14+-0.21782044*h1_15+-0.057544652*h1_16+0.24420041*h1_17+-0.10138292*h1_18+-0.2387106*h1_19+0.867847);\n",
        "h2_6 = tanh(-0.0077049686*h1_0+-0.05349668*h1_1+-0.11536639*h1_2+0.06141029*h1_3+-0.034344573*h1_4+0.21672213*h1_5+-0.09835135*h1_6+-0.25849992*h1_7+0.10576329*h1_8+-0.14288934*h1_9+0.027846925*h1_10+-0.20104973*h1_11+0.24784875*h1_12+0.03396087*h1_13+-0.016039118*h1_14+-0.03147038*h1_15+-0.023109786*h1_16+-0.118931994*h1_17+0.18256636*h1_18+0.09981429*h1_19+0.079372324);\n",
        "h2_7 = tanh(-0.009369999*h1_0+-2.4468672*h1_1+-0.41433397*h1_2+-0.6289744*h1_3+-1.8285819*h1_4+0.12905455*h1_5+0.83588725*h1_6+0.11491322*h1_7+0.3871084*h1_8+0.07153052*h1_9+0.013530584*h1_10+0.40614852*h1_11+0.10340061*h1_12+-0.04473306*h1_13+0.0075287875*h1_14+0.5593612*h1_15+0.34401885*h1_16+-0.5538749*h1_17+0.36615464*h1_18+-0.2666981*h1_19+0.20194086);\n",
        "h2_8 = tanh(-0.0017045025*h1_0+-1.1388719*h1_1+0.25932005*h1_2+-0.6482845*h1_3+2.263395*h1_4+0.115192235*h1_5+-0.40946937*h1_6+-0.038117886*h1_7+-0.03703431*h1_8+0.05965774*h1_9+0.0032721795*h1_10+0.2561857*h1_11+0.47324425*h1_12+-0.25200802*h1_13+0.0017081021*h1_14+0.29819545*h1_15+0.061945435*h1_16+0.13516657*h1_17+0.5409335*h1_18+-0.17400871*h1_19+0.11659722);\n",
        "h2_9 = tanh(0.014866875*h1_0+1.0399909*h1_1+1.0108095*h1_2+-0.0918755*h1_3+-0.7634763*h1_4+-0.49749368*h1_5+-0.7020006*h1_6+-0.16384888*h1_7+-0.07833025*h1_8+-0.23745225*h1_9+-0.023095092*h1_10+-0.29244414*h1_11+0.6255917*h1_12+0.41413808*h1_13+-0.019823061*h1_14+-1.0593235*h1_15+-0.42221433*h1_16+0.98025715*h1_17+-0.71444243*h1_18+0.84347373*h1_19+0.28510216);\n",
        "h2_10 = tanh(-0.013515852*h1_0+0.68840384*h1_1+-0.030184174*h1_2+-0.48098114*h1_3+0.08609854*h1_4+-0.50878614*h1_5+0.26547763*h1_6+-0.7151076*h1_7+0.111288495*h1_8+0.53379977*h1_9+0.022258151*h1_10+0.16971351*h1_11+-1.2486296*h1_12+0.9649942*h1_13+0.0028906881*h1_14+0.31582054*h1_15+0.24213083*h1_16+-0.27242163*h1_17+0.11330636*h1_18+0.17038865*h1_19+-0.42089102);\n",
        "h2_11 = tanh(-0.009949424*h1_0+-0.6704206*h1_1+0.13177924*h1_2+0.07316155*h1_3+-0.28207502*h1_4+-0.32401362*h1_5+0.020746209*h1_6+-0.05517531*h1_7+0.10340257*h1_8+0.29381263*h1_9+0.020497978*h1_10+0.031984854*h1_11+0.079552434*h1_12+-7.7884445*h1_13+0.01224806*h1_14+0.8700812*h1_15+-0.16562358*h1_16+0.14370379*h1_17+0.2389181*h1_18+-0.19771136*h1_19+0.33039534);\n",
        "h2_12 = tanh(-0.002431529*h1_0+0.30154988*h1_1+-0.45072728*h1_2+0.27823612*h1_3+-0.25173753*h1_4+-0.58188903*h1_5+0.39836177*h1_6+-0.060329597*h1_7+0.1667837*h1_8+-0.09419194*h1_9+0.0108116735*h1_10+-0.3581154*h1_11+0.15690842*h1_12+-0.41209573*h1_13+-0.0070331707*h1_14+-0.23976392*h1_15+0.16620094*h1_16+-0.034617994*h1_17+0.32754526*h1_18+0.8490298*h1_19+0.026407415);\n",
        "h2_13 = tanh(-0.7385622*h1_0+-0.25174448*h1_1+2.090295*h1_2+0.67499936*h1_3+0.07392262*h1_4+0.5413692*h1_5+-0.8287433*h1_6+2.2810504*h1_7+1.1917778*h1_8+1.3719273*h1_9+0.25762329*h1_10+-2.1111712*h1_11+1.6987114*h1_12+-0.437825*h1_13+1.7649944*h1_14+-2.2254016*h1_15+-2.3555171*h1_16+-0.7732424*h1_17+-1.0756916*h1_18+-0.1367373*h1_19+1.5085722);\n",
        "h2_14 = tanh(-0.002535408*h1_0+-4.2686224*h1_1+-0.045537975*h1_2+0.08043166*h1_3+1.7274952*h1_4+-0.1260494*h1_5+-0.23011239*h1_6+-0.21025337*h1_7+0.48537356*h1_8+0.39049447*h1_9+-0.006565428*h1_10+-0.293028*h1_11+-1.1253971*h1_12+-0.2890831*h1_13+0.0042421576*h1_14+0.4230598*h1_15+-0.17869289*h1_16+-0.034056116*h1_17+0.61135995*h1_18+0.6345532*h1_19+0.73144835);\n",
        "h2_15 = tanh(-0.5239093*h1_0+0.49806875*h1_1+0.53555894*h1_2+1.2645539*h1_3+1.011548*h1_4+0.39209503*h1_5+-0.78194916*h1_6+0.94610345*h1_7+0.9121405*h1_8+-1.4693716*h1_9+-0.58336824*h1_10+-1.1661471*h1_11+-0.16158457*h1_12+0.058512915*h1_13+1.3358645*h1_14+0.5742125*h1_15+-1.3325212*h1_16+1.1804186*h1_17+-1.4877121*h1_18+0.6357802*h1_19+1.1328585);\n",
        "h2_16 = tanh(0.0095406*h1_0+-0.53232694*h1_1+0.1535685*h1_2+0.26108417*h1_3+-0.08375187*h1_4+0.3462123*h1_5+-0.61755395*h1_6+0.0015145333*h1_7+-0.108780764*h1_8+0.2555477*h1_9+0.00023940884*h1_10+-0.26577523*h1_11+-0.27689674*h1_12+0.43049082*h1_13+-0.025424613*h1_14+0.24634649*h1_15+-0.22579487*h1_16+0.37249807*h1_17+-0.8058662*h1_18+-0.77451354*h1_19+0.5821276);\n",
        "y = -0.17525196*h2_0+-0.22995844*h2_1+0.0026147955*h2_2+0.091129646*h2_3+-0.4534783*h2_4+0.4119419*h2_5+-1.4205361e-05*h2_6+-0.11359831*h2_7+0.1762025*h2_8+-0.1002102*h2_9+-0.37161925*h2_10+0.2683793*h2_11+0.051897053*h2_12+0.19354156*h2_13+-0.10134394*h2_14+0.3937854*h2_15+-0.3296008*h2_16+0.3220947;\n",
        "\n",
        "        Id = pow(10, (y/normI + MinI)) ;\n",
        "\n",
        "if (Id <= 1e-15) begin //limit\n",
        "        Id = 1e-15;\n",
        "        //Id = Id;\n",
        "end\n",
        "else begin\n",
        "        Id = Id;\n",
        "end  //limit end\n",
        "\n",
        "\n",
        "        I(g, d) <+ Cgsd*ddt(Vg-Vd) ;\n",
        "        I(g, s) <+ Cgsd*ddt(Vg-Vs) ;\n",
        "\n",
        "if (Vgsraw >= Vgdraw) begin\n",
        "        I(d, s) <+ dir*Id*W ;\n",
        "\n",
        "end\n",
        "\n",
        "else begin\n",
        "        I(d, s) <+ dir*Id*W ;\n",
        "\n",
        "end\n",
        "\n",
        "end\n",
        "\n",
        "endmodule\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "print(verilog_code)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O8ZOvUesBszP"
      },
      "source": [
        "**Deprecated Version (Linear Regression)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ASPvpASs1kUi",
        "outputId": "c5c7ddd0-a242-42d6-a36f-32f8004863b4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [50/5000], Loss: 216.5890\n",
            "Epoch [100/5000], Loss: 35.0012\n",
            "Epoch [150/5000], Loss: 14.2757\n",
            "Epoch [200/5000], Loss: 5.4854\n",
            "Epoch [250/5000], Loss: 2.1224\n",
            "Epoch [300/5000], Loss: 1.0208\n",
            "Epoch [350/5000], Loss: 0.6775\n",
            "Epoch [400/5000], Loss: 0.5532\n",
            "Epoch [450/5000], Loss: 0.4914\n",
            "Epoch [500/5000], Loss: 0.4525\n",
            "Epoch [550/5000], Loss: 0.4252\n",
            "Epoch [600/5000], Loss: 0.4048\n",
            "Epoch [650/5000], Loss: 0.3887\n",
            "Epoch [700/5000], Loss: 0.3751\n",
            "Epoch [750/5000], Loss: 0.3628\n",
            "Epoch [800/5000], Loss: 0.3512\n",
            "Epoch [850/5000], Loss: 0.3399\n",
            "Epoch [900/5000], Loss: 0.3286\n",
            "Epoch [950/5000], Loss: 0.3174\n",
            "Epoch [1000/5000], Loss: 0.3062\n",
            "Epoch [1050/5000], Loss: 0.2950\n",
            "Epoch [1100/5000], Loss: 0.2838\n",
            "Epoch [1150/5000], Loss: 0.2727\n",
            "Epoch [1200/5000], Loss: 0.2616\n",
            "Epoch [1250/5000], Loss: 0.2506\n",
            "Epoch [1300/5000], Loss: 0.2397\n",
            "Epoch [1350/5000], Loss: 0.2289\n",
            "Epoch [1400/5000], Loss: 0.2183\n",
            "Epoch [1450/5000], Loss: 0.2078\n",
            "Epoch [1500/5000], Loss: 0.1976\n",
            "Epoch [1550/5000], Loss: 0.1875\n",
            "Epoch [1600/5000], Loss: 0.1777\n",
            "Epoch [1650/5000], Loss: 0.1682\n",
            "Epoch [1700/5000], Loss: 0.1589\n",
            "Epoch [1750/5000], Loss: 0.1499\n",
            "Epoch [1800/5000], Loss: 0.1411\n",
            "Epoch [1850/5000], Loss: 0.1327\n",
            "Epoch [1900/5000], Loss: 0.1245\n",
            "Epoch [1950/5000], Loss: 0.1167\n",
            "Epoch [2000/5000], Loss: 0.1092\n",
            "Epoch [2050/5000], Loss: 0.1020\n",
            "Epoch [2100/5000], Loss: 0.0951\n",
            "Epoch [2150/5000], Loss: 0.0886\n",
            "Epoch [2200/5000], Loss: 0.0823\n",
            "Epoch [2250/5000], Loss: 0.0764\n",
            "Epoch [2300/5000], Loss: 0.0708\n",
            "Epoch [2350/5000], Loss: 0.0655\n",
            "Epoch [2400/5000], Loss: 0.0606\n",
            "Epoch [2450/5000], Loss: 0.0559\n",
            "Epoch [2500/5000], Loss: 0.0515\n",
            "Epoch [2550/5000], Loss: 0.0475\n",
            "Epoch [2600/5000], Loss: 0.0437\n",
            "Epoch [2650/5000], Loss: 0.0401\n",
            "Epoch [2700/5000], Loss: 0.0369\n",
            "Epoch [2750/5000], Loss: 0.0339\n",
            "Epoch [2800/5000], Loss: 0.0311\n",
            "Epoch [2850/5000], Loss: 0.0286\n",
            "Epoch [2900/5000], Loss: 0.0262\n",
            "Epoch [2950/5000], Loss: 0.0241\n",
            "Epoch [3000/5000], Loss: 0.0222\n",
            "Epoch [3050/5000], Loss: 0.0204\n",
            "Epoch [3100/5000], Loss: 0.0189\n",
            "Epoch [3150/5000], Loss: 0.0175\n",
            "Epoch [3200/5000], Loss: 0.0162\n",
            "Epoch [3250/5000], Loss: 0.0151\n",
            "Epoch [3300/5000], Loss: 0.0141\n",
            "Epoch [3350/5000], Loss: 0.0132\n",
            "Epoch [3400/5000], Loss: 0.0124\n",
            "Epoch [3450/5000], Loss: 0.0117\n",
            "Epoch [3500/5000], Loss: 0.0111\n",
            "Epoch [3550/5000], Loss: 0.0105\n",
            "Epoch [3600/5000], Loss: 0.0101\n",
            "Epoch [3650/5000], Loss: 0.0097\n",
            "Epoch [3700/5000], Loss: 0.0094\n",
            "Epoch [3750/5000], Loss: 0.0091\n",
            "Epoch [3800/5000], Loss: 0.0088\n",
            "Epoch [3850/5000], Loss: 0.0086\n",
            "Epoch [3900/5000], Loss: 0.0084\n",
            "Epoch [3950/5000], Loss: 0.0083\n",
            "Epoch [4000/5000], Loss: 0.0082\n",
            "Epoch [4050/5000], Loss: 0.0081\n",
            "Epoch [4100/5000], Loss: 0.0080\n",
            "Epoch [4150/5000], Loss: 0.0079\n",
            "Epoch [4200/5000], Loss: 0.0078\n",
            "Epoch [4250/5000], Loss: 0.0078\n",
            "Epoch [4300/5000], Loss: 0.0078\n",
            "Epoch [4350/5000], Loss: 0.0077\n",
            "Epoch [4400/5000], Loss: 0.0077\n",
            "Epoch [4450/5000], Loss: 0.0077\n",
            "Epoch [4500/5000], Loss: 0.0077\n",
            "Epoch [4550/5000], Loss: 0.0077\n",
            "Epoch [4600/5000], Loss: 0.0077\n",
            "Epoch [4650/5000], Loss: 0.0077\n",
            "Epoch [4700/5000], Loss: 0.0076\n",
            "Epoch [4750/5000], Loss: 0.0076\n",
            "Epoch [4800/5000], Loss: 0.0076\n",
            "Epoch [4850/5000], Loss: 0.0076\n",
            "Epoch [4900/5000], Loss: 0.0076\n",
            "Epoch [4950/5000], Loss: 0.0076\n",
            "Epoch [5000/5000], Loss: 0.0076\n",
            "Learned linear coefficients (h1, h2, h3, h4, h5, h6): [[3.2403252e-01 1.2588283e-04 1.4926398e-01 1.6216135e-06 5.8017345e-03\n",
            "  3.4838660e-05]]\n",
            "Learned bias: [0.369416]\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "X_tensor = torch.tensor(X, dtype=torch.float32)\n",
        "Y_tensor = torch.tensor(Y, dtype=torch.float32)\n",
        "\n",
        "# Define the model\n",
        "class LinearModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LinearModel, self).__init__()\n",
        "        self.linear = nn.Linear(6, 1)  # 6 inputs (t_stress, t_rec, etc.), 1 output (deltaV)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linear(x)\n",
        "\n",
        "# Instantiate the model\n",
        "model = LinearModel()\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 5000\n",
        "for epoch in range(num_epochs):\n",
        "    # Forward pass\n",
        "    outputs = model(X_tensor)\n",
        "    loss = criterion(outputs, Y_tensor)\n",
        "\n",
        "    # Backward and optimize\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if (epoch+1) % 50 == 0:\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "# Print learned weights and biases\n",
        "print(\"Learned linear coefficients (h1, h2, h3, h4, h5, h6):\", model.linear.weight.data.numpy())\n",
        "print(\"Learned bias:\", model.linear.bias.data.numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rjD077d3381W",
        "outputId": "ead20c1e-027c-46e5-ac60-530137e872f3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Estimated Output delta_V: 0.45497649908065796\n"
          ]
        }
      ],
      "source": [
        "input_array = np.array([[0.0001, 0.0002, 0.5, 100, 1.7, 25]])  # Sample input\n",
        "\n",
        "# Convert input data to a tensor\n",
        "input_tensor = torch.tensor(input_array, dtype=torch.float32)\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# No need to track gradients for making predictions\n",
        "with torch.no_grad():\n",
        "    # Predict the output using the model\n",
        "    predicted_output = model(input_tensor)\n",
        "\n",
        "# Print the predicted delta_V value\n",
        "print(\"Estimated Output delta_V:\", predicted_output.item())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9OZZWfblrhuz",
        "outputId": "3b31f421-93f6-4151-b35e-2e8776ffba7b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 10/100, Loss: 0.048087798058986664\n",
            "Epoch 20/100, Loss: 0.07116486877202988\n",
            "Epoch 30/100, Loss: 0.08174236863851547\n",
            "Epoch 40/100, Loss: 0.10612807422876358\n",
            "Epoch 50/100, Loss: 0.05975770205259323\n",
            "Epoch 60/100, Loss: 0.04767672345042229\n",
            "Epoch 70/100, Loss: 0.06521065533161163\n",
            "Epoch 80/100, Loss: 0.04492233693599701\n",
            "Epoch 90/100, Loss: 0.09233981370925903\n",
            "Epoch 100/100, Loss: 0.052101973444223404\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import numpy as np\n",
        "\n",
        "# Define the MLP neural network class\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MLP, self).__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(5, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)\n",
        "\n",
        "# Example data\n",
        "np.random.seed(42)\n",
        "x_data = np.random.rand(1000, 5)  # 5D Input (t_st, t_rec, t_cycle, Vov, temp)\n",
        "y_data = np.random.rand(1000, 1)  # 1D Output(deltaV)\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "x_tensor = torch.FloatTensor(x_data)\n",
        "y_tensor = torch.FloatTensor(y_data)\n",
        "\n",
        "# Create dataset and dataloader\n",
        "dataset = TensorDataset(x_tensor, y_tensor)\n",
        "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "# Initialize the MLP\n",
        "model = MLP()\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 100\n",
        "for epoch in range(num_epochs):\n",
        "    for inputs, targets in dataloader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}')\n",
        "\n",
        "# Save the trained model\n",
        "torch.save(model.state_dict(), 'mlp_regression_model.pth')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}